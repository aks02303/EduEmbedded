So let me just conclude this lecture with 
some results on the parsing problem using 
the reranking models that I just 
described to you, in conjunction with the 
perception. 
So in one set of experiments by myself 
and Terry Koon, we started with a 
baseline model which was electrolyzed. 
PCFG. 
Which, at least, at the time, was very 
close to the state of the art in parsing. 
And that scores around 88% precision and 
recall. 
Remember, f measure is a kind of average 
of precision and recall, in recovering 
sub constituents within a parse true. 
The reranked model that I just described 
scores 89.5% f-measure, which is about 
11% relative error reduction. 
So about 11% of errors have been 
corrected by the reranking model. 
So that's a fairly significant 
improvement, given that these models are 
starting to reach quite high levels of 
accuracy. 
This is actually a pretty significant 
improvement, and indeed I had developed 
these lexicalized P, PCFG's during my 
Ph.D thesis, and it was very, very hard 
to push these any further other than this 
88.2% measure which we see here. 
Here are some other results more recently 
from Eugene Charniak and Mark Johnson in 
2005. 
They employed a similar approach. 
But they had better and best lists, 
better features, and also importantly a 
better baseline model than this model 
I've shown you here. 
And they pushed accuracy from about 89.7% 
to 91% accuracy. 
This is actually very, very close to the 
state of the art in parsing performance. 
So, the reranking model, again, gives a 
pretty significant gain and actually 
produced one of the very best results 
we've seen on parsing. 
What I've shown you, though, in this 
lecture, is a quite new way of thinking 
about these supervised learning problems 
that we see in natural language 
processing. 
This idea of global linear models defined 
through gen f and v, and finally the 
perception algorithm as one way of 
training these parameters v. 
They give significant improvements on 
these reranking problems, but perhaps 
most importantly they're going to open up 
a whole new way of thinking about 
algorithms for problems such as 
translation or tagging and parsing. 
And we'll see how we can apply these 
models in several other contexts in the 
final week of lectures, which is the next 
week of this class. 

