So that is basically the complete 
definition for the model form for 
log-linear models. 
Again, we have some conditional 
probability of y given x under parameter 
values v. 
To calculate this, we compute the inner 
product between d and f. 
Remember this is equal to sum k equals 1 
to m, v sub k times f sub k of x y, and 
we compute the normalization term by 
summing over all possible labels y primed 
and taking e of e to the power of v dot f 
of x and y primed. 
And this transmission basically takes us 
from the scores to a well formed 
distribution over the possible values for 
Y. 
So the next thing I want to talk about is 
how we actually estimate these parameters 
the from training examples. 
So where in general we are going to 
assume that the set of features is fixed. 
So that's being defined coming into the 
problem. 
And the parameter estimation problem is 
going to be to estimate these parameters 
v from our training examples. 
Before I talk about parameter estimation 
I want to talk about one other thing 
which is where log-linear models get 
their name from, why these things are 
called log-linear models. 
So that the reason for that is the 
following. 
And why the name? 
So if you recall, we had p y given x 
under v is equal to e to the v.f x y over 
sum y primed e to the v.f x y primed. 
So let's look at what happens if I take 
the log of this. 
Okay. 
So I take the log of P, I'm assuming the 
log is natural log. 
So the log of this is going to be log Of 
e to the f v dot f x y, minus the log of 
sum y primed e v dot f x y primed. 
This is just using the usual rules of 
taking a log of a ratio of two terms. 
And of course this is equal to V dot F X 
Y. 
Because the log, and assuming this is 
again, this is natural log, and the 
exponent cancel each other essentially. 
And so, what we end up with is this log 
probability, in two terms. 
Here we have the inner product between v 
and f, applied to x and y of the elements 
we're interested in. 
And here we have log of this 
normalization constant. 
And so this is a linear term, that's why 
we call these thing log linear models. 
Because we end with something which has a 
linear term as this first term. 
This normalization term is a function of 
X until it doesn't end on Y. 
That's the second term in this 
expression. 
Okay, so that's the reason for the name, 
and as we will see when we derive 
parameter estimates, it's going to be 
useful to bear in mind that when we take 
the log of p, we end up with these two 
terms. 
It'll be easy to manipulate things in 
this form. 

