So next I want to talk about some key
challenges in NLP answering the question
why is NLP hard.
And we're actually going to focus on one
particular problem, namely the problem of
ambiguity which is seen time and time
again in natural language problems in
applications.
So to illustrate this we'll take an
example sentence.
This is actually from Livian Lee, and the
sentence here is, At last, a computer that
understands you like your mother.
And I think this was taken from some
marketing blog for a natural language
understanding system sometime back in the
1980s I believe.
The problem, of course, with this sentence
is that it is actually ambigu, ambiguous.
There are at least three re, readings or
interpretations of this sentence.
So let's go over that.
So the intended meaning was probably this.
It's saying that the computer understands
you as well as your mother understands
you.
But if you look a little harder, you'll
see that there are a couple of other
possible interpretations.
So one is, that the computer understands
the fact that you like your mother.
So at last a computer that understands you
like your mother.
Another interpretation, the computer
understands you as well as it understands
your mother.
So at last a computer that understands you
like your mother.
And then, even if we look at these 2
interpretations, 1 and 3 alone.
There's a question of, of whether this
means, you know?
Does the computer understand you well is
the context.
Does your mother understand you well?
Without knowing that, you can't answer
that question.
And then we have a similar ambiguity here.
So ambiguity also occurs at the acoustic
level.
And this is a, a key problem in speech
recognition.
So again, if we come back to this example
sentence, then from a purely acoustic
point of view this sequence like your, can
be confused with other things.
So, for example like cured is a valid
sequence of two words in English.
And acoustically, these two things might
be quite confusable.
Of course, this is much more plausible as
a sentence in English than this.
But nevertheless, if a speech recognizer
was to rely on acoustic information alone,
it's going to have to deal with all kinds
of ambiguities like this where two words
or two sequences of words are confusable.
It's interesting to note that many of
these ambiguities manifiest themselves at
the syntatic level.
What I mean by that is they manifest
themselves in the parsing problem that I
showed your earlier.
So different structures at the parsing
level, again these, these [unknown] parse
tree I showed you, lead to different
interpretations.
So in fact these two interpretations where
the computer understands you, as well as
it understands your mother.
This is it understands the fact that you
like your mother.
Actually, correspond to two quite
different syntactic structures.
So one of the key problems in natural
language parsing, is essentially
disambiguation.
Choosing between different syntactic
structures corresponding to different
interpretations.
Here's yet another level of ambiguity in
language.
This is roughly speaking, what we might
call the semantic level.
And this is an instance of what is often
referred to as word sense ambiguity.
So, if you look at it in a dictionary,
you, if you look at many words in English
or other languages, they have multiple
different meanings, depending on the
context.
So again mother was a word in that
sentence I showed you previously if look
in a dictionary you'll find the
conventional or rather the, the, the
frequent usage of this word.
But you also find a much less much less
frequent usage which is given here.
And a natural language processing system
will have to disambiguate between these
two meanings.
Here's a bit more on word sense ambiguity,
so if I take the sentence, "They put money
in the bank", it's pretty clear here what
we mean by bank.
But there is an interpretation where the
money is buried in mud, so we're going to
have disambiguate this word to work out
which of these interpretations is
intended.
Here's another example, I saw her duck
with a telescope.
So this sentence is actually ambiguous in
multiple ways.
I could be looking at her duck using a
telescope.
I could be looking at her duck who has a
telescope.
You can go through many other examples.
But there's a key word sense ambiguity
here which is the word duck.
Duck can refer either to the animal or it
can refer to the process of ducking, the
verb to duck and that leads to two quite
different interpretations here.
So yet another level of ambiguity is at
the discourse level and what I am going to
show you here is an instance of an
anaphora.
This is a problem of a pronoun for
example, she and revolving it resolving it
to the entity that it refers to in a word.
So, let's again assume that we have our
sentence, except here I have Alice says,
as the, the start of the sentence.
And let's say the continuation is, but she
doesn't know any details.
Actually, I have two possible
continuations here.
But she doesn't know any details, or but
she doesn't understand me at all.
So she could refer to two different
things.
It could refer to your mother.
Or it could refer, at refer to Alice.
And so there's an ambiguity here that we
need to resolve.
We need to figure out which one of these
two entities in the past she is referring
to.
And actually it's going to differ for
these two cases.
If I say, but she doesn't know any
details, by far the most plausible
interpretation is that she is referring to
Alice.
But if I say, but she doesn't understand
me at all, in this case by far the most
likely interpretation is that she refers
to your mother.
So this is again a very challenging
proposition, taking pronouns and figuring
out what entity they refer to in the past
discourse.
So the final thing I want to talk about in
this lecture is the contents of the
course.
What will this course actually be about?
And so we're going to cover several topics
in this course.
One thing we'll look at is the basic, and
[inaudible] sub problems that I mentioned
earlier.
Problems such as part of speech tagging,
parsing, models for word sense
disambiguation, and so on.
A second major focus of this talk will be
on machine learning or statistical methods
for natural language processing.
So machine learning techniques have become
extremely prevalent for all of the natural
language applications and problems that I
described earlier.
As one example, modern machine translation
systems are trained automatically from
vast numbers of example translations.
Statistical methods are used to induce
dictionaries and other kind of models for
that translation problem.
So we'll talk about many basic
mathematical or computational models based
on machine learning techniques applied to
language.
These include probabilistic context free
grammars hidden markov models.
We'll talk about estivmation and smoothing
techniques right at the start of the
calss.
We'll talk about a famous algorithm called
the EM algorithm which is widely applied
in speech recognition and also machine
translation.
We'll talk about a very important class of
model, called log-linear models later in
the class, and so on.
And finally we will talk about various
applications including information
extraction, machine translation, and
possibly natural language interfaces.
So here is a syllabus for the class, we're
going to start off with a problem called
the language-modeling problem, which will
be in some sense a warm-up.
Introducing many important ideas,
particularly an idea called smooth
destination.
We'll then talk about tagging problems,
such as part of speech tagging and
identity recognition.
And we'll talk about hidden Markov models,
which are a key model for these tagging
problems.
We'll talk about the parsing problem that
I described earlier, and we'll talk about
models for that problem.
We'll then have a few lectures on machine
translation where you'll see more or less
from the ground up how we can build a
modern machine translation system.
Next we'll talk about log-linear models
and discriminative methods.
These are a key model in statistical
natural language nowadays.
And they're applied to several problems
which we'll go over in these lectures.
And then finally, to finish up, we'll talk
about semi-supervised and unsupervised
learning for NLP, which is a, a very
important topic and a huge area of con
research.
So in terms of prerequisites for the
class, coming into the class you should
know some basic linear algebra.
Some basic probability will be key, so you
should know what discreet distribution is,
what a random variable is.
And finally, some basic knowledge of
algorithms in computer science.
And secondly, there are going to be some
programming assignments on the course, so
you should have some basic programming
skills.
For example, if you can program in Python
or Java, that should be plenty for this
course.
One of these two program languages will be
plenty for this course.
Finally, in terms of background reading
for the course, there are a couple of
resources.
So over the years I have developed some
fairly comprehensive notes for many of the
topics.
Actually for all of the topic's you'll see
in the course, so if you go to my webpage
you'll find a link to those notes, and
thou, they should be very useful in, as
background for reading lectures.
And in addition, as additional context I'd
recommend you Jurafsky and Martin speech
and language processing text book, this is
by no means essential, but if you want to,
want to read more on the subject, this
could be very useful.
