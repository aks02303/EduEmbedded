So, let me finish this segment with some 
results for global linear models, taken 
from McDonald's implementation of these 
types of models. 
So, what I'm showing here, is a table 
with dependency accuracy. 
So, this is the proportion, of 
dependencies that we propose that are 
actually correct. 
This is actually an evaluation measure we 
saw way back in the course, when we 
looked at lexicalized context-free 
grammars. 
We talked about evaluating the a, a, 
accuracy of a parser by looking at the 
number of dependencies it, it got 
correct. 
This model is a lexicalized PCFG, that I 
developed for my PhD thesis. 
And of course, once we have a parse tree 
from a lexicalized PCFG, we can extract 
the dependencies from it, using the 
methods I described earlier in this 
segment. 
And also the methods I described in the 
section on lexicalized PCFGs. 
And this model scores just over 91% in 
terms of dependency, accuracy. 
Here are a couple of results from 
McDonald from 2005. 
The simple model that I just described, 
gets close to 91% accuracy. 
And for a slightly richer model, which 
I'll talk about in a second, called a 
second order dependency model, we can get 
about 91 and a half percent accuracy. 
So, this is pretty impressive. 
The dependency parsing models actually 
are extremely competitive with 
lexicalized PCFGs, in terms of recovering 
these dependency relations. 
And there are very, some, there are some 
very significant advantages in dependency 
parsing approaches. 
They are quite simple. 
It really is just a matter of defining 
these features that I just showed you, 
implementing dynamic programming. 
And then finally implementing the 
perception algorithm, for example, for 
parameter estimation. 
They're not only simple, but they're very 
efficient, as I'd said before. 
They are cubic time in the length of a 
sentence. 
And remember that for PCFGs, for example, 
the running time was cubic in the length 
of the sentence, and also cubic in the 
number of symbols. 
So, G is the number of non-terminals. 
[SOUND] And this term adds a lot, this 
could easily be 10 cubed or 20 cubed, 50 
cubed, easily into the thousands. 
So, dependency parsers can be 
significantly more efficient than a full 
P, PCFG parser. 
Let me just talk a little bit about 
extensions to what I've just described to 
you. 
There's been a lot of interest also in 
non-projective dependency structures. 
So, actually defining gen to include 
these structures with the crossing 
dependencies. 
And for some languages, for example 
Czech, these kind of crossing 
dependencies, dependencies are seen 
frequently, and so it's important to 
allow them. 
And that leads to a whole new research 
topic, which is how do we find the 
highest-scoring non-projective structure. 
It turns out that dynamic programming 
doesn't really well, actually, dynamic 
programming can be employed in some 
cases. 
But there are also other interesting 
algorithms based on spanning tree 
algorithms, for example. 
Another very important extension is to 
look at second order and third order 
dependency parsing. 
What I mean by that is the following. 
So, the feature vector representations I 
have shown you, have looked at a single 
dependency at a time. 
So, it looks at the single dependency in 
isolation from all of the other 
dependencies in the sentence. 
So, we just look at this one dependency, 
and that's reflected because we have a 
local feature vector that looks at the 
sentence. 
And just the identity of the head and the 
modifier in that particular dependency. 
In higher order dependency parsing, for 
example second order dependency parsing, 
we start to look at dependencies 
together. 
So, we might actually have a feature, 
which is sensitive to the fact that I 
have these two dependencies together in 
the parse tree, or maybe even these three 
dependencies together in the parse tree. 
So, we can now have local feature 
vectors. 
Then maybe don't just look at the head 
and the modifier, but might also look at 
the grandparent. 
So, maybe this is h, m, and this is h 
primed. 
And might also look at some other 
modifier, maybe this modifier here. 
So, it's easy enough, actually, to extend 
the approaches I've described, to allow 
local feature vectors that look at larger 
pieces of dependency structure. 
And this involves some complications to 
the depend, the dynamic, dynamic 
programming algorithms that are used, but 
these models still remain tractable. 
With these kind of modifications, the 
state-of-the-art is around 93 to 94%, at 
least on English Wall Street Journal data 
sets in terms of recovering dependency 
annotations. 
And this really is a pretty impressive 
level of accuracy. 

