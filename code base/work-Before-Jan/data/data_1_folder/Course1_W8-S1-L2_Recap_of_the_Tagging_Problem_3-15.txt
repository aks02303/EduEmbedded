So first, let's give a quick recap of the 
tagging problem. 
So, remember we considered a couple of 
very important examples of tagging 
problems much earlier in this class. 
And the first one we looked at was 
Part-of-Speech tagging. 
So the problem here is to take a sentence 
as input, and provide an output where 
each word in the sentence now has an 
associated tag. 
For example, profits is a noun sword is a 
verb at is a preposition and so on and so 
on. 
So extracting the problem is to taking a 
sequence of words as input and produce a 
sequence of tags as the output of this 
model. 
So each word gets a single tag, the other 
important tagging problem we looked at 
was the problem of named entity 
recognition. 
So in this case the input is again a 
sentence and the output from the model is 
a segmentation of that sentence. 
Where we identify important segments 
corresponding, for example, to companies 
such as Boeing. 
Locations, such as Wall Street or people, 
such as Alan Mulally in this particular 
example. 
And we described how the named entity 
recognition problem can also be framed as 
tagging problem. 
Let me just remind you of that. 
Where each word is tagged with [SOUND] 
one of a number of possible tags listed 
down here. 
For example, some words are tagged as NA, 
meaning that they're not part of an 
entity. 
Whereas, other words are tagged as the 
start of a company or the continuation of 
a company or the start of a location, 
continuation of a location, or the start 
of a person and the continuation of the 
person. 
So that's just a reminder that we can 
frame this named entity problem, this 
named entity recognition problem, which 
is a segmentation problem, actually as a 
tagging problem. 
So, as before we're going to treat this 
problem as a machine learning problem and 
so, our goal is the following. 
We have some training set which consists 
of, perhaps, a quite large quantity of 
example sentences where each sentence has 
the underlying tags marked, like this. 
And from this training set, we need to 
induce a function or an algorithm that 
maps new sentences to their underlying 
tag sequences. 
And in, I think it was the second week of 
class we saw hidden Markov Models and the 
Viterbi Algorithm as one way of achieving 
this goal. 
So recall in Hidden Markov Models, we 
would read off parameter estimates by 
taking counts from our training samples. 
And then for a new test example we would 
use the Viterbi Algorithm to find the 
most likely text sequence under the 
underlying model. 

