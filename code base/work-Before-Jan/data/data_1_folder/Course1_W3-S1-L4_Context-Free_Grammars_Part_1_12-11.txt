Okay, so in this next segment, I'm going
to describe context free grammars, which
are a very important formosum used within
the passing problem.
So first give a formal definition.
And one thing I should note, is that
context-free grammars are not only used in
linguistics.
They're also of central importance in
computer science.
Both in theoretical computer science, and
also in programming languages, where they
play a very, very important role.
Okay, so a context free grammar has the
following components.
So context free grammar, g, is a[UNKNOWN],
contexting of N, sigma, R, and S, where we
have the following elements of
this[UNKNOWN].
So N is going to be a finite set of what
we call non-terminal symbols.
Sigma is going to be a finite set of what
are called terminal symbols.
R is a set of rules.
Come back to this in a second.
And finally, s is some member of n, which
is referred to as a distinguished or star
symbol in the grammar.
So each of these rules takes the following
form.
We have something on the left hand side of
the rule x, x has to be non-terminal.
And then on the right hand side of the
rule, we have a sequence of items, Y one
through Y n.
Where each of these Y's can either be a
non-terminal N, or it can be an element of
sigma.
Notice that n could be 0.
So actually this right hand side of the
rule could just be what we often write as
epsilon, which is the empty sequence,
sequence at length 0.
So that's a very abstract definition.
Let me give you a concrete example.
So here is an extremely simple grammar for
English, but it is used for illustrating
the definition that I just showed you.
So I have the four elements of the context
free grammar, N, S, Sigma, and R.
So N in this case, as I said, is a finite
set of what are called nonterminals...
So we have symbols like S, NP, VP, that
you've seen appearing in those pals trees
that I showed you earlier, and notice we
also have parts of speech.
Things like DT, Vi, Vt, NN, IN.
These are actually going to be parts of
speech in the grammar.
The, the distinguish start, start symbol
is simply going to be S.
Okay?
And then finally we have a set of words in
the language.
So Sigma might consist of the following
set of words.
It's to be a very small vocabulary here.
Moristic grammar would be have a much,
much larger vocabulary.
And sorry, finally we have a set of rules.
Okay?
So here are the rules in this particular
example grammar.
And as I said, on the left hand side of
the rule, I always see a nonterminal.
For example, For example S VP.
Or NP or PP and so on.
And on the right-hand side of each rule, I
see a sequence of zero or more symbols,
where each symbol could be any element of
N or sigma.
So in particular, I have S goes to NP VP,
here.
Vp goes to Vi, VP goes to Vt NP, and so on
and so on.
Over here I have rules where actually the
left-hand side is a single symbol, for
example Vi, and the right-hand side is a
word, for example, sleeps.
So, this is one simple example grammar.
Later in the class we'll talk a lot more
about what these symbols mean but just
briefly.
S basically stands for sentence, vp is an
abbreviations for verb phrase, np is a
noun phrase, pp is a prepositional phrase,
and so on and so on.
So that's a forma definition of context
free grammars and also an example of a
particular context free grammar.
The next crucial concept Is going to be
the idea of a derivation, or more
specifically, more specifically what's
called a leftmost derivation.
So a derivation is a sequence of strings,
s1 through sn, where this sequence of
strings has the following property.
So s1 has to be equal to s, this
distinguished start symbol in the grammar.
As N has to be made up of elements of
sigma alone.
So sigma star is just the set of all
possible strings which can be derived from
sigma So if I have sigma is D dog A, then
sigma star includes strings like just the
empty string epsilon, A dog thur, two
words strings like a dog, the dog and so
on, basically any string that you can form
through a finite sequence of these words
and sigma where we also include the empty
string sigma epsilon.
Okay.
So SN has got to be a sentence, for
example, the man sleeps running on the sum
sigma.
And then each intermediate Si, for i is
equal to two to n, is derived from Si
minus 1.
When kicking the left most terminal X in
its i minus 1 and replacing it by some
beta where X goes to Beta is a rule in R.
So here's an example, the derivation and I
will show go with this example in much
detail in next line.
So we start off with S, the first string,
the next string in this derivation is NP
VP, S is taken and replaced by NP VP.
Now take this NP and replace it by D N
,and so on and so on.
Let me actually go to an example will,
where I'll illustrate this a little bit
more carefully.
Okay, so we always start in an in a
derivation with the distinguished start
symbol, for example S.
And then each step we're going to choose
some rule under the grammar.
So we might for example choose S goes to
NP VP.
And in this case we replace S with the
right hand side of this rule.
The NV goes to VP.
So, the critical idea and the derivation
is that each point we take the left most
non-terminal in the current string and we
find some rules where that non-terminal we
write is the sequence of non-terminals and
we simply replace it.
So the next term that we are going modify
is NP, so we pick some rule from the
grammar, for example, NP goes to the N,
replace NP with N and now we have this N
VP Again, I picked the leftmost element.
Dt in this case.
I picked some rule.
In this case DT goes to there.
And now I have the N VP.
And we can keep going like this.
And finally, we end up with a sequence of
words.
Okay, so a complete derivation always ends
up with a string where every words in that
string is a word in a language, every
elements of this string is a member of
sigma, Okay, so the context free grammar
will define a set of va, valid derivations
where any derivation is valid if it starts
with S, ends with some sequence of words,
and goes through this process where at
every point we replace the left most known
terminal Using some rule in the
context-free grammar.
It's very useful to represent these
derivations as parse trees.
So this parse tree, which I've shown you
here, is nothing more than a
representation of the derivation.
So we have S at the very root of the tree.
Notice we chose an S goes to NP VP as the
first rule and that's reflected with the
fact we have S dominating NP followed by
VP.
Assuming we have NP dominating determinate
noun because that's the next rule we, we
showed here and so you can go through each
of these rules used and see that it has
that it corresponds to some sub fragment
Within this tree.
Okay, so given a context-free grammar, we
have a set of valid derivations under the
grammar.
And this set can be infinite.
In fact in almost in every, almost all
interesting cases, it's going to be an
infinite set of possible derivations.
So a couple more definitions, or one more
definition, so we say a string s So a
sentence, so this could for example be,
the dog laughs.
We say a string S in the language defined
by the CFG if there is at least one
derivation that yields S.
Okay, so a CFG defines a language.
A language is a set of strings, and a
string is in the language if there's at
least one parse tree for the string.
Critically, a very interesting property is
that, Some strings in the language may
actually have more than one possible
derivation and this leads directly to the
problem of ambiguity.
Let me givbe you an example of this.
So here's a very simple sentence.
He drove down the street in the car.
And here I have one possible parse tree
for this particular sentence.
But, under most grammars for English,
there will also be a second possible parse
tree, which I've shown you here.
So let's go back and forth between these
two.
In this particular pass tree, this
prepositional phrase is seen lower in the
tree, and in particular, I have a rule, NP
goes to NP prepositional phrase at this,
low level in the tree.
Whereas, if I go back to the previous pass
tree, you'll see again we have this
prepositional phrase, in the car But it's
a little higher in the tree.
In, in particular we have a rule, VP goes
to VP-prepositional phrase.
So basically there is some ambiguity about
whether this prepositional phrase is seen
high in the tree, attaching to this VP.
Or if it's seen lower in the tree,
attaching to the noun phrase down here.
And that actually corresponds to two
different possible interpretations of this
sentence.
So under this parse tree, in the car is
modifying this entire verb phrase down the
street.
And this basically corresponds to by far
the most likely interpretation, which is
that I'm doing the driving in the car.
Let's look at the second parse tree,
though.
We have, in this case, in the car
modifying street.
And so there is a possible, although
highly implausible interpretation, where
the street that I'm driving down is
actually located in the car.
So I have a street in the car.
You could imagine some crazy world where
that would happen.
And that's what this attatchment
corresponds to.
This is where the prepositional phrase in
the car modifies street.
And so we have a street in the car.
So, just to recap, critically, with an
underlying context-free grammar, you may
find there are multiple derivations for a
particular sentence.
And those multiple derivations very often
correspond to different interpretation of
the underlying sentence.
