So that was a description of a set of 
potential features in our Log-Linear 
attacker. 
This is just a recap of local linear 
models then take features and produce 
conditional probability distributions of 
the form PY given X under a parameter 
vector v. 
So we've already said that we have 
features f sub k for k equals 1 to m, and 
this gives us a feature vector. 
And recall that in Log-Linear models, we 
also assume we have a parameter vector, V 
in r to the m. 
So, if m equals 3 for example, we might 
have f of xy is equal to 1, 0, 1 and we 
might have v is equal to 1, 2, 3. 
We can take the inner product of these 
two things, so v of f xy is equal to 1 
times 1 plus 0 times 2 plus 1 times 3, so 
that is equal to 4. 
And we can compute this inner product for 
every possible label or in conjunction 
with x. 
So this conditional probability, as we 
described last week in the lectures on 
log-linear models, is to find out e to 
the power of v of f divide it by a 
normalization term, which involves the 
sum of all possible labels and e of v of 
f with that particular label. 
How do we estimate the parameters of 
these models? 
Where do these v values come from? 
They typically come through learning 
parameters on a set of training examples 
in particular using the regularized log 
likelihood methods we described last week 
in the class. 
So, abstractly we assume some training 
set consisting of Xi, Yi pairs for i 
equals 1 to n. 
Then we choose our parameter values v 
star to be the Vs that maximize the sum 
of two terms. 
So firstly, I have the sum of log 
probabilities, my training examples. 
And secondly, you have a negative term. 
So lambda is some constant which is 
greater than zero dictating the relative 
weight of this term. 
And what I have here is basically the 
length squared of my parameter vector v. 
Or more directly, as written here, the 
sum of squared parameter values. 
So this regularize is going to keep 
parameter values small. 
Whereas, this term is going to encourage 
the parameter values to fit the training 
sample as well. 
And as we said before, this regularize 
term generally it leads to much better 
generalization in cases where we could 
potentially have very, very large numbers 
of features. 
So concretely, in log-linear taggers, our 
training set, these Xi, Yi pairs is going 
to be a set of examples, Wwhere each Xi 
consists of a history. 
For example, DT, JJ, the red dog 3. 
So remember, a history consists of a pair 
of tags, the sentence, and the position 
that is being tagged. 
And each Yi is going to consist of a part 
of speech tag. 
So we can take our training examples, our 
tag sentences. 
And for every position in our training 
samples, we can extract a history. 
We can extract a tag, and we can use 
those history tag pairs as the training 
examples for training the parameters of a 
log linear model. 

