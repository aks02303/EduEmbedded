,file_name,text,course_name,temp,week,section,lesson,course_title,week_no,section_no,lesson_no,text1,text_topics
0,Course1_W1-S1-L1_Introduction_Part_1_11-17,okay welcome natural language processing name michael collins im professor computer science columbia university ive taught course several years recently columbia mit natural language processing think tremendously exciting field builds insights computer science linguistics well see increasingly probability statistics also huge impact daily lives many many applications technologies making use basic ideas natural language processing introductory lecture going cover basic points first question going ask natural language processing well discuss key applications nlp also key problems solved natural language processing second question well consider nlp hard well consider key challenges well find natural language processing finally ill talk little bit course kind material well cover course general expect taking course high level natural language processing concerns use computers processing human natural languages one side problem often referred natural language understanding take text input computer processes text something useful hand often referred natural language generation computer sense produces language communicating human user first consider key applications nlp one oldest applications problem great importance machine translation problem mapping sentences one language sentences another language challenging task remarkable progress made last years area example translation google translate many familiar translation arabic english translations perfect still understand great deal said original language later course well actually go key steps building model machine translation system second example application often referred information extraction problem case take text input produce structured basically database representation key content text particular example input job posting output captures various important aspects posting example industry involved position involved location company salary youll see information pulled document salary case comes portion critical example natural lang language standing problem promise sense understand input unstructured text turn structured data base kind representation theres clear motivation particular problem information extraction weve performed step example perform complex searches say want find jobs advertising sector paying least certain salary particular location would search difficult formulate using regular search engine first run information extraction system websites job postings find web perform database query perform much complex searches one addition might able perform st statistical queries might able ask know number jobs accounting changed years number jobs software engineering boston area posted last year another key application natural language processing text summarization problem case take single document potentially group several documents try condense summary sense preserves main information documents actually example screenshot system developed columbia called news blaster actually multidocument system take multiple documents news story produce condensed summary main content documents particular example large group documents vaccination program summary attempts capture main information documents summarization clear motivation making sense vast amount data text available web news sources useful able summarize data another key application called dialogue systems systems human actually interact computer achieve task example ive shown flight domain user attempting book flight user might come query system system goes processes query sense understands query particular case realizes theres piece missing information namely day flight system responds query day flying user provides information system returns list flights dialog systems basic problem build system user interact computer using natural language notice type system involves natural language understanding components understand user saying theres also importantly natural language generation component going generate text cases example clarification questions weve shown addition applications ive described well also consider basic natural language processing problems depend many applications first well talk something called tagging problem abstractly tagging problems take following form input sequence case sequence letters output going tagged sequence letter input associated tag probably best illustrated couple examples first one partofspeech tagging problem case take sentences input example profits soared boeing co tag word input part speech n stands noun v stands verb p stands preposition adv stands adverb one sort basic problems natural language processing perform mapping high accuracy actually useful across wide range applications second example tagging problem whats called named entity recognition basic problem take sentences input identify basic entities sentence entities things like companies boeing co locations wall street might also example identify people named entity recognition basic problem clearly useful many applications able identify companies locations people entity types problem could problem could framed tagging problem word input tagged either belonging named entity na means part entity might part company sc means first word company start company cc means continuation company similarly sl means start location cl means continuation location abstractly tagging problem problem mapping input sequence items usually words tag sequence word sequence associated tag two key problems many many others another basic problem natural language processing problem natural language parsing goes back work linguistics going back sort foundation modern linguistics noam chomskys work problem take sentence input map output usually referred parse tree well talk lot later course high level parse tree essentially gives hierarchical decomposition sentence corresponding grammatical structure weve recovered kind representations example recover basic grammatical relations fact boeing subject verb fact prepositional phrase seattle modifier located basic problem natural language processing well see recover representations high accuracy useful across wide range applications gain first basic step natural language understanding trying make sense underlying sentence means also quite challenging problem,Course1,W1-S1-L1,W1,S1,L1,Introduction,1,1,1,okay welcom natur languag process name michael collin im professor comput scienc columbia univers ive taught cours sever year recent columbia mit natur languag process think tremend excit field build insight comput scienc linguist well see increasingli probabl statist also huge impact daili live mani mani applic technolog make use basic idea natur languag process introductori lectur go cover basic point first question go ask natur languag process well discuss key applic nlp also key problem solv natur languag process second question well consid nlp hard well consid key challeng well find natur languag process final ill talk littl bit cours kind materi well cover cours gener expect take cours high level natur languag process concern use comput process human natur languag one side problem often refer natur languag understand take text input comput process text someth use hand often refer natur languag gener comput sens produc languag commun human user first consid key applic nlp one oldest applic problem great import machin translat problem map sentenc one languag sentenc anoth languag challeng task remark progress made last year area exampl translat googl translat mani familiar translat arab english translat perfect still understand great deal said origin languag later cours well actual go key step build model machin translat system second exampl applic often refer inform extract problem case take text input produc structur basic databas represent key content text particular exampl input job post output captur variou import aspect post exampl industri involv posit involv locat compani salari youll see inform pull document salari case come portion critic exampl natur lang languag stand problem promis sens understand input unstructur text turn structur data base kind represent there clear motiv particular problem inform extract weve perform step exampl perform complex search say want find job advertis sector pay least certain salari particular locat would search difficult formul use regular search engin first run inform extract system websit job post find web perform databas queri perform much complex search one addit might abl perform st statist queri might abl ask know number job account chang year number job softwar engin boston area post last year anoth key applic natur languag process text summar problem case take singl document potenti group sever document tri condens summari sens preserv main inform document actual exampl screenshot system develop columbia call news blaster actual multidocu system take multipl document news stori produc condens summari main content document particular exampl larg group document vaccin program summari attempt captur main inform document summar clear motiv make sens vast amount data text avail web news sourc use abl summar data anoth key applic call dialogu system system human actual interact comput achiev task exampl ive shown flight domain user attempt book flight user might come queri system system goe process queri sens understand queri particular case realiz there piec miss inform name day flight system respond queri day fli user provid inform system return list flight dialog system basic problem build system user interact comput use natur languag notic type system involv natur languag understand compon understand user say there also importantli natur languag gener compon go gener text case exampl clarif question weve shown addit applic ive describ well also consid basic natur languag process problem depend mani applic first well talk someth call tag problem abstractli tag problem take follow form input sequenc case sequenc letter output go tag sequenc letter input associ tag probabl best illustr coupl exampl first one partofspeech tag problem case take sentenc input exampl profit soar boe co tag word input part speech n stand noun v stand verb p stand preposit adv stand adverb one sort basic problem natur languag process perform map high accuraci actual use across wide rang applic second exampl tag problem what call name entiti recognit basic problem take sentenc input identifi basic entiti sentenc entiti thing like compani boe co locat wall street might also exampl identifi peopl name entiti recognit basic problem clearli use mani applic abl identifi compani locat peopl entiti type problem could problem could frame tag problem word input tag either belong name entiti na mean part entiti might part compani sc mean first word compani start compani cc mean continu compani similarli sl mean start locat cl mean continu locat abstractli tag problem problem map input sequenc item usual word tag sequenc word sequenc associ tag two key problem mani mani other anoth basic problem natur languag process problem natur languag pars goe back work linguist go back sort foundat modern linguist noam chomski work problem take sentenc input map output usual refer pars tree well talk lot later cours high level pars tree essenti give hierarch decomposit sentenc correspond grammat structur weve recov kind represent exampl recov basic grammat relat fact boe subject verb fact preposit phrase seattl modifi locat basic problem natur languag process well see recov represent high accuraci use across wide rang applic gain first basic step natur languag understand tri make sens underli sentenc mean also quit challeng problem,[ 4 13  1  8  2]
1,Course1_W1-S1-L2_Introduction_Part_2_10-28,next want talk key challenges nlp answering question nlp hard actually going focus one particular problem namely problem ambiguity seen time time natural language problems applications illustrate well take example sentence actually livian lee sentence last computer understands like mother think taken marketing blog natural language understanding system sometime back believe problem course sentence actually ambigu ambiguous least three readings interpretations sentence lets go intended meaning probably saying computer understands well mother understands look little harder youll see couple possible interpretations one computer understands fact like mother last computer understands like mother another interpretation computer understands well understands mother last computer understands like mother even look interpretations alone theres question whether means know computer understand well context mother understand well without knowing cant answer question similar ambiguity ambiguity also occurs acoustic level key problem speech recognition come back example sentence purely acoustic point view sequence like confused things example like cured valid sequence two words english acoustically two things might quite confusable course much plausible sentence english nevertheless speech recognizer rely acoustic information alone going deal kinds ambiguities like two words two sequences words confusable interesting note many ambiguities manifiest syntatic level mean manifest parsing problem showed earlier different structures parsing level unknown parse tree showed lead different interpretations fact two interpretations computer understands well understands mother understands fact like mother actually correspond two quite different syntactic structures one key problems natural language parsing essentially disambiguation choosing different syntactic structures corresponding different interpretations heres yet another level ambiguity language roughly speaking might call semantic level instance often referred word sense ambiguity look dictionary look many words english languages multiple different meanings depending context mother word sentence showed previously look dictionary youll find conventional rather frequent usage word also find much less much less frequent usage given natural language processing system disambiguate two meanings heres bit word sense ambiguity take sentence put money bank pretty clear mean bank interpretation money buried mud going disambiguate word work interpretations intended heres another example saw duck telescope sentence actually ambiguous multiple ways could looking duck using telescope could looking duck telescope go many examples theres key word sense ambiguity word duck duck refer either animal refer process ducking verb duck leads two quite different interpretations yet another level ambiguity discourse level going show instance anaphora problem pronoun example revolving resolving entity refers word lets assume sentence except alice says start sentence lets say continuation doesnt know details actually two possible continuations doesnt know details doesnt understand could refer two different things could refer mother could refer refer alice theres ambiguity need resolve need figure one two entities past referring actually going differ two cases say doesnt know details far plausible interpretation referring alice say doesnt understand case far likely interpretation refers mother challenging proposition taking pronouns figuring entity refer past discourse final thing want talk lecture contents course course actually going cover several topics course one thing well look basic inaudible sub problems mentioned earlier problems part speech tagging parsing models word sense disambiguation second major focus talk machine learning statistical methods natural language processing machine learning techniques become extremely prevalent natural language applications problems described earlier one example modern machine translation systems trained automatically vast numbers example translations statistical methods used induce dictionaries kind models translation problem well talk many basic mathematical computational models based machine learning techniques applied language include probabilistic context free grammars hidden markov models well talk estivmation smoothing techniques right start calss well talk famous algorithm called em algorithm widely applied speech recognition also machine translation well talk important class model called loglinear models later class finally talk various applications including information extraction machine translation possibly natural language interfaces syllabus class going start problem called languagemodeling problem sense warmup introducing many important ideas particularly idea called smooth destination well talk tagging problems part speech tagging identity recognition well talk hidden markov models key model tagging problems well talk parsing problem described earlier well talk models problem well lectures machine translation youll see less ground build modern machine translation system next well talk loglinear models discriminative methods key model statistical natural language nowadays theyre applied several problems well go lectures finally finish well talk semisupervised unsupervised learning nlp important topic huge area con research terms prerequisites class coming class know basic linear algebra basic probability key know discreet distribution random variable finally basic knowledge algorithms computer science secondly going programming assignments course basic programming skills example program python java plenty course one two program languages plenty course finally terms background reading course couple resources years developed fairly comprehensive notes many topics actually topics youll see course go webpage youll find link notes thou useful background reading lectures addition additional context id recommend jurafsky martin speech language processing text book means essential want want read subject could useful,Course1,W1-S1-L2,W1,S1,L2,Introduction,1,1,2,next want talk key challeng nlp answer question nlp hard actual go focu one particular problem name problem ambigu seen time time natur languag problem applic illustr well take exampl sentenc actual livian lee sentenc last comput understand like mother think taken market blog natur languag understand system sometim back believ problem cours sentenc actual ambigu ambigu least three read interpret sentenc let go intend mean probabl say comput understand well mother understand look littl harder youll see coupl possibl interpret one comput understand fact like mother last comput understand like mother anoth interpret comput understand well understand mother last comput understand like mother even look interpret alon there question whether mean know comput understand well context mother understand well without know cant answer question similar ambigu ambigu also occur acoust level key problem speech recognit come back exampl sentenc pure acoust point view sequenc like confus thing exampl like cure valid sequenc two word english acoust two thing might quit confus cours much plausibl sentenc english nevertheless speech recogn reli acoust inform alon go deal kind ambigu like two word two sequenc word confus interest note mani ambigu manifiest syntat level mean manifest pars problem show earlier differ structur pars level unknown pars tree show lead differ interpret fact two interpret comput understand well understand mother understand fact like mother actual correspond two quit differ syntact structur one key problem natur languag pars essenti disambigu choos differ syntact structur correspond differ interpret here yet anoth level ambigu languag roughli speak might call semant level instanc often refer word sens ambigu look dictionari look mani word english languag multipl differ mean depend context mother word sentenc show previous look dictionari youll find convent rather frequent usag word also find much less much less frequent usag given natur languag process system disambigu two mean here bit word sens ambigu take sentenc put money bank pretti clear mean bank interpret money buri mud go disambigu word work interpret intend here anoth exampl saw duck telescop sentenc actual ambigu multipl way could look duck use telescop could look duck telescop go mani exampl there key word sens ambigu word duck duck refer either anim refer process duck verb duck lead two quit differ interpret yet anoth level ambigu discours level go show instanc anaphora problem pronoun exampl revolv resolv entiti refer word let assum sentenc except alic say start sentenc let say continu doesnt know detail actual two possibl continu doesnt know detail doesnt understand could refer two differ thing could refer mother could refer refer alic there ambigu need resolv need figur one two entiti past refer actual go differ two case say doesnt know detail far plausibl interpret refer alic say doesnt understand case far like interpret refer mother challeng proposit take pronoun figur entiti refer past discours final thing want talk lectur content cours cours actual go cover sever topic cours one thing well look basic inaud sub problem mention earlier problem part speech tag pars model word sens disambigu second major focu talk machin learn statist method natur languag process machin learn techniqu becom extrem preval natur languag applic problem describ earlier one exampl modern machin translat system train automat vast number exampl translat statist method use induc dictionari kind model translat problem well talk mani basic mathemat comput model base machin learn techniqu appli languag includ probabilist context free grammar hidden markov model well talk estivm smooth techniqu right start calss well talk famou algorithm call em algorithm wide appli speech recognit also machin translat well talk import class model call loglinear model later class final talk variou applic includ inform extract machin translat possibl natur languag interfac syllabu class go start problem call languagemodel problem sens warmup introduc mani import idea particularli idea call smooth destin well talk tag problem part speech tag ident recognit well talk hidden markov model key model tag problem well talk pars problem describ earlier well talk model problem well lectur machin translat youll see less ground build modern machin translat system next well talk loglinear model discrimin method key model statist natur languag nowaday theyr appli sever problem well go lectur final finish well talk semisupervis unsupervis learn nlp import topic huge area con research term prerequisit class come class know basic linear algebra basic probabl key know discreet distribut random variabl final basic knowledg algorithm comput scienc secondli go program assign cours basic program skill exampl program python java plenti cours one two program languag plenti cours final term background read cours coupl resourc year develop fairli comprehens note mani topic actual topic youll see cours go webpag youll find link note thou use background read lectur addit addit context id recommend jurafski martin speech languag process text book mean essenti want want read subject could use,[ 4  8  1  0 14]
2,Course1_W1-S2-L1_Introduction_to_the_Language_Modeling_Problem_Part_1_6-17,okay first topic going cover course problem language modeling language modeling one oldest problems studied statistical natural language processing basic problem useful problem language models used wide range natural language applications going cover number things im firstly going define basic problem well talk important class language models called trigram language models extremely widely used well talk evaluate different language models measure effectiveness different language models finally well talk couple estimation techniques language modeling firstly something called linear interpolation secondly something called discounting methods methods widely used within language modeling see later class theyre also useful many problems natural language processing basic estimation techniques widely used areas get us started couple definitions going assume set v finite set going include words language interest imagine constructing language model english example might set containing words man telescope uncommon set really quite large might easily contain thousands ten thousands possible words language given underlying set v im going use v dagger symbol refer set possible sentences strings language wellformed sentence takes following form zero words word drawn set v followed special symbol stop symbol okay use stop symbol end sentence initially going look little peculiar well see soon convenient include symbol start develop probabilistic model language modeling problem recap sentence could sequence words could sentence makes sense example sentence might sentence really really doesnt make sense get sequences like stop sequence words drawn vocabulary followed stop well also include sentence stop symbol alone case sentence basically zero length words stop completely precise given definitions define language modeling problem im going assume training sample example sentences language interested lets assume thats english example might collect sentences youve seen new york times last years might collect large set example sentences world wide web think many examples training sample quite large concrete mid example pretty common make use know roughly million words data training samples end wasnt uncommon use maybe billion words often chosen newspaper data example recently last several years people started using web data construct language models might even get scenario hundreds billions words potential training data main point training samples get quite large given training sample task following want learn distribution p sentences language okay p going function satisfies conditions firstly sentence x remember dagger set possible sentences language sentence x p x greater equal secondly sum sentences language something sums value okay p wellformed distribution sentences language task going take training sample example sentences input outputs function p output process examples might example assign probability minus sentence composing word followed stop might assign times minus particular sentence assign probability every sentence language roughly speaking would like good language model assign high probability sentences likely english low probability sentences unlikely english example sentence pretty illformed youre unknown unlikely see sentence relatively low probability,Course1,W1-S2-L1,W1,S2,L1,Introduction,1,2,1,okay first topic go cover cours problem languag model languag model one oldest problem studi statist natur languag process basic problem use problem languag model use wide rang natur languag applic go cover number thing im firstli go defin basic problem well talk import class languag model call trigram languag model extrem wide use well talk evalu differ languag model measur effect differ languag model final well talk coupl estim techniqu languag model firstli someth call linear interpol secondli someth call discount method method wide use within languag model see later class theyr also use mani problem natur languag process basic estim techniqu wide use area get us start coupl definit go assum set v finit set go includ word languag interest imagin construct languag model english exampl might set contain word man telescop uncommon set realli quit larg might easili contain thousand ten thousand possibl word languag given underli set v im go use v dagger symbol refer set possibl sentenc string languag wellform sentenc take follow form zero word word drawn set v follow special symbol stop symbol okay use stop symbol end sentenc initi go look littl peculiar well see soon conveni includ symbol start develop probabilist model languag model problem recap sentenc could sequenc word could sentenc make sens exampl sentenc might sentenc realli realli doesnt make sens get sequenc like stop sequenc word drawn vocabulari follow stop well also includ sentenc stop symbol alon case sentenc basic zero length word stop complet precis given definit defin languag model problem im go assum train sampl exampl sentenc languag interest let assum that english exampl might collect sentenc youv seen new york time last year might collect larg set exampl sentenc world wide web think mani exampl train sampl quit larg concret mid exampl pretti common make use know roughli million word data train sampl end wasnt uncommon use mayb billion word often chosen newspap data exampl recent last sever year peopl start use web data construct languag model might even get scenario hundr billion word potenti train data main point train sampl get quit larg given train sampl task follow want learn distribut p sentenc languag okay p go function satisfi condit firstli sentenc x rememb dagger set possibl sentenc languag sentenc x p x greater equal secondli sum sentenc languag someth sum valu okay p wellform distribut sentenc languag task go take train sampl exampl sentenc input output function p output process exampl might exampl assign probabl minu sentenc compos word follow stop might assign time minu particular sentenc assign probabl everi sentenc languag roughli speak would like good languag model assign high probabl sentenc like english low probabl sentenc unlik english exampl sentenc pretti illform your unknown unlik see sentenc rel low probabl,[ 4  8 14 13 12]
3,Course1_W1-S2-L2_Introduction_to_the_Language_Modeling_Problem_Part_2_7-12,soon start talk techniques solve precisely problem problem taking training samples input returning function p output first question ask really know earth would want first sight seems like rather strange problem considering actually theres strong motivation considering two reasons ill give considering language modeling problem first language models actually useful wide range applications speech recognition really first application language models language models critical modern speech recognizers examples optical character recognition handwriting recognition another example well see later course machine translation short language models actually useful many applications ill come back point detail second reason studying language models estimation techniques develop later lecture useful problems nlp example well see problems part speech tagging natural language pausing machine translation estimation techniques described applied directly let go back first issue describe little detail language models relevant problem speech recognition fairly high level sketch hopefully youll get basic idea basic problem speech recognition follows input acoustic recording actually somebody speaking one axis time axis amplitude energy speech recognizer typically go preprocessing steps something like following would typically split sequence relatively short time periods often called frames frame might example around milliseconds long frame might perform kind fourier analysis get energies different frequencies within frame details arent important kind preprocessing might carry performed preprocessing task map acoustic input words actually spoken lets say sake example recognized speech actually spoken case speech recognizer takes acoustic sequences input outputs sentence sequence words output practice often many possible alternative sentences could spoken quite confusable another example sentence might wreck nice beach famous example speech processing community issue two sentences quite similar acoustic point view simply look measure compatible sentence acoustics versus sentence quite possible might confuse two sentences one example sentence practice many many many possibilities might reasonable degree fit acoustic input might quite confusable true sentence recognized speech case language model actually evaluate probability p sentences language model adds useful information whole process fact sentence recognize speech probably probably sentence wreck nice beach laugh language model going provide us additional information terms likelihood probability different sentences language going many others might look acoustically like like good match input completely unlikely sentences english practice modern speech recognizers use two sources information firstly way evaluating well sentences match input acoustic point view secondly also language model gives essentially prior probability different sentences language useful getting rid kind confusions okay finally lets talk kind naive method language modeling get us ground though experiement say n training sentences maybe million sentences new york new york times example sentence x xn ill define c x xn number times sentence seen training example okay simple estimate following define p simply c n okay simply count number times sentences seen divide total number sentences seen training corpus language model verify p always greater equal zero also sum sentences p sum one perfectly wellformed language model clear deficiencies importantly assign probability sentence seen training sample know continuiusly seeing new sentences language model really ability generalize new sentences important question lecture essentially build models improve upon naive estimate particular models generalize well new test sentences,Course1,W1-S2-L2,W1,S2,L2,Introduction,1,2,2,soon start talk techniqu solv precis problem problem take train sampl input return function p output first question ask realli know earth would want first sight seem like rather strang problem consid actual there strong motiv consid two reason ill give consid languag model problem first languag model actual use wide rang applic speech recognit realli first applic languag model languag model critic modern speech recogn exampl optic charact recognit handwrit recognit anoth exampl well see later cours machin translat short languag model actual use mani applic ill come back point detail second reason studi languag model estim techniqu develop later lectur use problem nlp exampl well see problem part speech tag natur languag paus machin translat estim techniqu describ appli directli let go back first issu describ littl detail languag model relev problem speech recognit fairli high level sketch hope youll get basic idea basic problem speech recognit follow input acoust record actual somebodi speak one axi time axi amplitud energi speech recogn typic go preprocess step someth like follow would typic split sequenc rel short time period often call frame frame might exampl around millisecond long frame might perform kind fourier analysi get energi differ frequenc within frame detail arent import kind preprocess might carri perform preprocess task map acoust input word actual spoken let say sake exampl recogn speech actual spoken case speech recogn take acoust sequenc input output sentenc sequenc word output practic often mani possibl altern sentenc could spoken quit confus anoth exampl sentenc might wreck nice beach famou exampl speech process commun issu two sentenc quit similar acoust point view simpli look measur compat sentenc acoust versu sentenc quit possibl might confus two sentenc one exampl sentenc practic mani mani mani possibl might reason degre fit acoust input might quit confus true sentenc recogn speech case languag model actual evalu probabl p sentenc languag model add use inform whole process fact sentenc recogn speech probabl probabl sentenc wreck nice beach laugh languag model go provid us addit inform term likelihood probabl differ sentenc languag go mani other might look acoust like like good match input complet unlik sentenc english practic modern speech recogn use two sourc inform firstli way evalu well sentenc match input acoust point view secondli also languag model give essenti prior probabl differ sentenc languag use get rid kind confus okay final let talk kind naiv method languag model get us ground though experi say n train sentenc mayb million sentenc new york new york time exampl sentenc x xn ill defin c x xn number time sentenc seen train exampl okay simpl estim follow defin p simpli c n okay simpli count number time sentenc seen divid total number sentenc seen train corpu languag model verifi p alway greater equal zero also sum sentenc p sum one perfectli wellform languag model clear defici importantli assign probabl sentenc seen train sampl know continuiusli see new sentenc languag model realli abil gener new sentenc import question lectur essenti build model improv upon naiv estim particular model gener well new test sentenc,[ 4  8  1 14 13]
4,Course1_W1-S2-L3_Markov_Processes_Part_1_8-56,okay previous segments lecture gave basic definition language modeling problem segment want talk trigram language models ive said extremely important widely used type language model trigram language models build heavily idea markov processes important concept probability statistics lets first talk markov processes well describe use construct trigram language model markov process following scenario sequence random variables x x xn random variable take value finite set v example v might set words language interested well assume length n fixed every sequence length example might every sequence length well first cover case length fixed well go generalize allowing n vary allowing value n random variable n fixed goal build model joint probability x taking value little x x taking value little x x n taking value little xn values set v okay joint probability distribution values n variables worth noting huge number possible values xy makes little xn fact vocabulary v whose size size v ive written theyre size v power n different possible sequences distribution large set possibilities thats think first position v possibilities think second position v possibilities nth position v possibilities end v power n okay go modeling joint probability basically going steps deriving whats called first order markov process first step use chain rule probabilities decompose entire expression product expressions let explain little bit whats going say using chain rule remember events b want say whats joint probability events b happening example might joint probability x equal little x x equal little x considering first random variables sequence chain rule says following decompose product terms first one p multiply conditional probability p b given follows directly definition conditional probabilities apply simple case sequence length means decompose probablity x taken takes value x multiplied probability x takes value x given x equal x okay thats chain rule applied two events apply longer sequences joint probability events b c chain rule says decompose p times p e given times p c given b notice first probability probability b probability c point condition previous events particular sequesntial order apply sequence length three expression written actually exactly ive written times p x equals x given x equals x x equal x okay important realize kind decomposition using patr using chain rule exact always take joint probability events decompose following way ive written simply chain rule applied full sequence random variables x x n really generalization two cases ive shown arbitrary n first term p x equals x product terms one position equals n term probability xi equals xi condition previous values random variables sequence critical realize equality exact always take joint probability form rewrite using chain rule form thats first step driving first order markov process lets go second step make critically markov assumption ok equality exact quality follows markov assumption assumption first order assumption well see call first order second saying position range n sequence x xi probability xi xi given previous values sequence equal probability xi equals xi conditioned previous value minus weve essentially made assumption random variable position basically depends value random variable little precise independent assumption saying random variable conditionally independent previous random variable random variables condition everything minus irrelevant leaves us expression say joint probability sequence equal product terms x x equals x product equals n point probability xi equal little xi given xi minus equal xi minus okay clearly huge assumption im assuming random variable depends previous value going useful considerably simplifies model well see considerably simplifies number parameters underlying model,Course1,W1-S2-L3,W1,S2,L3,Markov,1,2,3,okay previou segment lectur gave basic definit languag model problem segment want talk trigram languag model ive said extrem import wide use type languag model trigram languag model build heavili idea markov process import concept probabl statist let first talk markov process well describ use construct trigram languag model markov process follow scenario sequenc random variabl x x xn random variabl take valu finit set v exampl v might set word languag interest well assum length n fix everi sequenc length exampl might everi sequenc length well first cover case length fix well go gener allow n vari allow valu n random variabl n fix goal build model joint probabl x take valu littl x x take valu littl x x n take valu littl xn valu set v okay joint probabl distribut valu n variabl worth note huge number possibl valu xy make littl xn fact vocabulari v whose size size v ive written theyr size v power n differ possibl sequenc distribut larg set possibl that think first posit v possibl think second posit v possibl nth posit v possibl end v power n okay go model joint probabl basic go step deriv what call first order markov process first step use chain rule probabl decompos entir express product express let explain littl bit what go say use chain rule rememb event b want say what joint probabl event b happen exampl might joint probabl x equal littl x x equal littl x consid first random variabl sequenc chain rule say follow decompos product term first one p multipli condit probabl p b given follow directli definit condit probabl appli simpl case sequenc length mean decompos probabl x taken take valu x multipli probabl x take valu x given x equal x okay that chain rule appli two event appli longer sequenc joint probabl event b c chain rule say decompos p time p e given time p c given b notic first probabl probabl b probabl c point condit previou event particular sequesnti order appli sequenc length three express written actual exactli ive written time p x equal x given x equal x x equal x okay import realiz kind decomposit use patr use chain rule exact alway take joint probabl event decompos follow way ive written simpli chain rule appli full sequenc random variabl x x n realli gener two case ive shown arbitrari n first term p x equal x product term one posit equal n term probabl xi equal xi condit previou valu random variabl sequenc critic realiz equal exact alway take joint probabl form rewrit use chain rule form that first step drive first order markov process let go second step make critic markov assumpt ok equal exact qualiti follow markov assumpt assumpt first order assumpt well see call first order second say posit rang n sequenc x xi probabl xi xi given previou valu sequenc equal probabl xi equal xi condit previou valu minu weve essenti made assumpt random variabl posit basic depend valu random variabl littl precis independ assumpt say random variabl condit independ previou random variabl random variabl condit everyth minu irrelev leav us express say joint probabl sequenc equal product term x x equal x product equal n point probabl xi equal littl xi given xi minu equal xi minu okay clearli huge assumpt im assum random variabl depend previou valu go use consider simplifi model well see consider simplifi number paramet underli model,[ 4  7  1 14 13]
5,Course1_W1-S2-L4_Markov_Processes_Part_2_6-28,okay weve defined firstorder markov processes well see generalize called secondorder markov processes fact straight forward generalization ive shown task model joint distribution sequence n random variables secondorder markov process make following assumption joint probability product terms probability x equal little x probability x equal x condition x equals x elements along sequence equals n value ith random variable depends previous two random variables remember firstorder markov process would term alone conditioned also add term two positions back sense slightly powerful model capture broader class distributions conditioning little bit information essentially conditioning previous two elements sequence rather previous one condition previous element minus called firstorder process condition previous two elements secondorder process go fact define third order fourth order markov processes natural way thats essentially thats secondorder markov process make things little bit simpler note terms little awkward make things little bit simpler actually going write secondorder markov process follows product equals n point ith conditioned value minus also minus im careful essentially define couple random variables x minus x essentially start sequence random variables always take value star think always starting markov process xi sorry x minus equal star x given star x next element maybe thats word x next element okay makes things slightly simpler basically notational point view ive assumed length sequence n fixed really wed like generalize kind model length sequence n also random variable means wed like define distribution possible sequences sequence length also vary see get getting closer closer language modeling problem language models essential model distribution sentences length sentence vary theres actually simple solution direct extension secondorder markov processes firstorder markov processes matter well essentially say nth random variable always equal stop stop special symbol special regular vocabulary markov process member set v sort additional symbol set v ever seen end sequence use markov process exactly say joint probability equal product terms equals n point condition value ith random variable previous two random variables assuming xn equal stop way writing probability sequence length get distribution length sequences intuitively whats going process point im generating value ith random variable conditioned previous two random variables think generating sequence random variables left right order first x x x xn point theres possibility thread random variable stop see stop symbol stop output sequence output process sampling process little bit formally show quite mild conditions addition stipulation equal stop wellformed distribution possible sequences varying length dont want go details thats fairly straightforward application theory behind markov process show wellformed distribution main thing think intuition point generating symbol xi ever generate stop immediately terminate process,Course1,W1-S2-L4,W1,S2,L4,Markov,1,2,4,okay weve defin firstord markov process well see gener call secondord markov process fact straight forward gener ive shown task model joint distribut sequenc n random variabl secondord markov process make follow assumpt joint probabl product term probabl x equal littl x probabl x equal x condit x equal x element along sequenc equal n valu ith random variabl depend previou two random variabl rememb firstord markov process would term alon condit also add term two posit back sens slightli power model captur broader class distribut condit littl bit inform essenti condit previou two element sequenc rather previou one condit previou element minu call firstord process condit previou two element secondord process go fact defin third order fourth order markov process natur way that essenti that secondord markov process make thing littl bit simpler note term littl awkward make thing littl bit simpler actual go write secondord markov process follow product equal n point ith condit valu minu also minu im care essenti defin coupl random variabl x minu x essenti start sequenc random variabl alway take valu star think alway start markov process xi sorri x minu equal star x given star x next element mayb that word x next element okay make thing slightli simpler basic notat point view ive assum length sequenc n fix realli wed like gener kind model length sequenc n also random variabl mean wed like defin distribut possibl sequenc sequenc length also vari see get get closer closer languag model problem languag model essenti model distribut sentenc length sentenc vari there actual simpl solut direct extens secondord markov process firstord markov process matter well essenti say nth random variabl alway equal stop stop special symbol special regular vocabulari markov process member set v sort addit symbol set v ever seen end sequenc use markov process exactli say joint probabl equal product term equal n point condit valu ith random variabl previou two random variabl assum xn equal stop way write probabl sequenc length get distribut length sequenc intuit what go process point im gener valu ith random variabl condit previou two random variabl think gener sequenc random variabl left right order first x x x xn point there possibl thread random variabl stop see stop symbol stop output sequenc output process sampl process littl bit formal show quit mild condit addit stipul equal stop wellform distribut possibl sequenc vari length dont want go detail that fairli straightforward applic theori behind markov process show wellform distribut main thing think intuit point gener symbol xi ever gener stop immedi termin process,[ 4  1 14 13 12]
6,Course1_W1-S2-L5_Trigram_Language_Models_9-40,well see apply ideas directly language modeling problem well see trigram language models derived direct application secondorder markov processes trigram language model consist things firstly well finite set v going vocabulary language model exactly way might words like beckham might fairly large set might thousands even tens thousands words second part language model set parameters every sequence three words u v w referred trigram sequence three words parameter qw given uv well see soon role parameters take w could v sorry element v stop symbol u v could element element v addition special star symbols saw secondorder markov process given definitions model defines distribution follows sentence x xn xi v equals n minus xn equals stop probability trigram language model define product terms equals n point q xi given x minus x minus ill illustrate second example basically parameters correspond directly probability xi equal xi given xi minus equal xi minus xi minus equal xi minus basically rewrite secondorder markov process ive showed previous section thats formal definition things become lot clearer go example end really simple model lets take simpler sentence example sentence dog barks stop symbol end sentence well always case probability assigned sentence language model product terms weve notice words dog barks stop point condition previous words sequence example barks conditioned dog previous words sequence essentially treating sentences generated secondorder markov process word sentence chosen conditioned two previous words sentence trigram language model lets talk briefly assumption word depends two previous words clearly naive assumption fact possible cons construct kinds examples independence assumption clearly violated fact later course see models example probabilistic variance context free grammars arguably much realistic models language capture much longer range dependencies previous two words sentence said trigram language models tremendously useful turns quite difficult improve upon benefit considerable simplicity estimate trigram parameters model estimation problem well come little later lecture summarize sentence probability product terms one q parameters trigram sentence condition word previous two words leaves us remaining estimation problem want estimate q parameters model example might want estimate probability laughs given previous words dog well spend quite lot time estimation problem turns quite challenging program lets talk first though natural estimate quantity often referred maximum likelihood estimate really intuitive estimate recall weve assumed training set example sentences language might typically maybe million tens millions maybe even billion words sentences training samples derive counts example counts dog would number times ive seen word followed dog training samples count dog laughs would number times ive seen trigram sequence followed dog followed laughs maximum likelihood estimate ive written example ratio two terms ratio trigram divided ratio often call bigram sequence two words conditioning upon thats intuitive natural estimate many people would come first guess estimate parameters model maximum like estimators useful form starting point estimation methods develop clear problem following huge number parameters model even though weve made assumption condition previous two words point still large number parameters define n vocabulary size well n possibilities word n possibilities word n possibilities word plus plus take stop stop symbols start symbols account good estimate n times n times n n cubed parameters model example vocabulary size cubed thats times different parameters large number parameters irrespective much training data going large number comparison number training examnples manifests following way many cases counts numerator may equal simply havent seen particular triagram training data case estimate equal thats problematic many trigrams possible maybe times see trigram zero times training doesnt mean say estimate equal still cases denominator may zero may ive never seen words followed dog case ratio completely undefined estimate really falls apart main point large number parameters large amount training data would mean many counts equal zero lead estimates unrealistically low actually ill defined well soon see modify styles estimates ways make quite robust problems,Course1,W1-S2-L5,W1,S2,L5,Trigram,1,2,5,well see appli idea directli languag model problem well see trigram languag model deriv direct applic secondord markov process trigram languag model consist thing firstli well finit set v go vocabulari languag model exactli way might word like beckham might fairli larg set might thousand even ten thousand word second part languag model set paramet everi sequenc three word u v w refer trigram sequenc three word paramet qw given uv well see soon role paramet take w could v sorri element v stop symbol u v could element element v addit special star symbol saw secondord markov process given definit model defin distribut follow sentenc x xn xi v equal n minu xn equal stop probabl trigram languag model defin product term equal n point q xi given x minu x minu ill illustr second exampl basic paramet correspond directli probabl xi equal xi given xi minu equal xi minu xi minu equal xi minu basic rewrit secondord markov process ive show previou section that formal definit thing becom lot clearer go exampl end realli simpl model let take simpler sentenc exampl sentenc dog bark stop symbol end sentenc well alway case probabl assign sentenc languag model product term weve notic word dog bark stop point condit previou word sequenc exampl bark condit dog previou word sequenc essenti treat sentenc gener secondord markov process word sentenc chosen condit two previou word sentenc trigram languag model let talk briefli assumpt word depend two previou word clearli naiv assumpt fact possibl con construct kind exampl independ assumpt clearli violat fact later cours see model exampl probabilist varianc context free grammar arguabl much realist model languag captur much longer rang depend previou two word sentenc said trigram languag model tremend use turn quit difficult improv upon benefit consider simplic estim trigram paramet model estim problem well come littl later lectur summar sentenc probabl product term one q paramet trigram sentenc condit word previou two word leav us remain estim problem want estim q paramet model exampl might want estim probabl laugh given previou word dog well spend quit lot time estim problem turn quit challeng program let talk first though natur estim quantiti often refer maximum likelihood estim realli intuit estim recal weve assum train set exampl sentenc languag might typic mayb million ten million mayb even billion word sentenc train sampl deriv count exampl count dog would number time ive seen word follow dog train sampl count dog laugh would number time ive seen trigram sequenc follow dog follow laugh maximum likelihood estim ive written exampl ratio two term ratio trigram divid ratio often call bigram sequenc two word condit upon that intuit natur estim mani peopl would come first guess estim paramet model maximum like estim use form start point estim method develop clear problem follow huge number paramet model even though weve made assumpt condit previou two word point still larg number paramet defin n vocabulari size well n possibl word n possibl word n possibl word plu plu take stop stop symbol start symbol account good estim n time n time n n cube paramet model exampl vocabulari size cube that time differ paramet larg number paramet irrespect much train data go larg number comparison number train examnpl manifest follow way mani case count numer may equal simpli havent seen particular triagram train data case estim equal that problemat mani trigram possibl mayb time see trigram zero time train doesnt mean say estim equal still case denomin may zero may ive never seen word follow dog case ratio complet undefin estim realli fall apart main point larg number paramet larg amount train data would mean mani count equal zero lead estim unrealist low actual ill defin well soon see modifi style estim way make quit robust problem,[ 4  1 14 13 12]
7,Course1_W1-S2-L6_Evaluating_Language_Models-_Perplexity_12-36,okay previous sections defined language modeling problem introduced important class models trigram models language modeling soon describe estimation methods alleviate problems sparse data described previous session get estimation want describe evaluate effectiveness different language models describe important measure called perplexity widely used measure performances language model lets define perplexity ive said weve assumed language modeling problem training data consisting many sentences language interested addition going assume test data sentences using low refer number sentences sm sentence language example might dog laughs stop could sentence critically assume parameters language model estimated training examples well assume test data separate held set examples used estimation parameters test data new unseen examples given test data natural look probability language model assigns test data sentences product equals p si p product terms example given star star times q dog given star times function p function parameters good language model intuitively assign high probability possible test data sentences think value measure well language model predicts test data sentences actually slightly convenient instead looking product look log product take log usual rules logs products end following sum equals log probability language model sentence si higher quantity log probability better language model model test data sentences would actually perfectly good measure quality language model going transform quantity couple steps convenient turn convenient first im going define value l dividing log probability capital total number words test data interpreted average log probability word word test data basically quantity normalized respect length test data sentences sense stable respect length test examples log way going defined log base complexity defined minus l take log probability divide number words raise minus l one important point perplexity higher quantity log probability better fit model test examples means minus l lower quantities perplexity better higher quantities lower value perplexity better language model fit test data examples standard measure quality language model lets talk little bit giving complexity values also talking little bit important intuition behind complexity one thought experiment following say vocabulary v find capsule n size v plus lets say dumbest possible language model parameter capital n remember w case value v stop symbol language model simply assigns uniformed distribution possible words position completely ignores previous two words addition model relative frequency different words fact word english might frequent word english said dumbest possible language model could think easy enough go equations slide showed previously recover fact l case log n perplexity minus l perplexity case comes simply n basically vocabulary size plus think perplexity measure sense effective vocabulary size dumbest possible model simply get vocabulary size statistical models improve upon words frequent others condition context give little bit intuition values perplexity taken paper joshua goodman call bit progress language modeling considered assessing vocabulary size believe newspaper text numbers trigram model defined conditioned word previous two words appropriate estimation techniques well describe little later lecture perplexity came notice vastly smaller vocabulary size weve done much much better simply predicting uniform distribution word term sequence see statistical information given great benefits terms perpliexity goodman also looks models one bigram model bigram model similar trigram model point condition previous word xi minus essentially first order mark process rember trigram model second order mark process conditioning less information previous word course lose statistical power perplexity go bit pretty significant change also looked unigram models case probability sentence product terms q x actually completely ignores context parameter q x possible word predicting ignoring previous two words model ability model differences frequencies different words ability model context perplexity case close youll notice really dramatically larger value trigram model certainly gotten lot benefit conditional two previous words still rather better words vocabulary size short trigram language models give considerable improvements uniform distribution vocabulary unigram biagram models course go call gram models condition previous words gram models given enough training data see imrovements trigrams getting pretty close state art problem want finish segment little bit history behind idea language models critical reference due shannon really one earliest pieces work problemthis cole shannon really one huge figure information theory huge early figure inf information theory actually experiments looked basically bigram trigram language models building building mark processes starting point interesting paper well worth reading amongst things actually measures well humans perform language models actually looks well humans predict next word maybe next letter text turns humans really perform quite quite well probelm probably better trigram models ive described actually probably better language model weve come point thats one early piece research another interesting reference due chomsky syntactic structures hugely influential book modern linguistics wonderful book highly recommend interested language detailed argument role probability statistics linguistics specifically role grammaticality predicting whether sentence grammatical involved argument provoke lot provoked lot discussion wont get arguments probability really doesnt much grammaticality interesting argument said wont get get details said think entirely clear think hes much sharons experiments mark process language mind clear arguments processes fail capture kind longer distance dependencies critical part language said later course see powerful models contextfree grammars begin get interesting long range dependencies said said trigram language models tremendously useful practice applications speech recognition machine translation play critical role theyve hard improve upon,Course1,W1-S2-L6,W1,S2,L6,Evaluating,1,2,6,okay previou section defin languag model problem introduc import class model trigram model languag model soon describ estim method allevi problem spars data describ previou session get estim want describ evalu effect differ languag model describ import measur call perplex wide use measur perform languag model let defin perplex ive said weve assum languag model problem train data consist mani sentenc languag interest addit go assum test data sentenc use low refer number sentenc sm sentenc languag exampl might dog laugh stop could sentenc critic assum paramet languag model estim train exampl well assum test data separ held set exampl use estim paramet test data new unseen exampl given test data natur look probabl languag model assign test data sentenc product equal p si p product term exampl given star star time q dog given star time function p function paramet good languag model intuit assign high probabl possibl test data sentenc think valu measur well languag model predict test data sentenc actual slightli conveni instead look product look log product take log usual rule log product end follow sum equal log probabl languag model sentenc si higher quantiti log probabl better languag model model test data sentenc would actual perfectli good measur qualiti languag model go transform quantiti coupl step conveni turn conveni first im go defin valu l divid log probabl capit total number word test data interpret averag log probabl word word test data basic quantiti normal respect length test data sentenc sens stabl respect length test exampl log way go defin log base complex defin minu l take log probabl divid number word rais minu l one import point perplex higher quantiti log probabl better fit model test exampl mean minu l lower quantiti perplex better higher quantiti lower valu perplex better languag model fit test data exampl standard measur qualiti languag model let talk littl bit give complex valu also talk littl bit import intuit behind complex one thought experi follow say vocabulari v find capsul n size v plu let say dumbest possibl languag model paramet capit n rememb w case valu v stop symbol languag model simpli assign uniform distribut possibl word posit complet ignor previou two word addit model rel frequenc differ word fact word english might frequent word english said dumbest possibl languag model could think easi enough go equat slide show previous recov fact l case log n perplex minu l perplex case come simpli n basic vocabulari size plu think perplex measur sens effect vocabulari size dumbest possibl model simpli get vocabulari size statist model improv upon word frequent other condit context give littl bit intuit valu perplex taken paper joshua goodman call bit progress languag model consid assess vocabulari size believ newspap text number trigram model defin condit word previou two word appropri estim techniqu well describ littl later lectur perplex came notic vastli smaller vocabulari size weve done much much better simpli predict uniform distribut word term sequenc see statist inform given great benefit term perpliex goodman also look model one bigram model bigram model similar trigram model point condit previou word xi minu essenti first order mark process rember trigram model second order mark process condit less inform previou word cours lose statist power perplex go bit pretti signific chang also look unigram model case probabl sentenc product term q x actual complet ignor context paramet q x possibl word predict ignor previou two word model abil model differ frequenc differ word abil model context perplex case close youll notic realli dramat larger valu trigram model certainli gotten lot benefit condit two previou word still rather better word vocabulari size short trigram languag model give consider improv uniform distribut vocabulari unigram biagram model cours go call gram model condit previou word gram model given enough train data see imrov trigram get pretti close state art problem want finish segment littl bit histori behind idea languag model critic refer due shannon realli one earliest piec work problemthi cole shannon realli one huge figur inform theori huge earli figur inf inform theori actual experi look basic bigram trigram languag model build build mark process start point interest paper well worth read amongst thing actual measur well human perform languag model actual look well human predict next word mayb next letter text turn human realli perform quit quit well probelm probabl better trigram model ive describ actual probabl better languag model weve come point that one earli piec research anoth interest refer due chomski syntact structur huge influenti book modern linguist wonder book highli recommend interest languag detail argument role probabl statist linguist specif role grammat predict whether sentenc grammat involv argument provok lot provok lot discuss wont get argument probabl realli doesnt much grammat interest argument said wont get get detail said think entir clear think he much sharon experi mark process languag mind clear argument process fail captur kind longer distanc depend critic part languag said later cours see power model contextfre grammar begin get interest long rang depend said said trigram languag model tremend use practic applic speech recognit machin translat play critic role theyv hard improv upon,[ 4  8 14 13 12]
8,Course1_W1-S3-L1_Linear_Interpolation_Part_1_7-46,okay far lecture described basic language modelling problem weve introduced important class language models namely trigram language models finally weve spoken evaluate different language models using measure called perplexity final part lecture im going talk estimation techniques trigram models namely linear interpolation discounting methods well first focus method called linear interpolation recap one primary challenges involved estimation trigram language modes sparse data problem described earlier lecture lets go argument natural estimate trigram language model whats called maximum likelihood estimate general form follows im estimating parameter q word wi conditioned previous words wi minus wi minus simply define parameter estimate ratio two terms numerator often called trigram count simply number times ive seen sequence words wi minus wi minus wi training corpus dot denominator bigram count simply number times ive seen words wi minus wi minus training data lets take specific parameter example say want estimate parameter corresponding probability laughs given previous two words dog ratio two counts trigram count bigram count said earlier general models going large number parameters vocabulary size n roughly n cubed parameters model one example n cubed thats around times parameters even large training sets use nowadays estimate parameters language model large number inevitably many counts used estimates equal zero lead kinds problems many cases q parameters equal zero happens count numerator trigram count equal zero still bigram count equal zero count denominator estimate completely undefined leads us estimation methods going consider ive said first going consider method called linear interpolation first lets get definitions q sub ml going maximum likelihood estimate trigram parameter estimate based ratio counts showed previous slide addition trigram estimate also describe called bigram unigram estimates follows bigram estimate game use q sub ml looks word wi previous word wi minus estimate conditions previous one word opposed previous two words context simply defined ratio counts numerator bigram count denominator unigram count go step unigram estimate actually estimate probability word completely ignores context even going condition previous word context find ratio counts numerator unigram count thats simply number times ive seen word wi corpus denominator expression going number total number words corpus total number words training data simply ratio frequency word wi total number words ive seen corpus look three different estimates corresponding three different levels different strengths weaknesses tradeoff often referred biasvariance tradeoff statistics trigram estimate benefit conditions lot context namely previous two words relatively low bias given enough training samples counts relatively high converge reasonable estimate probability wi given context contrast look unigram estimate completely ignores context itll fail capture contextual effects itll converge less good estimator number trading samples increases conversely however trigram maximum likelihood estimator problem many counts equal zero need large number training samples get accurate estimate trigram maximum length estimate conversely unigram estimate counts converge rather quickly expected values estimate quickly converge true unigram distribution underlying data bigram estimate somewhere two extremes conditions reasonable amount context converges reasonably quickly true underlying value wed really like come estimator trades different strength weaknesses three estimators linear interpolation comes play,Course1,W1-S3-L1,W1,S3,L1,Linear,1,3,1,okay far lectur describ basic languag model problem weve introduc import class languag model name trigram languag model final weve spoken evalu differ languag model use measur call perplex final part lectur im go talk estim techniqu trigram model name linear interpol discount method well first focu method call linear interpol recap one primari challeng involv estim trigram languag mode spars data problem describ earlier lectur let go argument natur estim trigram languag model what call maximum likelihood estim gener form follow im estim paramet q word wi condit previou word wi minu wi minu simpli defin paramet estim ratio two term numer often call trigram count simpli number time ive seen sequenc word wi minu wi minu wi train corpu dot denomin bigram count simpli number time ive seen word wi minu wi minu train data let take specif paramet exampl say want estim paramet correspond probabl laugh given previou two word dog ratio two count trigram count bigram count said earlier gener model go larg number paramet vocabulari size n roughli n cube paramet model one exampl n cube that around time paramet even larg train set use nowaday estim paramet languag model larg number inevit mani count use estim equal zero lead kind problem mani case q paramet equal zero happen count numer trigram count equal zero still bigram count equal zero count denomin estim complet undefin lead us estim method go consid ive said first go consid method call linear interpol first let get definit q sub ml go maximum likelihood estim trigram paramet estim base ratio count show previou slide addit trigram estim also describ call bigram unigram estim follow bigram estim game use q sub ml look word wi previou word wi minu estim condit previou one word oppos previou two word context simpli defin ratio count numer bigram count denomin unigram count go step unigram estim actual estim probabl word complet ignor context even go condit previou word context find ratio count numer unigram count that simpli number time ive seen word wi corpu denomin express go number total number word corpu total number word train data simpli ratio frequenc word wi total number word ive seen corpu look three differ estim correspond three differ level differ strength weak tradeoff often refer biasvari tradeoff statist trigram estim benefit condit lot context name previou two word rel low bia given enough train sampl count rel high converg reason estim probabl wi given context contrast look unigram estim complet ignor context itll fail captur contextu effect itll converg less good estim number trade sampl increas convers howev trigram maximum likelihood estim problem mani count equal zero need larg number train sampl get accur estim trigram maximum length estim convers unigram estim count converg rather quickli expect valu estim quickli converg true unigram distribut underli data bigram estim somewher two extrem condit reason amount context converg reason quickli true underli valu wed realli like come estim trade differ strength weak three estim linear interpol come play,[ 4 14 13 12 11]
9,Course1_W1-S3-L2_Linear_Interpolation_Part_2_11-35,linear interpolation going come estimate takes account maximum length estimates trigram bigram unigram levels new estimate actually going weighted average three maximum length estimatessound additional parameters lambda lambda lambda dictate relative weights maximal likelihood estimates constraints lambda values sum greater equal example might lambda equals lambda equals lambda equals third basically means give onethird weight maximum likelihood estimates lets go specific example specific parameter say want estimate parameter corresponding probability word laughs given previous words dog definition assuming lambda values equal third would rd times maximum like estimate laughs given dog rd times maximum like estimate laughs given word dog finally rd times maximum length estimate laughs without contextual sensitivity motivation step practice end new estimator incorporates information three maximal likelyg estimates thus sense incorporates strengths three estimates new estimator sensitive previous two words context estimator also robust incorporate information robust estimates bigram unigram level shortly well see actually estimate lambda parameters using data first want show important point parameter estimate q fact valid estimate important verify estimate correctly defines distribution mean following define v primed vocabulary stop symbol uv bigram conditioning want make sure sum words w v primed something sums thats fairly simple show im going go slide steps proving property ive done substituted q expression showed previous slide interpolated estimate lambda one lambda two lambda three three paramaters dictating relative weight three estimates three maximum length estimates trigram bigram unigram estimate first thing notice lambda paramaters vary w varies brought outside respective sums expression simplifies slightly lambda times sum w plus lambda sum w plus lambda timesunknown w unigram estimate next notice simple enough show sums equal thats maximum couldg estimate correctly define distribution equivalently go back definitions terms counts convince property end simply lambda lambda lambda terms definition remember constraint lambdas terms sum also shown property need directly define distribution uv bigram w set v primed value greater equal thats thats trivial show case final question need answer actually estimate lambda values generally done follows firstly well take part training set take sentence take sentences training set hold called validation data visualize follows imagine training data consists many millions sentences might take portion relatively small portion roughly speaking say percent percent data use call validation data counts used maximum likelihood estimates taken main portion training data addition trigram c w w w ill define c primed number times trigram seen validation portion training data proceed follows going define function l function parameters basically measures well model fits validation data l defined follows sum trigrams w w w c primed going number times particular trigram seen validation data many counts course going zero log q w given w w parameter estimate particular q q defined theunknown fact hidden expression depends theunknown defined way average maximum likely hood estimates lander parameters vary q values vary function value l vary well set optimization problem attempt maximize l constraints lambda positive also sum interpret l measure well particular values lambda lambda lambda fit validation data fact fairly easy exercise show maximize function respect lambdas also minimize complexity model validation data actually trying pick lambdas minimize complexity language model hence fit validation data well possible im going go details maximization performed practice fact fairly simple operation solve problem want talk one last issue important practice intended sketch hopefully get idea practice actually important allow lambda vary little bit particular allow vary depending different counts used estimate heres done practice say conditioning particular bigram wi minus wi minus actually going partition different bigrams depending account definition partitioned four different subsets function pi takes bigrammers imput returns bigram count returns count returns counts returns otherwise partition generally chosen hand probably fairly typical kind definition might see weve defined partition give slightly refined version linear interpolation landers vary depending value pi parameters lander sub one lambda one sub two lambda one sub three three parameters used count bigram equal zero parameters lambda two one lambda two two lambda two three use three parameters count one two notice lambdas vary depending partition bigram falls whats motivation step basically means lambdas vary depending partition bigram falls hence vary depending count underlying bigram actually important practice landers optimized using validation data similar way way showed previous slide one crucial aspect particular bigram counter equal zero parameter lambda actually equal maximum likely estimate actually defined particular case,Course1,W1-S3-L2,W1,S3,L2,Linear,1,3,2,linear interpol go come estim take account maximum length estim trigram bigram unigram level new estim actual go weight averag three maximum length estimatessound addit paramet lambda lambda lambda dictat rel weight maxim likelihood estim constraint lambda valu sum greater equal exampl might lambda equal lambda equal lambda equal third basic mean give onethird weight maximum likelihood estim let go specif exampl specif paramet say want estim paramet correspond probabl word laugh given previou word dog definit assum lambda valu equal third would rd time maximum like estim laugh given dog rd time maximum like estim laugh given word dog final rd time maximum length estim laugh without contextu sensit motiv step practic end new estim incorpor inform three maxim likelyg estim thu sens incorpor strength three estim new estim sensit previou two word context estim also robust incorpor inform robust estim bigram unigram level shortli well see actual estim lambda paramet use data first want show import point paramet estim q fact valid estim import verifi estim correctli defin distribut mean follow defin v prime vocabulari stop symbol uv bigram condit want make sure sum word w v prime someth sum that fairli simpl show im go go slide step prove properti ive done substitut q express show previou slide interpol estim lambda one lambda two lambda three three paramat dictat rel weight three estim three maximum length estim trigram bigram unigram estim first thing notic lambda paramat vari w vari brought outsid respect sum express simplifi slightli lambda time sum w plu lambda sum w plu lambda timesunknown w unigram estim next notic simpl enough show sum equal that maximum couldg estim correctli defin distribut equival go back definit term count convinc properti end simpli lambda lambda lambda term definit rememb constraint lambda term sum also shown properti need directli defin distribut uv bigram w set v prime valu greater equal that that trivial show case final question need answer actual estim lambda valu gener done follow firstli well take part train set take sentenc take sentenc train set hold call valid data visual follow imagin train data consist mani million sentenc might take portion rel small portion roughli speak say percent percent data use call valid data count use maximum likelihood estim taken main portion train data addit trigram c w w w ill defin c prime number time trigram seen valid portion train data proceed follow go defin function l function paramet basic measur well model fit valid data l defin follow sum trigram w w w c prime go number time particular trigram seen valid data mani count cours go zero log q w given w w paramet estim particular q q defin theunknown fact hidden express depend theunknown defin way averag maximum like hood estim lander paramet vari q valu vari function valu l vari well set optim problem attempt maxim l constraint lambda posit also sum interpret l measur well particular valu lambda lambda lambda fit valid data fact fairli easi exercis show maxim function respect lambda also minim complex model valid data actual tri pick lambda minim complex languag model henc fit valid data well possibl im go go detail maxim perform practic fact fairli simpl oper solv problem want talk one last issu import practic intend sketch hope get idea practic actual import allow lambda vari littl bit particular allow vari depend differ count use estim here done practic say condit particular bigram wi minu wi minu actual go partit differ bigram depend account definit partit four differ subset function pi take bigramm imput return bigram count return count return count return otherwis partit gener chosen hand probabl fairli typic kind definit might see weve defin partit give slightli refin version linear interpol lander vari depend valu pi paramet lander sub one lambda one sub two lambda one sub three three paramet use count bigram equal zero paramet lambda two one lambda two two lambda two three use three paramet count one two notic lambda vari depend partit bigram fall what motiv step basic mean lambda vari depend partit bigram fall henc vari depend count underli bigram actual import practic lander optim use valid data similar way way show previou slide one crucial aspect particular bigram counter equal zero paramet lambda actual equal maximum like estim actual defin particular case,[ 4 14 13 12 11]
10,Course1_W1-S3-L3_Discounting_Methods_Part_1_9-26,okay final part lecture going describe second estimation technique whats often called discounting method build many intuitions linear interpolation well see slightly different way things also often used practice okay understand discounting methods lets first start intuition illustrated example ive shown ive shown counts biagrams first word biagram word example followed dog seen times followed woman seen times ill assume example list ive shown bigrams whose count greater zero full list words seen one times following word ten ten words total number times ive seen word going sum counts actually case consider maximum likelihood estimates case going example divided probability dog finder probability woman following falling ratio count number times ive seen word one thing observe general estimates going systematically high particularly cases large vocabulary remember might thousand tens thousands possible words forming word weve seen word times lucky words actually seen word thats rather informal description actually show rather formally estimates going systematically high thats particularly true low count estimates example probability street falling going rather high estimate case discounting methods build intuition following way going define new discounted counts ill use count star refer discounted count simply count x minus bigram x whos count looking back example count example discounted count going similarly original count translates discounted count cases count one discounted count define estimates based ratio discounted count number times weve seen word example estimate probability dog given probability country given notice essentially lowered estimates discounting methods weve done well see actually missing left probability mass mean well sum terms end expression something like plus finally plus times thats actually value calculation actually less series probabilities sum less leaves called missing probability mass whose value minus equal sense probability mass left discounting method basic idea discounting methods going take missing probability mass divide words vocabulary arent list ie words count following word equal unlucky words never seen bigram first word little bit formally well define word wi minus alpha wi minus missing probability mass defined minus sum w count star w minus w divided count wi minus particular example showed verify missing probability mass fact lets see derive final estimate based discounting method following idea dividing missing mass words whose counts particular contextual word method going back katz often called katz backoff model deriving bigram estimator q bo going backed estimate wi conditioned wi minus going condition previous word moment well see define trigram estimator similar way first well consider bigram case okay particular word wi minus ill define sets big wi minus set words whose bigram count greater example word would words ive shown previous slide contrast b w minus set words count equal set words never seen following particular word interested im going define missing probability mass expression going minus sum words whose count greater zero count star w minus w divided count w minus probability mass left okay given definitions backoff estimate takes two forms depending whether word set word set b word set simply take discounted count might example divided number times weve seen wi minus might example conversely word second set b means count equal following remember alpha wi minus one missing probability mass ive said somehow going divide different words set b divide proportion maximum likelihood estimate unigram level qmlfwy unigram maximum likely estimate showed earlier numerator thats denominator sum words b qml w normalization turn ensures im splitting alphas proportion unigram maximum likely estimate see weve defined discounted counts example subtracting value like count training data really quite simple derive estimation method works really quite well practice,Course1,W1-S3-L3,W1,S3,L3,Discounting,1,3,3,okay final part lectur go describ second estim techniqu what often call discount method build mani intuit linear interpol well see slightli differ way thing also often use practic okay understand discount method let first start intuit illustr exampl ive shown ive shown count biagram first word biagram word exampl follow dog seen time follow woman seen time ill assum exampl list ive shown bigram whose count greater zero full list word seen one time follow word ten ten word total number time ive seen word go sum count actual case consid maximum likelihood estim case go exampl divid probabl dog finder probabl woman follow fall ratio count number time ive seen word one thing observ gener estim go systemat high particularli case larg vocabulari rememb might thousand ten thousand possibl word form word weve seen word time lucki word actual seen word that rather inform descript actual show rather formal estim go systemat high that particularli true low count estim exampl probabl street fall go rather high estim case discount method build intuit follow way go defin new discount count ill use count star refer discount count simpli count x minu bigram x who count look back exampl count exampl discount count go similarli origin count translat discount count case count one discount count defin estim base ratio discount count number time weve seen word exampl estim probabl dog given probabl countri given notic essenti lower estim discount method weve done well see actual miss left probabl mass mean well sum term end express someth like plu final plu time that actual valu calcul actual less seri probabl sum less leav call miss probabl mass whose valu minu equal sens probabl mass left discount method basic idea discount method go take miss probabl mass divid word vocabulari arent list ie word count follow word equal unlucki word never seen bigram first word littl bit formal well defin word wi minu alpha wi minu miss probabl mass defin minu sum w count star w minu w divid count wi minu particular exampl show verifi miss probabl mass fact let see deriv final estim base discount method follow idea divid miss mass word whose count particular contextu word method go back katz often call katz backoff model deriv bigram estim q bo go back estim wi condit wi minu go condit previou word moment well see defin trigram estim similar way first well consid bigram case okay particular word wi minu ill defin set big wi minu set word whose bigram count greater exampl word would word ive shown previou slide contrast b w minu set word count equal set word never seen follow particular word interest im go defin miss probabl mass express go minu sum word whose count greater zero count star w minu w divid count w minu probabl mass left okay given definit backoff estim take two form depend whether word set word set b word set simpli take discount count might exampl divid number time weve seen wi minu might exampl convers word second set b mean count equal follow rememb alpha wi minu one miss probabl mass ive said somehow go divid differ word set b divid proport maximum likelihood estim unigram level qmlfwi unigram maximum like estim show earlier numer that denomin sum word b qml w normal turn ensur im split alpha proport unigram maximum like estim see weve defin discount count exampl subtract valu like count train data realli quit simpl deriv estim method work realli quit well practic,[ 4 14 13 12 11]
11,Course1_W1-S3-L4_Discounting_Methods_Part_2_3-34,method extended fairly natural way katz backoff models trigram case going define estimate wi given two previous words derived similar way bigram case im going define sets b depend two words conditioned first set set words w trigram count greater zero second set b set words w trigram count equal zero look backoff estimate first case follows come across word wi trigram cant count greater take ratio discounted count minus count going something like count trigram minus constant say simply count bigram conditioned definition gives us missing probability mass expression missing mess alpha wi minus wi minus going minus sum words whose count greater zero estimator showed count star divided count missing probability mass word seen set b count equal zero define backoff estimate following way going split missing mass words set b going split proportion backoff estimate bigram level estimator showed previous slide essentially recursive estimate going look backoff estimate level bigram level numerator backoff estimate normalization term sum words b backoff term simply taking missing probability mass alpha splitting proportion backed estimates bigram thats essentially trigram backoff method one parameter approach value say im using discounting value subtract account form discounted count typically value one way choose value optimization validation data test series values one see well language model forms validation set data pick discount value gives best performance validation data value wouldnt untypical usually somewhere around,Course1,W1-S3-L4,W1,S3,L4,Discounting,1,3,4,method extend fairli natur way katz backoff model trigram case go defin estim wi given two previou word deriv similar way bigram case im go defin set b depend two word condit first set set word w trigram count greater zero second set b set word w trigram count equal zero look backoff estim first case follow come across word wi trigram cant count greater take ratio discount count minu count go someth like count trigram minu constant say simpli count bigram condit definit give us miss probabl mass express miss mess alpha wi minu wi minu go minu sum word whose count greater zero estim show count star divid count miss probabl mass word seen set b count equal zero defin backoff estim follow way go split miss mass word set b go split proport backoff estim bigram level estim show previou slide essenti recurs estim go look backoff estim level bigram level numer backoff estim normal term sum word b backoff term simpli take miss probabl mass alpha split proport back estim bigram that essenti trigram backoff method one paramet approach valu say im use discount valu subtract account form discount count typic valu one way choos valu optim valid data test seri valu one see well languag model form valid set data pick discount valu give best perform valid data valu wouldnt untyp usual somewher around,[ 4 14 13 12 11]
12,Course1_W1-S3-L5_Summary_2-31,okay completes lecture summarize main lessons weve learned three steps deriving language models ive shown first step expand joint probability sequence words w w wn using chain rule probabilities showed portion lecture markov processes second step make markov independence assumptions particular assuming probability word wi conditioned entire previous sequence minus previous words actually depends previous two words sequence call secondorder markov assumption final step deriving estimates smooth diagram estimates essentially using low order accounts done either method linear interpolation discounting method showed briefly language modelling huge industry theres lot research improved methods language modelling areas particular interest methods model underlying topic documents longrange features document conditioning previous two words certainly limiting cases might want condition fact document sports politics general topic influence words seen document might important condition might condition words outside twoword window theres considerable interest problem another type model well see later class language models built based syntactic models language models explicitly trying incorporate grammatical information information sentences grammatical versus nongrammatical language models often capture long range features fall outside twoway window rule though quite quite difficult improve upon language models theyre simple theyre efficient get us long way many problems,Course1,W1-S3-L5,W1,S3,L5,Summary,1,3,5,okay complet lectur summar main lesson weve learn three step deriv languag model ive shown first step expand joint probabl sequenc word w w wn use chain rule probabl show portion lectur markov process second step make markov independ assumpt particular assum probabl word wi condit entir previou sequenc minu previou word actual depend previou two word sequenc call secondord markov assumpt final step deriv estim smooth diagram estim essenti use low order account done either method linear interpol discount method show briefli languag model huge industri there lot research improv method languag model area particular interest method model underli topic document longrang featur document condit previou two word certainli limit case might want condit fact document sport polit gener topic influenc word seen document might import condit might condit word outsid twoword window there consider interest problem anoth type model well see later class languag model built base syntact model languag model explicitli tri incorpor grammat inform inform sentenc grammat versu nongrammat languag model often captur long rang featur fall outsid twoway window rule though quit quit difficult improv upon languag model theyr simpl theyr effici get us long way mani problem,[4 2 1 5 8]
13,Course1_W10-S1-L1_Introduction_1-02,last week class going go global linear models show extend applied couple important problems first one im going describe segment tagging problems actually going describe perceptron algorithm showed last time conjunction global linear models applied tagging problem second problem well look week dependency parsing new type parsing problem problem global linear models perception algorithm applied interesting thing well see cases well make use global linear models conjunction dynamic programming algorithms youve seen earlier course well introduce powerful new way applying global linear models various problems,Course1,W10-S1-L1,W10,S1,L1,Introduction,10,1,1,last week class go go global linear model show extend appli coupl import problem first one im go describ segment tag problem actual go describ perceptron algorithm show last time conjunct global linear model appli tag problem second problem well look week depend pars new type pars problem problem global linear model percept algorithm appli interest thing well see case well make use global linear model conjunct dynam program algorithm youv seen earlier cours well introduc power new way appli global linear model variou problem,[ 1  4 14  5 13]
14,Course1_W10-S1-L2_Recap_of_GLMs_7-40,start let give recap global linear models work remember three components global linear model feature vector f function gen parameter vector v let remind f function maps inputoutput pair feature vector lets take pausing example case might x equals sentence thats going input going parse tree sentence remember f going map entire xy pair feature vector might example feature vector particular parse tree conjunction particular sentence gen function takes input sound maps set candidate structures x example could sentence like dog barks sound gen x going set parse trees assuming looking parsing problem saw last time various ways might define gen x might example define gen x set possible parse trees context free grammar particular context free grammar finally v parameter vector v going sort parameters dimensionality f five features would five parameter values v example could take inner product v f get score said last lecture interpreted measure plausible particular structure conjoined particular input x saw last time way put three components together follows set possible inputs example set possible sentences language set x use refer set possible outputs example set possible parse trees goal learn function capital f takes input x maps output function defined three components gen f v follows given x enumerate every one candidate structures arg max members gen x structure calculate score inner product return highest scoring structure thats arg max intuitive sense enumerate candidates turn calculate score inner product return high scoring candidate critical question course given set training examples examples inputoutput pairs actually set parameters v solved perceptron algorithm ill give recap algorithm second first lets cough look schematically figure showed last weeks lecture view whole process follows input sentence function gen enumerates set possible structures case one two three four five six possible trees structures mapped feature vector mapping f feature vector six feature vectors one trees case calculate score structure inner product f v example first tree might get score second tree score third one finally select highest scoring tree first one case score final output model see parameters v varied put different weights either positive negative different features model scores change training parameters model intuitively going try look values v correctly recover correct paths training examples saw last time simple yet effective algorithm parameter estimation global linear models variant perceptron algorithm lets look training set set examples xiyi equals n example might sentences paired trees sound might hundred thousand maybe even tens thousands examples like initially set parameters equal throughout algorithm im going im going define capital f x exactly way arg max function capital specifies number iterations perceptron might typically maybe iterations going specify many passes actually make training data make big passes training data pass cycle examples n calculate output model example z sub going parse tree current highest scoring tree model zi equal truth yi leave parameters unchanged remember broken dont try fix dont make mistake leave parameters unchanged hand zi equal yi perform similar simple parameter updates say new value v old value v plus feature vector xi together yi subtract feature vector xi conjunction zi remember vectors dimensional space initially might look odd add parameter feature vectors perfectly legitimate operation vectors dimensionality space intuitively going increase weight features seen truth structure yi decrease weight features seen incorrectly proposed structure zi thats perceptron said simple algorithm yet effective algorithm lecture going see extend basic approach problem tagging,Course1,W10-S1-L2,W10,S1,L2,Recap,10,1,2,start let give recap global linear model work rememb three compon global linear model featur vector f function gen paramet vector v let remind f function map inputoutput pair featur vector let take paus exampl case might x equal sentenc that go input go pars tree sentenc rememb f go map entir xy pair featur vector might exampl featur vector particular pars tree conjunct particular sentenc gen function take input sound map set candid structur x exampl could sentenc like dog bark sound gen x go set pars tree assum look pars problem saw last time variou way might defin gen x might exampl defin gen x set possibl pars tree context free grammar particular context free grammar final v paramet vector v go sort paramet dimension f five featur would five paramet valu v exampl could take inner product v f get score said last lectur interpret measur plausibl particular structur conjoin particular input x saw last time way put three compon togeth follow set possibl input exampl set possibl sentenc languag set x use refer set possibl output exampl set possibl pars tree goal learn function capit f take input x map output function defin three compon gen f v follow given x enumer everi one candid structur arg max member gen x structur calcul score inner product return highest score structur that arg max intuit sens enumer candid turn calcul score inner product return high score candid critic question cours given set train exampl exampl inputoutput pair actual set paramet v solv perceptron algorithm ill give recap algorithm second first let cough look schemat figur show last week lectur view whole process follow input sentenc function gen enumer set possibl structur case one two three four five six possibl tree structur map featur vector map f featur vector six featur vector one tree case calcul score structur inner product f v exampl first tree might get score second tree score third one final select highest score tree first one case score final output model see paramet v vari put differ weight either posit neg differ featur model score chang train paramet model intuit go tri look valu v correctli recov correct path train exampl saw last time simpl yet effect algorithm paramet estim global linear model variant perceptron algorithm let look train set set exampl xiyi equal n exampl might sentenc pair tree sound might hundr thousand mayb even ten thousand exampl like initi set paramet equal throughout algorithm im go im go defin capit f x exactli way arg max function capit specifi number iter perceptron might typic mayb iter go specifi mani pass actual make train data make big pass train data pass cycl exampl n calcul output model exampl z sub go pars tree current highest score tree model zi equal truth yi leav paramet unchang rememb broken dont tri fix dont make mistak leav paramet unchang hand zi equal yi perform similar simpl paramet updat say new valu v old valu v plu featur vector xi togeth yi subtract featur vector xi conjunct zi rememb vector dimension space initi might look odd add paramet featur vector perfectli legitim oper vector dimension space intuit go increas weight featur seen truth structur yi decreas weight featur seen incorrectli propos structur zi that perceptron said simpl algorithm yet effect algorithm lectur go see extend basic approach problem tag,[ 5  4  0 10 14]
15,Course1_W10-S1-L3_GLMs_for_Tagging_Part_1_5-26,heres quick reminder entailed tagging problems abstractly tagging problem take sequence words letters case input output produce tagged sequence element sequence associated tag example cdc one first examples saw partofspeech tagging case input sentence output tagger tagged sentence word gets tag example profits tagged n noun soared tagged v verb tagged preposition second crucial example looked problem named entity recognition problem input model sentence sequence words output tagged sequence tags encode named entities found within sentence word entity tag na whereas words start company tag sc words continuation company tag cc similarly stop location continue location stop person continue person weve seen two basic models tagging problem hit mark models hmms recently saw log linear taggers sound first two methods weve seen lecture going see third method global linear models glms percetron applied tagging problems going set glm tagging okay input model sentence lets say n equals example might sentence w equal w w w might sentence like dog barks example define set possible tags lets assume three three parts speech tags n v first thing going define function gen remember gen going take sentences input map set candidate structures particular case going use definition gen sentence simply returns possible tag sequences length n particular example gen sentence w w w would set consists tag sequences length three using three tags okay theyre actually thats members gen case okay gen really simple function could think simply lists possible tag sequences input sentence critically size gen going exponential length sentence fact size gen sentence x going number possible tags size set thats number possible tags raised power n n length sentence possible tags first position possible tags second position possible tags third position think global linear models weve presented far algorithms weve looked explicitly enumerate different members gen score function vf xy clearly isnt going tractable definition gen n gets appreciable length size set simply large us brute force numerate different elements set gen well see though particular definition function f things work actually apply global linear models efficient way im going talk next,Course1,W10-S1-L3,W10,S1,L3,GLMs,10,1,3,here quick remind entail tag problem abstractli tag problem take sequenc word letter case input output produc tag sequenc element sequenc associ tag exampl cdc one first exampl saw partofspeech tag case input sentenc output tagger tag sentenc word get tag exampl profit tag n noun soar tag v verb tag preposit second crucial exampl look problem name entiti recognit problem input model sentenc sequenc word output tag sequenc tag encod name entiti found within sentenc word entiti tag na wherea word start compani tag sc word continu compani tag cc similarli stop locat continu locat stop person continu person weve seen two basic model tag problem hit mark model hmm recent saw log linear tagger sound first two method weve seen lectur go see third method global linear model glm percetron appli tag problem go set glm tag okay input model sentenc let say n equal exampl might sentenc w equal w w w might sentenc like dog bark exampl defin set possibl tag let assum three three part speech tag n v first thing go defin function gen rememb gen go take sentenc input map set candid structur particular case go use definit gen sentenc simpli return possibl tag sequenc length n particular exampl gen sentenc w w w would set consist tag sequenc length three use three tag okay theyr actual that member gen case okay gen realli simpl function could think simpli list possibl tag sequenc input sentenc critic size gen go exponenti length sentenc fact size gen sentenc x go number possibl tag size set that number possibl tag rais power n n length sentenc possibl tag first posit possibl tag second posit possibl tag third posit think global linear model weve present far algorithm weve look explicitli enumer differ member gen score function vf xy clearli isnt go tractabl definit gen n get appreci length size set simpli larg us brute forc numer differ element set gen well see though particular definit function f thing work actual appli global linear model effici way im go talk next,[ 1  4  5 13 14]
16,Course1_W10-S1-L4_GLMs_for_Tagging_Part_2_7-35,goal define function f takes sentence example dog barks conjunction tag sequence example dnv return future venture actually going use lot machinery developed log linear taggers little earlier course use directly concepts saw first critical concept idea history basically captures context play making tagging decision saw lectures ago lock linear models history tuple consisting pair tags minus minus example tagging word base case would minus minus equal determinant followed adjective two tags entire sentence case entire sentence second element tuple slightly careful specify position sentence thats tagged case th word tagged word number thats fourth element history consider possible tag choose particular location basically going use history encapsulate information goes decision recall weve made assumption history looks previous two tags case lock linear models motivated concerns efficiency allowed us use viterbi algorithm decoding problem multimedia taggers going use assumption well see global linear models also allows us apply dynamic programming algorithms go actually reusing completely technology saw log linear taggers define ill call local features remember global feature vector looks entire x pair x example sentence say dog barks tag sequence feature x global look entire tax sequence im going define local features look history tag pair theyr going look small window fact theyre going look sequence three tags three consecutive tags history contains two previous tags look third tag ill use notation little g local features g sub function takes history tag pair input returns value output often natural language local features binary features indicator functions ask particular questions history conjunction tag proposed examples saw actually log linear tagging lecture g example might feature one current word w tagged word base tag proposed v b g might function current word ends ing tag proposed vbg also contextual features look tag proposed previous two tags history heres feature one trigram tags previous tag proposed sequence determiner adjective verb saw lecture local linear models one great thing kind feature vector definitions essentially ask question history conjunction tag proposed might useful disambiguating tag current position think entire xy pair okay im emphasizing goal define feature vector representation entire input x entire output sort x together sentence particular tag sequence n words sentence n history tag pairs x pair particular sentence one two three four five six words convenience ill number six history tag pairs particular example first history tag pair tagging decision nnp thats tag first word history previous two tags start symbols star symbols entire sentence position one second historytag pair rb thats tag second word entire sentence position two previous two tags star np third tagging decision vb thats identity third tag previous two tags nnp rb see determiner fourth position adjective fifth position base sixth position cases history sentence course staying constant position indicated particular tag also previous tags positions obviously changed modify index position sentence decisions positions going local feature vector g im going feature vector first decision consists history conjunction tag nnp history particular history feature vector g second history conjunction rb going six local feature vectors corresponding six different history tag pairs particular syntax sentence let show leverage local feature vectors defining global feature vector maps entire tag sequence conjunction sentence feature vector,Course1,W10-S1-L4,W10,S1,L4,GLMs,10,1,4,goal defin function f take sentenc exampl dog bark conjunct tag sequenc exampl dnv return futur ventur actual go use lot machineri develop log linear tagger littl earlier cours use directli concept saw first critic concept idea histori basic captur context play make tag decis saw lectur ago lock linear model histori tupl consist pair tag minu minu exampl tag word base case would minu minu equal determin follow adject two tag entir sentenc case entir sentenc second element tupl slightli care specifi posit sentenc that tag case th word tag word number that fourth element histori consid possibl tag choos particular locat basic go use histori encapsul inform goe decis recal weve made assumpt histori look previou two tag case lock linear model motiv concern effici allow us use viterbi algorithm decod problem multimedia tagger go use assumpt well see global linear model also allow us appli dynam program algorithm go actual reus complet technolog saw log linear tagger defin ill call local featur rememb global featur vector look entir x pair x exampl sentenc say dog bark tag sequenc featur x global look entir tax sequenc im go defin local featur look histori tag pair theyr go look small window fact theyr go look sequenc three tag three consecut tag histori contain two previou tag look third tag ill use notat littl g local featur g sub function take histori tag pair input return valu output often natur languag local featur binari featur indic function ask particular question histori conjunct tag propos exampl saw actual log linear tag lectur g exampl might featur one current word w tag word base tag propos v b g might function current word end ing tag propos vbg also contextu featur look tag propos previou two tag histori here featur one trigram tag previou tag propos sequenc determin adject verb saw lectur local linear model one great thing kind featur vector definit essenti ask question histori conjunct tag propos might use disambigu tag current posit think entir xy pair okay im emphas goal defin featur vector represent entir input x entir output sort x togeth sentenc particular tag sequenc n word sentenc n histori tag pair x pair particular sentenc one two three four five six word conveni ill number six histori tag pair particular exampl first histori tag pair tag decis nnp that tag first word histori previou two tag start symbol star symbol entir sentenc posit one second historytag pair rb that tag second word entir sentenc posit two previou two tag star np third tag decis vb that ident third tag previou two tag nnp rb see determin fourth posit adject fifth posit base sixth posit case histori sentenc cours stay constant posit indic particular tag also previou tag posit obvious chang modifi index posit sentenc decis posit go local featur vector g im go featur vector first decis consist histori conjunct tag nnp histori particular histori featur vector g second histori conjunct rb go six local featur vector correspond six differ histori tag pair particular syntax sentenc let show leverag local featur vector defin global featur vector map entir tag sequenc conjunct sentenc featur vector,[ 1  5  4 14 13]
17,Course1_W10-S1-L5_GLMs_for_Tagging_Part_3_7-06,going see construct global feature vector f maps entire sentence paired entire tag sequence vector local feature vectors g way simply sum local feature vectors different positions sentence sum equal n n length input sentence length tag sequence assume h sub ith history history ith position equals ti ith tag simply sum feature vectors global feature vector sound equal sum local feature vectors sound example take history tagged pairs g h one sound lets say feature vector equal one might sound f particular word sequence sound paired particular tag sequence would sum feature vectors first position id sum second position third position final position global feature vector formed sum local feature vectors typically weve seen many examples log linear taggers class also features gave earlier lecture typically local features indi indicator functions depending whether question history conjunction tag true false example g might indicator function current word wi ends ing tag proposed vbg happens global features become counts sums local features th component global feature vector remember f sound equal sum equals one n g h going sum g features applied every possible position tag sequence particular case f going number times word ending ing tagged vbg entire sequence local feature vectors indicated functions global feature vectors actually counts different occurrences within tagged sequence put things together well see way defining global features global linear model leads convenient property spite size gen remember going exponential sound n n length sentence spite size going able efficiently calculate function f takes word sequence sentence input proves tag sequence output work remember global linear model function f going highest scoring tag sequence arg max possible tag sequences gen exponential text sequences score within arg max vf f global feature vector definition takes entire sentence paired entire tagged sequence returns feature vectors example something like substitute definition showed f going sum equals n g hi comma ti hi ith history tagged sequence ti ith tag tagged sequence substituting definition f take sum local feature vectors blankaudio course rewrite slightly taking v inside sum inner product v sum local feature vectors equal sum inner products v g fairly straight forward results linear algebra term interpreted score sound hi conjunction ti basically score indicating plausible part particular history tag pair final problem end end problem finding highest scoring tag sequence tagged sequence scored sum local scores history tag pair input weve gone point shouldnt surprise dynamic programming used find arg max fact simple task modify viterbi algorithm originally saw h ms saw log linear models easy modify finding highest scoring tag sequence scores,Course1,W10-S1-L5,W10,S1,L5,GLMs,10,1,5,go see construct global featur vector f map entir sentenc pair entir tag sequenc vector local featur vector g way simpli sum local featur vector differ posit sentenc sum equal n n length input sentenc length tag sequenc assum h sub ith histori histori ith posit equal ti ith tag simpli sum featur vector global featur vector sound equal sum local featur vector sound exampl take histori tag pair g h one sound let say featur vector equal one might sound f particular word sequenc sound pair particular tag sequenc would sum featur vector first posit id sum second posit third posit final posit global featur vector form sum local featur vector typic weve seen mani exampl log linear tagger class also featur gave earlier lectur typic local featur indi indic function depend whether question histori conjunct tag true fals exampl g might indic function current word wi end ing tag propos vbg happen global featur becom count sum local featur th compon global featur vector rememb f sound equal sum equal one n g h go sum g featur appli everi possibl posit tag sequenc particular case f go number time word end ing tag vbg entir sequenc local featur vector indic function global featur vector actual count differ occurr within tag sequenc put thing togeth well see way defin global featur global linear model lead conveni properti spite size gen rememb go exponenti sound n n length sentenc spite size go abl effici calcul function f take word sequenc sentenc input prove tag sequenc output work rememb global linear model function f go highest score tag sequenc arg max possibl tag sequenc gen exponenti text sequenc score within arg max vf f global featur vector definit take entir sentenc pair entir tag sequenc return featur vector exampl someth like substitut definit show f go sum equal n g hi comma ti hi ith histori tag sequenc ti ith tag tag sequenc substitut definit f take sum local featur vector blankaudio cours rewrit slightli take v insid sum inner product v sum local featur vector equal sum inner product v g fairli straight forward result linear algebra term interpret score sound hi conjunct ti basic score indic plausibl part particular histori tag pair final problem end end problem find highest score tag sequenc tag sequenc score sum local score histori tag pair input weve gone point shouldnt surpris dynam program use find arg max fact simpl task modifi viterbi algorithm origin saw h ms saw log linear model easi modifi find highest score tag sequenc score,[ 5  1  4 10  2]
18,Course1_W10-S1-L6_GLMs_for_Tagging_Part_4_6-00,weve seen define global features f sum local features g efficiently calculate function sentences tag sequences using viterbi algorithm critical two reasons one want apply global li linear model new test sentence clearly need calculate function f need able calculate efficiently secondly think perceptron algorithm main computational step perceptron algorithm training example find highest scoring tax sequence current parameter settings going repeatedly calculate argmax therefore critical efficiently let talk briefly perceptron works types models recap perceptron algorithm inputs xiyi equals n tagging case sentences paired entire tag sequences set v equal define fx argmax iterate members gen score member find highest scoring structure actual parameter estimation step take big passes data go examples one one equals n critical step point find highest scoring tax sequence current model correct make updates parameters tagging case algorithm looks like following training set set training examples consisting word sequences paired tag sequences used n sub length ith example ith sentence ith tag sequence initially set parameters v equal im going take big iterations training set visit examples one one take equals n im iterating training samples first thing calculate highest scoring tag sequence score vf going assume f sound equal sum local feature vectors means critically calculate high scoring text sequence current parameters using dynamic programming using viterbi algorithm output model z equal target output they’re yeah want tag errors sequence proposed simple update say v equals v plus f minus f f looks correct tag sequence f looks incorrect tag sequence high level critical thing weve leveraged kind definition f sum local feature vectors sense use dynamic programming find highest scoring tag sequence training example current parameters use standard perceptron updates heres results perceptron couple data sets comparing loglinear tagger first partofspeech tagging data see error rate perception competitive actually slightly lower result log linear tagger secondly problem noun phrase chunking actually problem taking sentence input recovering noun phrase boundaries look noun phrases non recursive low level noun phrases like dog cat segmentation problem way identity recognition segmentation problem treated tagging problem way maybe identity recognition could treated tagging problem noun phrase chunking see similar levels accuracy perhaps slightly higher accuracy perceptron perceptron certainly competitive algorithm particular domain tagging problems offers interesting alternative loglinear tagger simple algorithm really rather simple implement perhaps importantly way thinking global linear models gen exponential science f defined sum local feature vectors well see powerful idea tagging one first problems addressed using kind technique next segment class well look dependency parsing another application technique idea decomposing f sum g really employed effectively,Course1,W10-S1-L6,W10,S1,L6,GLMs,10,1,6,weve seen defin global featur f sum local featur g effici calcul function sentenc tag sequenc use viterbi algorithm critic two reason one want appli global li linear model new test sentenc clearli need calcul function f need abl calcul effici secondli think perceptron algorithm main comput step perceptron algorithm train exampl find highest score tax sequenc current paramet set go repeatedli calcul argmax therefor critic effici let talk briefli perceptron work type model recap perceptron algorithm input xiyi equal n tag case sentenc pair entir tag sequenc set v equal defin fx argmax iter member gen score member find highest score structur actual paramet estim step take big pass data go exampl one one equal n critic step point find highest score tax sequenc current model correct make updat paramet tag case algorithm look like follow train set set train exampl consist word sequenc pair tag sequenc use n sub length ith exampl ith sentenc ith tag sequenc initi set paramet v equal im go take big iter train set visit exampl one one take equal n im iter train sampl first thing calcul highest score tag sequenc score vf go assum f sound equal sum local featur vector mean critic calcul high score text sequenc current paramet use dynam program use viterbi algorithm output model z equal target output they’r yeah want tag error sequenc propos simpl updat say v equal v plu f minu f f look correct tag sequenc f look incorrect tag sequenc high level critic thing weve leverag kind definit f sum local featur vector sens use dynam program find highest score tag sequenc train exampl current paramet use standard perceptron updat here result perceptron coupl data set compar loglinear tagger first partofspeech tag data see error rate percept competit actual slightli lower result log linear tagger secondli problem noun phrase chunk actual problem take sentenc input recov noun phrase boundari look noun phrase non recurs low level noun phrase like dog cat segment problem way ident recognit segment problem treat tag problem way mayb ident recognit could treat tag problem noun phrase chunk see similar level accuraci perhap slightli higher accuraci perceptron perceptron certainli competit algorithm particular domain tag problem offer interest altern loglinear tagger simpl algorithm realli rather simpl implement perhap importantli way think global linear model gen exponenti scienc f defin sum local featur vector well see power idea tag one first problem address use kind techniqu next segment class well look depend pars anoth applic techniqu idea decompos f sum g realli employ effect,[ 1  5  4 10  0]
19,Course1_W10-S2-L1_Introduction_0-37,next segment class going look global linear models third problem mainly dependency parsing well see make use many ideas saw last segment considered perceptron tagging problems well see segment develop different approach parsing pcfgs lexicalized pcfgs saw earlier course dependency parsing models actually widely used research also real world applications,Course1,W10-S2-L1,W10,S2,L1,Introduction,10,2,1,next segment class go look global linear model third problem mainli depend pars well see make use mani idea saw last segment consid perceptron tag problem well see segment develop differ approach pars pcfg lexic pcfg saw earlier cours depend pars model actual wide use research also real world applic,[14  0  1  4  5]
20,Course1_W10-S2-L2_The_Dependency_Parsing_Problem_Part_1_5-21,segment well first define dependency parsing problem well talk global linear models applied dependency parsing finally well talk results work ryan mcdonald others amongst first apply actually first apply global linear models dependency parsing problem dependency parse looks like following dependency structures essentially alternative form syntactic representation treelike structures saw earlier course remember earlier course saw kind tree structures derived example contextfree grammar dependency structures another way representing syntactic information set dependency arcs dependency arc directed arc two symbols signifies theres grammatical relation two symbols rather two wor words root special symbol particular structure directed arc root verb saw sense root sentence main word sentence directed arc saw john john subject saw thats directed arc signifies directed arc saw movie movie object saw finally directed arc movie modifier actually determiner modifier word movie simplest case dependencies structures unlabeled means look like structure ive shown easy enough go beyond lebels arcs naturally thats natural thing labeled structures might label subject arc object art arc maybe would main verb arc would say determiner arc dt throughout lecture ill actually stick unlabeled case arcs dont labels easy extend techniques ill describe labeled case way labeled case little intuitive see exactly recovering kind dependencystyle annotations certainly carries lot useful syntactic information sentence analyzed sense quite equivalent parse trees saw earlier class fact little bit later ill talk directly derive dependency structures parse trees form weve seen earlier itll useful develop little bit notation dependency structures dependency going pair hm h index head word index modifier word ever one directed arrows head example saw modifier example john lets number words always number root zero four directed arcs dependency structure means actually four dependencies thats dependency dependency dependency dependency throughout lecture well represent dependency structures sets dependencies dependency hm pair h head modifier dependency parsing problem considering segment take sentence input produce one dependency structures output im going focus case arcs unlabeled thats simpler case easy enough extend approaches ill describe cases case arcs actually labels,Course1,W10-S2-L2,W10,S2,L2,The,10,2,2,segment well first defin depend pars problem well talk global linear model appli depend pars final well talk result work ryan mcdonald other amongst first appli actual first appli global linear model depend pars problem depend pars look like follow depend structur essenti altern form syntact represent treelik structur saw earlier cours rememb earlier cours saw kind tree structur deriv exampl contextfre grammar depend structur anoth way repres syntact inform set depend arc depend arc direct arc two symbol signifi there grammat relat two symbol rather two wor word root special symbol particular structur direct arc root verb saw sens root sentenc main word sentenc direct arc saw john john subject saw that direct arc signifi direct arc saw movi movi object saw final direct arc movi modifi actual determin modifi word movi simplest case depend structur unlabel mean look like structur ive shown easi enough go beyond lebel arc natur that natur thing label structur might label subject arc object art arc mayb would main verb arc would say determin arc dt throughout lectur ill actual stick unlabel case arc dont label easi extend techniqu ill describ label case way label case littl intuit see exactli recov kind dependencystyl annot certainli carri lot use syntact inform sentenc analyz sens quit equival pars tree saw earlier class fact littl bit later ill talk directli deriv depend structur pars tree form weve seen earlier itll use develop littl bit notat depend structur depend go pair hm h index head word index modifi word ever one direct arrow head exampl saw modifi exampl john let number word alway number root zero four direct arc depend structur mean actual four depend that depend depend depend depend throughout lectur well repres depend structur set depend depend hm pair h head modifi depend pars problem consid segment take sentenc input produc one depend structur output im go focu case arc unlabel that simpler case easi enough extend approach ill describ case case arc actual label,[14  4  0 13 12]
21,Course1_W10-S2-L3_The_Dependency_Parsing_Problem_Part_2_13-53,lets talk makes dependency structure well formed define set possible dependency structures given sentence going focus two constraints important first constraint directed arks form directed tree root symbol root tree basically means pick word sentence example movie theres going directed path root word case path first arc root saw arc sort moving every one words connected root directed path structure addition forms tree means example cycles paths cycles like second constrain well come back detail second crossing dependencies structures crossing dependencies would look like following structure like say root four words sentence see two dependencies actually cross definition im going use lecture going rule kind structures ill talk second come back simple example john saw mary actually one two three four five possible dependency structures case five structures satisfy two constraints showed correct one course sound four incorrect structures particular sentence parsing problem essentially going correspond searching though different possible structures finding one likely plausible model let talk little bit crossing dependencies ive shown dependencies pause structure one crossing dependencies notice crossing dependency structures allow crossing dependencies often referred nonprojected structures actually useful scenarios depending language constructions involved actually see kind nonprojective structures heres example english arguably correct dependency pause certainly crossing dependency purposes lecture going assume structures allowed theyre space possible dependency structures particular sentence thats pretty good approximately language like english right end lecture ill talk little bit extensions models allow structures case assume structures inaudible theres considerable interest last year years dependency parsing youll youll see later lecture certain advantages terms efficiency simplicity let talk little bit resources conference called conll called shared task basically friendly competition different groups released dependency parsing datasets actually languages arabic chinese czech danish dutch german japanese portuguese slovene spanish swedish turkish different groups developed dependency parsing systems compared results data sets similar setup seen conll protocol described todays lecture largely based phd thesis ryan mcdonald introduced global linear models dependency policy showed models successful well treating problem supervised learning task well assume trainings examples consisting sentences paired dependency structures training sample would look like theres question get resources get dependency banks theres couple ways datasets arise languages famous example czech people actually handconstructed dependency banks annotated large quantities data maybe thousands tens thousands sentences sentence annotated outline dependency structure think ana analog tree bank seen example penn wall street journal tree bank people annotated constituency trees cases various languages rather annotating kinds constituency trees directly annotated dependency structures languages dependency banks directly available may constituent tree banks like penn wall street journal tree bank hopefully wont suprised actually fairly straightforward extract dependency structures tree banks unknown pcfgs showed earlier course opened direct link constituency structures dependency structures considering lecture let talk little bit constituent based tree something like kind tree would find pen wall street journal treebank think example saw earlier core course weve lexicalized tree way described lectures lexicalized context free grammars nonterminal associated head word example np president vp sorry missing sh also p bar way showed earlier course looked lexicalized pcfgs rules propagated head words contextfree trees step lexicalization step used deriving lexicalized pcfgs also opens chance taking tree structures converting dependency structures remember dependency represented pair h h index head word index modifier word particular case example tree going converted following set dependencies let give example rule told goes p hillary v p told extract dependency told head hilliary modifier gives dependency two one directed arch told hillary similarly look rule told head clinton modifier arch two three words one two three four five six seven arch two three told clinton similarly arch told two four also modified told similarly go entire tree pulling dependencies right top tree told root entire sentence arch zero two root goes told summarize main point slide tree bank set sentences annotated contextfree trees convert socalled dependency bank step lexicalization exactly step lexicalization saw within context lexicalized pcfgs weve done training test data dependency pausing problem one interesting property dependency parsing compelling reason taking dependency parsing seriously efficiency find dependency parsing structures looked probabilistic context free grammars saw dynamic programming algorithm cky algorithm runtime cubic length sentence also cubic number nonterminals grammar n might example number nonterminals might depending define reasonably large number quite challenging actually pretty large number see g cubed term certainly lead problems terms efficiency lexicalized pcfg parsing least methods saw th n length sentence cubic number nonterminals grammer really getting expensive algorithms unlabeled dependency parsing gain generally based dynamic programming going see detail theyre fairly complex take time go post references rather beautiful algorithms dynamic programming dependently posing structures particular due jason eisner ill post papers remarkably algorithms cubic length sentence dependence size grammar fact sense dependency grammars dont really nonterminals dont really g terms might small constant front n cubed maybe factor eight something like constant way way smaller g cubed actually means dependency parsing algorithms practice extremely efficient major reason like said taking dependency parsing seriously two properties think beneficial theyre efficient parsing secondly useful representations sound recover kind dependency structures theyre useful many applications natural image processing,Course1,W10-S2-L3,W10,S2,L3,The,10,2,3,let talk make depend structur well form defin set possibl depend structur given sentenc go focu two constraint import first constraint direct ark form direct tree root symbol root tree basic mean pick word sentenc exampl movi there go direct path root word case path first arc root saw arc sort move everi one word connect root direct path structur addit form tree mean exampl cycl path cycl like second constrain well come back detail second cross depend structur cross depend would look like follow structur like say root four word sentenc see two depend actual cross definit im go use lectur go rule kind structur ill talk second come back simpl exampl john saw mari actual one two three four five possibl depend structur case five structur satisfi two constraint show correct one cours sound four incorrect structur particular sentenc pars problem essenti go correspond search though differ possibl structur find one like plausibl model let talk littl bit cross depend ive shown depend paus structur one cross depend notic cross depend structur allow cross depend often refer nonproject structur actual use scenario depend languag construct involv actual see kind nonproject structur here exampl english arguabl correct depend paus certainli cross depend purpos lectur go assum structur allow theyr space possibl depend structur particular sentenc that pretti good approxim languag like english right end lectur ill talk littl bit extens model allow structur case assum structur inaud there consider interest last year year depend pars youll youll see later lectur certain advantag term effici simplic let talk littl bit resourc confer call conll call share task basic friendli competit differ group releas depend pars dataset actual languag arab chines czech danish dutch german japanes portugues sloven spanish swedish turkish differ group develop depend pars system compar result data set similar setup seen conll protocol describ today lectur larg base phd thesi ryan mcdonald introduc global linear model depend polici show model success well treat problem supervis learn task well assum train exampl consist sentenc pair depend structur train sampl would look like there question get resourc get depend bank there coupl way dataset aris languag famou exampl czech peopl actual handconstruct depend bank annot larg quantiti data mayb thousand ten thousand sentenc sentenc annot outlin depend structur think ana analog tree bank seen exampl penn wall street journal tree bank peopl annot constitu tree case variou languag rather annot kind constitu tree directli annot depend structur languag depend bank directli avail may constitu tree bank like penn wall street journal tree bank hope wont supris actual fairli straightforward extract depend structur tree bank unknown pcfg show earlier cours open direct link constitu structur depend structur consid lectur let talk littl bit constitu base tree someth like kind tree would find pen wall street journal treebank think exampl saw earlier core cours weve lexic tree way describ lectur lexic context free grammar nontermin associ head word exampl np presid vp sorri miss sh also p bar way show earlier cours look lexic pcfg rule propag head word contextfre tree step lexic step use deriv lexic pcfg also open chanc take tree structur convert depend structur rememb depend repres pair h h index head word index modifi word particular case exampl tree go convert follow set depend let give exampl rule told goe p hillari v p told extract depend told head hilliari modifi give depend two one direct arch told hillari similarli look rule told head clinton modifi arch two three word one two three four five six seven arch two three told clinton similarli arch told two four also modifi told similarli go entir tree pull depend right top tree told root entir sentenc arch zero two root goe told summar main point slide tree bank set sentenc annot contextfre tree convert socal depend bank step lexic exactli step lexic saw within context lexic pcfg weve done train test data depend paus problem one interest properti depend pars compel reason take depend pars serious effici find depend pars structur look probabilist context free grammar saw dynam program algorithm cki algorithm runtim cubic length sentenc also cubic number nontermin grammar n might exampl number nontermin might depend defin reason larg number quit challeng actual pretti larg number see g cube term certainli lead problem term effici lexic pcfg pars least method saw th n length sentenc cubic number nontermin grammer realli get expens algorithm unlabel depend pars gain gener base dynam program go see detail theyr fairli complex take time go post refer rather beauti algorithm dynam program depend pose structur particular due jason eisner ill post paper remark algorithm cubic length sentenc depend size grammar fact sens depend grammar dont realli nontermin dont realli g term might small constant front n cube mayb factor eight someth like constant way way smaller g cube actual mean depend pars algorithm practic extrem effici major reason like said take depend pars serious two properti think benefici theyr effici pars secondli use represent sound recov kind depend structur theyr use mani applic natur imag process,[14  0  4 13 12]
22,Course1_W10-S2-L4_GLMs_for_Dependency_Parsing_Part_1_11-59,lets describe global linear models glms applied dependency parsing throughout section ill take x sentence x going sequence words x x x sub n really define global linear model need give two definitions firstly define gen takes sentence input returns set candidate dependency structures output secondly define f function takes sentence together dependency structure returns feature vector representation gen going simple simply defined set dependency structures sentence x example john saw mary gen sentence return possible dependency structures three words critically size gen exponential n n length sentence paused situation gen going large set possible structures least appreciable n sentences say words going extremely large set f course going function takes input sentence conjunction particular dependency structure returns feature vector might return vector like representation used learning algorithm used example perception algorithm setting parameters model well see way defining f gets us around problem exponential size gen use dynamic programming search likely tree model let talk define f key idea going define f sum local feature vectors remember last segment perception algorithm global linear models tagging saw similar thing f defined sum local feature x g dependency pausing case going one local feature vector dependency structure structure going mapped set dependencies dependency head modifier head index word zero root word modifier also index word sentence local feature vectors look information sentence look identities head modifier word let illustrate example sentence x dependency structure sentence x john saw movie x equals john x equal saw x equal x equal movie dependency structure basically equal set dependencies would useful number words example dependency corresponds directed arc saw movie going featurevector mapping takes x conjunction return feature vector definition previous slide defined sum dependencies g x h particular example going g x plus g x plus g x plus g x playing trick define global feature vector sum local feature vectors local feature vector directed arcs dependency unknown might example feature vectors like following noise maybe four feature vectors look like following case sum would one two three first position ones zeros second position one elsewhere third position would single third position final position also thats going final global feature vector sum local feature vectors seeing something common kind models local feature vectors ones zeros going features well talk second features ask various questions different dependencies global feature vector going count example three dependencies within structure first question true ie first question value let give example might define featured vectors maybe first feature first component going g x h going vector composed g x h g x h right way g sub x h dimensionality feature feature vector number features different functions g g gd look x h triple return value one function might following x sub h equal saw x sub equal movie okay particular would g john saw movie dependency thats particular dependency going one case okay particular function particular feature looks two words stand dependency relationship x h x returns theyre particular word otherwise would typically define feature like every possible pair words vocabulary okay weve seen often log linear models might large number possible features one pair words features useful certain pairs words likely stand dependency relationship pairs words much less likely stand dependency relation dependency relationship let give examples features common actually include words input x also include part speech tags sound provide useful information g h x heres another feature think x including words also part speech tags input could say something like one part speech tag h equal vbd parse speech tag equal nn otherwise kind features would introduced every possible pairs part speech tags underlying partofspeech tag set kind features useful certain partofspeech tags much likely seen dependency relationships ones another type feature might use sound could look distance two words okay could say one distance h difference absolute value difference equal otherwise sound feature returns value two words involved dependency far kind features useful certainly statistical tendency language dependencies close words thats actually empirical observation told across many languages see occasional dependencies long many short dependencies adjacent word dependency involving words within two three word window youll hopefully see examples considerable freedom defining features look sentence index head word index modifier word incorporate kinds information words x h x maybe part speech tags words maybe distance two words maybe surrounding information look part speech tags words left right two words stand teh dependency relationship,Course1,W10-S2-L4,W10,S2,L4,GLMs,10,2,4,let describ global linear model glm appli depend pars throughout section ill take x sentenc x go sequenc word x x x sub n realli defin global linear model need give two definit firstli defin gen take sentenc input return set candid depend structur output secondli defin f function take sentenc togeth depend structur return featur vector represent gen go simpl simpli defin set depend structur sentenc x exampl john saw mari gen sentenc return possibl depend structur three word critic size gen exponenti n n length sentenc paus situat gen go larg set possibl structur least appreci n sentenc say word go extrem larg set f cours go function take input sentenc conjunct particular depend structur return featur vector might return vector like represent use learn algorithm use exampl percept algorithm set paramet model well see way defin f get us around problem exponenti size gen use dynam program search like tree model let talk defin f key idea go defin f sum local featur vector rememb last segment percept algorithm global linear model tag saw similar thing f defin sum local featur x g depend paus case go one local featur vector depend structur structur go map set depend depend head modifi head index word zero root word modifi also index word sentenc local featur vector look inform sentenc look ident head modifi word let illustr exampl sentenc x depend structur sentenc x john saw movi x equal john x equal saw x equal x equal movi depend structur basic equal set depend would use number word exampl depend correspond direct arc saw movi go featurevector map take x conjunct return featur vector definit previou slide defin sum depend g x h particular exampl go g x plu g x plu g x plu g x play trick defin global featur vector sum local featur vector local featur vector direct arc depend unknown might exampl featur vector like follow nois mayb four featur vector look like follow case sum would one two three first posit one zero second posit one elsewher third posit would singl third posit final posit also that go final global featur vector sum local featur vector see someth common kind model local featur vector one zero go featur well talk second featur ask variou question differ depend global featur vector go count exampl three depend within structur first question true ie first question valu let give exampl might defin featur vector mayb first featur first compon go g x h go vector compos g x h g x h right way g sub x h dimension featur featur vector number featur differ function g g gd look x h tripl return valu one function might follow x sub h equal saw x sub equal movi okay particular would g john saw movi depend that particular depend go one case okay particular function particular featur look two word stand depend relationship x h x return theyr particular word otherwis would typic defin featur like everi possibl pair word vocabulari okay weve seen often log linear model might larg number possibl featur one pair word featur use certain pair word like stand depend relationship pair word much less like stand depend relat depend relationship let give exampl featur common actual includ word input x also includ part speech tag sound provid use inform g h x here anoth featur think x includ word also part speech tag input could say someth like one part speech tag h equal vbd pars speech tag equal nn otherwis kind featur would introduc everi possibl pair part speech tag underli partofspeech tag set kind featur use certain partofspeech tag much like seen depend relationship one anoth type featur might use sound could look distanc two word okay could say one distanc h differ absolut valu differ equal otherwis sound featur return valu two word involv depend far kind featur use certainli statist tendenc languag depend close word that actual empir observ told across mani languag see occasion depend long mani short depend adjac word depend involv word within two three word window youll hope see exampl consider freedom defin featur look sentenc index head word index modifi word incorpor kind inform word x h x mayb part speech tag word mayb distanc two word mayb surround inform look part speech tag word left right two word stand teh depend relationship,[ 5 14  4  1 13]
23,Course1_W10-S2-L5_GLMs_for_Dependency_Parsing_Part_2_8-28,lets describe feature factor definitions used within context global linear models dependency parsing recall run perceptron algorithm need able calculate function capital f x x sentence example john saw mary function going take sentence map dependency structure example structure john saw mary defined usual way local linear global linear models gen x defined set possible dependency structures input sentence im going take max possible structures going score structures using wf w parameter vector dimensional space f feature vector saw tagging critical compute function f x efficiently couple reasons one get new test example want able parse want able apply function okay second reason run perceptron algorithm learns way w examples need repeatedly calculate function train examples remember perceptron typically calculates output particular trend example current parameters simple update parameters make errors f x crucial within within perceptron algorithm lets emphasize set gen typically large exponential n n length sentence ive shown defined function capital f sorry lower case f feature vector function sum local feature vectors feature vectors takes sentence looks single dependency within structure returns feature vector problem reduced problem finding highest scoring structure gen score structure sum terms one dependencies im going explain algorithm ill try post paper specifies algorithm critically make use dynamic programming problem actually runs cubic time length sentence intuitively weve decomposed score entire structure sum local schools across dependencies within structure decomposition allows us use dynamic programming algorithms already quite similar algorithms saw pcfgs theyre similar cky algorithm leverage dependency structures really rather beautiful ways jason eisner wonderful papers algorithms summarize weve defined global feature vectors local feature vectors allows us find highest scoring dependency structure model efficiently using dynamic programming use dynamic programming method within perceptron algorithm train parameters w also decoding new test sentences finish segment let talk little bit kind features mcdonald actually looked like theyre theyre similar features showed remember feature vectors going take sentence head word modifier word define w ith word sentence ith tag part speech tag ith word going assume input dont sentence also part speech tags sentence part speech tags give us valuable information simple unigram features look identity head word identifier identity modifier word identity head tag identity modifier identity modifier tag interestingly bigram features rather similar features showed previous slide look identity tuple wh wm sub h sub could features true particular combination maybe saw john vbd nnp feature one saw dependency word saw tagged vbd word john tagged nnp also define features look various subsets tuple example might look two words involved might look saw john might look two parts speech involved might look fact john tagged nnp modifying vbd mcdonald also used contextual features example might look tag particular dependency word h word might interested part speech tag nth word part speech tag hth word might also look part speech tags immediate left right head modifier word example might look fortuple th th plus one thats part speech part speech tm minus one tm part speech part speech several kinds features define look part speech modifier word head word also part speech previous word next word cases clearly gives us richer context consider particular dependency finally important classic feature called inbetween features empirically statistically speaking parts speech example verb cause dependency relatively unlikely verb seen two words diminishes probability seeing dependency seems empirically look least english chances seeing dependency crossing certain parts speech verb much less chance crossing parts speech mcdonald introduced features would track tag head tag modifier also would check whether particular tag example vbd seen two words turned another useful feature type,Course1,W10-S2-L5,W10,S2,L5,GLMs,10,2,5,let describ featur factor definit use within context global linear model depend pars recal run perceptron algorithm need abl calcul function capit f x x sentenc exampl john saw mari function go take sentenc map depend structur exampl structur john saw mari defin usual way local linear global linear model gen x defin set possibl depend structur input sentenc im go take max possibl structur go score structur use wf w paramet vector dimension space f featur vector saw tag critic comput function f x effici coupl reason one get new test exampl want abl pars want abl appli function okay second reason run perceptron algorithm learn way w exampl need repeatedli calcul function train exampl rememb perceptron typic calcul output particular trend exampl current paramet simpl updat paramet make error f x crucial within within perceptron algorithm let emphas set gen typic larg exponenti n n length sentenc ive shown defin function capit f sorri lower case f featur vector function sum local featur vector featur vector take sentenc look singl depend within structur return featur vector problem reduc problem find highest score structur gen score structur sum term one depend im go explain algorithm ill tri post paper specifi algorithm critic make use dynam program problem actual run cubic time length sentenc intuit weve decompos score entir structur sum local school across depend within structur decomposit allow us use dynam program algorithm alreadi quit similar algorithm saw pcfg theyr similar cki algorithm leverag depend structur realli rather beauti way jason eisner wonder paper algorithm summar weve defin global featur vector local featur vector allow us find highest score depend structur model effici use dynam program use dynam program method within perceptron algorithm train paramet w also decod new test sentenc finish segment let talk littl bit kind featur mcdonald actual look like theyr theyr similar featur show rememb featur vector go take sentenc head word modifi word defin w ith word sentenc ith tag part speech tag ith word go assum input dont sentenc also part speech tag sentenc part speech tag give us valuabl inform simpl unigram featur look ident head word identifi ident modifi word ident head tag ident modifi ident modifi tag interestingli bigram featur rather similar featur show previou slide look ident tupl wh wm sub h sub could featur true particular combin mayb saw john vbd nnp featur one saw depend word saw tag vbd word john tag nnp also defin featur look variou subset tupl exampl might look two word involv might look saw john might look two part speech involv might look fact john tag nnp modifi vbd mcdonald also use contextu featur exampl might look tag particular depend word h word might interest part speech tag nth word part speech tag hth word might also look part speech tag immedi left right head modifi word exampl might look fortupl th th plu one that part speech part speech tm minu one tm part speech part speech sever kind featur defin look part speech modifi word head word also part speech previou word next word case clearli give us richer context consid particular depend final import classic featur call inbetween featur empir statist speak part speech exampl verb caus depend rel unlik verb seen two word diminish probabl see depend seem empir look least english chanc see depend cross certain part speech verb much less chanc cross part speech mcdonald introduc featur would track tag head tag modifi also would check whether particular tag exampl vbd seen two word turn anoth use featur type,[ 5 14  1  4 13]
24,Course1_W10-S2-L6_Experiments_with_GLMs_for_Dep,let finish segment results global linear models taken mcdonalds implementation types models im showing table dependency accuracy proportion dependencies propose actually correct actually evaluation measure saw way back course looked lexicalized contextfree grammars talked evaluating accuracy parser looking number dependencies got correct model lexicalized pcfg developed phd thesis course parse tree lexicalized pcfg extract dependencies using methods described earlier segment also methods described section lexicalized pcfgs model scores terms dependency accuracy couple results mcdonald simple model described gets close accuracy slightly richer model ill talk second called second order dependency model get half percent accuracy pretty impressive dependency parsing models actually extremely competitive lexicalized pcfgs terms recovering dependency relations significant advantages dependency parsing approaches quite simple really matter defining features showed implementing dynamic programming finally implementing perception algorithm example parameter estimation theyre simple theyre efficient id said cubic time length sentence remember pcfgs example running time cubic length sentence also cubic number symbols g number nonterminals sound term adds lot could easily cubed cubed cubed easily thousands dependency parsers significantly efficient full p pcfg parser let talk little bit extensions ive described theres lot interest also nonprojective dependency structures actually defining gen include structures crossing dependencies languages example czech kind crossing dependencies dependencies seen frequently important allow leads whole new research topic find highestscoring nonprojective structure turns dynamic programming doesnt really well actually dynamic programming employed cases also interesting algorithms based spanning tree algorithms example another important extension look second order third order dependency parsing mean following feature vector representations shown looked single dependency time looks single dependency isolation dependencies sentence look one dependency thats reflected local feature vector looks sentence identity head modifier particular dependency higher order dependency parsing example second order dependency parsing start look dependencies together might actually feature sensitive fact two dependencies together parse tree maybe even three dependencies together parse tree local feature vectors maybe dont look head modifier might also look grandparent maybe h h primed might also look modifier maybe modifier easy enough actually extend approaches ive described allow local feature vectors look larger pieces dependency structure involves complications depend dynamic dynamic programming algorithms used models still remain tractable kind modifications stateoftheart around least english wall street journal data sets terms recovering dependency annotations really pretty impressive level accuracy,Course1,W10-S2-L6,W10,S2,L6,Experiments,10,2,6,let finish segment result global linear model taken mcdonald implement type model im show tabl depend accuraci proport depend propos actual correct actual evalu measur saw way back cours look lexic contextfre grammar talk evalu accuraci parser look number depend got correct model lexic pcfg develop phd thesi cours pars tree lexic pcfg extract depend use method describ earlier segment also method describ section lexic pcfg model score term depend accuraci coupl result mcdonald simpl model describ get close accuraci slightli richer model ill talk second call second order depend model get half percent accuraci pretti impress depend pars model actual extrem competit lexic pcfg term recov depend relat signific advantag depend pars approach quit simpl realli matter defin featur show implement dynam program final implement percept algorithm exampl paramet estim theyr simpl theyr effici id said cubic time length sentenc rememb pcfg exampl run time cubic length sentenc also cubic number symbol g number nontermin sound term add lot could easili cube cube cube easili thousand depend parser significantli effici full p pcfg parser let talk littl bit extens ive describ there lot interest also nonproject depend structur actual defin gen includ structur cross depend languag exampl czech kind cross depend depend seen frequent import allow lead whole new research topic find highestscor nonproject structur turn dynam program doesnt realli well actual dynam program employ case also interest algorithm base span tree algorithm exampl anoth import extens look second order third order depend pars mean follow featur vector represent shown look singl depend time look singl depend isol depend sentenc look one depend that reflect local featur vector look sentenc ident head modifi particular depend higher order depend pars exampl second order depend pars start look depend togeth might actual featur sensit fact two depend togeth pars tree mayb even three depend togeth pars tree local featur vector mayb dont look head modifi might also look grandpar mayb h h prime might also look modifi mayb modifi easi enough actual extend approach ive describ allow local featur vector look larger piec depend structur involv complic depend dynam dynam program algorithm use model still remain tractabl kind modif stateoftheart around least english wall street journal data set term recov depend annot realli pretti impress level accuraci,[14  5  4  0 13]
25,Course1_W10-S2-L7_Summary_2-50,finally summarize weve seen weeks lectures global linear models require definitions function gen enumerates set candidates input function f maps candidate structure feature vector key idea tagging dependency parsing models described week follows firstly gen defined set possible structures example possible tag sequences possible dependency parses particular input means gen grows exponentially quickly respect size input gen exponential size grows exponentially fast respect length sentence get around weve defined global feature back f sum local feature vectors f takes input x inaudible structure input returns v vector defined sum local feature vectors look sub parts structure tagging case local fea feature axis g looked trigrams text dependency parsing case looked single dependencies within parsing given definition f use dynamic programming find highest scoring structure model thats critical need apply model new test examples moreover perception algorithm use parameter estimation requires us repeatedly decode training samples repeatedly find highest scoring structure model training sample turn finally worth noting weve seen perceptron algorithm parameter estimate certainly options terms estimated parameters global linear models one famous example conditional random fields essentially giant loglinear models use gradient ascent parameter estimation another option widely considered called largemargin methods related support vector machines covered perceptron relatively simple introduces many main ideas global linear models certainly computing parameter estimation methods advantages,Course1,W10-S2-L7,W10,S2,L7,Summary,10,2,7,final summar weve seen week lectur global linear model requir definit function gen enumer set candid input function f map candid structur featur vector key idea tag depend pars model describ week follow firstli gen defin set possibl structur exampl possibl tag sequenc possibl depend pars particular input mean gen grow exponenti quickli respect size input gen exponenti size grow exponenti fast respect length sentenc get around weve defin global featur back f sum local featur vector f take input x inaud structur input return v vector defin sum local featur vector look sub part structur tag case local fea featur axi g look trigram text depend pars case look singl depend within pars given definit f use dynam program find highest score structur model that critic need appli model new test exampl moreov percept algorithm use paramet estim requir us repeatedli decod train sampl repeatedli find highest score structur model train sampl turn final worth note weve seen perceptron algorithm paramet estim certainli option term estim paramet global linear model one famou exampl condit random field essenti giant loglinear model use gradient ascent paramet estim anoth option wide consid call largemargin method relat support vector machin cover perceptron rel simpl introduc mani main idea global linear model certainli comput paramet estim method advantag,[ 5 14  4  1 13]
26,Course1_W2-S1-L1_The_Tagging_Problem_10-01,okay next segment course going describe tagging problems tagging problems fundamental importance natural language processing look hidden markov models widely applied type model class problem ill first describe tagging problem ill give couple key examples natural language processing tagging problems well describe important paradigm thinking supervised learning problems paradigm generative models noisy channel model widely applied methodology developing methods supervised learning well see applied course finally ill describe hidden markov model taggers one instance generative models well give basic definitions well describe parameters estimation methods well describe viterbi algorithm algorithm great importance hidden markov models slide shows tagging problem called part speech tagging important problem natural language processing actually one earliest problems considered statistical machine learning approaches nlp statistical models problem go back late problem follows input model sentence sequence words output tag sequence mean output model word given associated tag profits example tag n look key stands noun soared tag v key means verb preposition thats p task take sentences input word word assign part speech word input sense challenging problem well turns ambiguity going play crucial role problem many problems natural language processing lets look words sentence take prophets example certainly noun context profits course also verb english say company profits endeavors profits verb context lets look others topping verb particular sentence also noun say example topping cake forecasts noun sentence also verb quarter noun much less frequent usage also verb results noun also verb rough estimate might find words english average take possible parts speech isnt true english true many languages probably languages addition english second example tagging problem problem named entity inaudible recognition important problem nlp actually first programming assignment course build complete name density recognizer youll build model task name density recognition problem problem case take sentence sequence words input output going identify named entities input sentence named entity might company location person three common entity types output weve identified boeing co company wall street location alan mulally person basic problem taking sentences input mark entities sentence output basic problem ininaudible useful wide range applications imagine kinds cases identifying entities would useful task first glance doesnt look like tagging problem tagging problem needs assign tag word input already segmentation identification subsegments sentence next slide ill show map problem directly tagging problem work ive basically shown example ive represented segmentation showed previous slide word word tag various tags na stands something part entity notice weve tagged every word sentence turn several words nas theyre part entity company entity type tags corresponding start company continuation company look boeing co weve tagged boeing start start compnay cc continuation company similarly wall street location wall start location street continuation alan mulally thing happens ive basically shown take named entity recognition problem map directly tagging problem going tag word turn input sequence inaudible goal going following going treat machine learning problem well assume training set set training examples actually set sentences taken one commonly used resource called wall street journal tree bank actually close training sentences training sentence consists full input sentence together unlerlying part speech tags training samples actually annotated hand human annotators gone sentence sentence marked kind annotations thats quite laborious job benefit readily available sort training examples problem problem wall street journal tree bank early example corpus form many many copera across many languages many different genres given training set problem going following learn function algorithm take new sentences input map sentence tax sequence going treat whole problem supervised learning problem develop little bit intuition might develop model problem kind information might useful developing model want talk two different types constraints play role part speech tagging problem actually tagging problem ill call different type constraints local versus contextual local constraints take following form look particular word english example considering part speech tagging problem preference modal verb okay much often seen verb english also see seen noun priority bias one part speech another part speech well refer kind local preference local preference words take one part speech another balance local constraints called contextual constraints part speech sequences much likely others one example noun much likely verb follow determiner okay determiner tag dt word like determiner im likely see noun noun im much like see verb okay contextual information sound text equal much likely others well balance two types constraints tagging models worth remembering preferences sometimes conflict take sentence trash garage consider word local preference modal verb clearly noun particular context thats surrounding words surrounding syntactic structure dictate really noun context two course two sources constraints local versus contextual well see build model actually balances two types constraints,Course1,W2-S1-L1,W2,S1,L1,The,2,1,1,okay next segment cours go describ tag problem tag problem fundament import natur languag process look hidden markov model wide appli type model class problem ill first describ tag problem ill give coupl key exampl natur languag process tag problem well describ import paradigm think supervis learn problem paradigm gener model noisi channel model wide appli methodolog develop method supervis learn well see appli cours final ill describ hidden markov model tagger one instanc gener model well give basic definit well describ paramet estim method well describ viterbi algorithm algorithm great import hidden markov model slide show tag problem call part speech tag import problem natur languag process actual one earliest problem consid statist machin learn approach nlp statist model problem go back late problem follow input model sentenc sequenc word output tag sequenc mean output model word given associ tag profit exampl tag n look key stand noun soar tag v key mean verb preposit that p task take sentenc input word word assign part speech word input sens challeng problem well turn ambigu go play crucial role problem mani problem natur languag process let look word sentenc take prophet exampl certainli noun context profit cours also verb english say compani profit endeavor profit verb context let look other top verb particular sentenc also noun say exampl top cake forecast noun sentenc also verb quarter noun much less frequent usag also verb result noun also verb rough estim might find word english averag take possibl part speech isnt true english true mani languag probabl languag addit english second exampl tag problem problem name entiti inaud recognit import problem nlp actual first program assign cours build complet name densiti recogn youll build model task name densiti recognit problem problem case take sentenc sequenc word input output go identifi name entiti input sentenc name entiti might compani locat person three common entiti type output weve identifi boe co compani wall street locat alan mulal person basic problem take sentenc input mark entiti sentenc output basic problem ininaud use wide rang applic imagin kind case identifi entiti would use task first glanc doesnt look like tag problem tag problem need assign tag word input alreadi segment identif subseg sentenc next slide ill show map problem directli tag problem work ive basic shown exampl ive repres segment show previou slide word word tag variou tag na stand someth part entiti notic weve tag everi word sentenc turn sever word na theyr part entiti compani entiti type tag correspond start compani continu compani look boe co weve tag boe start start compnay cc continu compani similarli wall street locat wall start locat street continu alan mulal thing happen ive basic shown take name entiti recognit problem map directli tag problem go tag word turn input sequenc inaud goal go follow go treat machin learn problem well assum train set set train exampl actual set sentenc taken one commonli use resourc call wall street journal tree bank actual close train sentenc train sentenc consist full input sentenc togeth unlerli part speech tag train sampl actual annot hand human annot gone sentenc sentenc mark kind annot that quit labori job benefit readili avail sort train exampl problem problem wall street journal tree bank earli exampl corpu form mani mani copera across mani languag mani differ genr given train set problem go follow learn function algorithm take new sentenc input map sentenc tax sequenc go treat whole problem supervis learn problem develop littl bit intuit might develop model problem kind inform might use develop model want talk two differ type constraint play role part speech tag problem actual tag problem ill call differ type constraint local versu contextu local constraint take follow form look particular word english exampl consid part speech tag problem prefer modal verb okay much often seen verb english also see seen noun prioriti bia one part speech anoth part speech well refer kind local prefer local prefer word take one part speech anoth balanc local constraint call contextu constraint part speech sequenc much like other one exampl noun much like verb follow determin okay determin tag dt word like determin im like see noun noun im much like see verb okay contextu inform sound text equal much like other well balanc two type constraint tag model worth rememb prefer sometim conflict take sentenc trash garag consid word local prefer modal verb clearli noun particular context that surround word surround syntact structur dictat realli noun context two cours two sourc constraint local versu contextu well see build model actual balanc two type constraint,[ 1  4 13  0  8]
27,Course1_W2-S1-L2_Generative_Models_for_Supervised_Learning_8-57,last section described tagging problem described essentially supervised learning problem example tag sequences want learn model function example tag sequences next section lecture want describe method deriving supervised learning algorithms called generative model said important kind supervised learning model well see come time time course supervised learning problems following setting going assume set training examples ill write xi yi equals xi referred input whereas yi referred label lets specific part speech training example might following x might following sentence dog laughs would underlying tag sequence sentence example determiner noun verb might x equal cat barks equal determine noun verb part speech tagging case training example consists sentence whats called input entire tag sequence associated label might hundred thousand maybe even tens thousands examples like given training examples supervised learning problems task learn function f maps inputs x labels f x want example part speech tagging learn function takes sentence input maps part speech tag sequence output first kind model might consider supervised learning whats called con conditional model goes follows first step learning step actually learn distribution py given x training samples well method takes training samples input returns distribution py given x output general distribution various parameters estimated training examples bit like trigram parameters saw first lectures language modeling okay thats step one learn conditional distribution py given x training example second step test input x simply define f x maximizes conditional probability applying model weve learned take input x input model search different labels return likely conditional model okay thats conditional model natural approach probably first things would think tasks ill show second alternative called generative models generative models assume scenario training data nothing changes task learn function maps input x input x labels f xsound generative models actually something slightly different showed learn joint distribution p x inputs x paired labeled remember previous slide p given x conditional model going instead going learn joint model p x given often model takes following form use bayes rule rather rule conditional probability factor two different terms p often referred prior thought measure prior likelihood label likely priori sort conditional generative model conditional probability x given probability generating x given well see soon well see many times course natural often come models kind form one interesting thing joint distributions theyre quite flexible sense general discriminative model showed earlier let illustrate important relationship given ive ive learned joint model always calculate condition probability given x using bayes rule follows py times p x given thats form ive shown im assuming using form denominator p x like said bayes rule p x derived sum p times p x given okay important point given joint distribution easily derive conditional distribution sense joint distribution little bit general conditional distribution theres actually lot back forth two model types often referred often discriminative model rather precise estimating py given x directly often referred discriminative model rather precise actually see lot discriminative models later course p x given sorry p x often referred generative model going see lecture pros cons two model types theres lot interesting research areas lot back forth two model types okay thats generative model learned joint distribution important question though apply new test example heres see something interesting going take input x going define f x simply maximizes probably given x substitute using bayes rule form showed previous slide assuming generative model important characteristic equation p x vary denominator actually constant respect notice taking arg max searching maximizes entire term denominator constant looking arg max simply discard look maximizes product two terms p p x x given convenient computational point view calculating px sometimes rather painful dont actually need applying type model,Course1,W2-S1-L2,W2,S1,L2,Generative,2,1,2,last section describ tag problem describ essenti supervis learn problem exampl tag sequenc want learn model function exampl tag sequenc next section lectur want describ method deriv supervis learn algorithm call gener model said import kind supervis learn model well see come time time cours supervis learn problem follow set go assum set train exampl ill write xi yi equal xi refer input wherea yi refer label let specif part speech train exampl might follow x might follow sentenc dog laugh would underli tag sequenc sentenc exampl determin noun verb might x equal cat bark equal determin noun verb part speech tag case train exampl consist sentenc what call input entir tag sequenc associ label might hundr thousand mayb even ten thousand exampl like given train exampl supervis learn problem task learn function f map input x label f x want exampl part speech tag learn function take sentenc input map part speech tag sequenc output first kind model might consid supervis learn what call con condit model goe follow first step learn step actual learn distribut py given x train sampl well method take train sampl input return distribut py given x output gener distribut variou paramet estim train exampl bit like trigram paramet saw first lectur languag model okay that step one learn condit distribut py given x train exampl second step test input x simpli defin f x maxim condit probabl appli model weve learn take input x input model search differ label return like condit model okay that condit model natur approach probabl first thing would think task ill show second altern call gener model gener model assum scenario train data noth chang task learn function map input x input x label f xsound gener model actual someth slightli differ show learn joint distribut p x input x pair label rememb previou slide p given x condit model go instead go learn joint model p x given often model take follow form use bay rule rather rule condit probabl factor two differ term p often refer prior thought measur prior likelihood label like priori sort condit gener model condit probabl x given probabl gener x given well see soon well see mani time cours natur often come model kind form one interest thing joint distribut theyr quit flexibl sens gener discrimin model show earlier let illustr import relationship given ive ive learn joint model alway calcul condit probabl given x use bay rule follow py time p x given that form ive shown im assum use form denomin p x like said bay rule p x deriv sum p time p x given okay import point given joint distribut easili deriv condit distribut sens joint distribut littl bit gener condit distribut there actual lot back forth two model type often refer often discrimin model rather precis estim py given x directli often refer discrimin model rather precis actual see lot discrimin model later cours p x given sorri p x often refer gener model go see lectur pro con two model type there lot interest research area lot back forth two model type okay that gener model learn joint distribut import question though appli new test exampl here see someth interest go take input x go defin f x simpli maxim probabl given x substitut use bay rule form show previou slide assum gener model import characterist equat p x vari denomin actual constant respect notic take arg max search maxim entir term denomin constant look arg max simpli discard look maxim product two term p p x x given conveni comput point view calcul px sometim rather pain dont actual need appli type model,[ 4  1 14 13 12]
28,Course1_W2-S1-L3_Hidden_Markov_Models_HMMs-_Basic_Definitions_12-00,okay next describe hidden markov models see instance generative modeling approach described hidden markov models basic idea following input sentence x consists n words word know could dog tag sequence yn might determine noun hmm going define joint distribution tags sorry word sequences paired tag sequences precisely instance generative models showed think word sequence input tag sequence quote label joint distribution things distribution possible word sequences x xn possible tag sequences yn two sequences length defined model form learned parameters model set training examples output model going input x sequence words output sequence going sequence tags n maximizes joint probability im basically going search possible tag sequences input sentence im going find tag sequence maximizes probability interesting problem number possible tag sequences grows quickly n exponential n brute force search possible tag sequences general going possible well see nice way around problem class hidden markup models okay slide given formal definition trigram hmms formal definition next slide well see example probably help clarify things lets go definition im going assume two sets v going set possible words language might contain example dog cat box ill use refer set possible tags example might determiner noun verb preposition adverb typically way might dont know order tens tags wall street journal part speech tagging corpusg referred earlier approximately tags theory quite bit corpusg language thats bad estimate okay two sets sentence sequence words x xn xi v tag sequence going sequence tags yn plus sort add yn plus yi equals n yn plus stop okay im going make use stop symbol saw previous lectures language modelling going play similar role saw language modelling ive extended definition joint distribution sequences sn together text sequences plus yn plus example might dog barks might determiner nn vb stop notice extra symbol end tag sequence define joint probability product two terms product equals n plus qyi given yi minus yi notice similar saw trigram language models fact well see soon nothing trigram model applied tag sequences thats first term second term follows product equals n e xi given yi e parameters example following might e given dt basically corresponds probability given tag dt see word sense probability tag dt emitting generating word okay one term word sequence parameters model follows trigram parameter every trigram tags u v u v set im going use start symbol star similar saw language models member could stop symbol thats first set parameters basically trigram parameters secondly word vocabulary tag conditional probability rather parameter corresponding conditional probability x given often referred emission parameters two parameter types model given parameters calculate joint probability word sequence x xn tag sequence yn plus product two terms thats little little abstract lets go concrete example illustrate definitions ive assumed sentence three words dog laughs tag sequence dnv stop determine n noun v verb calculate probability word sequence pairedd tag sequence definitions previous slide works follows first weg product q terms notice tag dnv stop associated q term qd times qn times qv times qstop point im conditioning previous two text star symbols similar way saw language modeling sort initial sequences sorry initial symbols star tag sequence given star star n given star v given dn finally stop given nv q parameters second part expression product e parameters one word input sentence e given word e dog given n word finally e laughs given v word intuitively parameters correspond probabilities first generating word generating word dog given underlying tag n finally generating word laughs giving underlying tag v remember name models hidden markup models lets think little bit actually get name itll also help us develop little bit intuition models product q terms essentially prior probability tag sequences probability n plus remember noisy channel model usually p xy equal p times p x given cue terms correspond precisely p entire text sequence second order markov chain weve basically applied idea markov models problem defining p exactly form saw triagram language models previous part course okay look expression actually probability x xn conditioned n plus basically assuming word xj chosen depending value yj weve made fairly strong independence assumptions word depends underlying public speech tag leads name hidden markov models sense markov chain hidden sense think generative process first choose sequence tags model tag generate associated word x xn xjs observed tag sequences ys unobserved problem test example following received sequence words x xn find likely sequence tags underlying words plus uncover likely settings hidden sequence,Course1,W2-S1-L3,W2,S1,L3,Hidden,2,1,3,okay next describ hidden markov model see instanc gener model approach describ hidden markov model basic idea follow input sentenc x consist n word word know could dog tag sequenc yn might determin noun hmm go defin joint distribut tag sorri word sequenc pair tag sequenc precis instanc gener model show think word sequenc input tag sequenc quot label joint distribut thing distribut possibl word sequenc x xn possibl tag sequenc yn two sequenc length defin model form learn paramet model set train exampl output model go input x sequenc word output sequenc go sequenc tag n maxim joint probabl im basic go search possibl tag sequenc input sentenc im go find tag sequenc maxim probabl interest problem number possibl tag sequenc grow quickli n exponenti n brute forc search possibl tag sequenc gener go possibl well see nice way around problem class hidden markup model okay slide given formal definit trigram hmm formal definit next slide well see exampl probabl help clarifi thing let go definit im go assum two set v go set possibl word languag might contain exampl dog cat box ill use refer set possibl tag exampl might determin noun verb preposit adverb typic way might dont know order ten tag wall street journal part speech tag corpusg refer earlier approxim tag theori quit bit corpusg languag that bad estim okay two set sentenc sequenc word x xn xi v tag sequenc go sequenc tag yn plu sort add yn plu yi equal n yn plu stop okay im go make use stop symbol saw previou lectur languag model go play similar role saw languag model ive extend definit joint distribut sequenc sn togeth text sequenc plu yn plu exampl might dog bark might determin nn vb stop notic extra symbol end tag sequenc defin joint probabl product two term product equal n plu qyi given yi minu yi notic similar saw trigram languag model fact well see soon noth trigram model appli tag sequenc that first term second term follow product equal n e xi given yi e paramet exampl follow might e given dt basic correspond probabl given tag dt see word sens probabl tag dt emit gener word okay one term word sequenc paramet model follow trigram paramet everi trigram tag u v u v set im go use start symbol star similar saw languag model member could stop symbol that first set paramet basic trigram paramet secondli word vocabulari tag condit probabl rather paramet correspond condit probabl x given often refer emiss paramet two paramet type model given paramet calcul joint probabl word sequenc x xn tag sequenc yn plu product two term that littl littl abstract let go concret exampl illustr definit ive assum sentenc three word dog laugh tag sequenc dnv stop determin n noun v verb calcul probabl word sequenc pairedd tag sequenc definit previou slide work follow first weg product q term notic tag dnv stop associ q term qd time qn time qv time qstop point im condit previou two text star symbol similar way saw languag model sort initi sequenc sorri initi symbol star tag sequenc given star star n given star v given dn final stop given nv q paramet second part express product e paramet one word input sentenc e given word e dog given n word final e laugh given v word intuit paramet correspond probabl first gener word gener word dog given underli tag n final gener word laugh give underli tag v rememb name model hidden markup model let think littl bit actual get name itll also help us develop littl bit intuit model product q term essenti prior probabl tag sequenc probabl n plu rememb noisi channel model usual p xy equal p time p x given cue term correspond precis p entir text sequenc second order markov chain weve basic appli idea markov model problem defin p exactli form saw triagram languag model previou part cours okay look express actual probabl x xn condit n plu basic assum word xj chosen depend valu yj weve made fairli strong independ assumpt word depend underli public speech tag lead name hidden markov model sens markov chain hidden sens think gener process first choos sequenc tag model tag gener associ word x xn xj observ tag sequenc ys unobserv problem test exampl follow receiv sequenc word x xn find like sequenc tag underli word plu uncov like set hidden sequenc,[ 1  4 14 13 12]
29,Course1_W2-S1-L4_Parameter_Estimation_in_HMMs_13-16,okay weve introduced hidden markov models previous section gave basic definitions models next section want talk models learned set training examples particular q e parameters saw trigram parameters emmission parameters estimated set training examples actually quite straightforward lets take q parameters start lets say example want estimate q parameter corresponding probability seeing tag vt given previous two tags tags use training corpus course induce counts different tag sequences ive shown essentially linear interpretation method estimation similar methods saw trigram language models last lecture course ml estimate maximum likelihood estimate actually trigram maximum likelihood estimate notice denominator number times ive seen dt followed followed jj thats conditioning numerator number times ive seen sequence tags trigram count bigram count ive taken ratio terms counts taken directly training samples consist entire sentences together part speech sequences similarly bigram maximum likely estimate ratio counts number times ive seen jj second tag sequence conditioning numerator number times ive seen jj followed vt finally unigram ml estimate numerator number times ive seen tag vt denominator total number tags ive seen training corpus linear ineterpolation mthod three smoothign parameters exactly saw language modeling lambdas satisfy constraints sum theyre greater equal estimated data similar way actually identical way showed language modeling problem basically direct application methods saw language modeling okay secondly weve got consider omission parameters example e base given vt probability tag vt emits word base derive maximum likelihood estimates take intuitive form denominator number times ive seen tag vt numerator simply number times ive seen tag vt paired word base simply take ratio terms blindingly simple estimate thats basically although theres one problem need worry thats fall e x given going zero x never seen training data saying im basically saying word x never seen training data going estimate submission parameters equal zero sense means useful information word actually going real problem models ill describe simple solution important lets come back example lets say sake argument test sentence would apply tagging models test sentence try find likely sequence public speech tags sentence likely given test sentence encounter words weve never seen training mulally example quite possibly never seen training data maybe even topping relatively infrequent word may seen training data actually look statistics english example even say million words training data tagging frequently encounter words testator youve never seen training data means lets take particular example e mulally given equal tags means defined sequence x xn equal equal easily verify p x xn yn plus equal tag sequences yn plus thats text sequence going involve emission parameter like equal probabilities taken course point model completely broken tag sequences probability zero fact think trying find likely tag sequence rather aug max tag sequences expression tie tag sequences get value zero okay model broken simple fix common method used follows first step split vocabulary two different sets well follows ill define socalled frequent words word occuring greater threshold example typical value might define frequent word word occurring five times training define low frequency words words going include words seen less five times training data also words seen zero times training data new words see test data example never seen training thats first step second step going define mapping map low frequency words words set small finite set typically depending spelling features words ill give example next slide basic idea take large set lowfrequency words might many many words fall class word map one say new words small finite set let let concrete giving example paper bikel others specifically problem named entity recognition defined mapping low frequency words roughly think roughly different possible word classes chosen hand using intuition insight problem attacking namedentity recognition partition low frequency words different classes chosen try preserve useful information named entity recognition task example see low frequency word consisting two digits map class called twodigitnum four digits fourdigitnum lets look others word capitals gets mapped word called allcaps anything first word sentence period mapped symbol called first word thats essentially even though word capitalized capitalization information useful case words start sentence capitalized word whose initial letter capital theres words capitalized thisunknown initcap lets continue example lets come back original form training data im looking main density problem assume training example somebody actually annotated different entities particular example transformation data looks like heres data looks like translate transformation number lowfrequency words mapped kind pseudowords words small set categories preserve spelling features words profits mapped first word boeing mapped initcap lowercase initcap human look data see even though weve discarded precise identity words weve still preserved quite bit information importantly information underlying spelling words useful named entity problem okay perform transformation training examples also test examples simply build hidden markov model tagger using data format inaudible parameters probability seeing first word given tag na emission parameter specifying probability seeing init cap given tag sc weve essentially finessed problem lowfrequency words words test state never seen training closing vocabulary mapping lowfrequency words much smaller set words preserve information spellings nice thing simple method weve done simply read straight forward maximum likelihood estimates different parameters downside clearly heuristic human expertise go design mapping small number word classes,Course1,W2-S1-L4,W2,S1,L4,Parameter,2,1,4,okay weve introduc hidden markov model previou section gave basic definit model next section want talk model learn set train exampl particular q e paramet saw trigram paramet emmiss paramet estim set train exampl actual quit straightforward let take q paramet start let say exampl want estim q paramet correspond probabl see tag vt given previou two tag tag use train corpu cours induc count differ tag sequenc ive shown essenti linear interpret method estim similar method saw trigram languag model last lectur cours ml estim maximum likelihood estim actual trigram maximum likelihood estim notic denomin number time ive seen dt follow follow jj that condit numer number time ive seen sequenc tag trigram count bigram count ive taken ratio term count taken directli train sampl consist entir sentenc togeth part speech sequenc similarli bigram maximum like estim ratio count number time ive seen jj second tag sequenc condit numer number time ive seen jj follow vt final unigram ml estim numer number time ive seen tag vt denomin total number tag ive seen train corpu linear ineterpol mthod three smoothign paramet exactli saw languag model lambda satisfi constraint sum theyr greater equal estim data similar way actual ident way show languag model problem basic direct applic method saw languag model okay secondli weve got consid omiss paramet exampl e base given vt probabl tag vt emit word base deriv maximum likelihood estim take intuit form denomin number time ive seen tag vt numer simpli number time ive seen tag vt pair word base simpli take ratio term blindingli simpl estim that basic although there one problem need worri that fall e x given go zero x never seen train data say im basic say word x never seen train data go estim submiss paramet equal zero sens mean use inform word actual go real problem model ill describ simpl solut import let come back exampl let say sake argument test sentenc would appli tag model test sentenc tri find like sequenc public speech tag sentenc like given test sentenc encount word weve never seen train mulal exampl quit possibl never seen train data mayb even top rel infrequ word may seen train data actual look statist english exampl even say million word train data tag frequent encount word testat youv never seen train data mean let take particular exampl e mulal given equal tag mean defin sequenc x xn equal equal easili verifi p x xn yn plu equal tag sequenc yn plu that text sequenc go involv emiss paramet like equal probabl taken cours point model complet broken tag sequenc probabl zero fact think tri find like tag sequenc rather aug max tag sequenc express tie tag sequenc get valu zero okay model broken simpl fix common method use follow first step split vocabulari two differ set well follow ill defin socal frequent word word occur greater threshold exampl typic valu might defin frequent word word occur five time train defin low frequenc word word go includ word seen less five time train data also word seen zero time train data new word see test data exampl never seen train that first step second step go defin map map low frequenc word word set small finit set typic depend spell featur word ill give exampl next slide basic idea take larg set lowfrequ word might mani mani word fall class word map one say new word small finit set let let concret give exampl paper bikel other specif problem name entiti recognit defin map low frequenc word roughli think roughli differ possibl word class chosen hand use intuit insight problem attack namedent recognit partit low frequenc word differ class chosen tri preserv use inform name entiti recognit task exampl see low frequenc word consist two digit map class call twodigitnum four digit fourdigitnum let look other word capit get map word call allcap anyth first word sentenc period map symbol call first word that essenti even though word capit capit inform use case word start sentenc capit word whose initi letter capit there word capit thisunknown initcap let continu exampl let come back origin form train data im look main densiti problem assum train exampl somebodi actual annot differ entiti particular exampl transform data look like here data look like translat transform number lowfrequ word map kind pseudoword word small set categori preserv spell featur word profit map first word boe map initcap lowercas initcap human look data see even though weve discard precis ident word weve still preserv quit bit inform importantli inform underli spell word use name entiti problem okay perform transform train exampl also test exampl simpli build hidden markov model tagger use data format inaud paramet probabl see first word given tag na emiss paramet specifi probabl see init cap given tag sc weve essenti finess problem lowfrequ word word test state never seen train close vocabulari map lowfrequ word much smaller set word preserv inform spell nice thing simpl method weve done simpli read straight forward maximum likelihood estim differ paramet downsid clearli heurist human expertis go design map small number word class,[ 4  1 14 13 12]
30,Course1_W2-S1-L5_The_Viterbi_Algorithm_for_HMMs_Part_1_14-07,okay far weve given basic definitions hmms ive described estimate parameters hmm basically know everything learn hidden markov model set training examples final critical problem apply model new test data sentences see viterbi algorithm come play famous algorithm example dynamic programming critical idea algorithms computer sscience really rather beautiful way applying models test data sentences whats problem left following input sentence x thorough xn example could dog laughs new sentence new test data sentence course would like map sequence tags example n v model probability defined hmm parameters estimated training corpus task find highestscoring tag sequence model particular sentence x xn okay going defined set possible tags lets say sake argument three tags n v going search valid tag sequences yi equals n yn plus equal stop going assume p takes form showed trigram hmm structure recap product q terms equals n plus remember yn plus always equal stop symbol product e terms going terms e given determined tag first critical thing realize critical motivation viterbi algorithm observation brute force search going hopelessly inefficient scenario brute force search mean search possible tag sequences simply enumerate tag sequence turn let illustrate example say dog laughs input x xn case lets say set possible part speech tags size n v example list possible tag sequences goes stop n stop v stop n stop okay valid tag sequence simply one three tags three possible tags positions followed stop sign could conceivably go tag sequences turn tag sequence evaluate value p example might find im making numbers could simply list possible tag sequences apply model form model calculate probabilities return highest one say one sake example problem approach number possible sequences grows quickly length sentence particular case power possible sequences choices choice first tag second tag third tag choices possibilities general case size number tags size set raised power n number possible sequences number possible sequences obviously grows exponentially quickly respect sentence length n appreciable value n say n equal number quickly gets control becomes hopelessly inefficient brute force search go back slide critical thing going leverage getting around problem probability distribution takes form critically trigram parameters depend subsequences length model particular structure actually allow us search likely tag sequence solve problem much much efficiently brute force search okay lets give definitions underlying viterbi algorithm efficient algorithm solves problems brute force search ill define n length input sentence input sentence x x x xn central definition going function r r going take sequence tags input always star star minus start symbols could sequence tags k length sequence case k equals r takes sequence star star n v basically calculates probability hmm using basically truncated expression first k terms truncated version expression saw full hmms products equals k q parameters product equals k admission parameters essentially probability sequence length k k equals particular example going define dynamic programming table pi k u v going maximum probability tag sequence ending tags u v position k unknown completely precise need couple definitions im going define sub k k equals minus n set possible tags position k think input sentence x x x xn lets say set possible tags say n v p positions n possible tags n v p positions minus single possible symbol star definitions reflect minus equal star set possible symbols tags position minus star start symbol k n sub k equal full set tags notation make things cleaner get full definitions okay pi k u v k take value n u takes value sk minus v takes value sk going maximum probability tag sequence ending tags u v position k precisely pi k u v max sequences k tags k proceeded minus star always assumed star yk minus equals u yk equals v r function ive shown thats formal definition let give particular example next slide lets number words lets say consider entry pi p correspond intuitively im going fix tags p lets assume set equal n v p look preceding positions possible tags position possibilities let write always star star two start symbols many possible different sequences tags end p position example could n v p p p thats one possible sequence many others probability calculated multiplying multiplying together trigram probabilities emission probabilities q terms e terms pi p going maximum probability tag sequences ends tags p position lets give recursive definition critical idea going pi values actually calculated efficiently using recursive definition given slide firstly base case going lets say pi star star equal basically reflects fact every text sequence starts star star beginning second definition interesting recursive definition saying value k u sk minus v sk remember k take value value range n u always going set sk minus going one tags possible position sk minus similarly v take value sk remember sk set possible tags allowable position k say say equal max tags position k minus set possible tags position k minus take max compute pi k minus w u q v given w u e xk given v next slide ill illustrate example justifies recursive definition recursive though pi value depends set previous pi values pi values position k minus recursive definition defining pis terms pis specifically pis position k minus rather abstract let describe exactly justified,Course1,W2-S1-L5,W2,S1,L5,The,2,1,5,okay far weve given basic definit hmm ive describ estim paramet hmm basic know everyth learn hidden markov model set train exampl final critic problem appli model new test data sentenc see viterbi algorithm come play famou algorithm exampl dynam program critic idea algorithm comput sscienc realli rather beauti way appli model test data sentenc what problem left follow input sentenc x thorough xn exampl could dog laugh new sentenc new test data sentenc cours would like map sequenc tag exampl n v model probabl defin hmm paramet estim train corpu task find highestscor tag sequenc model particular sentenc x xn okay go defin set possibl tag let say sake argument three tag n v go search valid tag sequenc yi equal n yn plu equal stop go assum p take form show trigram hmm structur recap product q term equal n plu rememb yn plu alway equal stop symbol product e term go term e given determin tag first critic thing realiz critic motiv viterbi algorithm observ brute forc search go hopelessli ineffici scenario brute forc search mean search possibl tag sequenc simpli enumer tag sequenc turn let illustr exampl say dog laugh input x xn case let say set possibl part speech tag size n v exampl list possibl tag sequenc goe stop n stop v stop n stop okay valid tag sequenc simpli one three tag three possibl tag posit follow stop sign could conceiv go tag sequenc turn tag sequenc evalu valu p exampl might find im make number could simpli list possibl tag sequenc appli model form model calcul probabl return highest one say one sake exampl problem approach number possibl sequenc grow quickli length sentenc particular case power possibl sequenc choic choic first tag second tag third tag choic possibl gener case size number tag size set rais power n number possibl sequenc number possibl sequenc obvious grow exponenti quickli respect sentenc length n appreci valu n say n equal number quickli get control becom hopelessli ineffici brute forc search go back slide critic thing go leverag get around problem probabl distribut take form critic trigram paramet depend subsequ length model particular structur actual allow us search like tag sequenc solv problem much much effici brute forc search okay let give definit underli viterbi algorithm effici algorithm solv problem brute forc search ill defin n length input sentenc input sentenc x x x xn central definit go function r r go take sequenc tag input alway star star minu start symbol could sequenc tag k length sequenc case k equal r take sequenc star star n v basic calcul probabl hmm use basic truncat express first k term truncat version express saw full hmm product equal k q paramet product equal k admiss paramet essenti probabl sequenc length k k equal particular exampl go defin dynam program tabl pi k u v go maximum probabl tag sequenc end tag u v posit k unknown complet precis need coupl definit im go defin sub k k equal minu n set possibl tag posit k think input sentenc x x x xn let say set possibl tag say n v p posit n possibl tag n v p posit minu singl possibl symbol star definit reflect minu equal star set possibl symbol tag posit minu star start symbol k n sub k equal full set tag notat make thing cleaner get full definit okay pi k u v k take valu n u take valu sk minu v take valu sk go maximum probabl tag sequenc end tag u v posit k precis pi k u v max sequenc k tag k proceed minu star alway assum star yk minu equal u yk equal v r function ive shown that formal definit let give particular exampl next slide let number word let say consid entri pi p correspond intuit im go fix tag p let assum set equal n v p look preced posit possibl tag posit possibl let write alway star star two start symbol mani possibl differ sequenc tag end p posit exampl could n v p p p that one possibl sequenc mani other probabl calcul multipli multipli togeth trigram probabl emiss probabl q term e term pi p go maximum probabl tag sequenc end tag p posit let give recurs definit critic idea go pi valu actual calcul effici use recurs definit given slide firstli base case go let say pi star star equal basic reflect fact everi text sequenc start star star begin second definit interest recurs definit say valu k u sk minu v sk rememb k take valu valu rang n u alway go set sk minu go one tag possibl posit sk minu similarli v take valu sk rememb sk set possibl tag allow posit k say say equal max tag posit k minu set possibl tag posit k minu take max comput pi k minu w u q v given w u e xk given v next slide ill illustr exampl justifi recurs definit recurs though pi valu depend set previou pi valu pi valu posit k minu recurs definit defin pi term pi specif pi posit k minu rather abstract let describ exactli justifi,[ 7  1  4 14 13]
31,Course1_W2-S1-L6_The_Viterbi_Algorithm_for_HMMs_Part_2_3-31,okay justify recursive definition going use example well use sentence seen lets see calculate value pi p recall going highest probability tag sequence ending tags p positions first lets see expression works instantiate particular pi value max w case looking simply set possible tags position equal set tags equal n v p max four possible tags pi w p q given w p finally e xk k case seventh word sentence given expression use calculate pi p could justify key insight think tag sequence ending p position included tag position max explicitly searching different possible values tag position possible value tag multiply pi value q value given previous two tags finally admission value given reason fix particular tag say n position following property holds highest probability path going tags n p positions probability pi n p times q given n p times e given thats highest score tag sequence ending n p positions well highest scoring tag sequence ending n p positions include highest scoring tag sequence ending positions n p positions thats probability multiply trigram parameter q emission parameter e take account probability tag emission probability thats highest probability tag sequence going n p simply search possible tags previous position take max gives us highest probability tag sequence ending p positions,Course1,W2-S1-L6,W2,S1,L6,The,2,1,6,okay justifi recurs definit go use exampl well use sentenc seen let see calcul valu pi p recal go highest probabl tag sequenc end tag p posit first let see express work instanti particular pi valu max w case look simpli set possibl tag posit equal set tag equal n v p max four possibl tag pi w p q given w p final e xk k case seventh word sentenc given express use calcul pi p could justifi key insight think tag sequenc end p posit includ tag posit max explicitli search differ possibl valu tag posit possibl valu tag multipli pi valu q valu given previou two tag final admiss valu given reason fix particular tag say n posit follow properti hold highest probabl path go tag n p posit probabl pi n p time q given n p time e given that highest score tag sequenc end n p posit well highest score tag sequenc end n p posit includ highest score tag sequenc end posit n p posit that probabl multipli trigram paramet q emiss paramet e take account probabl tag emiss probabl that highest probabl tag sequenc go n p simpli search possibl tag previou posit take max give us highest probabl tag sequenc end p posit,[ 7  1  4  6 14]
32,Course1_W2-S1-L7_The_Viterbi_Algorithm_for_HMMs_Part_3_7-33,viterbi alogrithm puts ideas together input algorithm sequence words x x xn output going maximum value sequence tags yn plus p x xn yn plus notice simply going return maximum probability sequence course reality want find tag sequence actually achieves max want calculate augmax fact well see soon well worry max well see next slide simple change algorithm allow us actually compute augmax okay input sentence x xn parameters q typically estimated theta algorithm proceeds follows initialization step set pi star star equals one remember base case recursion showed definitions sub k set possible tags position k minus equal star symbol reflecting fact star symbol positions minus everywhere else sk equals set tags example might n v p set possible tags algorithm going proceed fill pi values essentially left right go order k equals right way n point rep apply recursive definition okay position k consider possible tag pairs u v possible positions k minus k simply compute pi k uv using max ws sk minus recall set tags possible tag position k minus going pi value times q value times c value final step remember returning maximum probability tax sequence final step multiply stop probabilities okay max possible u v pairs end sequence position n make sure multiply stop probability q stop given u v sort pesky little bit bookkeeping need remember need worry stop symbol thats final step viterbi algorithm okay said real goal following take input sequence words x xn output augmax yn plus p x xn yn plus ive actually shown full viterbi algorithm makes use crucial idea dynamic programming called backpointers actually allow us recover augmax recover highest scoring tag sequence actually small modification algorithm showed algorithm inputs initialization point going make use pi values way definitions proceed left right k equals n v u v calculate pi value point change algorithm addition recover recording pi value k u v also store back pointer ill call bp kuv augmax thats actually going record tag achieved max tag likely position k minus given fact u v position k okay additional piece bookkeeping recording augmax point weve filled pi values backpointer values actually find highest scoring tag sequence going backwards sequence end outdoor heres works loop completed say yn minus one yn uv pair maximizes pie uv times q stop given u v okay found last two tags sequence go backwards sequence point saying yk backpointer k plus yk plus yk plus okay unraveling augmaxs find highest scoring tank sequence thats find return full viterbi algorithm makes use backpointers actually recover highest probability sequence input x xn let briefly talk runtime complex algorithm x actually order n times number tags cubed linear length sequence cubic number possible tags sequence get complexity well look basically n times number tags squared possible values entering loop okay enter loop many times point search possible tags get extra factor thats get n times cubed runtime algorithm critically linear length sentence actually enormous gain brute force complex number tags raised power n weve gone exponential time algorithm actually linear time algorithm n something cubic number tags thats dramatic improvement,Course1,W2-S1-L7,W2,S1,L7,The,2,1,7,viterbi alogrithm put idea togeth input algorithm sequenc word x x xn output go maximum valu sequenc tag yn plu p x xn yn plu notic simpli go return maximum probabl sequenc cours realiti want find tag sequenc actual achiev max want calcul augmax fact well see soon well worri max well see next slide simpl chang algorithm allow us actual comput augmax okay input sentenc x xn paramet q typic estim theta algorithm proce follow initi step set pi star star equal one rememb base case recurs show definit sub k set possibl tag posit k minu equal star symbol reflect fact star symbol posit minu everywher els sk equal set tag exampl might n v p set possibl tag algorithm go proceed fill pi valu essenti left right go order k equal right way n point rep appli recurs definit okay posit k consid possibl tag pair u v possibl posit k minu k simpli comput pi k uv use max ws sk minu recal set tag possibl tag posit k minu go pi valu time q valu time c valu final step rememb return maximum probabl tax sequenc final step multipli stop probabl okay max possibl u v pair end sequenc posit n make sure multipli stop probabl q stop given u v sort peski littl bit bookkeep need rememb need worri stop symbol that final step viterbi algorithm okay said real goal follow take input sequenc word x xn output augmax yn plu p x xn yn plu ive actual shown full viterbi algorithm make use crucial idea dynam program call backpoint actual allow us recov augmax recov highest score tag sequenc actual small modif algorithm show algorithm input initi point go make use pi valu way definit proceed left right k equal n v u v calcul pi valu point chang algorithm addit recov record pi valu k u v also store back pointer ill call bp kuv augmax that actual go record tag achiev max tag like posit k minu given fact u v posit k okay addit piec bookkeep record augmax point weve fill pi valu backpoint valu actual find highest score tag sequenc go backward sequenc end outdoor here work loop complet say yn minu one yn uv pair maxim pie uv time q stop given u v okay found last two tag sequenc go backward sequenc point say yk backpoint k plu yk plu yk plu okay unravel augmax find highest score tank sequenc that find return full viterbi algorithm make use backpoint actual recov highest probabl sequenc input x xn let briefli talk runtim complex algorithm x actual order n time number tag cube linear length sequenc cubic number possibl tag sequenc get complex well look basic n time number tag squar possibl valu enter loop okay enter loop mani time point search possibl tag get extra factor that get n time cube runtim algorithm critic linear length sentenc actual enorm gain brute forc complex number tag rais power n weve gone exponenti time algorithm actual linear time algorithm n someth cubic number tag that dramat improv,[ 7  1  4 14 13]
33,Course1_W2-S1-L8_Summary_1-50,summarize developed full approach tagging problem based hidden markov models lets go pros cons approach ive shown one big advantage hidden markov model taggers simple train simply matter compiling counts training corpus way described earlier deriving simple maximum likelihood estimates simple lineally interpolated estimates perform relatively well example named entity recognition task work cited beckel others late performed accuracy terms recovering named entities thats pretty good level performance main problem models said problem estimating parameters e word given tag saw rather heuristic method dealing low frequency low count words grouping words different classes depending spelling thats really black heart takes lot human intuition human intervention problem rather clumsy approach particular problem becomes increasingly difficult words input model complex later course well see tagging tasks approach really gets hand later class see alternative tagging methods slightly complex hidden markov models think much satisfactory solution particular problem,Course1,W2-S1-L8,W2,S1,L8,Summary,2,1,8,summar develop full approach tag problem base hidden markov model let go pro con approach ive shown one big advantag hidden markov model tagger simpl train simpli matter compil count train corpu way describ earlier deriv simpl maximum likelihood estim simpl lineal interpol estim perform rel well exampl name entiti recognit task work cite beckel other late perform accuraci term recov name entiti that pretti good level perform main problem model said problem estim paramet e word given tag saw rather heurist method deal low frequenc low count word group word differ class depend spell that realli black heart take lot human intuit human intervent problem rather clumsi approach particular problem becom increasingli difficult word input model complex later cours well see tag task approach realli get hand later class see altern tag method slightli complex hidden markov model think much satisfactori solut particular problem,[ 1  4 13 14 12]
34,Course1_W3-S1-L10_Examples_of_Ambiguity_5-56,final segment want talk ambiguity problem give examples ambiguity comes kind grammar ive shown okay first source ambiguity going consider part speech ambiguity actually something saw lectures part speech tagging earlier course corresponds observation many words english take multiple possible paths speech heres one example word duck singular noun also intransitive verb ambiguous path speech types part speech ambiguity frequently lead multiple possible parse structures particular sentence ive shown examples third phrase structure string saw duck telescope first structure duck singular noun structure basically corresponds interpretation somebody seeing duck using telescope see duck lets look second structure case duck entransitive verb actually duck sentence embedded within verb phrase fact sentence argument verb soar forming vp interpretation basically corresponds interpretation seeing somebody duck using telescope see person duck real duck world somebody ducking cant see see two different interpretations thats first source ambiguity part speech ambiguity second source prepositional phrase attachment ill recap example early lecture sentence drove road car noticed two possible structures sentence one car modifying drove corresponds natural interpretation im driving car said second somewhat crazy interpretation road actually located car case prepositional phrase actually underneath noun phrase want give second example prepositional phrase attachment ambiguity think illustrates interesting property sentence john believed shot bill plausible interpretation bill shooting preposition prepositional phrase bill modifies shot essentially okay somebody believes bill shot john john actually second interpretation much less intuitive least humans bill prepositional phrase modifies belief lets actually paraphrase two interpretations first bill shot john somebody believes bill shot john second interpretation bill believednoise john shot prepositional phrase bill could modify shot corresponding reading bill shooting could modify believed bill believing interesting example shows strong preference humans prepositional phrases modify recent verb interpretations bill shooting believing quite plausible states world using priori beliefs likely world either looks quite plausible yet humans strong preference shooting interpretation reason behind theres strong preference prepositions modify recent verb sentence lets move another example ambiguity using grammar showed concerns noun premodifiers think earlier slides showed structure something similar structure sequence words fast car mechanic actually n bar fast car structure corresponds interpretation mechanic works fast cars okay mechanic whose specialty fast cars example okay fast car entire subphrase premodifying mechanic grammar showed second structure ive shown case car mechanic fast adjective modifies entire sub screen car mechanic okay car mechanic addition fast ambiguity ambiguity mechanic works fast cars car mechanic fast end different structures kind ambiguity within noun premodifiers absolutely prevalent see everywhere sentences english languages,Course1,W3-S1-L10,W3,S1,L10,Examples,3,1,10,final segment want talk ambigu problem give exampl ambigu come kind grammar ive shown okay first sourc ambigu go consid part speech ambigu actual someth saw lectur part speech tag earlier cours correspond observ mani word english take multipl possibl path speech here one exampl word duck singular noun also intransit verb ambigu path speech type part speech ambigu frequent lead multipl possibl pars structur particular sentenc ive shown exampl third phrase structur string saw duck telescop first structur duck singular noun structur basic correspond interpret somebodi see duck use telescop see duck let look second structur case duck entransit verb actual duck sentenc embed within verb phrase fact sentenc argument verb soar form vp interpret basic correspond interpret see somebodi duck use telescop see person duck real duck world somebodi duck cant see see two differ interpret that first sourc ambigu part speech ambigu second sourc preposit phrase attach ill recap exampl earli lectur sentenc drove road car notic two possibl structur sentenc one car modifi drove correspond natur interpret im drive car said second somewhat crazi interpret road actual locat car case preposit phrase actual underneath noun phrase want give second exampl preposit phrase attach ambigu think illustr interest properti sentenc john believ shot bill plausibl interpret bill shoot preposit preposit phrase bill modifi shot essenti okay somebodi believ bill shot john john actual second interpret much less intuit least human bill preposit phrase modifi belief let actual paraphras two interpret first bill shot john somebodi believ bill shot john second interpret bill believednois john shot preposit phrase bill could modifi shot correspond read bill shoot could modifi believ bill believ interest exampl show strong prefer human preposit phrase modifi recent verb interpret bill shoot believ quit plausibl state world use priori belief like world either look quit plausibl yet human strong prefer shoot interpret reason behind there strong prefer preposit modifi recent verb sentenc let move anoth exampl ambigu use grammar show concern noun premodifi think earlier slide show structur someth similar structur sequenc word fast car mechan actual n bar fast car structur correspond interpret mechan work fast car okay mechan whose specialti fast car exampl okay fast car entir subphras premodifi mechan grammar show second structur ive shown case car mechan fast adject modifi entir sub screen car mechan okay car mechan addit fast ambigu ambigu mechan work fast car car mechan fast end differ structur kind ambigu within noun premodifi absolut preval see everywher sentenc english languag,[ 0  4 14 13 12]
35,Course1_W3-S1-L1_Introduction_0-28,okay next portion class going look parsing problem natural language processing lecture going discuss context free grammars important formulas natural language parsing parsing problem great importance relevance many applications nlp goes back fundamental ideas theoretical linguistics applied computational models,Course1,W3-S1-L1,W3,S1,L1,Introduction,3,1,1,okay next portion class go look pars problem natur languag process lectur go discuss context free grammar import formula natur languag pars pars problem great import relev mani applic nlp goe back fundament idea theoret linguist appli comput model,[ 0  4  8 14 13]
36,Course1_W3-S1-L2_Introduction_to_the_Parsing_Problem_Part_1_10-37,lecture im first going give introduction parsing problem well describe contextfree grammars ill give brief sketch apply contextfree grammars develop model grammatical structures seen english finally ill focus ambiguity describe examples ambiguous structures ambiguity extremely prevalent problem weve seen general language processing certainly prevalent problem natural language parsing nutshell definition pausing problem input take sentence example boeing located seattle output going produce object called parse tree parse tree tree structure words sentence leaves tree boeing located seattle see leaves see tree labels internal nodes np pp vp soon well describe exactly role different labels play high level kind hierarchical decompositoin sentence assocative tree stucture parsing problem roots theoritical linguistics really goes back start modern linguistics largely due chomsky one book highly recommend want learn sort roots modern linguistics book chomsky believe called syntactic structures hugely influential book really beautiful book since enormous amount research linguistic syntactic structures languages looking wide range formalisms ive listed example lexical functional grammar lfg headdriven phrasestructure grammar tree adjoining grammars categorial grammars formalism going concentrate today formalism context free grammars really fundamental sense formed basis modern modern formalisms turns going treat parsing problem supervised machine learning problem going assume training data consisting sentences paired underlying depository actually depository one example actually taken resource called penn wall street journal treebank dont expect read every word sentence around words length actually full syntactic structure sentence syntactic structures come training data theyve actually annotated hand penn wall street journal tree bank one early example resource call tree bank treebank collection training data consisting sentences paired parse trees penn treebank around sentences think roughly one million words data annotated hand humans actually gone sentence sentence understanding underlying linguistic theory annotated full parseg structures sentences clearly laborious task net result large amount training data train supervised learning model usual setup example take portion entire data set training sample maybe sentences might typical take portion test sample somehow train model aims take sentences input output produce parse structure output trained training examples test performance model test examples get accurate measure well model actually performing lets give sketch information represented parse tree structures focusing simple example sentence burglar robbed apartment go number levels representation structures okay first level look level word tree see basically part speech word dt determiner n burglar noun v robbed v first level tree simply encodes part speech text sequence input sentence exactly way saw part speech text sequences previous section course part speech checking thats first level look little higher going start see hierarchal grouping words phrases often called constituents take node tree take example np n np node indirectly dominates sequence words dominates two words burglar case means particular substring burglar identified constuant phrase type np np actually stands noun phrase later class well talk extensively different categories see main point get heirachical grouping np second np interacting dominates substring sentence case therefore followed apartment identify apartment another np go little higher tree vp dominates substring robbed apartment know substring type vp stands something called verb phrase finally top level tree stands sentence node pin directly dominates entire sub sequence dominates burglar apartment know substring finally sense importantly parse trees encode important grammatical relationships within sentence let come back example illustrate lets look little tree fragment sense template whenever see structure like word v vp np left simply read fact subject node verb two things stand subject verb relationship lets apply tree apply template see robbed verb moreover burglar subject verb trees make particular relationship subject verb completely transparent recover parse tree simply read identify fragments tree see template read relationships subject verb let give second important relationship whenever see substructure following form word see np right actually going direct object sorry going verb whenever see configuration read relationship verb direct object apply particular example see apartment direct object particular verb therere many templates corresponding many grammatical relationships seen english language looking short syntactic structures allow us read different grammatic relationships thats crucial allows us identify want identify robbing need find subject want identify robbed need find direct object objects example many examples lecture im going use really rather simple short sentences might almost look like trivial problem want emphasize get longer sentences say words length difficult job recover grammatic relationships parse structures invaluable revealing grammatical relationship subject verb verb direct object,Course1,W3-S1-L2,W3,S1,L2,Introduction,3,1,2,lectur im first go give introduct pars problem well describ contextfre grammar ill give brief sketch appli contextfre grammar develop model grammat structur seen english final ill focu ambigu describ exampl ambigu structur ambigu extrem preval problem weve seen gener languag process certainli preval problem natur languag pars nutshel definit paus problem input take sentenc exampl boe locat seattl output go produc object call pars tree pars tree tree structur word sentenc leav tree boe locat seattl see leav see tree label intern node np pp vp soon well describ exactli role differ label play high level kind hierarch decompositoin sentenc assoc tree stuctur pars problem root theorit linguist realli goe back start modern linguist larg due chomski one book highli recommend want learn sort root modern linguist book chomski believ call syntact structur huge influenti book realli beauti book sinc enorm amount research linguist syntact structur languag look wide rang formal ive list exampl lexic function grammar lfg headdriven phrasestructur grammar tree adjoin grammar categori grammar formal go concentr today formal context free grammar realli fundament sens form basi modern modern formal turn go treat pars problem supervis machin learn problem go assum train data consist sentenc pair underli depositori actual depositori one exampl actual taken resourc call penn wall street journal treebank dont expect read everi word sentenc around word length actual full syntact structur sentenc syntact structur come train data theyv actual annot hand penn wall street journal tree bank one earli exampl resourc call tree bank treebank collect train data consist sentenc pair pars tree penn treebank around sentenc think roughli one million word data annot hand human actual gone sentenc sentenc understand underli linguist theori annot full parseg structur sentenc clearli labori task net result larg amount train data train supervis learn model usual setup exampl take portion entir data set train sampl mayb sentenc might typic take portion test sampl somehow train model aim take sentenc input output produc pars structur output train train exampl test perform model test exampl get accur measur well model actual perform let give sketch inform repres pars tree structur focus simpl exampl sentenc burglar rob apart go number level represent structur okay first level look level word tree see basic part speech word dt determin n burglar noun v rob v first level tree simpli encod part speech text sequenc input sentenc exactli way saw part speech text sequenc previou section cours part speech check that first level look littl higher go start see hierarch group word phrase often call constitu take node tree take exampl np n np node indirectli domin sequenc word domin two word burglar case mean particular substr burglar identifi constuant phrase type np np actual stand noun phrase later class well talk extens differ categori see main point get heirach group np second np interact domin substr sentenc case therefor follow apart identifi apart anoth np go littl higher tree vp domin substr rob apart know substr type vp stand someth call verb phrase final top level tree stand sentenc node pin directli domin entir sub sequenc domin burglar apart know substr final sens importantli pars tree encod import grammat relationship within sentenc let come back exampl illustr let look littl tree fragment sens templat whenev see structur like word v vp np left simpli read fact subject node verb two thing stand subject verb relationship let appli tree appli templat see rob verb moreov burglar subject verb tree make particular relationship subject verb complet transpar recov pars tree simpli read identifi fragment tree see templat read relationship subject verb let give second import relationship whenev see substructur follow form word see np right actual go direct object sorri go verb whenev see configur read relationship verb direct object appli particular exampl see apart direct object particular verb therer mani templat correspond mani grammat relationship seen english languag look short syntact structur allow us read differ grammat relationship that crucial allow us identifi want identifi rob need find subject want identifi rob need find direct object object exampl mani exampl lectur im go use realli rather simpl short sentenc might almost look like trivial problem want emphas get longer sentenc say word length difficult job recov grammat relationship pars structur invalu reveal grammat relationship subject verb verb direct object,[ 0  4 14 13 12]
37,Course1_W3-S1-L3_Introduction_to_the_Parsing_Problem_Part_2_4-20,final thing want talk one direct application syntactic structures give little bit intuition might useful problem modeling differences word order different languages specifically problem machine translation lets say sake argument trying build machine translation system takes japanese input produces english output theres something interesting two languages english pretty rigidly englishs word order pretty rigidly subject fi verb followed object example say ibm bought lotus course subject verb object order far frequent word order english hand look japanese actually predominantly subject object verb particular sentence would ibm lotus bought paraphrase japanese word order would see youre going build machine translation system need worry differences word ordering obviously take japaneses input translate word also move words theyre theyre correct order english simple example might seem like trivial problem things quickly get hand longer sentences heres example english sources said ibm bought lotus yesterday sort see everythings scrambled sort fall kind word order said end sentence sources end subject verb end sentence whereas second position english look subclause thats basically translated paraphrase notice word start phrase english goes end japanese notice bought comes second position english least comes subjects object two things subject object verb basically kind reordering applied recursively sentence youre trying model differences word order two languages english japanese quite difficult complexities showed previous slide much complexity reduced syntactic structures ive shown essentially parse tree japanese word order sources yesterday ibm lotus bought said superficially looks difference word order english parse structures actually recover english word order rotating phrases within tree identify three spots mean pick verb move front sbar first thing would okay thats kind instance rotation swapping order two things similar operation node swap order pick move finally similar operation swap order two things move go steps actually recover english word order theres simple description differences word order particular example corresponds rotation different points tree reflecting differences word order english japanese example fact subject verb object english subject object verb japanese,Course1,W3-S1-L3,W3,S1,L3,Introduction,3,1,3,final thing want talk one direct applic syntact structur give littl bit intuit might use problem model differ word order differ languag specif problem machin translat let say sake argument tri build machin translat system take japanes input produc english output there someth interest two languag english pretti rigidli english word order pretti rigidli subject fi verb follow object exampl say ibm bought lotu cours subject verb object order far frequent word order english hand look japanes actual predominantli subject object verb particular sentenc would ibm lotu bought paraphras japanes word order would see your go build machin translat system need worri differ word order obvious take japanes input translat word also move word theyr theyr correct order english simpl exampl might seem like trivial problem thing quickli get hand longer sentenc here exampl english sourc said ibm bought lotu yesterday sort see everyth scrambl sort fall kind word order said end sentenc sourc end subject verb end sentenc wherea second posit english look subclaus that basic translat paraphras notic word start phrase english goe end japanes notic bought come second posit english least come subject object two thing subject object verb basic kind reorder appli recurs sentenc your tri model differ word order two languag english japanes quit difficult complex show previou slide much complex reduc syntact structur ive shown essenti pars tree japanes word order sourc yesterday ibm lotu bought said superfici look differ word order english pars structur actual recov english word order rotat phrase within tree identifi three spot mean pick verb move front sbar first thing would okay that kind instanc rotat swap order two thing similar oper node swap order pick move final similar oper swap order two thing move go step actual recov english word order there simpl descript differ word order particular exampl correspond rotat differ point tree reflect differ word order english japanes exampl fact subject verb object english subject object verb japanes,[ 8  4  0  3 14]
38,Course1_W3-S1-L4_Context-Free_Grammars_Part_1_12-11,okay next segment im going describe context free grammars important formosum used within passing problem first give formal definition one thing note contextfree grammars used linguistics theyre also central importance computer science theoretical computer science also programming languages play important role okay context free grammar following components context free grammar g aunknown contexting n sigma r following elements thisunknown n going finite set call nonterminal symbols sigma going finite set called terminal symbols r set rules come back second finally member n referred distinguished star symbol grammar rules takes following form something left hand side rule x x nonterminal right hand side rule sequence items one n ys either nonterminal n element sigma notice n could actually right hand side rule could often write epsilon empty sequence sequence length thats abstract definition let give concrete example extremely simple grammar english used illustrating definition showed four elements context free grammar n sigma r n case said finite set called nonterminals symbols like np vp youve seen appearing pals trees showed earlier notice also parts speech things like dt vi vt nn actually going parts speech grammar distinguish start start symbol simply going okay finally set words language sigma might consist following set words small vocabulary moristic grammar would much much larger vocabulary sorry finally set rules okay rules particular example grammar said left hand side rule always see nonterminal example example vp np pp righthand side rule see sequence zero symbols symbol could element n sigma particular goes np vp vp goes vi vp goes vt np rules actually lefthand side single symbol example vi righthand side word example sleeps one simple example grammar later class well talk lot symbols mean briefly basically stands sentence vp abbreviations verb phrase np noun phrase pp prepositional phrase thats forma definition context free grammars also example particular context free grammar next crucial concept going idea derivation specifically specifically whats called leftmost derivation derivation sequence strings sn sequence strings following property equal distinguished start symbol grammar n made elements sigma alone sigma star set possible strings derived sigma sigma dog sigma star includes strings like empty string epsilon dog thur two words strings like dog dog basically string form finite sequence words sigma also include empty string sigma epsilon okay sn got sentence example man sleeps running sum sigma intermediate si equal two n derived si minus kicking left terminal x minus replacing beta x goes beta rule r heres example derivation show go example much detail next line start first string next string derivation np vp taken replaced np vp take np replace n let actually go example ill illustrate little bit carefully okay always start derivation distinguished start symbol example step going choose rule grammar might example choose goes np vp case replace right hand side rule nv goes vp critical idea derivation point take left nonterminal current string find rules nonterminal write sequence nonterminals simply replace next term going modify np pick rule grammar example np goes n replace np n n vp picked leftmost element dt case picked rule case dt goes n vp keep going like finally end sequence words okay complete derivation always ends string every words string word language every elements string member sigma okay context free grammar define set va valid derivations derivation valid starts ends sequence words goes process every point replace left known terminal using rule contextfree grammar useful represent derivations parse trees parse tree ive shown nothing representation derivation root tree notice chose goes np vp first rule thats reflected fact dominating np followed vp assuming np dominating determinate noun thats next rule showed go rules used see corresponds sub fragment within tree okay given contextfree grammar set valid derivations grammar set infinite fact almost every almost interesting cases going infinite set possible derivations couple definitions one definition say string sentence could example dog laughs say string language defined cfg least one derivation yields okay cfg defines language language set strings string language theres least one parse tree string critically interesting property strings language may actually one possible derivation leads directly problem ambiguity let givbe example heres simple sentence drove street car one possible parse tree particular sentence grammars english also second possible parse tree ive shown lets go back forth two particular pass tree prepositional phrase seen lower tree particular rule np goes np prepositional phrase low level tree whereas go back previous pass tree youll see prepositional phrase car little higher tree particular rule vp goes vpprepositional phrase basically ambiguity whether prepositional phrase seen high tree attaching vp seen lower tree attaching noun phrase actually corresponds two different possible interpretations sentence parse tree car modifying entire verb phrase street basically corresponds far likely interpretation im driving car lets look second parse tree though case car modifying street possible although highly implausible interpretation street im driving actually located car street car could imagine crazy world would happen thats attatchment corresponds prepositional phrase car modifies street street car recap critically underlying contextfree grammar may find multiple derivations particular sentence multiple derivations often correspond different interpretation underlying sentence,Course1,W3-S1-L4,W3,S1,L4,Context-Free,3,1,4,okay next segment im go describ context free grammar import formosum use within pass problem first give formal definit one thing note contextfre grammar use linguist theyr also central import comput scienc theoret comput scienc also program languag play import role okay context free grammar follow compon context free grammar g aunknown context n sigma r follow element thisunknown n go finit set call nontermin symbol sigma go finit set call termin symbol r set rule come back second final member n refer distinguish star symbol grammar rule take follow form someth left hand side rule x x nontermin right hand side rule sequenc item one n ys either nontermin n element sigma notic n could actual right hand side rule could often write epsilon empti sequenc sequenc length that abstract definit let give concret exampl extrem simpl grammar english use illustr definit show four element context free grammar n sigma r n case said finit set call nontermin symbol like np vp youv seen appear pal tree show earlier notic also part speech thing like dt vi vt nn actual go part speech grammar distinguish start start symbol simpli go okay final set word languag sigma might consist follow set word small vocabulari morist grammar would much much larger vocabulari sorri final set rule okay rule particular exampl grammar said left hand side rule alway see nontermin exampl exampl vp np pp righthand side rule see sequenc zero symbol symbol could element n sigma particular goe np vp vp goe vi vp goe vt np rule actual lefthand side singl symbol exampl vi righthand side word exampl sleep one simpl exampl grammar later class well talk lot symbol mean briefli basic stand sentenc vp abbrevi verb phrase np noun phrase pp preposit phrase that forma definit context free grammar also exampl particular context free grammar next crucial concept go idea deriv specif specif what call leftmost deriv deriv sequenc string sn sequenc string follow properti equal distinguish start symbol grammar n made element sigma alon sigma star set possibl string deriv sigma sigma dog sigma star includ string like empti string epsilon dog thur two word string like dog dog basic string form finit sequenc word sigma also includ empti string sigma epsilon okay sn got sentenc exampl man sleep run sum sigma intermedi si equal two n deriv si minu kick left termin x minu replac beta x goe beta rule r here exampl deriv show go exampl much detail next line start first string next string deriv np vp taken replac np vp take np replac n let actual go exampl ill illustr littl bit care okay alway start deriv distinguish start symbol exampl step go choos rule grammar might exampl choos goe np vp case replac right hand side rule nv goe vp critic idea deriv point take left nontermin current string find rule nontermin write sequenc nontermin simpli replac next term go modifi np pick rule grammar exampl np goe n replac np n n vp pick leftmost element dt case pick rule case dt goe n vp keep go like final end sequenc word okay complet deriv alway end string everi word string word languag everi element string member sigma okay context free grammar defin set va valid deriv deriv valid start end sequenc word goe process everi point replac left known termin use rule contextfre grammar use repres deriv pars tree pars tree ive shown noth represent deriv root tree notic chose goe np vp first rule that reflect fact domin np follow vp assum np domin determin noun that next rule show go rule use see correspond sub fragment within tree okay given contextfre grammar set valid deriv grammar set infinit fact almost everi almost interest case go infinit set possibl deriv coupl definit one definit say string sentenc could exampl dog laugh say string languag defin cfg least one deriv yield okay cfg defin languag languag set string string languag there least one pars tree string critic interest properti string languag may actual one possibl deriv lead directli problem ambigu let givb exampl here simpl sentenc drove street car one possibl pars tree particular sentenc grammar english also second possibl pars tree ive shown let go back forth two particular pass tree preposit phrase seen lower tree particular rule np goe np preposit phrase low level tree wherea go back previou pass tree youll see preposit phrase car littl higher tree particular rule vp goe vppreposit phrase basic ambigu whether preposit phrase seen high tree attach vp seen lower tree attach noun phrase actual correspond two differ possibl interpret sentenc pars tree car modifi entir verb phrase street basic correspond far like interpret im drive car let look second pars tree though case car modifi street possibl although highli implaus interpret street im drive actual locat car street car could imagin crazi world would happen that attatch correspond preposit phrase car modifi street street car recap critic underli contextfre grammar may find multipl deriv particular sentenc multipl deriv often correspond differ interpret underli sentenc,[ 0  4 12 14 13]
39,Course1_W3-S1-L5_Context-Free_Grammars_Part_2_2-22,lets illustrate issue ambiguity following sentence quite short rather innocuous looking sentence announced program promote safety trucks vans human theres really plausible interpretation sentence really ambiguous well see actually several,Course1,W3-S1-L5,W3,S1,L5,Context-Free,3,1,5,let illustr issu ambigu follow sentenc quit short rather innocu look sentenc announc program promot safeti truck van human there realli plausibl interpret sentenc realli ambigu well see actual sever,[ 4  0 14 13 12]
40,Course1_W3-S1-L6_A_Simple_Grammar_for_English_Part_1_10-32,okay next portion class want give brief sketch syntax english actually going develop relatively simple context free grammar captures really important constructions rules english want emphasize really going scratch surfaceinaudible building full fact grammar english pretty formidable task many many many issues wont cover emphasize formidable task building grammar english might book brought product details amazon book called comprehensive grammar english language famous book english grammar close pages length weighs almost five pounds large book ten inches dimension inches dimension two inches thick book tries document grammar english huge spite size book take randomly drawn sentence say newspaper book reading quite like li quite likely would come across construction wasnt covered book end transcription says monumental task build grammar english lets proceed idea building first attempt context grammar cover really important contractions okay firstly lets talk basic parts speech parts speech conventions taken something called brown corpus corpus early actually group researchers took pretty diverse set sentences wide variety genres english annotated parts speech quite likely first part speech taken corpus tagging conventions used many corpus since look nouns actually sub divide nouns different subtypes singular nouns use tag nn singular nouns plural nouns use tag nns plural noun things like telescopes houses buildings singular nouns things like man dog park finally proper nouns use nnp part speech tag proper noun names things like smith gates ibm almost always capitalized words english moving second important part speech dt stands determiner determiners words like every determiners usually come nouns strings like man man man see determiner noun finally adjectives jj used refer adjective adjective could word like red green large idealistic adjectives often come determiners nouns strings like red telescope would one example determiner adjective noun okay im next going start build simple grammar called noun phrases noun phrase category np let show rules work give us several different noun phrases language okay rules parts speech rewriting various words example rule says part speech nn remember thats singular noun box nn rewriting car mechanic pigeon two words determiners dt rewrite us adjectives fast metal idealistic clay jj adjective symbol rewriting words okay lets see construct phrases type np lets start simple one use rule say noun phrase consists determiner followed category called n bar refer n bar n bar well see soon role plays determiner rewrite example n bar rewrite various ways well come rules later lets use first simplest rule n bar rewrites singular noun finally nn singular noun rewrite example car thats first simple example singular noun phrase corresponding car kind internal structure noun phrase generally consist main noun material words often called premodifiers come word case one modifier determinant grammar ive shown noun phrase always composed determinant followed category called nbar okay lets try second example np goes n bar sorry determiner bar im going make use rule says n bar formed adjective followed n bar use rule similarly use n ball goes n say jj goes fast nn goes car okay structure noun phrase fast car notice critically ive used rule says create n bar adjective followed another n bar okay go step important well see rule n bar goes jj n bar recursive repeatedly applied lets give third structure rule top im going apply adjective rule twice things like fast red car okay see could repeatedly apply adjective rule get multiple adjectives noun rule modifies noun okay thats example use rule lets move little little bit lets look rule applied rule saying n bar formed singular noun followed n bar lets give examples np goes determiner bar top im going use n bar goes nn n bar thats ive used rule heres example rule might used car factory factory makes cars notice car acting premodifier factory almost position adjective definitely noun rather similar construction one saw adjectives okay moving lets finally look rule says n bar actually formed n bar followed another n bar let give one example rule applied top always np goes determinant nbar im going make use rule maybe factory okay im going make use rule let write structure go string says fast car factory okay factory makes fast cars entire end bar fast cars premodified factory ive replaced car entire premodifier fast car okay category nbar see nbar category used intermediate category within nounphrases always see nbar following disseminar addition nbar used within trees,Course1,W3-S1-L6,W3,S1,L6,A,3,1,6,okay next portion class want give brief sketch syntax english actual go develop rel simpl context free grammar captur realli import construct rule english want emphas realli go scratch surfaceinaud build full fact grammar english pretti formid task mani mani mani issu wont cover emphas formid task build grammar english might book brought product detail amazon book call comprehens grammar english languag famou book english grammar close page length weigh almost five pound larg book ten inch dimens inch dimens two inch thick book tri document grammar english huge spite size book take randomli drawn sentenc say newspap book read quit like li quit like would come across construct wasnt cover book end transcript say monument task build grammar english let proceed idea build first attempt context grammar cover realli import contract okay firstli let talk basic part speech part speech convent taken someth call brown corpu corpu earli actual group research took pretti divers set sentenc wide varieti genr english annot part speech quit like first part speech taken corpu tag convent use mani corpu sinc look noun actual sub divid noun differ subtyp singular noun use tag nn singular noun plural noun use tag nn plural noun thing like telescop hous build singular noun thing like man dog park final proper noun use nnp part speech tag proper noun name thing like smith gate ibm almost alway capit word english move second import part speech dt stand determin determin word like everi determin usual come noun string like man man man see determin noun final adject jj use refer adject adject could word like red green larg idealist adject often come determin noun string like red telescop would one exampl determin adject noun okay im next go start build simpl grammar call noun phrase noun phrase categori np let show rule work give us sever differ noun phrase languag okay rule part speech rewrit variou word exampl rule say part speech nn rememb that singular noun box nn rewrit car mechan pigeon two word determin dt rewrit us adject fast metal idealist clay jj adject symbol rewrit word okay let see construct phrase type np let start simpl one use rule say noun phrase consist determin follow categori call n bar refer n bar n bar well see soon role play determin rewrit exampl n bar rewrit variou way well come rule later let use first simplest rule n bar rewrit singular noun final nn singular noun rewrit exampl car that first simpl exampl singular noun phrase correspond car kind intern structur noun phrase gener consist main noun materi word often call premodifi come word case one modifi determin grammar ive shown noun phrase alway compos determin follow categori call nbar okay let tri second exampl np goe n bar sorri determin bar im go make use rule say n bar form adject follow n bar use rule similarli use n ball goe n say jj goe fast nn goe car okay structur noun phrase fast car notic critic ive use rule say creat n bar adject follow anoth n bar okay go step import well see rule n bar goe jj n bar recurs repeatedli appli let give third structur rule top im go appli adject rule twice thing like fast red car okay see could repeatedli appli adject rule get multipl adject noun rule modifi noun okay that exampl use rule let move littl littl bit let look rule appli rule say n bar form singular noun follow n bar let give exampl np goe determin bar top im go use n bar goe nn n bar that ive use rule here exampl rule might use car factori factori make car notic car act premodifi factori almost posit adject definit noun rather similar construct one saw adject okay move let final look rule say n bar actual form n bar follow anoth n bar let give one exampl rule appli top alway np goe determin nbar im go make use rule mayb factori okay im go make use rule let write structur go string say fast car factori okay factori make fast car entir end bar fast car premodifi factori ive replac car entir premodifi fast car okay categori nbar see nbar categori use intermedi categori within nounphras alway see nbar follow disseminar addit nbar use within tree,[ 0  4  1 14 13]
41,Course1_W3-S1-L7_A_Simple_Grammar_for_English_Part_2_5-30,okay next going look prepositions called prepositional phases well see play role within grammar brown corpus uses tag refer whats called preposition preposition word beside preposition usually go front noun phrase say example man would prepositional phrase room would another prepositional phrase okay given idea prepositions lets see incorporate grammar ive shown grammar previous slide except im going add couple rules firstly going rule says pp proportional phrase formed followed amfers various rules specifying within proportion words probably roughly think hundred proposition english thats rough rule magnitude thats first rule saying propositional phrase formed proposition formed noun phrase example prepositional phrase follow following np incidentally ill sometimes use triangle notation mean subtree grammar havent fully specified would tedious write inter intermediate structure np would really application rules determinate n bar n boxed n n goes room useful shorthand want hide details prepositional phrase formed tag followed np means formed preposition followed noun phrase lets go little bit rules see use take game create propositional phrase np np could example car im going use rule say form n bar n bar followed pp finally ill use rule top np goes determinor n bar something like dog car ok whats going noun dog actually post modifier prepositional phrase comes noun modifies actually specifies dog car case rule n bar n bar pp says take prepositional phase use prepositional phrase post modifier particular n bar get full noun phrase noun phrase full sequence words dog car worth noting rules going recursive build larger sequences prepositional phrase modifiers say example dog okay notice used rule n bar goes n bar pp n goes n time actually going specify internal structure noun phrase dog park city oophs city sorry okay city noun phrase ive used triangle notation hide full structure entire noun phrase dog park city notice ive recursively applied prepositional phrase park city n bar ive used create put front n bar form noun phrase second prepositional phrase park city modifies dog see get entire chains prepositional phrases mod modifying way ive shown,Course1,W3-S1-L7,W3,S1,L7,A,3,1,7,okay next go look preposit call preposit phase well see play role within grammar brown corpu use tag refer what call preposit preposit word besid preposit usual go front noun phrase say exampl man would preposit phrase room would anoth preposit phrase okay given idea preposit let see incorpor grammar ive shown grammar previou slide except im go add coupl rule firstli go rule say pp proport phrase form follow amfer variou rule specifi within proport word probabl roughli think hundr proposit english that rough rule magnitud that first rule say proposit phrase form proposit form noun phrase exampl preposit phrase follow follow np incident ill sometim use triangl notat mean subtre grammar havent fulli specifi would tediou write inter intermedi structur np would realli applic rule determin n bar n box n n goe room use shorthand want hide detail preposit phrase form tag follow np mean form preposit follow noun phrase let go littl bit rule see use take game creat proposit phrase np np could exampl car im go use rule say form n bar n bar follow pp final ill use rule top np goe determinor n bar someth like dog car ok what go noun dog actual post modifi preposit phrase come noun modifi actual specifi dog car case rule n bar n bar pp say take preposit phase use preposit phrase post modifi particular n bar get full noun phrase noun phrase full sequenc word dog car worth note rule go recurs build larger sequenc preposit phrase modifi say exampl dog okay notic use rule n bar goe n bar pp n goe n time actual go specifi intern structur noun phrase dog park citi ooph citi sorri okay citi noun phrase ive use triangl notat hide full structur entir noun phrase dog park citi notic ive recurs appli preposit phrase park citi n bar ive use creat put front n bar form noun phrase second preposit phrase park citi modifi dog see get entir chain preposit phrase mod modifi way ive shown,[ 0 10  4  1 14]
42,Course1_W3-S1-L8_A_Simple_Grammar_for_English_Part_3_11-21,next set categories going look verbs verb phrases sentences first critical observation verbs english subcategorized distinct subtypes rather different properties im going use symbol vi refer intransitive verbs vt refer transitive verbs vd refer ditransitive verbs example intransitive verb would sleeps walks laughs transitive verb would something like sees saw likes ditransitive would gave well various rules grammar expressing example vi go sleeps might one rule grammar vi goes walks would vt goes sees similar entries saw likes would vd goes gave example given different verb types different ways constructing whats called verb phrase lets look first one phrase intransitive verb get structure like following vp vi underneath sleeps okay second example would following vp goers vt could example cs notice transitive verb always precedes noun phrase form verb phrase verb phrase made transitive verb followed noun phrase sees dog another verb phrase finally look ditransitive rule verb phrase something like following vd gave noun phrases following verb first might dog ball okay notice verb phrase generally made verb followed noun phrases least verb phrases weve seen form intransitive verb zero noun phrases transitive theres one ditransitive two different noun phrases okay thats first simple set rules vp actually well come back little later slides next thing going look rule construct sentence rules says symbol stands sentence formed noun phrase followed verb phrase actually use rule construct sentences like man sleeps notice goes np vp man vp goes sleeps similarly could np would man man sees dog could man gave dog ball okay starting see structure entire sentences top write np versus followed vp np critically subject case yesthis subject subject subject verb phrase generates verb followed zero noun phrases zero transitive case one transitive case two direct transitive case okay construct many sentences given basicals lets add one rule grammar concerning verb phrases says verb phrase composed verb phrase followed prepositional phrase notice see prepositional phrases cropping earlier seen rule said n bar rewrite n bar followed pp rule says vp formed vp followed pp okay let give examples apply rule sleeps prepositional phrase would structure sleeps car example okay well give another example could vp using ditransitive verb know gave dog ball prepositional phrase could example say wednesday would another prepositional phrase okay see basic structure vp formed vp followed another prepositional phrase prepositional phrase generally adds information example location verb sorry event described verb time event described verb phrase worth noting recursive rule apply multiple times could another application rule end transcription another propositional phrase good example say gave dog bowl wednesday car second prepositional phrase add yet information underlying event described verb okay moving weve seen construct sentence category going add another part speech ill use comp comp going rewrite words example like thats one complemiz complementizer well focus word gives new category well call sbar rule saying sbar formed complementizer followed example could sbar following structure thats going ill hide details parse tree example could man sleeps okay importing structures english youll frequently see well see subordinate clauses sentences formed word like followed sentnece lets extend grammar see exactly fit well actually need verb types remember earlier saw intransitive ditransitive transitive verbs im running namesunknown ill call v v v v new part speech reserved words like said reported would rules like thisv going reserved verbs like told v reserved actually quite unusual word bet well see unusual second new vp rules start use sbar string said man sleeps theres actually actually vp composed v type five said follow sbar critically knows possible type words said reported form phrase verb followed sbar okay sounknown look rule says verb type six followed np sbar form vp thats phrase like told dog mechanic likes pigeon okay sbar formed word complementizedg followed sentence np told actually take two arguments firstly np secondly sbar form verb phrase whos told told final rule says vp composed v type seven followed two known phrases sbar v example might best unusual think verbs take three arguments english think three maybe lets give example say bet pigeon mp also mp also sbar okay thats application rule via type entire phrase verb phrase ne next lets talk little bit coordination another part speech cc word various rules make use coordinators notice rules form nonterminal lefthand side composed two instances nonterminal coordinator create phrases like following man dog would instance rule using np notice rule basically glues together two noun phrases coordinator conjoin two phrases form larger phrase coordination type rule used different phrase types example say sleeps likes dog verb phrases sub segments verb phrases finally use rule says vp rewrites vp cc vp join two thing together,Course1,W3-S1-L8,W3,S1,L8,A,3,1,8,next set categori go look verb verb phrase sentenc first critic observ verb english subcategor distinct subtyp rather differ properti im go use symbol vi refer intransit verb vt refer transit verb vd refer ditransit verb exampl intransit verb would sleep walk laugh transit verb would someth like see saw like ditransit would gave well variou rule grammar express exampl vi go sleep might one rule grammar vi goe walk would vt goe see similar entri saw like would vd goe gave exampl given differ verb type differ way construct what call verb phrase let look first one phrase intransit verb get structur like follow vp vi underneath sleep okay second exampl would follow vp goer vt could exampl cs notic transit verb alway preced noun phrase form verb phrase verb phrase made transit verb follow noun phrase see dog anoth verb phrase final look ditransit rule verb phrase someth like follow vd gave noun phrase follow verb first might dog ball okay notic verb phrase gener made verb follow noun phrase least verb phrase weve seen form intransit verb zero noun phrase transit there one ditransit two differ noun phrase okay that first simpl set rule vp actual well come back littl later slide next thing go look rule construct sentenc rule say symbol stand sentenc form noun phrase follow verb phrase actual use rule construct sentenc like man sleep notic goe np vp man vp goe sleep similarli could np would man man see dog could man gave dog ball okay start see structur entir sentenc top write np versu follow vp np critic subject case yesthi subject subject subject verb phrase gener verb follow zero noun phrase zero transit case one transit case two direct transit case okay construct mani sentenc given basic let add one rule grammar concern verb phrase say verb phrase compos verb phrase follow preposit phrase notic see preposit phrase crop earlier seen rule said n bar rewrit n bar follow pp rule say vp form vp follow pp okay let give exampl appli rule sleep preposit phrase would structur sleep car exampl okay well give anoth exampl could vp use ditransit verb know gave dog ball preposit phrase could exampl say wednesday would anoth preposit phrase okay see basic structur vp form vp follow anoth preposit phrase preposit phrase gener add inform exampl locat verb sorri event describ verb time event describ verb phrase worth note recurs rule appli multipl time could anoth applic rule end transcript anoth proposit phrase good exampl say gave dog bowl wednesday car second preposit phrase add yet inform underli event describ verb okay move weve seen construct sentenc categori go add anoth part speech ill use comp comp go rewrit word exampl like that one complemiz complement well focu word give new categori well call sbar rule say sbar form complement follow exampl could sbar follow structur that go ill hide detail pars tree exampl could man sleep okay import structur english youll frequent see well see subordin claus sentenc form word like follow sentnec let extend grammar see exactli fit well actual need verb type rememb earlier saw intransit ditransit transit verb im run namesunknown ill call v v v v new part speech reserv word like said report would rule like thisv go reserv verb like told v reserv actual quit unusu word bet well see unusu second new vp rule start use sbar string said man sleep there actual actual vp compos v type five said follow sbar critic know possibl type word said report form phrase verb follow sbar okay sounknown look rule say verb type six follow np sbar form vp that phrase like told dog mechan like pigeon okay sbar form word complementizedg follow sentenc np told actual take two argument firstli np secondli sbar form verb phrase who told told final rule say vp compos v type seven follow two known phrase sbar v exampl might best unusu think verb take three argument english think three mayb let give exampl say bet pigeon mp also mp also sbar okay that applic rule via type entir phrase verb phrase ne next let talk littl bit coordin anoth part speech cc word variou rule make use coordin notic rule form nontermin lefthand side compos two instanc nontermin coordin creat phrase like follow man dog would instanc rule use np notic rule basic glue togeth two noun phrase coordin conjoin two phrase form larger phrase coordin type rule use differ phrase type exampl say sleep like dog verb phrase sub segment verb phrase final use rule say vp rewrit vp cc vp join two thing togeth,[ 0 10  4 14 13]
43,Course1_W3-S1-L9_A_Simple_Grammar_for_English_Part_4_2-20,thats simple grammar going show wanted emphasize weve really really scratched surface weve gotten started hopefully though thats given illustration might build grammar english let mention important things weve completely missed grammar grammar completely fails capture one important property language whats called agreement look two sentences subject verb critically english many many languages theres property subject verb agree pleural noun example dogs one form verb laugh singular noun dog different form verb theres notion agreement noun subject main verb sentence grammar ive shown completely failed capture constraint second interesting phenomenon whmovement led kinds interesting realizations structure language heres example say dog cat liked liked transitive verb sense theres gap filled dog dog really object dog thing thats liked think dog moved position think various accounts theres definitely kind hole filled phrase earlier called whmovement another example something weve missed know active versus passive sentences say dog saw cat versus cat seen dog really critical somehow relate two sentences identify sense come thing closely related related meaning perhaps even meaning youre interested reading strongly recommend picking introductory textbook syntax linguistics one book id highly recommend gives nice introduction formal grammar syntax language,Course1,W3-S1-L9,W3,S1,L9,A,3,1,9,that simpl grammar go show want emphas weve realli realli scratch surfac weve gotten start hope though that given illustr might build grammar english let mention import thing weve complet miss grammar grammar complet fail captur one import properti languag what call agreement look two sentenc subject verb critic english mani mani languag there properti subject verb agre pleural noun exampl dog one form verb laugh singular noun dog differ form verb there notion agreement noun subject main verb sentenc grammar ive shown complet fail captur constraint second interest phenomenon whmovement led kind interest realiz structur languag here exampl say dog cat like like transit verb sens there gap fill dog dog realli object dog thing that like think dog move posit think variou account there definit kind hole fill phrase earlier call whmovement anoth exampl someth weve miss know activ versu passiv sentenc say dog saw cat versu cat seen dog realli critic somehow relat two sentenc identifi sens come thing close relat relat mean perhap even mean your interest read strongli recommend pick introductori textbook syntax linguist one book id highli recommend give nice introduct formal grammar syntax languag,[ 0  4 14 13 12]
44,Course1_W3-S2-L1_Introduction_1-12,okay last segment class saw contextfree grammars saw ambiguity severe problem parsing natural languages problem given sentence several different possible parse trees grammar next segment going describe probabilistic contextfree grammars often abbreviated pcfgs pcfgs augment contextfree grammars adding probability rule grammar thereby assign probability every possible parse tree grammar well see allow us direct attack ambiguity problem simplest form actually perform rather poorly parsing model well see subsequent lectures class refinements actually form effective models pcfgs simple model theyre old model theyre crucial model forms basis many many parsing models used natural language processing,Course1,W3-S2-L1,W3,S2,L1,Introduction,3,2,1,okay last segment class saw contextfre grammar saw ambigu sever problem pars natur languag problem given sentenc sever differ possibl pars tree grammar next segment go describ probabilist contextfre grammar often abbrevi pcfg pcfg augment contextfre grammar ad probabl rule grammar therebi assign probabl everi possibl pars tree grammar well see allow us direct attack ambigu problem simplest form actual perform rather poorli pars model well see subsequ lectur class refin actual form effect model pcfg simpl model theyr old model theyr crucial model form basi mani mani pars model use natur languag process,[ 0  4  8 14 13]
45,Course1_W3-S2-L2_Basics_of_PCFGs_Part_1_9-43,well first introduce basic formalism pcfgs well talk crucial algorithm called cky algorithm actually dynamic programming algorithm allow us use pcfgs parse sentences pcfg youll see basically looks exactly contextfree grammar probabilities assigned rule grammar set contextfree rules well assume start symbol context free grammar red ive shown probability rule goes np vp example probability vp goes vi probability probabilities one critical property following take nonterminal grammar lets take dp example therere numbers possible ways expanding nonterminal vp expanded three different ways written vi vtnt vppp three possibilities notice probabilities associated different options sum one plus plus equal key constraint probabilities pcfg look nonterminal must probabilities summing heres another example mp write two different ways either duh duh determiner followed noun noun phrase followed prepositional phrase two probabilities sum one probabilities clear interpretation following conditional probability conditioned particular nonterminal multiple different ways writing nonterminal distribution different options different ways rewriting particular nonterminal okay following let illustrate definition text lets take particularunknown ground pretty simple notice everyone rules im using grammar thats parse tree underlying contextfree grammar going assign probility entire parse tree simply product probailties different rules look first rule goes np vp probability np goes determiner nn probability determinate goes probability calculate probability entire tree multiply together probabilities different rules vp goes vi probability vi goes sleeps probability okay thats final expression probability particular parse tree abstractly tree rules alpha goes beta alpha goes beta alpha n goes beta n left hand side non terminal rule sequence non terminals righthand side probability tree product parameters write q example q vp goes vt p equal zero point four grammar rule parameter q rule probability probability entire tree product q terms illustrated one useful intuition behind ptfg following think top stochastic processes sample pause trees derivations ptfg recall start symbol derivation always starts point derivation im going pick left nontermimal current derivation case im going use rule expand symbol lets say pick rule goes np vp think probabilistic process choose rule probability grammar fact theres way expanding np vp probability one always going rewrite np vp grammar showed previous slide next step process take leftmost nonterminal np think stochastic process choose rule expanding np different possibilities using distribution grammar case might choose np goes determine nn notice nn replaced two words write two symbols probability pick dt choose rule grammar actually rewrites though probability one probabilistic top process going terminate end sequence words point nonterminals left pharse tree underling course np p going something like probability going product different terms fact probabilistic context free grammar sample derivations context free grammar sort top process use probablistic context free grammar generate parsefrees okay crucial properties pcfgs firstly weve said assign probability every possible parse tree allowed underlying contextfree grammar calculate probability parse tree look rules parse tree multiply together different probabilities crucially purposes following say sentence lets say example dog saw man telescope sentence sentence may several different parsetrees underlying contextfree grammar say couple different parsetrees ill sketch cfg end transcription said ambiguity problem problem grammar generating parse tree ill cool entire set parse trees okay set possible parse trees input sentence critically probabilities rules grammar calculate probability parse trees might example calculate probability one maybe one get additional information parse tree grammar parse tree particular sentence different probability gives ranking different parse trees terms probability likelihood given ranking example simply output model parse highest probability tree model way choosing different parsetrees parse method well look well first somehow learn parameters pcfg datar given new sentence search parse tree highest probability grammar pcfg good job modeling probability different possible parse trees end accurate parser often resovles ambiguities described previous lecture one last definition sentence likely parsetree sentence look trees set set possible parsetrees choose highest probability tree pcfg,Course1,W3-S2-L2,W3,S2,L2,Basics,3,2,2,well first introduc basic formal pcfg well talk crucial algorithm call cki algorithm actual dynam program algorithm allow us use pcfg pars sentenc pcfg youll see basic look exactli contextfre grammar probabl assign rule grammar set contextfre rule well assum start symbol context free grammar red ive shown probabl rule goe np vp exampl probabl vp goe vi probabl probabl one critic properti follow take nontermin grammar let take dp exampl therer number possibl way expand nontermin vp expand three differ way written vi vtnt vppp three possibl notic probabl associ differ option sum one plu plu equal key constraint probabl pcfg look nontermin must probabl sum here anoth exampl mp write two differ way either duh duh determin follow noun noun phrase follow preposit phrase two probabl sum one probabl clear interpret follow condit probabl condit particular nontermin multipl differ way write nontermin distribut differ option differ way rewrit particular nontermin okay follow let illustr definit text let take particularunknown ground pretti simpl notic everyon rule im use grammar that pars tree underli contextfre grammar go assign probil entir pars tree simpli product probailti differ rule look first rule goe np vp probabl np goe determin nn probabl determin goe probabl calcul probabl entir tree multipli togeth probabl differ rule vp goe vi probabl vi goe sleep probabl okay that final express probabl particular pars tree abstractli tree rule alpha goe beta alpha goe beta alpha n goe beta n left hand side non termin rule sequenc non termin righthand side probabl tree product paramet write q exampl q vp goe vt p equal zero point four grammar rule paramet q rule probabl probabl entir tree product q term illustr one use intuit behind ptfg follow think top stochast process sampl paus tree deriv ptfg recal start symbol deriv alway start point deriv im go pick left nontermim current deriv case im go use rule expand symbol let say pick rule goe np vp think probabilist process choos rule probabl grammar fact there way expand np vp probabl one alway go rewrit np vp grammar show previou slide next step process take leftmost nontermin np think stochast process choos rule expand np differ possibl use distribut grammar case might choos np goe determin nn notic nn replac two word write two symbol probabl pick dt choos rule grammar actual rewrit though probabl one probabilist top process go termin end sequenc word point nontermin left phars tree underl cours np p go someth like probabl go product differ term fact probabilist context free grammar sampl deriv context free grammar sort top process use probablist context free grammar gener parsefre okay crucial properti pcfg firstli weve said assign probabl everi possibl pars tree allow underli contextfre grammar calcul probabl pars tree look rule pars tree multipli togeth differ probabl crucial purpos follow say sentenc let say exampl dog saw man telescop sentenc sentenc may sever differ parsetre underli contextfre grammar say coupl differ parsetre ill sketch cfg end transcript said ambigu problem problem grammar gener pars tree ill cool entir set pars tree okay set possibl pars tree input sentenc critic probabl rule grammar calcul probabl pars tree might exampl calcul probabl one mayb one get addit inform pars tree grammar pars tree particular sentenc differ probabl give rank differ pars tree term probabl likelihood given rank exampl simpli output model pars highest probabl tree model way choos differ parsetre pars method well look well first somehow learn paramet pcfg datar given new sentenc search pars tree highest probabl grammar pcfg good job model probabl differ possibl pars tree end accur parser often resovl ambigu describ previou lectur one last definit sentenc like parsetre sentenc look tree set set possibl parsetre choos highest probabl tree pcfg,[ 0  4  7 14 13]
46,Course1_W3-S2-L3_Basics_of_PCFGs_Part_2_8-26,next thing discuss actually learn pcfg data described earlier course resources available called treebanks extremely useful purpose early famous example treebank penn wall street journal treebank treebanks consist sentences word sentence together underlying parse tree parse trees actually annotated hand example case penn wsj treebank group people early university pennsylvania got together came set conventions based linguistic theory form structures went annotated sentences thats close million words data actually example parse trees learn rules parameters pcfg penn wsga treebank one earliest examples actually many different resources many different languages form blankaudio treebank learning pcfg actually extremely straight forward almost trivial therere two things really need learn one set underlying rules context free grammar probabilistic context free grammar might example learn rules like thing need learn parameters associated rules example terms rules pcfg learned simply take rules seen treebank learning quote learning context free grammar simply matter reading rules treebank reading context free rules estimate parameters remember q rule alpha goes beta probability associated rule going make use maximum likelhood estimates simple intuitive form estimate parameter sum rule alpha goes beta take ratio denominator number times weve seen alpha numerator number times weve seen entire rule example qml vp goes vt np would simply count vp goes vt np divided count vp counts taken directly example trees tree bank various guarantees kinds estimates one important one data looking treebank actually generate generated underlying pcfg could show training data size gets larger larger paramater estimates get closer closer true underlying probabilities pcfg generating data leads fairly fairly directly property distribution entire parse tree defined pcfg learned converges correct correct underlying distribution pcfg generating trading data bottom line though given tree bank easy learn pcfg simply read rules tree bank calculate maximum likely hood estimates amounts essentially count counting number times youve seen non terminals counting number times weve seen entire rules want talk one final technical property pcfgs goes back work booth thompson early step back think weve done really rather remarkable take given cfg set well formed parse trees cfg set parse trees quite complex structures fact many cfgs set infinite actually cfg showed earlier actually infinite set possible trees way calculating probability tree cfg example might weve done simply assigning probability every rule cfg given tree multiply together rule probabilities within tree get probability define correct distribution possible parse trees propability sum attempting define distribution infinite set infinite set quite complex structures booth thompson give conditions rule probabilities get proper distribution trees first one exactly showed earlier far important take non terminal example vp look rules non terminal left hand side example might following rule probabilities sum okay would valid example thats first condition thats really really need worry ill briefly mention though conditions rather technical well go quickly well somewhat interesting let give example grammar satisfies first condition actually define distribution possible trees rules grammar simple goes ss goes well generate parse trees like one right fact see infinite set possible parse trees grammar lets say choose probability rule probability rule satisfies condition rule probabilities sum actually finite tree going get probability cause goes going get probability cause goes actually going assign propability finite tree fail define distribution possible trees okay thats simple example grammar satisfies condition one illformed sense um second example ill briefly mention little bit surprising actually show certain setting rule parameters say actually show grammar also fails define well formed distribution trees show sum probabilities set finite sized trees actually sums less one particular case intuitively whats going grammars splitting quickly probability less one actually producing finite length parse tree okay wanted mention briefly sort curious practice never really practical concern worth back mind,Course1,W3-S2-L3,W3,S2,L3,Basics,3,2,3,next thing discuss actual learn pcfg data describ earlier cours resourc avail call treebank extrem use purpos earli famou exampl treebank penn wall street journal treebank treebank consist sentenc word sentenc togeth underli pars tree pars tree actual annot hand exampl case penn wsj treebank group peopl earli univers pennsylvania got togeth came set convent base linguist theori form structur went annot sentenc that close million word data actual exampl pars tree learn rule paramet pcfg penn wsga treebank one earliest exampl actual mani differ resourc mani differ languag form blankaudio treebank learn pcfg actual extrem straight forward almost trivial therer two thing realli need learn one set underli rule context free grammar probabilist context free grammar might exampl learn rule like thing need learn paramet associ rule exampl term rule pcfg learn simpli take rule seen treebank learn quot learn context free grammar simpli matter read rule treebank read context free rule estim paramet rememb q rule alpha goe beta probabl associ rule go make use maximum likelhood estim simpl intuit form estim paramet sum rule alpha goe beta take ratio denomin number time weve seen alpha numer number time weve seen entir rule exampl qml vp goe vt np would simpli count vp goe vt np divid count vp count taken directli exampl tree tree bank variou guarante kind estim one import one data look treebank actual gener gener underli pcfg could show train data size get larger larger paramat estim get closer closer true underli probabl pcfg gener data lead fairli fairli directli properti distribut entir pars tree defin pcfg learn converg correct correct underli distribut pcfg gener trade data bottom line though given tree bank easi learn pcfg simpli read rule tree bank calcul maximum like hood estim amount essenti count count number time youv seen non termin count number time weve seen entir rule want talk one final technic properti pcfg goe back work booth thompson earli step back think weve done realli rather remark take given cfg set well form pars tree cfg set pars tree quit complex structur fact mani cfg set infinit actual cfg show earlier actual infinit set possibl tree way calcul probabl tree cfg exampl might weve done simpli assign probabl everi rule cfg given tree multipli togeth rule probabl within tree get probabl defin correct distribut possibl pars tree propabl sum attempt defin distribut infinit set infinit set quit complex structur booth thompson give condit rule probabl get proper distribut tree first one exactli show earlier far import take non termin exampl vp look rule non termin left hand side exampl might follow rule probabl sum okay would valid exampl that first condit that realli realli need worri ill briefli mention though condit rather technic well go quickli well somewhat interest let give exampl grammar satisfi first condit actual defin distribut possibl tree rule grammar simpl goe ss goe well gener pars tree like one right fact see infinit set possibl pars tree grammar let say choos probabl rule probabl rule satisfi condit rule probabl sum actual finit tree go get probabl caus goe go get probabl caus goe actual go assign propabl finit tree fail defin distribut possibl tree okay that simpl exampl grammar satisfi condit one illform sens um second exampl ill briefli mention littl bit surpris actual show certain set rule paramet say actual show grammar also fail defin well form distribut tree show sum probabl set finit size tree actual sum less one particular case intuit what go grammar split quickli probabl less one actual produc finit length pars tree okay want mention briefli sort curiou practic never realli practic concern worth back mind,[ 0  4 14 13 12]
47,Course1_W3-S2-L4_The_CKY_Parsing_Algorithm_Part_1__7-31,okay far described basic formulas underlying pcfgs ive described learn pcfg tree bank set example trees last segment lecture want talk parsing pcfg problem taking sentence input example man saw dog telescope finding probable tree pcfg ok rather dumb kind brute force method would following given input sentence could somehow enumerate brute force possible parse trees sentence pcfg imagine algorithm simply listed possible past trees input sentence step one second step calculate probability trees would choose highest probability tree output parser kind bruteforce method simply enumerate trees calculate probability tree return highest scoring tree clear problem method following number possible parse trees sentence extremely large actually easy come grammars number possible parse trees sentence grows exponentially quickly respect length sentence bruteful search really becomes unfeasible kind grammars theres simply many pri trees search however rather beautiful solution relies dynamic programming im going show actually efficiently find highest probability tree pcfg without enumerate brute force every possible tree grammar said use dynamic programming way saw dynamic programming alrogithm hidden markov models avoided problem brute force search parsing algorithm im going describe well actually assume pcfg something called chomsky normal form means set restrictions rules pcfg context free grammar chomsky normal form consists following set nonterminal symbols set terminal symbols words grammar distinguished start symbol rules grammar astricted take one two forms firstly rule form x goes x nonterminals example vp goes vt np would perfectly valid rule three things nonterminals two children always two children definition similarly goes np vp unknown valid rule chomsky normal form second type rule form x goes x nonterminal terminal symbol example vt goes soar perfectly valid rule determiner goes rules phone nonterminal left hand side word right hand side cfg inunknown would chomsky normal form would set rules like associated probabilities okay pausing algorithm im going describe assume pcfg rules form might first glance seem big restriction turns isnt sense take pcfg convert equivalent pcfg chomsky normal form dont want go full details rather laborious ill give sketch kind tricks use take pcfg general form convert chomsky normal form pcfg let give one example lets say pcfg includes following rule vp goes say vt np propositional phrase okay lets say sake argument rule awkward thing rule non terminals right hand side violates restriction rules solution essentially convert sequence rules chomsky normal form let show works im actually going introduce new symbol grammar ill write first rule vp goes vt np prepositional phrase new symbol ive introduced vtnp second rule says vt hyphen np goes vt followed np probability one see basically split rule thee nonterminals right hand side two separate rules chomsky normal form previous grammar id parse tree would something like id structures different symbols like new parse tree essentially intermediate nonterminal structure would look something like okay im sorry attach prepositional phrase yep okay conversion ive done conversion run parsing algorithm new pcfg rules converted way ill recover trees like straight forward map back original format rules removing intermediate nonterminals okay short story grammar chomsky normal form use various methods convert grammar chomsky normal form course havent told deal unary rules things like vp goes vi probability something similar tricks use kind unary rules,Course1,W3-S2-L4,W3,S2,L4,The,3,2,4,okay far describ basic formula underli pcfg ive describ learn pcfg tree bank set exampl tree last segment lectur want talk pars pcfg problem take sentenc input exampl man saw dog telescop find probabl tree pcfg ok rather dumb kind brute forc method would follow given input sentenc could somehow enumer brute forc possibl pars tree sentenc pcfg imagin algorithm simpli list possibl past tree input sentenc step one second step calcul probabl tree would choos highest probabl tree output parser kind bruteforc method simpli enumer tree calcul probabl tree return highest score tree clear problem method follow number possibl pars tree sentenc extrem larg actual easi come grammar number possibl pars tree sentenc grow exponenti quickli respect length sentenc brute search realli becom unfeas kind grammar there simpli mani pri tree search howev rather beauti solut reli dynam program im go show actual effici find highest probabl tree pcfg without enumer brute forc everi possibl tree grammar said use dynam program way saw dynam program alrogithm hidden markov model avoid problem brute forc search pars algorithm im go describ well actual assum pcfg someth call chomski normal form mean set restrict rule pcfg context free grammar chomski normal form consist follow set nontermin symbol set termin symbol word grammar distinguish start symbol rule grammar astrict take one two form firstli rule form x goe x nontermin exampl vp goe vt np would perfectli valid rule three thing nontermin two children alway two children definit similarli goe np vp unknown valid rule chomski normal form second type rule form x goe x nontermin termin symbol exampl vt goe soar perfectli valid rule determin goe rule phone nontermin left hand side word right hand side cfg inunknown would chomski normal form would set rule like associ probabl okay paus algorithm im go describ assum pcfg rule form might first glanc seem big restrict turn isnt sens take pcfg convert equival pcfg chomski normal form dont want go full detail rather labori ill give sketch kind trick use take pcfg gener form convert chomski normal form pcfg let give one exampl let say pcfg includ follow rule vp goe say vt np proposit phrase okay let say sake argument rule awkward thing rule non termin right hand side violat restrict rule solut essenti convert sequenc rule chomski normal form let show work im actual go introduc new symbol grammar ill write first rule vp goe vt np preposit phrase new symbol ive introduc vtnp second rule say vt hyphen np goe vt follow np probabl one see basic split rule thee nontermin right hand side two separ rule chomski normal form previou grammar id pars tree would someth like id structur differ symbol like new pars tree essenti intermedi nontermin structur would look someth like okay im sorri attach preposit phrase yep okay convers ive done convers run pars algorithm new pcfg rule convert way ill recov tree like straight forward map back origin format rule remov intermedi nontermin okay short stori grammar chomski normal form use variou method convert grammar chomski normal form cours havent told deal unari rule thing like vp goe vi probabl someth similar trick use kind unari rule,[ 0  4 14 13 12]
48,Course1_W3-S2-L5_The_CKY_Parsing_Algorithm_Part_2__13-22,lets describe dynamic programming algorithm take pcfg chomsky normal form also sentence sentence like dog saw cat going two inputs algorithm going return highest score highest probability tree grammar ill focus problem problem simply computing highest probability tree grammar particular sentence remember set possible parse trees input sentence course really want calculate arg max tree actually achieves max well come back later first well considered problem computing maximum probability tree okay im going use n refer number words sentence ill refer wi ith word sentence w w right way wn thats input sentence n set nonterminals grammars start symbol grammar okay going make use dynamic programming ill use pi refer dynamic programming table pi going three indices x nonterminal going first index range n j also going value field n always less equal j okay consider entries pi ijx listening j well see second ill give example moment let give definition going defined maximum probability constituent nonterminal x spanning words j invclusive okay say sentence w w w w w w look pi np correspond think symbol np going many many different ways potentially np parse tree underneath spans words inclusive every one parse trees probability simply product rule probabilities within parse tree pi np going highest probability parse tree np spanning words given definition remember goal find highest score parse tree well definition pi ns going highest scoring parse tree root spanning words n inclusive calculate value essentially solved problem going see actually calculate pi values efficiently using dynamic programming method critically using recursive definition pi values initially build pi values small segments sentence gradually get bigger bigger pi value entire sentence reemphasize definition means heres concrete example number words sentence lets say pi vp means im going consider vps span words inclusive particular might least couple vps one telescope modifying saw instead telescope used see man one man telescope general correspond two different parse trees pi vp going highest probability different ways reaching vp spanning different words okay next going consider recursive definitions pi values actually drive dynamic programming algorithm okay first base case recursion range n nonterminal x define pi x simply parameter q x goes wi define q x goes wi equals rule seen grammar let give concrete example dog laughs example want compute pi word want compute pi nn going highest probability parse tree n root spanning words inclusive theres one way theres one possible parse tree parse tree single rule n goes dog going add value probability nn goes dog thats simple case rule grammar define equal zero reflecting fact maximum probability zero tree spanning single word dog nn root thats base case dynamic program recursive definition involved ill first give definition next slide ill give example try describe recursive definition correct okay case range n j range plus n nonterminal x actually really n minus sorry case strictly less j span example particular sentence might pi np example okay heres definition max im going max firstly ways rewriting x grammar max productions form x goes yz rule set remember grammar chomsky normal form consider rules binary two children variable take value range j minus would called split point see shortly im going search two choices choice rule choice value im going multiply q probability rule chosen pi plus j z z definition recursive value pi calculated based values pi particular based values pi based shorter segments length form plus j smaller length j okay thats rather abstract let give example illustrating definition actually correct heres example lets number words lets say sake argument want calculate pi vp thats going maximum probability tree vp root spanning words inclusive firstly im going write definition plays well justify lets say sake argument two rules expanding vp grammar one vp goes vt np one vp goes vp propositional phrase say sake argument probabilities respectively im going search two different things im going search possible rules grammar im going search values value actually means im going search range point j minus j thats going lets go options compute q vp goes vt np times pi vt times pi np thats one option calculate value case choose rule choose split point equal pi vt pi plus j z np another option q vp goes vt np times pi vt times pi np finally q vp goes vt np times pi vt times pi np ive fixed rule ive searched values similar thing q vp goes vp times pi vp times pi prepositional phrase thats ive chosen rule split point right way q vp goes vp pp times pi vp times pi prepositional phrase okay im going search different possibilities im going search possible rule choices expanding vp im also going search possible split points products going different value ill simply take final value pi vp max different values see actually implement algorithm going careful terms order calculate pi values well calculate pi values bottom ie smaller segments get larger segments ensure pi values lower levels calculated final pi value,Course1,W3-S2-L5,W3,S2,L5,The,3,2,5,let describ dynam program algorithm take pcfg chomski normal form also sentenc sentenc like dog saw cat go two input algorithm go return highest score highest probabl tree grammar ill focu problem problem simpli comput highest probabl tree grammar particular sentenc rememb set possibl pars tree input sentenc cours realli want calcul arg max tree actual achiev max well come back later first well consid problem comput maximum probabl tree okay im go use n refer number word sentenc ill refer wi ith word sentenc w w right way wn that input sentenc n set nontermin grammar start symbol grammar okay go make use dynam program ill use pi refer dynam program tabl pi go three indic x nontermin go first index rang n j also go valu field n alway less equal j okay consid entri pi ijx listen j well see second ill give exampl moment let give definit go defin maximum probabl constitu nontermin x span word j invclus okay say sentenc w w w w w w look pi np correspond think symbol np go mani mani differ way potenti np pars tree underneath span word inclus everi one pars tree probabl simpli product rule probabl within pars tree pi np go highest probabl pars tree np span word given definit rememb goal find highest score pars tree well definit pi ns go highest score pars tree root span word n inclus calcul valu essenti solv problem go see actual calcul pi valu effici use dynam program method critic use recurs definit pi valu initi build pi valu small segment sentenc gradual get bigger bigger pi valu entir sentenc reemphas definit mean here concret exampl number word sentenc let say pi vp mean im go consid vp span word inclus particular might least coupl vp one telescop modifi saw instead telescop use see man one man telescop gener correspond two differ pars tree pi vp go highest probabl differ way reach vp span differ word okay next go consid recurs definit pi valu actual drive dynam program algorithm okay first base case recurs rang n nontermin x defin pi x simpli paramet q x goe wi defin q x goe wi equal rule seen grammar let give concret exampl dog laugh exampl want comput pi word want comput pi nn go highest probabl pars tree n root span word inclus there one way there one possibl pars tree pars tree singl rule n goe dog go add valu probabl nn goe dog that simpl case rule grammar defin equal zero reflect fact maximum probabl zero tree span singl word dog nn root that base case dynam program recurs definit involv ill first give definit next slide ill give exampl tri describ recurs definit correct okay case rang n j rang plu n nontermin x actual realli n minu sorri case strictli less j span exampl particular sentenc might pi np exampl okay here definit max im go max firstli way rewrit x grammar max product form x goe yz rule set rememb grammar chomski normal form consid rule binari two children variabl take valu rang j minu would call split point see shortli im go search two choic choic rule choic valu im go multipli q probabl rule chosen pi plu j z z definit recurs valu pi calcul base valu pi particular base valu pi base shorter segment length form plu j smaller length j okay that rather abstract let give exampl illustr definit actual correct here exampl let number word let say sake argument want calcul pi vp that go maximum probabl tree vp root span word inclus firstli im go write definit play well justifi let say sake argument two rule expand vp grammar one vp goe vt np one vp goe vp proposit phrase say sake argument probabl respect im go search two differ thing im go search possibl rule grammar im go search valu valu actual mean im go search rang point j minu j that go let go option comput q vp goe vt np time pi vt time pi np that one option calcul valu case choos rule choos split point equal pi vt pi plu j z np anoth option q vp goe vt np time pi vt time pi np final q vp goe vt np time pi vt time pi np ive fix rule ive search valu similar thing q vp goe vp time pi vp time pi preposit phrase that ive chosen rule split point right way q vp goe vp pp time pi vp time pi preposit phrase okay im go search differ possibl im go search possibl rule choic expand vp im also go search possibl split point product go differ valu ill simpli take final valu pi vp max differ valu see actual implement algorithm go care term order calcul pi valu well calcul pi valu bottom ie smaller segment get larger segment ensur pi valu lower level calcul final pi valu,[ 7  0  4 14 13]
49,Course1_W3-S2-L6_The_CKY_Parsing_Algorithm_Part_3_10-07,lets next give justification recursive method calculating pi values correct ill use sentence example ill use pi vp example trying find highest probability vp spanning words three eight inclusive ill assume grammar two rules case going search two rules going search split point range values three seven basic intuition following words three four five six seven eight think vp spanning words make choice rule example might vp prepositional phrase make choice splitpoint lets take equals example split point mean means im considering case vp spans three five prepositional phrase spans six eight given choose rule vp goes vp prepositional phrase choose split point ie choose vp span words three five prepositional phrase span words six eight highest probability tree makes choices going q vp goes vp prepositional phase thats probability rule ill argue moment pi vp times pi six eight prepositional phrase two terms come come observation highest probability tree choice rule choice split point use highest probability tree vp pi term going correspond highest probability tree vp similarly use highest probability tree underneath prepositional phrase thats pi value comes calculate highest probability vp spanning words choice rule choice split point calculation im going carry searching different options fact max choice rule choice split point disreflects fact going search different composition decompositions form going search different choices rules different choices split point thereby find single best way reaching vp spanning words three eight inclusive essentially justification calculate pi values using recursive definition ive shown heres final algorithm puts ideals together input algorithm sentence thats sequence n words x xn addition pcfg consists nonterminals terminal symbols start symbol rules set parameters assuming course pcfg chomsky normal form first thing implement base case recursion n nonterminal define pi x q q goes xi rule occurs grammar zero otherwise thats base case showed earlier main loop algorithm implement recursive definition showed thing need careful fill pie values smaller segments get larger segments thats loop l essentially going length segment filling go causeg n minus l said j equals plus go firstly going set l equals going try equals j equals equals j equals equals j equals secondly one try l equals come back tryunknown j equals equals j equals notice segments length words haev segments lenght words making sure fill shorter segments longer segments means calculate pi j guarantee pi values lower filled applying recursive definition showed thing remember pi values going store maximum value probability subtree root x spanning words j really want recover tree achieves max storing back pointers similar way algorithms saw hmms addition storing pi j x bp back pointer j x arg max records rule split point actually achieved maximum value youve filled values straightforward use back pointers actually trace back pause tree highest probability tree grammar lastly lets talk run time time complex get algorithm actually cubic number words input sentence remember n number words also cubic number nonterminals grammar final run time algorithm let explain arrive number consider far algorithm order n squared choices j thats start point end point pie value calculating basically ordered n squared choices two values point n possible values x value x consider possible rules expanding x nsquared possible rules theres n choices theres n choices z also little n values puts us range j minus one j one okay implies finally n cubed times big n cubed times final complexity algorithm need emphasize way way better brute force search remember easy come grammars number possible parse trees input sentence exponential size input length sentence polynomial time algorithm cubic length sentence also cubic number non terminals grammar summarize weve seen lecture course pcfgs augments cfgs simply introducing probability rule grammar assign probability every possible parse tree grammar probability calculated product probabilities rules tree build parser based pcfg go following steps firstly learn pcfg treebank saw thats really quite trivial read rules treebank compute maximum likely estimates based simple counts secondly given new test data sentence use dynamic programming algorithm cky algorithm compute highest probability tree sentence pcfg thats essentially im going describe nest actually going look weaknesses pcfgs going give arguments really rather poor models language initially tried tree banks first became available results disappointing well see fairly simple ways augment pcfgs ways make much sc effective parsing models indeed many stateoftheart models parsing nowadays directly based ideas pcfgs youve seen lecture end transcription,Course1,W3-S2-L6,W3,S2,L6,The,3,2,6,let next give justif recurs method calcul pi valu correct ill use sentenc exampl ill use pi vp exampl tri find highest probabl vp span word three eight inclus ill assum grammar two rule case go search two rule go search split point rang valu three seven basic intuit follow word three four five six seven eight think vp span word make choic rule exampl might vp preposit phrase make choic splitpoint let take equal exampl split point mean mean im consid case vp span three five preposit phrase span six eight given choos rule vp goe vp preposit phrase choos split point ie choos vp span word three five preposit phrase span word six eight highest probabl tree make choic go q vp goe vp preposit phase that probabl rule ill argu moment pi vp time pi six eight preposit phrase two term come come observ highest probabl tree choic rule choic split point use highest probabl tree vp pi term go correspond highest probabl tree vp similarli use highest probabl tree underneath preposit phrase that pi valu come calcul highest probabl vp span word choic rule choic split point calcul im go carri search differ option fact max choic rule choic split point disreflect fact go search differ composit decomposit form go search differ choic rule differ choic split point therebi find singl best way reach vp span word three eight inclus essenti justif calcul pi valu use recurs definit ive shown here final algorithm put ideal togeth input algorithm sentenc that sequenc n word x xn addit pcfg consist nontermin termin symbol start symbol rule set paramet assum cours pcfg chomski normal form first thing implement base case recurs n nontermin defin pi x q q goe xi rule occur grammar zero otherwis that base case show earlier main loop algorithm implement recurs definit show thing need care fill pie valu smaller segment get larger segment that loop l essenti go length segment fill go causeg n minu l said j equal plu go firstli go set l equal go tri equal j equal equal j equal equal j equal secondli one tri l equal come back tryunknown j equal equal j equal notic segment length word haev segment lenght word make sure fill shorter segment longer segment mean calcul pi j guarante pi valu lower fill appli recurs definit show thing rememb pi valu go store maximum valu probabl subtre root x span word j realli want recov tree achiev max store back pointer similar way algorithm saw hmm addit store pi j x bp back pointer j x arg max record rule split point actual achiev maximum valu youv fill valu straightforward use back pointer actual trace back paus tree highest probabl tree grammar lastli let talk run time time complex get algorithm actual cubic number word input sentenc rememb n number word also cubic number nontermin grammar final run time algorithm let explain arriv number consid far algorithm order n squar choic j that start point end point pie valu calcul basic order n squar choic two valu point n possibl valu x valu x consid possibl rule expand x nsquar possibl rule there n choic there n choic z also littl n valu put us rang j minu one j one okay impli final n cube time big n cube time final complex algorithm need emphas way way better brute forc search rememb easi come grammar number possibl pars tree input sentenc exponenti size input length sentenc polynomi time algorithm cubic length sentenc also cubic number non termin grammar summar weve seen lectur cours pcfg augment cfg simpli introduc probabl rule grammar assign probabl everi possibl pars tree grammar probabl calcul product probabl rule tree build parser base pcfg go follow step firstli learn pcfg treebank saw that realli quit trivial read rule treebank comput maximum like estim base simpl count secondli given new test data sentenc use dynam program algorithm cki algorithm comput highest probabl tree sentenc pcfg that essenti im go describ nest actual go look weak pcfg go give argument realli rather poor model languag initi tri tree bank first becam avail result disappoint well see fairli simpl way augment pcfg way make much sc effect pars model inde mani stateoftheart model pars nowaday directli base idea pcfg youv seen lectur end transcript,[ 7  0  4 14 13]
50,Course1_W4-S1-L1_Weaknesses_of_PCFGs_14-59,okay last segment course saw probabilistic contextfree grammars pcfgs gave basic def basic definitions pcfgs saw estimate pcfg treebank saw apply pcfg new test sentence using dynamic programming algorithm cky algorithm recover likely parse tree given sentence let give historical background first treebanks created early early first time data form could learn example pcfg resources become available natural people consider pcfgs pcfgs actually around long time theyve known since least people immediately immediately apply pcfgs basically way showed last lecture performance really quite poor roughly accuracy least wall street journal treebank talking accuracy accuracy recovering basic subparts parse trees basic basic constituents within parse trees look parse trees kind accuracy really really quite poor pcfgs quite disappointment modern parsers perform considerably better round wall street journal dataset talking low terms accuracy english least im going next segment course describe properties pcfgs really explain low number two critical weaknesses one lack sensitivity lexical information two lack sensitivity structural frequencies well go formalize lexicalized pcfgs build directly ideas pcfgs yet much essentially stateoftheart performance well see raw vanilla pcfgs perform poorly possible refinements pcfgs build modern parser stateoftheart performance okay lets start talking weaknesses taking two things turn lets first talk problem lack sensitivity lexical information well first look independence assumptions made pcfg see lead real issues respect heres simple tree saw probability tree going product terms one q parameter rule see within tree striking pcfgs make strong independence assumptions particular look choice particular word tree condition nonterminal word ie part speech assumption pcfg choice word conditionally independent everything else tree condition part speech basically making assumption part speech carries information could probably possibly need identity word part speech extremely strong independence assumption make assumption word independent everything else tree condition nonterminal particular word independent words sentence extremely strong assumption bad assumption natural languages lets look couple examples independence assumption leads real problems going first start case prepositional phrase attachment ambiguity sentence workers dumped sacks bin theres prepositional phrase two possible attachments hence two possible parse trees first parse tree actually correct prepositional phrase attaches verb phase dumped sacks second parse tree prepositional phrase attaches noun phrase sacks okay look example closely list contextfree rules seen two trees showed previous side list list rules seen first parse tree list rules seen second parse tree first thing note many rules actually identical fact way two parse trees differ choice rule vp goes pp prepositional phase actually vp attachment prepositional phrase np goes np prepositional phase think way calculate probability trees simply multiply together probabilities individual rules decision two trees probable going come single parameters corresponding two rules different essentially q parameter np goes np prepositional phrase greater q vp goes vp prepositional phrase tree win otherwise tree win entire decision two trees comes two parameters attachment attachment decision basically made completely independent words particular strain look prepositional phrase attachment spectacularly bad thing let give bit context lets look main words attachment decision dumped sacks bin verb first noun called n preposition n statistically speaking make decision without knowledge words always go noun phrase attachment always go verb phrase attachment might get around accuracy attachment probability problem might get around attachments correct reflects fact fact pretty much frequency noun phrase attachments versus verb phrase attachments may maybe slight bias towards one however look four words involved attachment decision get around accuracy still perfect considerably better perfect prepositional phrase attachment ambiguity still difficult problem nevertheless much much better accuracy goes back work early days statistical parsing people considered isolated problem given four words predict whether attach noun verb used various machine learning methods used supervised data examples like label label case would verb label verb attachment would come collect thousands examples like see could use machine learning make prediction words completely really rather badly build classifier actually looks words much much better anyway bottom line pcfgs make decision without reference lexicalized information completely independent words know suboptimal decision case heres second example case coordination ambiguity phrase dogs houses cats theres ambiguity noun phrase cats coordinated first example coordinated dogs houses second example coordinated houses another classical example ambiguity kind coordination ambiguities come everywhere play game simply list rules two analyses ive shown case see following set rules first analysis set rules second analysis rules actually identical exactly set rules two parse trees showed identical probability pcfg simply tie case two different parse trees even though parse trees apply rules different orders thats get different analysis set rules two parse trees probabilities two parse trees pcfg completely fails display preference one parse tree particular completely ignores lexical information thats couple examples ignoring lexical information leads real problems many many others let go second cause problems pcfgs failure model structural preferences phenomenon called close attachment im going show slide lets assume sequence words noun preposition noun preposition noun example might president company africa prepositional phrase attachment ambiguity prepositional phrase africa could attach recent noun company could attach distant noun president either company africa president africa thats ambiguity two structures close attachment said attachment prepositional phrase attaches closest possible attachment point called close attachment second example attachment unknown verify parse trees exactly set contextfree rules therefore receive identical probability pcfg pcfg fails distinguish two things look statistics first structure close attachment significantly frequent second structure actually occurs twice often second kind structure even look words theres fairly significant structural bias kind close attachments opposed attachments close attachment preference becomes even pronounced look examples involving attachments verbs actually example sentence id shown earlier class ambiguity preposition phrase bill either attach shot close attachment attach believed shot interpretation bill shooting believed interpretation bill believes john shot okay theres two possible analyses humans think would strong preference shooting analysis might might even see believing analysis case look pcfg see two analyses identical sets rules pcfg going assign probability analyses however look kind case prepositional phrase two different verbs attach close attachment least wall street journal data times likely significant statistical preference close attachment pcfg completely fails capture okay ive outlined weaknesses pcfgs lack sensitivities lexical information failure model kinds structural preferences next segment course going look refinements pcfgs fix many problems lead much much accurate parsing models,Course1,W4-S1-L1,W4,S1,L1,Weaknesses,4,1,1,okay last segment cours saw probabilist contextfre grammar pcfg gave basic def basic definit pcfg saw estim pcfg treebank saw appli pcfg new test sentenc use dynam program algorithm cki algorithm recov like pars tree given sentenc let give histor background first treebank creat earli earli first time data form could learn exampl pcfg resourc becom avail natur peopl consid pcfg pcfg actual around long time theyv known sinc least peopl immedi immedi appli pcfg basic way show last lectur perform realli quit poor roughli accuraci least wall street journal treebank talk accuraci accuraci recov basic subpart pars tree basic basic constitu within pars tree look pars tree kind accuraci realli realli quit poor pcfg quit disappoint modern parser perform consider better round wall street journal dataset talk low term accuraci english least im go next segment cours describ properti pcfg realli explain low number two critic weak one lack sensit lexic inform two lack sensit structur frequenc well go formal lexic pcfg build directli idea pcfg yet much essenti stateoftheart perform well see raw vanilla pcfg perform poorli possibl refin pcfg build modern parser stateoftheart perform okay let start talk weak take two thing turn let first talk problem lack sensit lexic inform well first look independ assumpt made pcfg see lead real issu respect here simpl tree saw probabl tree go product term one q paramet rule see within tree strike pcfg make strong independ assumpt particular look choic particular word tree condit nontermin word ie part speech assumpt pcfg choic word condit independ everyth els tree condit part speech basic make assumpt part speech carri inform could probabl possibl need ident word part speech extrem strong independ assumpt make assumpt word independ everyth els tree condit nontermin particular word independ word sentenc extrem strong assumpt bad assumpt natur languag let look coupl exampl independ assumpt lead real problem go first start case preposit phrase attach ambigu sentenc worker dump sack bin there preposit phrase two possibl attach henc two possibl pars tree first pars tree actual correct preposit phrase attach verb phase dump sack second pars tree preposit phrase attach noun phrase sack okay look exampl close list contextfre rule seen two tree show previou side list list rule seen first pars tree list rule seen second pars tree first thing note mani rule actual ident fact way two pars tree differ choic rule vp goe pp preposit phase actual vp attach preposit phrase np goe np preposit phase think way calcul probabl tree simpli multipli togeth probabl individu rule decis two tree probabl go come singl paramet correspond two rule differ essenti q paramet np goe np preposit phrase greater q vp goe vp preposit phrase tree win otherwis tree win entir decis two tree come two paramet attach attach decis basic made complet independ word particular strain look preposit phrase attach spectacularli bad thing let give bit context let look main word attach decis dump sack bin verb first noun call n preposit n statist speak make decis without knowledg word alway go noun phrase attach alway go verb phrase attach might get around accuraci attach probabl problem might get around attach correct reflect fact fact pretti much frequenc noun phrase attach versu verb phrase attach may mayb slight bia toward one howev look four word involv attach decis get around accuraci still perfect consider better perfect preposit phrase attach ambigu still difficult problem nevertheless much much better accuraci goe back work earli day statist pars peopl consid isol problem given four word predict whether attach noun verb use variou machin learn method use supervis data exampl like label label case would verb label verb attach would come collect thousand exampl like see could use machin learn make predict word complet realli rather badli build classifi actual look word much much better anyway bottom line pcfg make decis without refer lexic inform complet independ word know suboptim decis case here second exampl case coordin ambigu phrase dog hous cat there ambigu noun phrase cat coordin first exampl coordin dog hous second exampl coordin hous anoth classic exampl ambigu kind coordin ambigu come everywher play game simpli list rule two analys ive shown case see follow set rule first analysi set rule second analysi rule actual ident exactli set rule two pars tree show ident probabl pcfg simpli tie case two differ pars tree even though pars tree appli rule differ order that get differ analysi set rule two pars tree probabl two pars tree pcfg complet fail display prefer one pars tree particular complet ignor lexic inform that coupl exampl ignor lexic inform lead real problem mani mani other let go second caus problem pcfg failur model structur prefer phenomenon call close attach im go show slide let assum sequenc word noun preposit noun preposit noun exampl might presid compani africa preposit phrase attach ambigu preposit phrase africa could attach recent noun compani could attach distant noun presid either compani africa presid africa that ambigu two structur close attach said attach preposit phrase attach closest possibl attach point call close attach second exampl attach unknown verifi pars tree exactli set contextfre rule therefor receiv ident probabl pcfg pcfg fail distinguish two thing look statist first structur close attach significantli frequent second structur actual occur twice often second kind structur even look word there fairli signific structur bia kind close attach oppos attach close attach prefer becom even pronounc look exampl involv attach verb actual exampl sentenc id shown earlier class ambigu preposit phrase bill either attach shot close attach attach believ shot interpret bill shoot believ interpret bill believ john shot okay there two possibl analys human think would strong prefer shoot analysi might might even see believ analysi case look pcfg see two analys ident set rule pcfg go assign probabl analys howev look kind case preposit phrase two differ verb attach close attach least wall street journal data time like signific statist prefer close attach pcfg complet fail captur okay ive outlin weak pcfg lack sensit lexic inform failur model kind structur prefer next segment cours go look refin pcfg fix mani problem lead much much accur pars model,[ 0  4 14 13 12]
51,Course1_W4-S2-L1_Introduction_00-17,next segment class going talk lexicalized pcfgs lexicalized probabilistic contextfree grammars fix many problems described earlier pcfgs lead much better parsing performance,Course1,W4-S2-L1,W4,S2,L1,Introduction,4,2,1,next segment class go talk lexic pcfg lexic probabilist contextfre grammar fix mani problem describ earlier pcfg lead much better pars perform,[ 0 14 13 12 11]
52,Course1_W4-S2-L2_Lexicalization_of_a_Treebank_10-44,well first talk lexicalize treebank well give definitions lexicalized pcfgs well talk parameter estimation models well actually make direct use smoothing techniques saw language modeling first lecture class well talk also parsing models finally ill talk bit accuracy models comparing example pcfgs going depth actually evaluate different parsing models okay first key idea idea lexacalized pcfgs add annotations specifying whats called head rule contextfree grammar ive shown simple grammar im going assume rules fall two types either part speech rewriting word nonterminal rewriting sequence nonterminals could nonterminals like prepositional phrase parts speech like vi vt key idea going rule grammar identify one children rule whats called head rule ive used red denote head rule vp head vi head vp vt head vp choosing one children head rule sense additional piece information contextfree grammar dont rules also annotation specifying heads thats abstract idea let talk little bit comes actually core idea linguistics idea head rules goes back early work linguistics intuitions sense head sort core rules important part verb verb phrase example always verb righthand side rule whether intransitive verb transitive ditransitive verb verb sense semantic center rule important part rule similarly argue vp sort semantic center rule predicate rule also noun phrases rightmost noun important part rule head annotations actually often present treebanks certainly werent present pan wall street journal treebank treebank used many original experiments statistical parsing annotations recovered set rules let give examples might example noun phrases set deterministic rules recover head noun phrase example would make annotations ive shown first rule says rule contains singular noun plural noun proper noun choose rightmost nouns would apply two rules contain one three categories case return rightmost category head rule fails however go next rule says rule contains np choose leftmost np going apply cases like following structure like doesnt contain one three categories however contain np case take leftmost np head thats rule far common cases noun phrases least wall street journal treebank either one three categories noun phrase righthand side rule funny corner cases go cases rule contains jj adjective choose rightmost jj theyre awesome rather strange noun phrases composed dont contain nouns contain adjectives cd number something like something like finally default rules saying choose rightmost child noun noun phrase heres second example example verb phrases specifying set rules recover heads verb phrases theyre awfully similar showed first rule says rule contains intransitive verb transitive verb choose leftmost vi vt would fact wed probably careful specify verb categories might several different subcategories verb hand rule contains vp choose left vp okay structure like following rule fails vp doesnt dominate vi vt instead case choose leftmost vp head rule finally default case saying dont find categories choose leftmost child main motivation going following moment ill show anotations heads rules use propagate lexical information tree actually transform tree bank kind structures kind unknown pcfg structures rules np goes np vp going add lexical information tree particular nonterminal tree example vp well add lexical item questioned drawn somewhere subtree question came word head annotations used define headwords flow tree one sense view new structures entire new set nonterminals whereas nonterminals like vp nonterminals like questioned np lawyer might around nonterminals original grammar new grammar nonterminals times vocabulary size could easily thousands nonterminals formal sense nothing changed weve vastly increased number nonterminals grammar see adding lexicon information nonterminals higher tree allow us sensitive lexical information higher levels tree thats going key key idea lexicalized pcfgs make pcfgs sensitive lexical information process performed lets look example critical idea propagate lexical items bottom tree constituent receives headword head child lets look tree little bit detail first case simple whenever part speech example dt simply get lexical head lexical item word goes straight parts speech immediately receives headword word said way words often referred headwords headword constituent lets go little higher tree lets take noun phrase noun phrase following rule np goes determiner nn lets assume head rules identified nn head particular rule well rules propagating lex lexical items trees say words propagated head parent witness example propagated tree like noun phrase receives head word head child rule similarly look vp goes vt np lets say sake argument vt identified head rule means vp gets word questioned similar step finally goes np vp lets assume vp head lexical item gets propagated really simple propagate lexical items bottom trees using head annotations nonterminal receives head word head child final result weve transformed trees way nonterminal includes lexical information taken subtree,Course1,W4-S2-L2,W4,S2,L2,Lexicalization,4,2,2,well first talk lexic treebank well give definit lexic pcfg well talk paramet estim model well actual make direct use smooth techniqu saw languag model first lectur class well talk also pars model final ill talk bit accuraci model compar exampl pcfg go depth actual evalu differ pars model okay first key idea idea lexac pcfg add annot specifi what call head rule contextfre grammar ive shown simpl grammar im go assum rule fall two type either part speech rewrit word nontermin rewrit sequenc nontermin could nontermin like preposit phrase part speech like vi vt key idea go rule grammar identifi one children rule what call head rule ive use red denot head rule vp head vi head vp vt head vp choos one children head rule sens addit piec inform contextfre grammar dont rule also annot specifi head that abstract idea let talk littl bit come actual core idea linguist idea head rule goe back earli work linguist intuit sens head sort core rule import part verb verb phrase exampl alway verb righthand side rule whether intransit verb transit ditransit verb verb sens semant center rule import part rule similarli argu vp sort semant center rule predic rule also noun phrase rightmost noun import part rule head annot actual often present treebank certainli werent present pan wall street journal treebank treebank use mani origin experi statist pars annot recov set rule let give exampl might exampl noun phrase set determinist rule recov head noun phrase exampl would make annot ive shown first rule say rule contain singular noun plural noun proper noun choos rightmost noun would appli two rule contain one three categori case return rightmost categori head rule fail howev go next rule say rule contain np choos leftmost np go appli case like follow structur like doesnt contain one three categori howev contain np case take leftmost np head that rule far common case noun phrase least wall street journal treebank either one three categori noun phrase righthand side rule funni corner case go case rule contain jj adject choos rightmost jj theyr awesom rather strang noun phrase compos dont contain noun contain adject cd number someth like someth like final default rule say choos rightmost child noun noun phrase here second exampl exampl verb phrase specifi set rule recov head verb phrase theyr aw similar show first rule say rule contain intransit verb transit verb choos leftmost vi vt would fact wed probabl care specifi verb categori might sever differ subcategori verb hand rule contain vp choos left vp okay structur like follow rule fail vp doesnt domin vi vt instead case choos leftmost vp head rule final default case say dont find categori choos leftmost child main motiv go follow moment ill show anot head rule use propag lexic inform tree actual transform tree bank kind structur kind unknown pcfg structur rule np goe np vp go add lexic inform tree particular nontermin tree exampl vp well add lexic item question drawn somewher subtre question came word head annot use defin headword flow tree one sens view new structur entir new set nontermin wherea nontermin like vp nontermin like question np lawyer might around nontermin origin grammar new grammar nontermin time vocabulari size could easili thousand nontermin formal sens noth chang weve vastli increas number nontermin grammar see ad lexicon inform nontermin higher tree allow us sensit lexic inform higher level tree that go key key idea lexic pcfg make pcfg sensit lexic inform process perform let look exampl critic idea propag lexic item bottom tree constitu receiv headword head child let look tree littl bit detail first case simpl whenev part speech exampl dt simpli get lexic head lexic item word goe straight part speech immedi receiv headword word said way word often refer headword headword constitu let go littl higher tree let take noun phrase noun phrase follow rule np goe determin nn let assum head rule identifi nn head particular rule well rule propag lex lexic item tree say word propag head parent wit exampl propag tree like noun phrase receiv head word head child rule similarli look vp goe vt np let say sake argument vt identifi head rule mean vp get word question similar step final goe np vp let assum vp head lexic item get propag realli simpl propag lexic item bottom tree use head annot nontermin receiv head word head child final result weve transform tree way nontermin includ lexic inform taken subtre,[ 0  4 14 13 12]
53,Course1_W4-S2-L3_Lexicalized_PCFGs-_Basic_Definitions_12-40,weve seen lexicalize treebank add lexical annotations nonterminal treebank next lets talk lexicalized probabilistic contextfree grammars ill give basic definitions okay firstly recap lets remind context free grammar chomsky normal form cfg chomsky normal form consists following n set nonterminal symbols grammar sigma set terminal symbols words distinguished start symbol always occurs root tree finally set rules r rule r must take one two different forms either form x goes one two three elements x one two nonterminal example would goes n p v p consists x goes x nonterminal word example dt goes saw given pcfg chomsky normal form use dynamic programming find highest probability parse order cubic time length sentence n length sentence cubic time number non terminals grammar heres definition lexicalized context free grammars chomsky normal form closely related definition gave previous slide lexicalized cfg assume set non terminal symbols set terminal symbols start symbol rules grammar takes slightly different form three different cases lets go simplest case first say rule x non terminal h word x h write h example goes one example rule x equals rules seen leaves tree part speech lexical item simply rewrites lexical item lets look two remaining cases lets look one first says take three nonterminals x two words h w lexicalized rule xh rewrites yhyw let give example rule could example vp saw goes vt saw mp dog critically im going subscript arrows one specify two children head rule saw left hand side rule saw first non terminal subscript arrow one say thats head word came head rule particular example x equals vp equal vt equal np h equal saw w equal dog case similar give example rule example could saw goes two n p man v p saw notice case head word comes second child thats arrows annotated way annotation arrows sort pesky detail important important voids ambiguities example np dog goes nn dog nn dog unlikely possible h equal dog w equal dog careful specify two children head word came heres example lexicalized contextfree grammar trumpski normal form let show polis tree ssaw goes example npman vpsaw well mark say head came head phrase n p man could go n n man example ive used rule im using rule letsunknown head man could go derivations lexicalized context free grammars look similar derivations regular context free grammars head words associated non terminals tree introduce parameters probabilities lexicalized pcfgs important emphasize similarities differences regular pcfgs regular pcfg saw parameters like following example q goes np vp basically conditional probability given going multiple ways rewriting np vp going one going probability choose oppose alternatives parameters lexicalized pcfg associated entire rules heres example parameter parameter rule saw goes npinaudible vp saw going interpretation conditional probability case interpretation given saw left hand side rule many many different possibilities one choose second nonterminal head choose np man vp saw two children going many many poss possibilities definite similarities two models fact technically speaking lexicalized pcfg really special case pcfg qualitatively something really changed weve gone relatively small number parameters original model large number parameters one parameter every possible lexicalized rule going careful estimate parameters models well see smoothing techniques saw language modeling first segment course applied directly problem next lets talk little bit parsing lexicalized cfgs dont want go tremendous detail parsing algorithms lexicalized pcfgs want give sketch main point use similar dynamic programming algorithm dynamic programming algorithm saw regular p f gs new form grammar ive shown similar chomsky normal form cfg whereas rules like goes npvp order n cubed possible rules n number noon terminals n times n times n new grammar things like saw goes np dog vp saw think number choices well three nonterminals np vp n cubed choices also rule two words head word saw modifier word dog im going sigma squared many many possible rules okay ive gone grammar order n cubed rules grammar many rules naively pick dynamic programming algorithm saw probabilistic context free grammars apply new grammar rules really like regular context free rules treat non terminals much larger set non terminals naively youll end following running time remember order n cubed times number nonterminals cubed run time dynamic programing new run time sigma squared coming terrible idea capillary size huge sigma could easily order much likely thousands even tens thousands theres simple observation gives us least plausibly efficient algorithm particular sentence like dog saw cat immediately discard rules following questioned goes two mp dog vp questioned immediately discard rule word questioned one nonterminals seen nowhere sentence know rule never used pausing sentence actually restrict much smaller set rules order n squared times n cubed capital n choices three non terminals two words rather drawn full vocabulary drawn one n words sentence okay end much smaller set rules restricting grammar simple way actually going lead run time order n fifth times n cubed come well order n cubed dependence dynamic programming algorithm sentence length multiply number non terminals rules grammar get order n cubed times n squared number possible rules say run time get thats really sketch precise definition algorithm hopefully youll get idea important point end pausing lexicalized pcfgs relatively efficiently gone n cubed dependance sentence length n fifth thats expensive still manageable youre careful things,Course1,W4-S2-L3,W4,S2,L3,Lexicalized,4,2,3,weve seen lexic treebank add lexic annot nontermin treebank next let talk lexic probabilist contextfre grammar ill give basic definit okay firstli recap let remind context free grammar chomski normal form cfg chomski normal form consist follow n set nontermin symbol grammar sigma set termin symbol word distinguish start symbol alway occur root tree final set rule r rule r must take one two differ form either form x goe one two three element x one two nontermin exampl would goe n p v p consist x goe x nontermin word exampl dt goe saw given pcfg chomski normal form use dynam program find highest probabl pars order cubic time length sentenc n length sentenc cubic time number non termin grammar here definit lexic context free grammar chomski normal form close relat definit gave previou slide lexic cfg assum set non termin symbol set termin symbol start symbol rule grammar take slightli differ form three differ case let go simplest case first say rule x non termin h word x h write h exampl goe one exampl rule x equal rule seen leav tree part speech lexic item simpli rewrit lexic item let look two remain case let look one first say take three nontermin x two word h w lexic rule xh rewrit yhyw let give exampl rule could exampl vp saw goe vt saw mp dog critic im go subscript arrow one specifi two children head rule saw left hand side rule saw first non termin subscript arrow one say that head word came head rule particular exampl x equal vp equal vt equal np h equal saw w equal dog case similar give exampl rule exampl could saw goe two n p man v p saw notic case head word come second child that arrow annot way annot arrow sort peski detail import import void ambigu exampl np dog goe nn dog nn dog unlik possibl h equal dog w equal dog care specifi two children head word came here exampl lexic contextfre grammar trumpski normal form let show poli tree ssaw goe exampl npman vpsaw well mark say head came head phrase n p man could go n n man exampl ive use rule im use rule letsunknown head man could go deriv lexic context free grammar look similar deriv regular context free grammar head word associ non termin tree introduc paramet probabl lexic pcfg import emphas similar differ regular pcfg regular pcfg saw paramet like follow exampl q goe np vp basic condit probabl given go multipl way rewrit np vp go one go probabl choos oppos altern paramet lexic pcfg associ entir rule here exampl paramet paramet rule saw goe npinaud vp saw go interpret condit probabl case interpret given saw left hand side rule mani mani differ possibl one choos second nontermin head choos np man vp saw two children go mani mani poss possibl definit similar two model fact technic speak lexic pcfg realli special case pcfg qualit someth realli chang weve gone rel small number paramet origin model larg number paramet one paramet everi possibl lexic rule go care estim paramet model well see smooth techniqu saw languag model first segment cours appli directli problem next let talk littl bit pars lexic cfg dont want go tremend detail pars algorithm lexic pcfg want give sketch main point use similar dynam program algorithm dynam program algorithm saw regular p f gs new form grammar ive shown similar chomski normal form cfg wherea rule like goe npvp order n cube possibl rule n number noon termin n time n time n new grammar thing like saw goe np dog vp saw think number choic well three nontermin np vp n cube choic also rule two word head word saw modifi word dog im go sigma squar mani mani possibl rule okay ive gone grammar order n cube rule grammar mani rule naiv pick dynam program algorithm saw probabilist context free grammar appli new grammar rule realli like regular context free rule treat non termin much larger set non termin naiv youll end follow run time rememb order n cube time number nontermin cube run time dynam program new run time sigma squar come terribl idea capillari size huge sigma could easili order much like thousand even ten thousand there simpl observ give us least plausibl effici algorithm particular sentenc like dog saw cat immedi discard rule follow question goe two mp dog vp question immedi discard rule word question one nontermin seen nowher sentenc know rule never use paus sentenc actual restrict much smaller set rule order n squar time n cube capit n choic three non termin two word rather drawn full vocabulari drawn one n word sentenc okay end much smaller set rule restrict grammar simpl way actual go lead run time order n fifth time n cube come well order n cube depend dynam program algorithm sentenc length multipli number non termin rule grammar get order n cube time n squar number possibl rule say run time get that realli sketch precis definit algorithm hope youll get idea import point end paus lexic pcfg rel effici gone n cube depend sentenc length n fifth that expens still manag your care thing,[ 0  4 14 13 12]
54,Course1_W4-S2-L4_Parameter_Estimation_in_Lexicalized_PCFGs_Part_1_5-28,next lets talk estimate parameters lexicalized pcfg firstly want emphasize weve weve ended going much better parse model language raw pcfgs ive picked example sentence example parse tree case propositional phrase ambiguity sentence man saw dog telescope full pals tree lexicalized glory nonterminals associated lexical item probability tree going product terms one q parameter rule seen tree ive listed important ones terms example root tree ssaw goes npman vpsaw parameter q rule tree recall applying regular pcfgs without lyscolization would simple rules g goes npvp vp goes vp prepositional phrase rules would associated probabilities argued rules rather insensitive lexical information see rules actually incorporate rich sources lexical information take example rule involving preposition vp goes vp prepositional phrase v p saw goes v p saw p p see parameters explicitly model dependencies within lexical items example dependency saw dependencies linked particular grammatical relations example ability say likely prepositional phrase preposition modify verb phrase head saw thats basically rule saying theres another example top tree probability saw associated man theunknown relationship man basically subject saw tree see parameters model direct access important lexical information challenge course going large number rules parameters model going quite careful estimate set training samples lets talk done model eugene charniak famous model form late one first lexicalized pcfg models one first models show much improved performance irregular pcfgs heres example parameter ill uses example throughout section parameter associated saw going np man vp saw first step going actually decompose product two terms two parameters make job sim sim simpler first parameter corresponds probability given saw probability wri rewriting np v p okay think r rewriting saw many possible ways rewriting parameter corresponding choice rule ignoring second lexical item man associated rule second parameter interpreted follows say saw weve actually chosen rule weve chosen np vp saw head vp two arrow choice lexical item going fill position rule potentially possible word could fall position parameter corresponding probability man chosen position within rule kind parameters crucial basically model case probably particular word man play particular role namely subject verb saw see direct model dependencies lexical items saw man dependencies useful disambiguation thats step one first step take parameter actually decompose products two different terms,Course1,W4-S2-L4,W4,S2,L4,Parameter,4,2,4,next let talk estim paramet lexic pcfg firstli want emphas weve weve end go much better pars model languag raw pcfg ive pick exampl sentenc exampl pars tree case proposit phrase ambigu sentenc man saw dog telescop full pal tree lexic glori nontermin associ lexic item probabl tree go product term one q paramet rule seen tree ive list import one term exampl root tree ssaw goe npman vpsaw paramet q rule tree recal appli regular pcfg without lyscol would simpl rule g goe npvp vp goe vp preposit phrase rule would associ probabl argu rule rather insensit lexic inform see rule actual incorpor rich sourc lexic inform take exampl rule involv preposit vp goe vp preposit phrase v p saw goe v p saw p p see paramet explicitli model depend within lexic item exampl depend saw depend link particular grammat relat exampl abil say like preposit phrase preposit modifi verb phrase head saw that basic rule say there anoth exampl top tree probabl saw associ man theunknown relationship man basic subject saw tree see paramet model direct access import lexic inform challeng cours go larg number rule paramet model go quit care estim set train sampl let talk done model eugen charniak famou model form late one first lexic pcfg model one first model show much improv perform irregular pcfg here exampl paramet ill use exampl throughout section paramet associ saw go np man vp saw first step go actual decompos product two term two paramet make job sim sim simpler first paramet correspond probabl given saw probabl wri rewrit np v p okay think r rewrit saw mani possibl way rewrit paramet correspond choic rule ignor second lexic item man associ rule second paramet interpret follow say saw weve actual chosen rule weve chosen np vp saw head vp two arrow choic lexic item go fill posit rule potenti possibl word could fall posit paramet correspond probabl man chosen posit within rule kind paramet crucial basic model case probabl particular word man play particular role name subject verb saw see direct model depend lexic item saw man depend use disambigu that step one first step take paramet actual decompos product two differ term,[ 0  4 14 13 12]
55,Course1_W4-S2-L5_Parameter_Estimation_in_Lexicalized_PCFGs_Part_2_9-08,lets look second step deriving estimate first step break parameter product two different terms essentially using chain rrule probabilities first predict rule predict lexical condition rule together head word second step use smoothed estimation parameter estimates two parameters left well apply exactly ideas saw language modeling example example im going use linear interpolation going two parameters lambda lambda greater equal lambda plus lambda equal two maximum likelihood estimates look first one going estimated count saw headed saw rewriting np vp second child vp head divided count saw maximum likelihood estimate basically estimate probability saw rewriting particular rule natural symbol intuitive estimate second one basically backed estimate completely ignores lexical information going back away conditioning word saw going count goes np vp divided count notice almost identical estimate youd see regular pcfg essentially interpolating two different estimates one conditions lexicon information isnt present pcfg throws away lexicon information usual arguments estimate tend robust counts larger well fewer problems counts zero whereas estimate detailed takes account lexical information useful interpolating two estimates get bounce get sense strengths two estimates robustness also sensitivity lexical information lets look second parameter parameter well use similar method smoothed estimate im using parameters lambda lambda lambda distinguish lambda lambda lambda lambda lambda greater equal lambda plus lambda plus lambda equal usual constraints lets look different maximum likelihood estimates first one basically think schematically saw np vp saw maximum likelihood estimate going ratio two counts first count going number times ive seen entire configuration numerator im going count number times ive seen man particular position okay estimate basically conditioning entire structure entire rule head word saw second level throw away word saw simply say given structure head wood head wood whats probability seeing man position final level given np probability man head word np okay weve gone detailed estimate basically saying whats probability man subject saw one saying whats probability man subject irrespective head word finally saying whats probability man head np usual arguments estimate robust much less detailed less robust robust detailed inter attempt gain strengths estimates interpolating three things put altogether see know goal estimate parameter entire rule first decompose product two different parameters weve estimated using smoothing techniques look back counts weve used model weve used everything extremely detailed accounts number times man saw seen together particular configuration coarse accounts things like number times ive seen goes np vp essentially everything count see estimation regular pcfg right detailed lexicalized counts thats basically basically estimate parameters lexicalized pcfg tree bank read counts estimate parameters way ive shown details im going go important getting algorithms work really well let briefly mention give reference want feed read one although ive shown case lexicalized pcfgs chomsky normal form important deal rules one child example rule unknown tree bank one two three four children righthand side rule various ways simple way bi binarize rules flat rule like unknown head words brevity convert structure like follows intermediate cases might vp underbar something use vp underbar signify intermediate nonterminal within vp weve essentially done binarize grammar adding intermediate rules build structure thats simple way dealing ways sophisticated work slightly better turns addition associating headword example told nonterminal useful also propagate part speech information headword example v preposition prp personal pronoun complimentizer gives little bit information lexicalized rules dont lexical item also part speech help build refined estimators using smooth estimation techniques start rely parts speech useful another thing need modify grammars encode preferences close attachment going back example john believed shot bill much likely attach shot rather believed still havent really addressed problem modifications lexicalized pcfgs allow us learn kind preferences also useful want read ill post link paper wrote actually thesis work originates build built one lexicalized pcfg pauses goes details driving model takes account different steps,Course1,W4-S2-L5,W4,S2,L5,Parameter,4,2,5,let look second step deriv estim first step break paramet product two differ term essenti use chain rrule probabl first predict rule predict lexic condit rule togeth head word second step use smooth estim paramet estim two paramet left well appli exactli idea saw languag model exampl exampl im go use linear interpol go two paramet lambda lambda greater equal lambda plu lambda equal two maximum likelihood estim look first one go estim count saw head saw rewrit np vp second child vp head divid count saw maximum likelihood estim basic estim probabl saw rewrit particular rule natur symbol intuit estim second one basic back estim complet ignor lexic inform go back away condit word saw go count goe np vp divid count notic almost ident estim youd see regular pcfg essenti interpol two differ estim one condit lexicon inform isnt present pcfg throw away lexicon inform usual argument estim tend robust count larger well fewer problem count zero wherea estim detail take account lexic inform use interpol two estim get bounc get sens strength two estim robust also sensit lexic inform let look second paramet paramet well use similar method smooth estim im use paramet lambda lambda lambda distinguish lambda lambda lambda lambda lambda greater equal lambda plu lambda plu lambda equal usual constraint let look differ maximum likelihood estim first one basic think schemat saw np vp saw maximum likelihood estim go ratio two count first count go number time ive seen entir configur numer im go count number time ive seen man particular posit okay estim basic condit entir structur entir rule head word saw second level throw away word saw simpli say given structur head wood head wood what probabl see man posit final level given np probabl man head word np okay weve gone detail estim basic say what probabl man subject saw one say what probabl man subject irrespect head word final say what probabl man head np usual argument estim robust much less detail less robust robust detail inter attempt gain strength estim interpol three thing put altogeth see know goal estim paramet entir rule first decompos product two differ paramet weve estim use smooth techniqu look back count weve use model weve use everyth extrem detail account number time man saw seen togeth particular configur coars account thing like number time ive seen goe np vp essenti everyth count see estim regular pcfg right detail lexic count that basic basic estim paramet lexic pcfg tree bank read count estim paramet way ive shown detail im go go import get algorithm work realli well let briefli mention give refer want feed read one although ive shown case lexic pcfg chomski normal form import deal rule one child exampl rule unknown tree bank one two three four children righthand side rule variou way simpl way bi binar rule flat rule like unknown head word breviti convert structur like follow intermedi case might vp underbar someth use vp underbar signifi intermedi nontermin within vp weve essenti done binar grammar ad intermedi rule build structur that simpl way deal way sophist work slightli better turn addit associ headword exampl told nontermin use also propag part speech inform headword exampl v preposit prp person pronoun compliment give littl bit inform lexic rule dont lexic item also part speech help build refin estim use smooth estim techniqu start reli part speech use anoth thing need modifi grammar encod prefer close attach go back exampl john believ shot bill much like attach shot rather believ still havent realli address problem modif lexic pcfg allow us learn kind prefer also use want read ill post link paper wrote actual thesi work origin build built one lexic pcfg paus goe detail drive model take account differ step,[ 4  0 14 13 12]
56,Course1_W4-S2-L6_Evaluation_of_Lexicalized_PCFGs_Part_1_9-32,okay finish segment lexicalized pcfgs want talk evaluate different pausing models give insight accurate lexicalized pcfgs pausing ill actually describe two ways evaluating pauser heres first heres basis first widely method key insight take parse tree example tree represent set constituents constitutent consists label start point end point look np example number words sentence one two three four five end piece bands words one two inclusive constitutent one two similarly np vp spanning words three five spanning words one five particular path tree four different constituents note include parts speech within definition look levels higher tree parts speech short story take parse tree map set constituents human annotated parse trees act gold standard test data also output parser weve built trying evaluate lets see use idea evaluate parser basically calculating precision recall constituents ive defined lets illustrate specific example general ill situation gold standard tree test data sentence okay test sentence human annotated tree taken correct parser output also tree maybe exactly gold standard tree maybe partially correct may include substructures youve also seen gold standard tree trees matched representation described represented set constituents constituent consists label start point end point define various numbers ill define g number constituents seen gold standard tree case nd ill define p number constituents output parser case one two three four five six finally ill define c number constituents actually correct case also six mean correct means look constituent example see thats also gold tree thats correct constituent assuming go turn ill find theyre actually correct constituent worth noting method general enough allow cases number constituents two trees different important reality often happens aside rules binary branching really restriction binary branching rules number constiutents trees would actually identical reality tree banks allow rules nonbianary branching may end different numbers constituents two parse trees okay weve calculated three numbers g p c recall defined times c g basically saying gold standard constituents many recover correctly case times precision times c p case times saying constituents recover percentage correct particular example constituents recover actually correct ive recovered six seven think thats around gold standard constituents eight calculations im top head equal okay weve errors terms recall terms precision let give indication well different policies work particular metric using dataset weve talked lot penn wall street journal treebank using sentences thats think order million words something around training data sentences test data examples emphasize going consist sentence sequence words human annotated tree going able run parser two half thousand sentences compare parser outputs trees using recall precision precision well see constituents propose parser proportion correct thats precision also proportion gold standard constituents recover correctly thats recall okay first result pcfg following really see poorly pcfg performs said really disappointment first applied low terms recall precision parse trees look really quite dreadful actually look outputinaudible pcfg paper think first really accurate tree bank parser due david madigan created collaboration group ibm research produced huge amount seminal work early statistical natural language processing one things looked statistical parsing percent recall precision really breakthrough result way higher low numbers different model model ive models ive described far model based decision trees theyre sort bottom pausers going details main important point really first serious treatment parser really worked quite well lexicalized p c f gs described results work caseunknown thesis developing lexicalized p c f g give results signifigantly higher getting real conversations really number think terms accuracy lexicalized pcfgs recent results pushed performance maybe range ive listed different cases two methods make use discrimitive discriminative estimation methods well see methods later course see log linear models conditional random fields petros method makes use called latent variable pcfgs cases close connections lexicalized pcfgs certainly pcfgs later case,Course1,W4-S2-L6,W4,S2,L6,Evaluation,4,2,6,okay finish segment lexic pcfg want talk evalu differ paus model give insight accur lexic pcfg paus ill actual describ two way evalu pauser here first here basi first wide method key insight take pars tree exampl tree repres set constitu constitut consist label start point end point look np exampl number word sentenc one two three four five end piec band word one two inclus constitut one two similarli np vp span word three five span word one five particular path tree four differ constitu note includ part speech within definit look level higher tree part speech short stori take pars tree map set constitu human annot pars tree act gold standard test data also output parser weve built tri evalu let see use idea evalu parser basic calcul precis recal constitu ive defin let illustr specif exampl gener ill situat gold standard tree test data sentenc okay test sentenc human annot tree taken correct parser output also tree mayb exactli gold standard tree mayb partial correct may includ substructur youv also seen gold standard tree tree match represent describ repres set constitu constitu consist label start point end point defin variou number ill defin g number constitu seen gold standard tree case nd ill defin p number constitu output parser case one two three four five six final ill defin c number constitu actual correct case also six mean correct mean look constitu exampl see that also gold tree that correct constitu assum go turn ill find theyr actual correct constitu worth note method gener enough allow case number constitu two tree differ import realiti often happen asid rule binari branch realli restrict binari branch rule number constiut tree would actual ident realiti tree bank allow rule nonbianari branch may end differ number constitu two pars tree okay weve calcul three number g p c recal defin time c g basic say gold standard constitu mani recov correctli case time precis time c p case time say constitu recov percentag correct particular exampl constitu recov actual correct ive recov six seven think that around gold standard constitu eight calcul im top head equal okay weve error term recal term precis let give indic well differ polici work particular metric use dataset weve talk lot penn wall street journal treebank use sentenc that think order million word someth around train data sentenc test data exampl emphas go consist sentenc sequenc word human annot tree go abl run parser two half thousand sentenc compar parser output tree use recal precis precis well see constitu propos parser proport correct that precis also proport gold standard constitu recov correctli that recal okay first result pcfg follow realli see poorli pcfg perform said realli disappoint first appli low term recal precis pars tree look realli quit dread actual look outputinaud pcfg paper think first realli accur tree bank parser due david madigan creat collabor group ibm research produc huge amount semin work earli statist natur languag process one thing look statist pars percent recal precis realli breakthrough result way higher low number differ model model ive model ive describ far model base decis tree theyr sort bottom pauser go detail main import point realli first seriou treatment parser realli work quit well lexic p c f gs describ result work caseunknown thesi develop lexic p c f g give result signifigantli higher get real convers realli number think term accuraci lexic pcfg recent result push perform mayb rang ive list differ case two method make use discrimit discrimin estim method well see method later cours see log linear model condit random field petro method make use call latent variabl pcfg case close connect lexic pcfg certainli pcfg later case,[ 0  4 14 13 12]
57,Course1_W4-S2-L7_Evaluation_of_Lexicalized_PCFGs_Part_2_11-28,thats first evaluation method wanted describe idea looking precision recall recovering constituents want look second type evaluation based accuracy recovering called dependencies actually useful way getting insight accurate parser constructions well constructions well let take tree man saw dog telescope could gold standard tree could output localized gpc pauser see nontermials lexicalized key idea going abe convert tree set dependencies lets see okay basically grammar chomsky normal form every binary rule going single dependency lets skip one well go next second one first dependency says basically said column shows head word going show modifier word remember rules grammar take following form xh goes yh yw xh goes yw lets write yw two h okay essentially rule x goes w example head word h word w h w rule first dependency saying saw head man modifier rule go np vp dependency extracted rule top tree basically saying dependency relationship saw man two words sentence think rule production label type dependency involved basically signifying subject verb dependency goes back showed early parsing slides whenever see configuration kind form subject im sorry verb subject seeing dependencies reveal exactly kind grammatical relations ive slightly careful give word involved also position sentence one two three four five six seven eight saw position three case word fact word repeated multiple word repeated multiple times okay convert tree set dependencies basically represents critical grammatical relations within particular parse tree calculate precision recall convert gold standard trees dependency representations also convert parser output dependency relations see accurate recovering kind dependencies one thing note following go back slide second youll notice one two three four five six seven eight dependency one two three four five six seven eight words thats always going case im always going number dependencies words okay none business different power structure different number dependencies said special dependency word root trees saw ill specify rule case root head word root okay make sure also take account word root tree okay every tree whether gold standard tree parser output going number dependencies equal number words sentence actually report accuracy terms accurate recovering dependencies get dependency correct get two words dependency also get label ie rule correct well look back results phd thesis parser around dependency accuracy okay one nice property dependencies delve little deeper looking accuracy specifically precision recall recovering different recovery types example could look subject verb dependencies precisely could look depedencies label whenever see label know dependency involving subjects verb calculate precision recall terms recovering dependencies particular label recall subject verb would number subject verb dependencies correct divided number subject verb dependencies gold standard precision would number subject verb dependencies get correct number subject verb dependencies parsers output notice numbers may equal may ave different numbers subjectverb dependencies parsers go back idea calculating recall precision nice thing measure go dependency type dependency type figure dependencies recover high accuraciy dependencyies actually problematic numbers fact look subjectverb pairs dependency following form pretty well get recall precision look objectverb pairs basically going type vp goes vt np whenever see dependency like relationship verb object example say dog saw man im going dependency soar man labeled vp vt np man object soar recovered pretty high accuracy recall precision look arguments verbs general look productions involving verbs rules form add going dependency verb modified way perform pretty well percent recall precision problem problem cases though look prepositional phrase attachments going rules following form xh goes yh ppw x could noun phrase could verb phrase case prepositional phrase modifying something else see score low terms recall precision see thats pretty stark drop subject verb accuracies object verb accuracies reflects fact pp attachments challenging problem reality get high accuracy really need full world knowledge likely seman semantically speaking likely using telescope see likely looking something holding telescope kind decisions difficult thats reflected drop accuracy look coordination things like dog dogs houses cat cats could modify houses dogs even lower accuracy difficult take home think core structure policies quite good basic relations like subject verb object verb arguments verbs recovered high accuracy modifiers prepositional phrase attachments called nation ambiguities cases causing difficulty old results actually parser spite date like although think looked multiple parsers youd still see similar split hard sorry easy high accuracy dependencies lower accuracy dependencies summarize saw key weakness pcfgs lack sensitivity lexical information lecture ive described lexicalized pcfgs way getting around problem key steps firstly lexicalize tree bank using head rules allowed us go rules like goes npvp bs lexicalize rules like sore goes np dog vp sore use trick vastly increase number non terminals also number rules grammar second step estimate parameters lexicalized pcfg using smoothed estimation large number rules form poses challenges parameter estimation use smooth estimation techniques come robust estimators kind rule parameters finally terms accuracy pcfgs throughout recovering constituents dependencies saw cause unknownstructure core grammatical relationships like subject verb verb object recovered quite high accuracy cases prepositional phrases attachment still fairly challenging,Course1,W4-S2-L7,W4,S2,L7,Evaluation,4,2,7,that first evalu method want describ idea look precis recal recov constitu want look second type evalu base accuraci recov call depend actual use way get insight accur parser construct well construct well let take tree man saw dog telescop could gold standard tree could output local gpc pauser see nontermi lexic key idea go abe convert tree set depend let see okay basic grammar chomski normal form everi binari rule go singl depend let skip one well go next second one first depend say basic said column show head word go show modifi word rememb rule grammar take follow form xh goe yh yw xh goe yw let write yw two h okay essenti rule x goe w exampl head word h word w h w rule first depend say saw head man modifi rule go np vp depend extract rule top tree basic say depend relationship saw man two word sentenc think rule product label type depend involv basic signifi subject verb depend goe back show earli pars slide whenev see configur kind form subject im sorri verb subject see depend reveal exactli kind grammat relat ive slightli care give word involv also posit sentenc one two three four five six seven eight saw posit three case word fact word repeat multipl word repeat multipl time okay convert tree set depend basic repres critic grammat relat within particular pars tree calcul precis recal convert gold standard tree depend represent also convert parser output depend relat see accur recov kind depend one thing note follow go back slide second youll notic one two three four five six seven eight depend one two three four five six seven eight word that alway go case im alway go number depend word okay none busi differ power structur differ number depend said special depend word root tree saw ill specifi rule case root head word root okay make sure also take account word root tree okay everi tree whether gold standard tree parser output go number depend equal number word sentenc actual report accuraci term accur recov depend get depend correct get two word depend also get label ie rule correct well look back result phd thesi parser around depend accuraci okay one nice properti depend delv littl deeper look accuraci specif precis recal recov differ recoveri type exampl could look subject verb depend precis could look deped label whenev see label know depend involv subject verb calcul precis recal term recov depend particular label recal subject verb would number subject verb depend correct divid number subject verb depend gold standard precis would number subject verb depend get correct number subject verb depend parser output notic number may equal may ave differ number subjectverb depend parser go back idea calcul recal precis nice thing measur go depend type depend type figur depend recov high accuraciy dependencyi actual problemat number fact look subjectverb pair depend follow form pretti well get recal precis look objectverb pair basic go type vp goe vt np whenev see depend like relationship verb object exampl say dog saw man im go depend soar man label vp vt np man object soar recov pretti high accuraci recal precis look argument verb gener look product involv verb rule form add go depend verb modifi way perform pretti well percent recal precis problem problem case though look preposit phrase attach go rule follow form xh goe yh ppw x could noun phrase could verb phrase case preposit phrase modifi someth els see score low term recal precis see that pretti stark drop subject verb accuraci object verb accuraci reflect fact pp attach challeng problem realiti get high accuraci realli need full world knowledg like seman semant speak like use telescop see like look someth hold telescop kind decis difficult that reflect drop accuraci look coordin thing like dog dog hous cat cat could modifi hous dog even lower accuraci difficult take home think core structur polici quit good basic relat like subject verb object verb argument verb recov high accuraci modifi preposit phrase attach call nation ambigu case caus difficulti old result actual parser spite date like although think look multipl parser youd still see similar split hard sorri easi high accuraci depend lower accuraci depend summar saw key weak pcfg lack sensit lexic inform lectur ive describ lexic pcfg way get around problem key step firstli lexic tree bank use head rule allow us go rule like goe npvp bs lexic rule like sore goe np dog vp sore use trick vastli increas number non termin also number rule grammar second step estim paramet lexic pcfg use smooth estim larg number rule form pose challeng paramet estim use smooth estim techniqu come robust estim kind rule paramet final term accuraci pcfg throughout recov constitu depend saw caus unknownstructur core grammat relationship like subject verb verb object recov quit high accuraci case preposit phrase attach still fairli challeng,[14  0  4 13 12]
58,Course1_W5-S1-L1_Opening_Comments_0-25,okay next lectures going talk machine translation machine translation problem automatically translating one language another language one oldest problems artificial intelligence computer science think fascinating challenging problem problem clearly huge impacts implications,Course1,W5-S1-L1,W5,S1,L1,Opening,5,1,1,okay next lectur go talk machin translat machin translat problem automat translat one languag anoth languag one oldest problem artifici intellig comput scienc think fascin challeng problem problem clearli huge impact implic,[ 8  4 14 13 12]
59,Course1_W5-S1-L2_introduction_2-03,heres example translation system many would seen google translate case translating arabic english next lectures go main steps building modern translation system particular describing kind statistical methods google people use build translation systems first lecture translation want go number introductory topics first want talk challenges machine translation makes translation difficult problem want describe ill call classical approach machine translation rulebased approaches used early days translation still quite wide widely used give us grounding history machine translation approaches people thought first decades machine translation final part lecture want give brief introduction statistical translation systems statistical translation systems learn translation models large numbers examples translations go back early seminal seminal work researchers ibm developed first statistical translation systems last two decades theres considerable interest systems statistical methods form basis many state art systems including example google translate part lecture ill give brief introduction statistical mt next lectures well go quite detail describing models actually used statistical mt systems,Course1,W5-S1-L2,W5,S1,L2,introduction,5,1,2,here exampl translat system mani would seen googl translat case translat arab english next lectur go main step build modern translat system particular describ kind statist method googl peopl use build translat system first lectur translat want go number introductori topic first want talk challeng machin translat make translat difficult problem want describ ill call classic approach machin translat rulebas approach use earli day translat still quit wide wide use give us ground histori machin translat approach peopl thought first decad machin translat final part lectur want give brief introduct statist translat system statist translat system learn translat model larg number exampl translat go back earli semin semin work research ibm develop first statist translat system last two decad there consider interest system statist method form basi mani state art system includ exampl googl translat part lectur ill give brief introduct statist mt next lectur well go quit detail describ model actual use statist mt system,[ 8  4 14 13 12]
60,Course1_W5-S1-L3_Challenges_in_MT_8-06,okay lets talk challenges machine translation surprisingly problems ive described concerning ambiguity last lectures direct consequences machine translation problem fact ambiguity one main problems main challenges translation systems heres first example examples lexical ambiguity examples taken article bonnie dorr others several examples subsequent slides taken paper look word like book english two quite distinct meanings illustrated examples say book flight verb one meaning say read book noun quite different meaning many languages try translate book another language end different lexical choices depending sense book actually used particular example spanish book flight get one translation read book get another translation translation essentially going require us resolve lexical ambiguity two uses book actually used particular context well known difficult problem many examples lets look third example kill actually cases kill actually verb kill man versus kill process theyre somewhat different senses english could argue theyre similar theyre really rather distinct senses certainly go spanish youll end two quite different translations two different senses bottom line many cases translate accurately need resolve kind lexical ambiguities source language side another challenge weve described actually earlier course languages different languages differing word orders ill give brief recap argument argument saw lectures ago consider translation english japanese english systematically subject verb object say example dog saw cat whereas japanese predominantly subject object verb would say dog cat saw simple examples heres another one get longer sentences see kinds word order differences lead really quite complex differences word order two languages accurate translation going model differences word order challenging problem heres another example article dorr et al interesting property see syntactic structure necessarily preserved across translations different languages express concepts fundamentally different ways english sentence bottle floated cave spanish translation heres actually paraphrase translation youll see translation literally bottle entered cave floating whats going verb floated sense become essentially adverbial modifier floating adverb modifying entered preposition sense translated entered certainly strong influence choice verb case see kind verb becoming adverb preposition strong impact choice main verb clause theres necessarily direct correspondence syntactic structure two translations pose major challenges translation systems syntactic ambiguity surprisingly caused problems another example dorr et al little bit grim example john hit dog stick two possible analysis john could either using stick hitting dog could actually stick theres prepositional phrase ambiguity could either two attachments spanish side well actually get two quite different translations depending attachment first case attached well get sentence attaches dog end different translation short story particular case get accurate translation likely resolve prepositional phrasing attachment ambiguity correctly heres another example example need resolve pronouns reference translation pronoun resolution problem spoke think first introductory lecture class lets go example particular case english sentence computer outputs data fast pronoun could potentially refer either data computer two possibilities context clear refers computer particular example makes lot sense computer fast makes much less sense data fast pronoun resolution problem taking pronoun like deciding noun phrase context refers particular translation spanish translated es okay lets look second example computer outputs data stored ascii two potential noun phrases could refer case clear context refers data makes sense data stored ascii doesnt make sense computer stored ascii okay notice second case actually translated different pronoun spanish estan well first case es refers single noun phrase translated sense second case refers plural noun phrase get different form basically able translate word correctly spanish actually going first resolve noun phrase english refers decide whether noun phrase refers plural singular thereby choose one two different pronoun forms problem ambiguity sentences ambiguous least resolve pronouns refer depending disambiguation decisions end different translations,Course1,W5-S1-L3,W5,S1,L3,Challenges,5,1,3,okay let talk challeng machin translat surprisingli problem ive describ concern ambigu last lectur direct consequ machin translat problem fact ambigu one main problem main challeng translat system here first exampl exampl lexic ambigu exampl taken articl bonni dorr other sever exampl subsequ slide taken paper look word like book english two quit distinct mean illustr exampl say book flight verb one mean say read book noun quit differ mean mani languag tri translat book anoth languag end differ lexic choic depend sens book actual use particular exampl spanish book flight get one translat read book get anoth translat translat essenti go requir us resolv lexic ambigu two use book actual use particular context well known difficult problem mani exampl let look third exampl kill actual case kill actual verb kill man versu kill process theyr somewhat differ sens english could argu theyr similar theyr realli rather distinct sens certainli go spanish youll end two quit differ translat two differ sens bottom line mani case translat accur need resolv kind lexic ambigu sourc languag side anoth challeng weve describ actual earlier cours languag differ languag differ word order ill give brief recap argument argument saw lectur ago consid translat english japanes english systemat subject verb object say exampl dog saw cat wherea japanes predominantli subject object verb would say dog cat saw simpl exampl here anoth one get longer sentenc see kind word order differ lead realli quit complex differ word order two languag accur translat go model differ word order challeng problem here anoth exampl articl dorr et al interest properti see syntact structur necessarili preserv across translat differ languag express concept fundament differ way english sentenc bottl float cave spanish translat here actual paraphras translat youll see translat liter bottl enter cave float what go verb float sens becom essenti adverbi modifi float adverb modifi enter preposit sens translat enter certainli strong influenc choic verb case see kind verb becom adverb preposit strong impact choic main verb claus there necessarili direct correspond syntact structur two translat pose major challeng translat system syntact ambigu surprisingli caus problem anoth exampl dorr et al littl bit grim exampl john hit dog stick two possibl analysi john could either use stick hit dog could actual stick there preposit phrase ambigu could either two attach spanish side well actual get two quit differ translat depend attach first case attach well get sentenc attach dog end differ translat short stori particular case get accur translat like resolv preposit phrase attach ambigu correctli here anoth exampl exampl need resolv pronoun refer translat pronoun resolut problem spoke think first introductori lectur class let go exampl particular case english sentenc comput output data fast pronoun could potenti refer either data comput two possibl context clear refer comput particular exampl make lot sens comput fast make much less sens data fast pronoun resolut problem take pronoun like decid noun phrase context refer particular translat spanish translat es okay let look second exampl comput output data store ascii two potenti noun phrase could refer case clear context refer data make sens data store ascii doesnt make sens comput store ascii okay notic second case actual translat differ pronoun spanish estan well first case es refer singl noun phrase translat sens second case refer plural noun phrase get differ form basic abl translat word correctli spanish actual go first resolv noun phrase english refer decid whether noun phrase refer plural singular therebi choos one two differ pronoun form problem ambigu sentenc ambigu least resolv pronoun refer depend disambigu decis end differ translat,[ 8  0  4 14 13]
61,Course1_W5-S1-L4_Classical_Approaches_to_MT_Part_1_8-02,okay next segment going go classical machine translation im going give high level description various systems people developed first decades machine translation thisll give us useful context thinking might design different machine translation systems systems im going describe rule based methods methods humans would try hand build translation systems example compiling dictionaries bilingual dictionaries specifying words lang one language would translate words another language early machine translation systems made use method called direct machine translation methods essentially performed translation process word word sentence one language youd essentially word word try map words language little analysis source language text okay imagine might useful maybe pause least tag source language set text get idea structure underlying sentence youre trying translate direct machine translation systems really away analysis form relied large bilingual dictionaries ill give example next slide word source language would specify set rules translate word target language terms reordering theres always problem words sentence one language may want reorder things may want move things around direct translation systems usually simple set reordering rules applied wordforword translation performed example english see adjective like blue noun dog whereas french sorry french terrible think correct would see dog translating word blue translating word notice order reversed would need set rules cover kind simple reordering phenomena example set direct translation rules taken jurafsky martin textbook originally think russian system thats actually think fascinating look set rules translating word much many english russian imagine somebody human looked lot example translations maybe used intuition two languages derive set rules form translation based surrounding context rules say proceeding word much would translate one way hand preceding word much might translate different way otherwise word much rather many would go rules deriving kind rules pretty painstaking task imagine reasonable translation system might least thousand words language need translate would compile set rules different word language direct machine translation systems rather limited let go key problems face one following difficult nearly impossible capture kind word order differences saw diff earlier different languages english japanese example could imagine trying write set rules given english string predict word order target side japanese side thats hard analysis english string dont know words verbs versus versus nouns certainly syntactic structure thats one challenge second one words translated without knowledge role syntactic role play sentence take word example english take two quite different syntactic roles one complementizer clause say said like ice cream one particular sense word hand say like ice cream actually determiner would complementizer would determiner many cases translate different language end different translations two different context analysis english source language theres really way able distinguish two cases translate correctly problems direct translation systems led people consider called transfer based approaches heres sketch work sentence say english trying map sentence french sake example transfer based systems would form translation three different steps first thing would english side analysis example might find parse tree english representation syntactic semantic structure english thats analysis stage next stage perform called transfer transform english parse treesound somehow transform french english french parse tree perform transfer step use set rules maybe rather similar example showed russian earlier rules critically refer syntactic structure english side information hopefully perform better job transferring across french french side finally stage called generation take french parse tree produce french strings usually least syntactic structure simple step remove tree produce string transfer based approaches might use various levels representation intermediate stage might use syntactic structures might use something closer symantic structure closure representation meaning two languages,Course1,W5-S1-L4,W5,S1,L4,Classical,5,1,4,okay next segment go go classic machin translat im go give high level descript variou system peopl develop first decad machin translat thisll give us use context think might design differ machin translat system system im go describ rule base method method human would tri hand build translat system exampl compil dictionari bilingu dictionari specifi word lang one languag would translat word anoth languag earli machin translat system made use method call direct machin translat method essenti perform translat process word word sentenc one languag youd essenti word word tri map word languag littl analysi sourc languag text okay imagin might use mayb paus least tag sourc languag set text get idea structur underli sentenc your tri translat direct machin translat system realli away analysi form reli larg bilingu dictionari ill give exampl next slide word sourc languag would specifi set rule translat word target languag term reorder there alway problem word sentenc one languag may want reorder thing may want move thing around direct translat system usual simpl set reorder rule appli wordforword translat perform exampl english see adject like blue noun dog wherea french sorri french terribl think correct would see dog translat word blue translat word notic order revers would need set rule cover kind simpl reorder phenomena exampl set direct translat rule taken jurafski martin textbook origin think russian system that actual think fascin look set rule translat word much mani english russian imagin somebodi human look lot exampl translat mayb use intuit two languag deriv set rule form translat base surround context rule say proceed word much would translat one way hand preced word much might translat differ way otherwis word much rather mani would go rule deriv kind rule pretti painstak task imagin reason translat system might least thousand word languag need translat would compil set rule differ word languag direct machin translat system rather limit let go key problem face one follow difficult nearli imposs captur kind word order differ saw diff earlier differ languag english japanes exampl could imagin tri write set rule given english string predict word order target side japanes side that hard analysi english string dont know word verb versu versu noun certainli syntact structur that one challeng second one word translat without knowledg role syntact role play sentenc take word exampl english take two quit differ syntact role one complement claus say said like ice cream one particular sens word hand say like ice cream actual determin would complement would determin mani case translat differ languag end differ translat two differ context analysi english sourc languag there realli way abl distinguish two case translat correctli problem direct translat system led peopl consid call transfer base approach here sketch work sentenc say english tri map sentenc french sake exampl transfer base system would form translat three differ step first thing would english side analysi exampl might find pars tree english represent syntact semant structur english that analysi stage next stage perform call transfer transform english pars treesound somehow transform french english french pars tree perform transfer step use set rule mayb rather similar exampl show russian earlier rule critic refer syntact structur english side inform hope perform better job transfer across french french side final stage call gener take french pars tree produc french string usual least syntact structur simpl step remov tree produc string transfer base approach might use variou level represent intermedi stage might use syntact structur might use someth closer symant structur closur represent mean two languag,[ 8  4  0 14 13]
62,Course1_W5-S1-L5_Classical_Approaches_to_MT_Part_2_5-56,one useful diagram people often use characterize different translation systems called translation pyramid looks like following english lets say im trying translate french direct translate systems sort directly translate english french draw arrow transferbased systems following analysis schematically get point little bit pyramid transfer stage english parse tree example french parse tree finally generation stage intuition behind pyramid bit analysis less far go transfer state made made transfer prompt lot easier purely direct case course might various levels analysis might method going bit higher pyramid case becomes even easier translate course may harder form deeper analysis theres tradeoff final type translation system takes idea extreme whats called interlinguabased translation case two phases first step analysis step going analyze source language sentence hope languageindependent representation meaning actually think point apex pyramid languageindependent representation meaning actually analysis step go way languageindependent representation secondly generation step take language independent representation generate foreign language okay critically going depend definition interlingua representation meeting languages independent language potential advantages method one following transferbased system lets say n different languages n equals n equals want build translation systems different pairs languages transferbased systems every different pair languages going build different transfer lexicon going rebuild transfer component system case n squared approximately equal case n squared approximately equal okay thats lot work building different transfer components different language pairs appealing thing interlinguabased system need build n analysis components n languages build component takes say english resolves language independent representation need n generation components lets remember generation component would take languageindependent representation generate say french string english string okay weve done away transfer component theory need n analysis components n generation components saving downside interlinguabased approach really difficult come truly languageindependent representation meaning might even philosophical point view impossible problem heres one example difficult present different concepts interlingua different languages tend break concepts world quite different ways let give example english single word wall german therere actually two words wall one wall internal house wall within house one external wall wall outside house two different words two different concepts japanese two words brother one word elder brother one word younger brother spanish two words english word leg use one word leg human another word leg animal leg piece furniture table example go language language find language way breaking world different concepts youre careful interlingua simply going intersection ways different ways breaking things well put another way difficult imagine priori way different ways could break concepts world every time add new language translation system chances surprised find distinction made language hadnt thought designed interlingua bottom line designing interligua might might difficult task kind conceptual problems,Course1,W5-S1-L5,W5,S1,L5,Classical,5,1,5,one use diagram peopl often use character differ translat system call translat pyramid look like follow english let say im tri translat french direct translat system sort directli translat english french draw arrow transferbas system follow analysi schemat get point littl bit pyramid transfer stage english pars tree exampl french pars tree final gener stage intuit behind pyramid bit analysi less far go transfer state made made transfer prompt lot easier pure direct case cours might variou level analysi might method go bit higher pyramid case becom even easier translat cours may harder form deeper analysi there tradeoff final type translat system take idea extrem what call interlinguabas translat case two phase first step analysi step go analyz sourc languag sentenc hope languageindepend represent mean actual think point apex pyramid languageindepend represent mean actual analysi step go way languageindepend represent secondli gener step take languag independ represent gener foreign languag okay critic go depend definit interlingua represent meet languag independ languag potenti advantag method one follow transferbas system let say n differ languag n equal n equal want build translat system differ pair languag transferbas system everi differ pair languag go build differ transfer lexicon go rebuild transfer compon system case n squar approxim equal case n squar approxim equal okay that lot work build differ transfer compon differ languag pair appeal thing interlinguabas system need build n analysi compon n languag build compon take say english resolv languag independ represent need n gener compon let rememb gener compon would take languageindepend represent gener say french string english string okay weve done away transfer compon theori need n analysi compon n gener compon save downsid interlinguabas approach realli difficult come truli languageindepend represent mean might even philosoph point view imposs problem here one exampl difficult present differ concept interlingua differ languag tend break concept world quit differ way let give exampl english singl word wall german therer actual two word wall one wall intern hous wall within hous one extern wall wall outsid hous two differ word two differ concept japanes two word brother one word elder brother one word younger brother spanish two word english word leg use one word leg human anoth word leg anim leg piec furnitur tabl exampl go languag languag find languag way break world differ concept your care interlingua simpli go intersect way differ way break thing well put anoth way difficult imagin priori way differ way could break concept world everi time add new languag translat system chanc surpris find distinct made languag hadnt thought design interlingua bottom line design interligua might might difficult task kind conceptu problem,[ 8  4 14 13 12]
63,Course1_W5-S1-L6_Introduction_to_Statistical_MT_12-31,final segment want make brief introduction statistical methods machine translation statistical methods focus next lectures class heres basic idea key observation socalled parallel corpora available many language pairs parallel corpus lets say english french consists set example translations elements corpus english sentence paired french sentence sentence pairs sentences english associated translation french basically going treat translation supervised learning problem e f examples heres english sentence f french sentence assuming trying build translation system english french french english might many thousands example translations turns kind corpora exist across many language pairs one classic example early work ibm really disseminal work area translation used corpus called canadian hansards proceedings canadian parliament anything said canadian parliament written english french actually millions example translations english french many language pairs resources example europarl corpus corpus drawn european parliament contains think european languages translations different languages simultaneously first statistical machine translation systems came early largely due researchers ibm said basic idea take examples translations try learn statistical model given new sentence say french translate english surface sort radical rather preposterous view completely different rule based approaches developed prior early actually highly successful approach last years theres huge amount research statistical machine translation gotten point commercial system google translate used precisely technology idea actually old one goes back warren weaver second world war suggested applying statistical methods cryptography translation actually go next slide quote heres quote warren weaver letter norbert wiener spurred great successes second world war statistical methods used decrypt encrypted messages says following one naturally wonders problem translation could conceivably treated problem cryptography look article russian say really written english coded strange symbols proceed decode warren weaver sort preposterous idea using statistical cryptographic methods try perform translation lets talk little bit ibm researchers actually envisioned translation process idea apply noisy channel model saw lectures tagging models idea apply noisy channel model translation convention lets assume goal build translation system french english want something maps french sentence english sentence e know natural way try try build conditional model pe given f french sentence f consider every possible sentence english turn large set course infinite set sentences assume sentences probability e f conditional probability given im translating f e output noisy channel model going use generative model going use bayes rule kind flip problem around heres goes noisy channel model translation going two components p e language model assign probabilities sentences example like dog laughs stop example might well trigram language model exactly saw first week class okay model learn large amounts english data alone second part model whats called translation model model p f given e notice reversed direction often see noisy channel model pf given e english sentence example dog laughs consider possible french sentences gain infinite set going model pf given e french sentence conditioned english sentence okay model going estimated set translation examples okay going estimated bilingual corpus parallel corpus okay saw noisy channel model two models actually derive p given f using bayes rule usual rules probability pef f pe times p f given divided sum english sentences p times p f given e okay given new french sentence f going output likely translation model means going search sentence e maximizes condition probability plug formula notice denominator constant respect english sentence searching constant term isnt needed considering argmax saw exactly trick saw noisy channel model hidden alcohol earlier class end following problem remember problem take french sentence f produce translation e going search sentence e maximizes product two terms term language model term prior probability english sentence e second term conditional probability probability french sentence f given english sentence e course weve seen build language model using trigram model next lectures well see build models pf given e translation model thats going critical new component translation systems notes noisy channel model said language model estimated data actually dont need parallel corpus estimate parameters language model means actually leverage potentially large quantities text english alone translation model trained parallel corpus consisting french english sentence pairs notice even though goal translate f french english weve used noisy channel model estimate p f given e okay things flipped translation model sense backwards one big advantage model makes use language model english sentences gives us strong source information sentences likely grammatical fluent english side make lot difficiencies translation model next class well talk build model f given e finally going left problem given french sentence f finding english sentence maximizes product terms also challenging problem set searching large set possible translations well talk lot problem little later class heres example noisy channel approach tutorial philip cohen kevin knight imagine trying translate case spanish english many many possible translations many possible english sentences weve shown possibilities case weve shown conditional probability spanish given english translation model okay might get various terms notice ps given e pe given okay part translation model actually backwards really need thats one component model evaluate probability different translations one component take account p given e notice use part model alone actually come translation pretty bad translation heres full model notice evaluate probability endless alternatives actually take product two terms p sq e times p e model trigram language model english sentence english sentences calculate probability trigram language model sentences course much likely others particular sentence much likely sentences multiply together two terms youll find highest probability translation actually one high level important part evaluate plausibility examples translation input multiply two things firstly p e prior probability language model seeing sentence english secondly p given e probability seeing spanish sentence given english sentence underlying process,Course1,W5-S1-L6,W5,S1,L6,Introduction,5,1,6,final segment want make brief introduct statist method machin translat statist method focu next lectur class here basic idea key observ socal parallel corpora avail mani languag pair parallel corpu let say english french consist set exampl translat element corpu english sentenc pair french sentenc sentenc pair sentenc english associ translat french basic go treat translat supervis learn problem e f exampl here english sentenc f french sentenc assum tri build translat system english french french english might mani thousand exampl translat turn kind corpora exist across mani languag pair one classic exampl earli work ibm realli dissemin work area translat use corpu call canadian hansard proceed canadian parliament anyth said canadian parliament written english french actual million exampl translat english french mani languag pair resourc exampl europarl corpu corpu drawn european parliament contain think european languag translat differ languag simultan first statist machin translat system came earli larg due research ibm said basic idea take exampl translat tri learn statist model given new sentenc say french translat english surfac sort radic rather preposter view complet differ rule base approach develop prior earli actual highli success approach last year there huge amount research statist machin translat gotten point commerci system googl translat use precis technolog idea actual old one goe back warren weaver second world war suggest appli statist method cryptographi translat actual go next slide quot here quot warren weaver letter norbert wiener spur great success second world war statist method use decrypt encrypt messag say follow one natur wonder problem translat could conceiv treat problem cryptographi look articl russian say realli written english code strang symbol proceed decod warren weaver sort preposter idea use statist cryptograph method tri perform translat let talk littl bit ibm research actual envis translat process idea appli noisi channel model saw lectur tag model idea appli noisi channel model translat convent let assum goal build translat system french english want someth map french sentenc english sentenc e know natur way tri tri build condit model pe given f french sentenc f consid everi possibl sentenc english turn larg set cours infinit set sentenc assum sentenc probabl e f condit probabl given im translat f e output noisi channel model go use gener model go use bay rule kind flip problem around here goe noisi channel model translat go two compon p e languag model assign probabl sentenc exampl like dog laugh stop exampl might well trigram languag model exactli saw first week class okay model learn larg amount english data alon second part model what call translat model model p f given e notic revers direct often see noisi channel model pf given e english sentenc exampl dog laugh consid possibl french sentenc gain infinit set go model pf given e french sentenc condit english sentenc okay model go estim set translat exampl okay go estim bilingu corpu parallel corpu okay saw noisi channel model two model actual deriv p given f use bay rule usual rule probabl pef f pe time p f given divid sum english sentenc p time p f given e okay given new french sentenc f go output like translat model mean go search sentenc e maxim condit probabl plug formula notic denomin constant respect english sentenc search constant term isnt need consid argmax saw exactli trick saw noisi channel model hidden alcohol earlier class end follow problem rememb problem take french sentenc f produc translat e go search sentenc e maxim product two term term languag model term prior probabl english sentenc e second term condit probabl probabl french sentenc f given english sentenc e cours weve seen build languag model use trigram model next lectur well see build model pf given e translat model that go critic new compon translat system note noisi channel model said languag model estim data actual dont need parallel corpu estim paramet languag model mean actual leverag potenti larg quantiti text english alon translat model train parallel corpu consist french english sentenc pair notic even though goal translat f french english weve use noisi channel model estim p f given e okay thing flip translat model sens backward one big advantag model make use languag model english sentenc give us strong sourc inform sentenc like grammat fluent english side make lot diffici translat model next class well talk build model f given e final go left problem given french sentenc f find english sentenc maxim product term also challeng problem set search larg set possibl translat well talk lot problem littl later class here exampl noisi channel approach tutori philip cohen kevin knight imagin tri translat case spanish english mani mani possibl translat mani possibl english sentenc weve shown possibl case weve shown condit probabl spanish given english translat model okay might get variou term notic ps given e pe given okay part translat model actual backward realli need that one compon model evalu probabl differ translat one compon take account p given e notic use part model alon actual come translat pretti bad translat here full model notic evalu probabl endless altern actual take product two term p sq e time p e model trigram languag model english sentenc english sentenc calcul probabl trigram languag model sentenc cours much like other particular sentenc much like sentenc multipli togeth two term youll find highest probabl translat actual one high level import part evalu plausibl exampl translat input multipli two thing firstli p e prior probabl languag model see sentenc english secondli p given e probabl see spanish sentenc given english sentenc underli process,[ 8  4  3  1 14]
64,Course1_W5-S2-L1_Introduction_3-24,okay next segment class going talk ibm translation models ibm models go back late early seminal really starting whole statistical approach translation problem form central part modern statistical translation systems well cover models detail segment class recap slide last segment class recap noisy channel model introduced ibm researches translation noisy channel model said two components firstly p e language model model assigns probability sentence english secondly p f given e whats called translation model assigns model french sentence given english sentence one note throughout lecture ill follow convention ill always assume translating french sentence english sentence goal okay course might languages translating purposes lecture well always assume translating french english okay language model strings english translation model said sense backwards says whats conditional probability french sentence given english sentence translate model search english sentence maximizes product p e p f given e evaluate particular sentence f different english translations take terms account roadmap next lectures course translation follows im first going describe ibm models ibm went series models think going first two essentially introduce many important ideas actually model pretty decent model problem going looking well go describe phrasebased models phrasedbased models invented around late theyre sense second generation school translation systems first generation make direct use ideas ibm models ibm models sense form basis phrasebased models phrasebased models work much much better ibm models fact form basis many modern statistical translation systems example google translate heavily built technology well first talk ibm model well talk ibm model finally well talk parameter estimation models actually learn parameters models data consisting example translations,Course1,W5-S2-L1,W5,S2,L1,Introduction,5,2,1,okay next segment class go talk ibm translat model ibm model go back late earli semin realli start whole statist approach translat problem form central part modern statist translat system well cover model detail segment class recap slide last segment class recap noisi channel model introduc ibm research translat noisi channel model said two compon firstli p e languag model model assign probabl sentenc english secondli p f given e what call translat model assign model french sentenc given english sentenc one note throughout lectur ill follow convent ill alway assum translat french sentenc english sentenc goal okay cours might languag translat purpos lectur well alway assum translat french english okay languag model string english translat model said sens backward say what condit probabl french sentenc given english sentenc translat model search english sentenc maxim product p e p f given e evalu particular sentenc f differ english translat take term account roadmap next lectur cours translat follow im first go describ ibm model ibm went seri model think go first two essenti introduc mani import idea actual model pretti decent model problem go look well go describ phrasebas model phrasedbas model invent around late theyr sens second gener school translat system first gener make direct use idea ibm model ibm model sens form basi phrasebas model phrasebas model work much much better ibm model fact form basi mani modern statist translat system exampl googl translat heavili built technolog well first talk ibm model well talk ibm model final well talk paramet estim model actual learn paramet model data consist exampl translat,[ 8  4  1 14 13]
65,Course1_W5-S2-L2_IBM_Model_1_Part_1_13-06,okay lets talk ibm model one critical question define model p f given e might example english sentence like dog barks e french sentence say sentence believe reasonable translation french french terrible want assign pair sentences conditional probability generally one many possible translations english wanted find distribution possible french sentences paired english sentence ive seen throughout english sentence looking l words french sentence looking words l length two sentences might try model probability directly intermediate structure turns difficult thing absolutely critical idea ibm model define idea whats called alignment sentences okay case number french words equals number english words l equals alignment going sequence values case basically french word specifies english word aligned heres one possible alignment say le aligned word aligned line word aligned okay formally alignment variables take values l l length english sentences talk second included particular line equals equals equals possible alignment point say example equal means actually assume word aligned whats called null word ibm researchers found useful include extra word null could used generate french words sentence intuitively corresponds french words natural word align english okay many possible alignments possible words sorry l words english side e possible words french side french words aligned l plus l l plus possible alignments word im choosing alignment words l plus number possible alignments alignment structure french word aligned single english word heres example example englishfrench sentence case number english words one two three four five six l equals number french words one two three four five six seven equals seven one alignment following lets number english words french words basically alignment going sequence numbers example specifying word word french english word aligned saying word aligned word english word french aligned word word french aligned word word french aligned word next words aligned english word okay one possible setting alignment variables thats probably pretty good setting alignment variables okay reasonable job saying various words french correspond english side thats one possible alignment let show another possible alignment one saying every french word aligned english word draw like follows okay french word aligned single english word case theyre aligned word one english side clearly bad explanation translation case bad alignment next idea define model assigns conditional probability alignment paired translation f conditioned two things conditioned english sentence length french translation visualize follows might english sentence example dog laughs lets say equal know three french words f f f case okay words could take value unknown could different words french thats one choice positions going choose french word addition going choose alignment space possible alignments example might showed alignment words le would one particular choice would probability alignments case sorry conditioned dog laughs fact french translation also length okay actually going decompose product two models well describe soon define two models first going distribution possible alignments conditioned english sentence length french going define distribution possible alignments remember l plus pair possible values secondly going define second model conditioned alignment english sentence english sentence length assigns probability possible french translation english sentence weve done recover sort end goal define probability french sentence given english sentence summing possible alignments follows usual rules probability entire product pf given e usual rules probability marginalize l alignment variable sum get model p f given e okay thats basic trick introducing alignments models important thing allows us introduce intermediate variables alignments give us explanation translation process well see end natural parameterization two models sum space possible alignments get final distribution f need bottom line estimating probability f given e directly difficult instead come model p f given e sorry conditioned length guess say p f given em equal sum p f given e important byproduct process model form actually given english sentence french sentence find likely alignment english sentence french sentence every possible alignment two sentences evaluate probability pick likely alignment using usual rules probability say p given f e model term numerator denominator sum possible alignments p f given e equation follows usual rules probability given f e pair find likely alignment amongst space possible alignments im going go detail done described notes provided accompany accompany class actually ibm models one two compute likely alignment efficiently fact nowadays ibm models rarely ever used actual translation even though originally designed translation theyre rarely used translation play critical role allowing us recover alignments sentences weve trained parameters ibm model sentence sentence pair training set find likely alignment sentence pairs heres actual example frenchenglish translation using ibm alignment models translation models french english ive shown likely alignment actually fact english word alignment french word fact model trained opposite direction p e give f sum alignments p e given f kind model trained reverse direction alignment english word aligned single french word lets look saying aligned le know french english pretty much correct council aligned word alignments actually quite good become important phrase based translation systems well see next couple lectures class essentially given us anchor words english aligned words french okay bottom line weve trained ibm models one two look training data sentences consists sentence pairs actually find likely alignments point much better handle two sentences correspond,Course1,W5-S2-L2,W5,S2,L2,IBM,5,2,2,okay let talk ibm model one critic question defin model p f given e might exampl english sentenc like dog bark e french sentenc say sentenc believ reason translat french french terribl want assign pair sentenc condit probabl gener one mani possibl translat english want find distribut possibl french sentenc pair english sentenc ive seen throughout english sentenc look l word french sentenc look word l length two sentenc might tri model probabl directli intermedi structur turn difficult thing absolut critic idea ibm model defin idea what call align sentenc okay case number french word equal number english word l equal align go sequenc valu case basic french word specifi english word align here one possibl align say le align word align line word align okay formal align variabl take valu l l length english sentenc talk second includ particular line equal equal equal possibl align point say exampl equal mean actual assum word align what call null word ibm research found use includ extra word null could use gener french word sentenc intuit correspond french word natur word align english okay mani possibl align possibl word sorri l word english side e possibl word french side french word align l plu l l plu possibl align word im choos align word l plu number possibl align align structur french word align singl english word here exampl exampl englishfrench sentenc case number english word one two three four five six l equal number french word one two three four five six seven equal seven one align follow let number english word french word basic align go sequenc number exampl specifi word word french english word align say word align word english word french align word word french align word word french align word next word align english word okay one possibl set align variabl that probabl pretti good set align variabl okay reason job say variou word french correspond english side that one possibl align let show anoth possibl align one say everi french word align english word draw like follow okay french word align singl english word case theyr align word one english side clearli bad explan translat case bad align next idea defin model assign condit probabl align pair translat f condit two thing condit english sentenc length french translat visual follow might english sentenc exampl dog laugh let say equal know three french word f f f case okay word could take valu unknown could differ word french that one choic posit go choos french word addit go choos align space possibl align exampl might show align word le would one particular choic would probabl align case sorri condit dog laugh fact french translat also length okay actual go decompos product two model well describ soon defin two model first go distribut possibl align condit english sentenc length french go defin distribut possibl align rememb l plu pair possibl valu secondli go defin second model condit align english sentenc english sentenc length assign probabl possibl french translat english sentenc weve done recov sort end goal defin probabl french sentenc given english sentenc sum possibl align follow usual rule probabl entir product pf given e usual rule probabl margin l align variabl sum get model p f given e okay that basic trick introduc align model import thing allow us introduc intermedi variabl align give us explan translat process well see end natur parameter two model sum space possibl align get final distribut f need bottom line estim probabl f given e directli difficult instead come model p f given e sorri condit length guess say p f given em equal sum p f given e import byproduct process model form actual given english sentenc french sentenc find like align english sentenc french sentenc everi possibl align two sentenc evalu probabl pick like align use usual rule probabl say p given f e model term numer denomin sum possibl align p f given e equat follow usual rule probabl given f e pair find like align amongst space possibl align im go go detail done describ note provid accompani accompani class actual ibm model one two comput like align effici fact nowaday ibm model rare ever use actual translat even though origin design translat theyr rare use translat play critic role allow us recov align sentenc weve train paramet ibm model sentenc sentenc pair train set find like align sentenc pair here actual exampl frenchenglish translat use ibm align model translat model french english ive shown like align actual fact english word align french word fact model train opposit direct p e give f sum align p e given f kind model train revers direct align english word align singl french word let look say align le know french english pretti much correct council align word align actual quit good becom import phrase base translat system well see next coupl lectur class essenti given us anchor word english align word french okay bottom line weve train ibm model one two look train data sentenc consist sentenc pair actual find like align point much better handl two sentenc correspond,[ 3  4  8 14 13]
66,Course1_W5-S2-L3_IBM_Model_1_Part_2_9-01,okay lets looks ibm model defines two probabilities remember two parts model conditional probability alignment given english sentence length french sentence secondly p f given e im going go two side model turn firstly alignment model ibm model simplest model think assuming every possible alignment equally likely remember english sentences null word ill call e e e e e l french sentence lengths positions french side going assume positions l plus alignments word example aligned different things probability l plus l plus possibilities null word plus english positions french words choose overall probability going l plus n assuming every possible alignment equally likely dumbest possible model major simplifying assumption gets things started get us ground lets talk second part model model p f given aem whats going english sentence e example might dog barks ill write null th word sentence length french sentence say equals positions alignments sake argument lets assume alignment first word french aligned first english second word french second word english third word french third word english okay calculate distribution possible french sentences let give particular example lets assume f f finally f calculate conditional probability particular french sentence conditioning entire english sentence alignment length french sentence particular case going product three terms well le given intuitively parameter corresponding conditional probability french word le given aligned english word probability english word emitting french word le translation second term second word given dog final translation third word given barks one parameter french words condition identity french word english word aligned abstract form follows product j equals remember length french sentence going position position french sentence point f sub j thats jth word conditioned e sub j identity english word jth french word aligned essentially assuming french words filled turn independently french words actually conditioned identity english word aligned heres another example goes coming back example showed earlier assume english sentence french sentence english length six french word french length seven lets say say alignment ill draw alignment looks like particular case sorry also conditioning case particular case probability seeing particular french sentence conditioned e alignment product terms notice one term french word one term seven french words point condition english word aligned le lined example programme lined program intuitively ibm model corresponds following generative process english string example might dog barks length english string example equals three possible french words heres proceed firstly pick alignment uniformly random space possible alignment might example pick alignment like okay done fill french words product probabilities might example pick three words andcough part expression would probability le given dog times chien given times aboie given course unlikely alignment given two sentences nevertheless possible intuitively second step generative processes french position choose french word random distribution defined example word chosen distribution word given dog word chosen distribution word given first step choose alignment uniform random second word second step fill french words one one conditioning english words aligned final result model following forms conditional probability f given e product two terms firstly p secondly f given e thats application chain rule given particular form ibm model final expression following corresponds uniform probability alignments corresponds picking french word based english word aligned later class well see estimate parameters okay going parallel corpus input model going get f e english words e french words f like f get conditional probability ive shown one entry model actually trained real data english word position list probable words french trained parameters model see actually done reasonable job know french seems like reasonable set translation probabilities position translated word translated position french possibilities like word okay typical kind lexicon might get output process english word get probability every possible french word,Course1,W5-S2-L3,W5,S2,L3,IBM,5,2,3,okay let look ibm model defin two probabl rememb two part model condit probabl align given english sentenc length french sentenc secondli p f given e im go go two side model turn firstli align model ibm model simplest model think assum everi possibl align equal like rememb english sentenc null word ill call e e e e e l french sentenc length posit french side go assum posit l plu align word exampl align differ thing probabl l plu l plu possibl null word plu english posit french word choos overal probabl go l plu n assum everi possibl align equal like dumbest possibl model major simplifi assumpt get thing start get us ground let talk second part model model p f given aem what go english sentenc e exampl might dog bark ill write null th word sentenc length french sentenc say equal posit align sake argument let assum align first word french align first english second word french second word english third word french third word english okay calcul distribut possibl french sentenc let give particular exampl let assum f f final f calcul condit probabl particular french sentenc condit entir english sentenc align length french sentenc particular case go product three term well le given intuit paramet correspond condit probabl french word le given align english word probabl english word emit french word le translat second term second word given dog final translat third word given bark one paramet french word condit ident french word english word align abstract form follow product j equal rememb length french sentenc go posit posit french sentenc point f sub j that jth word condit e sub j ident english word jth french word align essenti assum french word fill turn independ french word actual condit ident english word align here anoth exampl goe come back exampl show earlier assum english sentenc french sentenc english length six french word french length seven let say say align ill draw align look like particular case sorri also condit case particular case probabl see particular french sentenc condit e align product term notic one term french word one term seven french word point condit english word align le line exampl programm line program intuit ibm model correspond follow gener process english string exampl might dog bark length english string exampl equal three possibl french word here proceed firstli pick align uniformli random space possibl align might exampl pick align like okay done fill french word product probabl might exampl pick three word andcough part express would probabl le given dog time chien given time aboi given cours unlik align given two sentenc nevertheless possibl intuit second step gener process french posit choos french word random distribut defin exampl word chosen distribut word given dog word chosen distribut word given first step choos align uniform random second word second step fill french word one one condit english word align final result model follow form condit probabl f given e product two term firstli p secondli f given e that applic chain rule given particular form ibm model final express follow correspond uniform probabl align correspond pick french word base english word align later class well see estim paramet okay go parallel corpu input model go get f e english word e french word f like f get condit probabl ive shown one entri model actual train real data english word posit list probabl word french train paramet model see actual done reason job know french seem like reason set translat probabl posit translat word translat posit french possibl like word okay typic kind lexicon might get output process english word get probabl everi possibl french word,[ 3  4  8 14 13]
67,Course1_W5-S2-L4_IBM_Model_2_11-27,lets move onto ibm model ibm model well see fairly straightforward extension model difference going come alignment model ibm model introduces called alignment sometimes called distortion parameters okay im going use q parameters going different indices length french sentence length english sentence j index french word index english word let give example come back example dog barks say equal three french words lets say sort second french word lets say align third english word probability happening q thats english position three conditioned french position two length two sentences l equal three case okay conditioned length english length french sentence conditioned position french word considering position two particular example distribution possible alignments zero taken null word one possibility well come concrete example moment abstractly write alignment going sequence variables specifying french words aligned calculate probability alignment multiply together q variable product j equals add q sub j going position alignment jth word condition position j length l length english french sentences final form model going joint probability f conditioned e length product j equals q times terms exactly saw fj given e sub j sorry j lets work example consider example english sentence french sentence lengths english french respectively lets consider particular alignment showed draw particular alignment whats probability alignment condition english word length french sentence going product q term first term probability position one french aligned position two english conditioned two sentence lengths youll see similar term position french positions aligned respectively product q terms gives us probability alignment condition english sentence length french sentence model incredibly naive misses kinds important linguistic problems linguistic point view really realistic model translation important point spite gets us ground actually quite successful recovering good alignments different sentences training samples alignments form critical component phrase based translation systems well see little later class naive model practice quite good job heres second part model remember second part define conditional probability french sentence given recondition english alignment two lengths case exactly model remember alignment following one term french word point condition english word aligned parameters reflecting conditional probability example le translated second part model ibm model finally get intuitions lets consider generative process underlying model ive shown basically assuming french string f generated english string e using following process conditioning e sentence example dog barks going kind condition length french sentence example equals first step choose alignment set possible alignments probability word french sentence choose alignment english word okay dictated alignment parameter q parameters second step fill french words one one conditioned english word aligned something like thats second step process final result joint probability f conditioned english sentence e length sentence n product two model terms end model form product j equals n j distortion parameter also translation parameter q sub j condition position lengths secondly translation parameter probability fj emitted e sub aj probability emitting french word english word actually aligned final model form ibm model final point wanted make ibm model following parameters q well actually talk next estimate parameters training examples parameters straightforward find likely alignment given sentence pair im going describe process takes english sentence paired french sentence input output produce alignment example might produce following alignment word french sentence weve chosen one english words align parameters ibm model straightforward find likely alignment model let show works key key definition following given sentence pair e el f fm aj j equals simply find alignment maximizes product two terms firstly q given j l secondly fj given ea find alignment maximizes product let illustrate particular example okay say want align word french could potentially aligned one english words could aligned null okay one possible alignments possible choose lets go turn see expression scored null word evaluate q given third word french sentence length english length french distortion parameter basically specifying probability position three french rely position zero translation parameter given null look second word case going evaluate q given aligning first position english given next word q given given word second position go right end till final word implemented q given times given implemented alternative words could align calculate two firms two terms firstly q reflecting likelihood position reflecting likely french word generated english word take max find maximum values take single alignment maximizes product case youd hope word natural translation okay recap main message parameters easy recover alignments easy fill alignments training examples alignment specifies french word alignment one english words english could come could include null position,Course1,W5-S2-L4,W5,S2,L4,IBM,5,2,4,let move onto ibm model ibm model well see fairli straightforward extens model differ go come align model ibm model introduc call align sometim call distort paramet okay im go use q paramet go differ indic length french sentenc length english sentenc j index french word index english word let give exampl come back exampl dog bark say equal three french word let say sort second french word let say align third english word probabl happen q that english posit three condit french posit two length two sentenc l equal three case okay condit length english length french sentenc condit posit french word consid posit two particular exampl distribut possibl align zero taken null word one possibl well come concret exampl moment abstractli write align go sequenc variabl specifi french word align calcul probabl align multipli togeth q variabl product j equal add q sub j go posit align jth word condit posit j length l length english french sentenc final form model go joint probabl f condit e length product j equal q time term exactli saw fj given e sub j sorri j let work exampl consid exampl english sentenc french sentenc length english french respect let consid particular align show draw particular align what probabl align condit english word length french sentenc go product q term first term probabl posit one french align posit two english condit two sentenc length youll see similar term posit french posit align respect product q term give us probabl align condit english sentenc length french sentenc model incred naiv miss kind import linguist problem linguist point view realli realist model translat import point spite get us ground actual quit success recov good align differ sentenc train sampl align form critic compon phrase base translat system well see littl later class naiv model practic quit good job here second part model rememb second part defin condit probabl french sentenc given recondit english align two length case exactli model rememb align follow one term french word point condit english word align paramet reflect condit probabl exampl le translat second part model ibm model final get intuit let consid gener process underli model ive shown basic assum french string f gener english string e use follow process condit e sentenc exampl dog bark go kind condit length french sentenc exampl equal first step choos align set possibl align probabl word french sentenc choos align english word okay dictat align paramet q paramet second step fill french word one one condit english word align someth like that second step process final result joint probabl f condit english sentenc e length sentenc n product two model term end model form product j equal n j distort paramet also translat paramet q sub j condit posit length secondli translat paramet probabl fj emit e sub aj probabl emit french word english word actual align final model form ibm model final point want make ibm model follow paramet q well actual talk next estim paramet train exampl paramet straightforward find like align given sentenc pair im go describ process take english sentenc pair french sentenc input output produc align exampl might produc follow align word french sentenc weve chosen one english word align paramet ibm model straightforward find like align model let show work key key definit follow given sentenc pair e el f fm aj j equal simpli find align maxim product two term firstli q given j l secondli fj given ea find align maxim product let illustr particular exampl okay say want align word french could potenti align one english word could align null okay one possibl align possibl choos let go turn see express score null word evalu q given third word french sentenc length english length french distort paramet basic specifi probabl posit three french reli posit zero translat paramet given null look second word case go evalu q given align first posit english given next word q given given word second posit go right end till final word implement q given time given implement altern word could align calcul two firm two term firstli q reflect likelihood posit reflect like french word gener english word take max find maximum valu take singl align maxim product case youd hope word natur translat okay recap main messag paramet easi recov align easi fill align train exampl align specifi french word align one english word english could come could includ null posit,[ 3  4  8 14 13]
68,Course1_W5-S2-L5_The_EM_Algorithm_for_IBM_Model_2_Part_1_5-09,final segment lecture want talk estimate q pr q parameters ibm model particular going focus something called em algorithm estimation parameters model definition parameter estimation problem input parameter estimation algorithm assume set sentence pairs im going use e k f k refer kth training example example consists english sentence could example could dog french sentence assumed translation english sentence heres another example might example find hundredth training sample consists english sentence together frame sentence output model want parameters form tfe qijlm example might estimate le given might estimate q given remember distortion alignment parameter key challenge problem assuming alignments training samples okay ideal situation somebody would annotated alignments alignments would present training samples thats really unrealistic assumption extremely laborious task annotate alignments know typical training set sizes easily know order hundreds thousands sentences might sentences training data much expect humans annotate alignments training examples ready significant subsetable training samples thats going chee key challenge going mean need use something called em algorithm really rather remarkable algorithm used estimation kind scenario part data hidden hidden missing alignments unobserved lets warm em algorithm first consider case alignments observed okay lets humor idealized setting actually training examples alignments training example would triple e f example might hundredth training sample specifying english sentence french sentence alignment two sentences particular line would say first word french aligned two second word aligned three okay said training example triple e f seeing alignments case deriving maximumlikelihood parameter estimates basically trivial looks much like parameter estimates weve seen earlier course ml estimates ratios counts taken training data example want estimate probability le given set ratio two terms numerator number times two words aligned seen seen aligned training data denominator number times see english word thats particular example definition numerator number times ive seen e aligned f denominator number times ive seen e similarly maximum length estimate distortion parameters simple ratio counts numerator count number times ive seen word french aligned word j english given two sentence lengths l respectively denominator basically number times ive seen ith position align anything given lengths l two sentences thats warm case alignments observed im actually going write pseudocode absolutely explicit counts calculated main reason though well see em algorithm sense well certainly closely related pseudocode im show algorithm im going shown show theres couple twists,Course1,W5-S2-L5,W5,S2,L5,The,5,2,5,final segment lectur want talk estim q pr q paramet ibm model particular go focu someth call em algorithm estim paramet model definit paramet estim problem input paramet estim algorithm assum set sentenc pair im go use e k f k refer kth train exampl exampl consist english sentenc could exampl could dog french sentenc assum translat english sentenc here anoth exampl might exampl find hundredth train sampl consist english sentenc togeth frame sentenc output model want paramet form tfe qijlm exampl might estim le given might estim q given rememb distort align paramet key challeng problem assum align train sampl okay ideal situat somebodi would annot align align would present train sampl that realli unrealist assumpt extrem labori task annot align know typic train set size easili know order hundr thousand sentenc might sentenc train data much expect human annot align train exampl readi signific subset train sampl that go chee key challeng go mean need use someth call em algorithm realli rather remark algorithm use estim kind scenario part data hidden hidden miss align unobserv let warm em algorithm first consid case align observ okay let humor ideal set actual train exampl align train exampl would tripl e f exampl might hundredth train sampl specifi english sentenc french sentenc align two sentenc particular line would say first word french align two second word align three okay said train exampl tripl e f see align case deriv maximumlikelihood paramet estim basic trivial look much like paramet estim weve seen earlier cours ml estim ratio count taken train data exampl want estim probabl le given set ratio two term numer number time two word align seen seen align train data denomin number time see english word that particular exampl definit numer number time ive seen e align f denomin number time ive seen e similarli maximum length estim distort paramet simpl ratio count numer count number time ive seen word french align word j english given two sentenc length l respect denomin basic number time ive seen ith posit align anyth given length l two sentenc that warm case align observ im actual go write pseudocod absolut explicit count calcul main reason though well see em algorithm sens well certainli close relat pseudocod im show algorithm im go shown show there coupl twist,[ 3  4 14 13 12]
69,Course1_W5-S2-L6_The_EM_Algorithm_for_IBM_Model_2_Part_2_8-37,okay im going give algorithm explicitly calculates counts referring main motivation going em algorithm essentially going variant algorithm input training corpus im assuming idealized scenario alignment training sample training sample consists f e triple fk sentence french im going assume sub k length french sentence ek english sentence ill assume l sub k length sentence finally alignment variables amk one example might e equal dog f equal alignment equal say basically corresponds alignment okay algorithm going iterate corpus run corpus calculate counts finally maximum likelihood estimates going simple count ratios tmll f given e ratio counts similarly distortion parameters ratios counts okay lets talk inner loop counts actually calculated im going pass training sentence turn training sentence im going consider every possible french position right mk every possible english position lk im assuming null word also possibility usual im going increment counts critical definition delta variables k example number french position j english position going see delta variables appear central way em algorithm well introduce next deltas defined basically indicator functions indicating alignments present delta k j ith french word aligned jth english word kth example lets let illustrate particular example delta equal french word aligned french word english similarly delta equal finally delta j equal j okay deltas actually quite simple indicate alignments actually present present inner loop basically going iterate possible j positions going increment counts let actually trace trace counts actually incremented particular example go things youll find c le equal c le plus thats increment similar line going get c equal c plus going get c given thats length two sentences going c given plus finally c equals c plus counts incremented based delta equal second set counts youre going find c dog chien equals c dog chien plus c dog equals c dog plus theres couple counts actually eight counts total incremented particular training example see basically rather laborious way making sure counts correct items example align le incremented particular example line le okay recap meant sort explicit description maximum likelihood estimate parameter estimation method maximum likelihood parameter estimates strong assumption know alignments training samples em algorithm actually going closely related ive shown recall going assume training examples alignments training example english sentence paired french sentence gain n training samples critically alignments omitted em algorithm going take input output q parameters final output going proceed slightly different way first major difference algorithm showed algorithm going iterative going start q parameters example first iteration might random parameters okay start random initialization step going use q parameters calculate counts moment ill describe going calculate counts actually based data rk fk pairs together current guess parameters give us counts counts well reestimate q parameters going iterative method iteration start values q calculate new values q next step going take new q parameters take data calculate counts calculate q variables well keep iterating like sense weve reached convergence typical kind models run maybe dont know iterations fairly common ibm models okay thats basic idea going iterative algorithm random random initialization recalculate q parameters step going process thing thats really going change counts calculated remember data doesnt include alignment variables instead going use q parameters actually calculate deltas remember delta k j equal k equal j okay indicator functions making use alignment variables going replace deltas values actually calculated based current parameters data ill moment talk much way calculating delta actually done intuition behind,Course1,W5-S2-L6,W5,S2,L6,The,5,2,6,okay im go give algorithm explicitli calcul count refer main motiv go em algorithm essenti go variant algorithm input train corpu im assum ideal scenario align train sampl train sampl consist f e tripl fk sentenc french im go assum sub k length french sentenc ek english sentenc ill assum l sub k length sentenc final align variabl amk one exampl might e equal dog f equal align equal say basic correspond align okay algorithm go iter corpu run corpu calcul count final maximum likelihood estim go simpl count ratio tmll f given e ratio count similarli distort paramet ratio count okay let talk inner loop count actual calcul im go pass train sentenc turn train sentenc im go consid everi possibl french posit right mk everi possibl english posit lk im assum null word also possibl usual im go increment count critic definit delta variabl k exampl number french posit j english posit go see delta variabl appear central way em algorithm well introduc next delta defin basic indic function indic align present delta k j ith french word align jth english word kth exampl let let illustr particular exampl delta equal french word align french word english similarli delta equal final delta j equal j okay delta actual quit simpl indic align actual present present inner loop basic go iter possibl j posit go increment count let actual trace trace count actual increment particular exampl go thing youll find c le equal c le plu that increment similar line go get c equal c plu go get c given that length two sentenc go c given plu final c equal c plu count increment base delta equal second set count your go find c dog chien equal c dog chien plu c dog equal c dog plu there coupl count actual eight count total increment particular train exampl see basic rather labori way make sure count correct item exampl align le increment particular exampl line le okay recap meant sort explicit descript maximum likelihood estim paramet estim method maximum likelihood paramet estim strong assumpt know align train sampl em algorithm actual go close relat ive shown recal go assum train exampl align train exampl english sentenc pair french sentenc gain n train sampl critic align omit em algorithm go take input output q paramet final output go proceed slightli differ way first major differ algorithm show algorithm go iter go start q paramet exampl first iter might random paramet okay start random initi step go use q paramet calcul count moment ill describ go calcul count actual base data rk fk pair togeth current guess paramet give us count count well reestim q paramet go iter method iter start valu q calcul new valu q next step go take new q paramet take data calcul count calcul q variabl well keep iter like sens weve reach converg typic kind model run mayb dont know iter fairli common ibm model okay that basic idea go iter algorithm random random initi recalcul q paramet step go process thing that realli go chang count calcul rememb data doesnt includ align variabl instead go use q paramet actual calcul delta rememb delta k j equal k equal j okay indic function make use align variabl go replac delta valu actual calcul base current paramet data ill moment talk much way calcul delta actual done intuit behind,[ 3  4 14 13 12]
70,Course1_W5-S2-L7_The_EM_Algorithm_for_IBM_Model_2_Part_3_9-28,okay lets describe em algorithm input algorithm training corpus consisting sentence pairs f k kth french sentence e k kth english sentence initialization step going initialize q parameters initial value might example choose random initial values parameters algorithm proceeds follows take capital iterations data said might typically training ibm models okay iteration first set counts equal end iteration going recalculate q parameters based counts calculate middle portion im going describe counts actually calculated okay initially set counts equal pass every training example might find example like one showed consider every position french string every position paired sorry every consider every position french string paired every possible position english strings loops j updates accounts actually identical algorithm showed previously updates c c plus delta critical differences deltas calculated remember delta k j equal k equals j otherwise lucky scenario alignments fill deltas either depending whether alignment training data course em algorithm going assume alignments instead calculate delta values based current parameters expression based qs ts form previous iteration okay qs ts common parameter values going able calculate deltas using expression ill describe next slide use deltas increment counts okay lets describe calculate deltas using expression showed previous slide illustrate expression im going use particular example lets consider third french word lets number english side lets first consider delta th training example thats mean im going consider position lets first posi consider value intuitively corresponds word aligned null word word zero expression say well numerator multiply two things together firstly q given thats im aligning word french word english thats im considering term given null thats numerator basically im considering alignment null im multiplying get associate distortion parameter also translation parameter look denominator whole thing going divided expression star let write star start actually going sum terms going consider possible english positions actually going q given times given null plus q given times given position one english distortion parameter translation parameter plus given times given basically going possible english words multiply q parameter thats calculate denominator denominator essentially going normalization constant well see soon similarly delta going equal q given times given divide star sum delta going q given times given third word sorry second word divided star fill values delta delta delta right way delta verify different deltas sum sense theyre fractional count say sum define probability distribution different possible alignments particular french word deltas actually direct probabilistic interpretation follow follows delta k j probability ith french word aligned jth english word conditioned english sentence e k french sentence k current parameters q ill write semicolon followed q mean talking probabilities model defined common parameter basically weve done algorithm hallucinated delta values calculating probability alignments common parameters recall use deltas calculating counts used reestimate q parameters recap go back algorithm im going show diagrams remind start initial values q variables also data consists e k f k k equals n two inputs calculate counts often referred expected counts theyre expected counts previous model previous q values using definition delta makes direct use q previous iteration calculate counts calculate new values q variables using simple expressions thats first iteration second iteration repeat two inputs data together current parameter parameters calculate expected counts expected counts reestimate q keep going process referred em algorithm least instance em algorithm applied ibm translation models,Course1,W5-S2-L7,W5,S2,L7,The,5,2,7,okay let describ em algorithm input algorithm train corpu consist sentenc pair f k kth french sentenc e k kth english sentenc initi step go initi q paramet initi valu might exampl choos random initi valu paramet algorithm proce follow take capit iter data said might typic train ibm model okay iter first set count equal end iter go recalcul q paramet base count calcul middl portion im go describ count actual calcul okay initi set count equal pass everi train exampl might find exampl like one show consid everi posit french string everi posit pair sorri everi consid everi posit french string pair everi possibl posit english string loop j updat account actual ident algorithm show previous updat c c plu delta critic differ delta calcul rememb delta k j equal k equal j otherwis lucki scenario align fill delta either depend whether align train data cours em algorithm go assum align instead calcul delta valu base current paramet express base qs ts form previou iter okay qs ts common paramet valu go abl calcul delta use express ill describ next slide use delta increment count okay let describ calcul delta use express show previou slide illustr express im go use particular exampl let consid third french word let number english side let first consid delta th train exampl that mean im go consid posit let first posi consid valu intuit correspond word align null word word zero express say well numer multipli two thing togeth firstli q given that im align word french word english that im consid term given null that numer basic im consid align null im multipli get associ distort paramet also translat paramet look denomin whole thing go divid express star let write star start actual go sum term go consid possibl english posit actual go q given time given null plu q given time given posit one english distort paramet translat paramet plu given time given basic go possibl english word multipli q paramet that calcul denomin denomin essenti go normal constant well see soon similarli delta go equal q given time given divid star sum delta go q given time given third word sorri second word divid star fill valu delta delta delta right way delta verifi differ delta sum sens theyr fraction count say sum defin probabl distribut differ possibl align particular french word delta actual direct probabilist interpret follow follow delta k j probabl ith french word align jth english word condit english sentenc e k french sentenc k current paramet q ill write semicolon follow q mean talk probabl model defin common paramet basic weve done algorithm hallucin delta valu calcul probabl align common paramet recal use delta calcul count use reestim q paramet recap go back algorithm im go show diagram remind start initi valu q variabl also data consist e k f k k equal n two input calcul count often refer expect count theyr expect count previou model previou q valu use definit delta make direct use q previou iter calcul count calcul new valu q variabl use simpl express that first iter second iter repeat two input data togeth current paramet paramet calcul expect count expect count reestim q keep go process refer em algorithm least instanc em algorithm appli ibm translat model,[ 3  4  7  8 14]
71,Course1_W5-S2-L8_The_EM_Algorithm_for_IBM_Model_2_Part_4_4-52,let briefly talk justification algorithm theres actually much detail algorithm justification notes posted class let give sketch said training samples e k f k pairs k equals n define something called log likelihood function function parameters q parameters basically going measure well parameters fit training examples higher values l mean l q values better job modeling training examples looking defined simply defined log probability data precisely sum n examples example calculate conditional probability f k given e k model take log log likelihood basically log probability sum log probabilities recall ibm models probability following form sum possible alignments p f k comma given e k sum within log course p probability p going product q terms particular example reflecting e k f k particular line looking see clearly directly function q parameters vary q parameters probabilities vary likelihood function may go maximum likelihood estimates defined q pair maximize function maximumlikelihood estimation would like find parameters make data probable possible sense fit data well possible measure fit loglikelihood formally derive many formal guarantees maximumlikelihood estimates example showing converge closer closer true upper length parameters get training data happens likelihood function quite nasty quite difficult optimize know nice looking functions single maximum convex function imagine single parameters function l nice well behaved single maximum essentially hill climbing method general guaranteed get maximum function unfortunately loglikelihood function nearly nice schematically looks rather like might many different points local optimum hill climbing method general likely get stuck one nonoptimal points schematic im showing functions theres single parameter reality multidimensional functions q might thousands parameters kind surfaces become extremely complex many many local optima okay makes optimization likelihood function hard facts almost certainly provably hard mp hard something similar guarantee em algorithm know defines sequence parameters start q point calculate new q keep going like maybe tens hundreds iterations em algorithm guaranteed limit converge one local optima likelihood function ideal ideally wed like get global maximum practice works quite well quite en effective algorithm one thing note property point covert going depend initial parameter values sometimes careful initialize parameters theres lot discussion notes posted along class,Course1,W5-S2-L8,W5,S2,L8,The,5,2,8,let briefli talk justif algorithm there actual much detail algorithm justif note post class let give sketch said train sampl e k f k pair k equal n defin someth call log likelihood function function paramet q paramet basic go measur well paramet fit train exampl higher valu l mean l q valu better job model train exampl look defin simpli defin log probabl data precis sum n exampl exampl calcul condit probabl f k given e k model take log log likelihood basic log probabl sum log probabl recal ibm model probabl follow form sum possibl align p f k comma given e k sum within log cours p probabl p go product q term particular exampl reflect e k f k particular line look see clearli directli function q paramet vari q paramet probabl vari likelihood function may go maximum likelihood estim defin q pair maxim function maximumlikelihood estim would like find paramet make data probabl possibl sens fit data well possibl measur fit loglikelihood formal deriv mani formal guarante maximumlikelihood estim exampl show converg closer closer true upper length paramet get train data happen likelihood function quit nasti quit difficult optim know nice look function singl maximum convex function imagin singl paramet function l nice well behav singl maximum essenti hill climb method gener guarante get maximum function unfortun loglikelihood function nearli nice schemat look rather like might mani differ point local optimum hill climb method gener like get stuck one nonoptim point schemat im show function there singl paramet realiti multidimension function q might thousand paramet kind surfac becom extrem complex mani mani local optima okay make optim likelihood function hard fact almost certainli provabl hard mp hard someth similar guarante em algorithm know defin sequenc paramet start q point calcul new q keep go like mayb ten hundr iter em algorithm guarante limit converg one local optima likelihood function ideal ideal wed like get global maximum practic work quit well quit en effect algorithm one thing note properti point covert go depend initi paramet valu sometim care initi paramet there lot discuss note post along class,[ 4  5  3 14 13]
72,Course1_W5-S2-L9_Summary_1-48,finally summarize key ideas saw ibm translation models really single key idea introduce alignment variables specifying words one language aligned words another language second make use translation parameters example probability dog translated word chien also distortion parameters example probability position one french aligned position two english saw parameter estimation algorithm em algorithm iterative algorithm training q parameters starts initial values q recalculates using method described typically run several iterations convergence critically recovered q parameters using em algorithm go back training examples fill alignments earlier showed recover likely alignment sentence q parameters actually idea models currently use machine translations systems critical component allow us recover alignments training examples next lecture course well talk phrasebased systems phrasebased systems going make direct use alignments recovered using ibm models,Course1,W5-S2-L9,W5,S2,L9,Summary,5,2,9,final summar key idea saw ibm translat model realli singl key idea introduc align variabl specifi word one languag align word anoth languag second make use translat paramet exampl probabl dog translat word chien also distort paramet exampl probabl posit one french align posit two english saw paramet estim algorithm em algorithm iter algorithm train q paramet start initi valu q recalcul use method describ typic run sever iter converg critic recov q paramet use em algorithm go back train exampl fill align earlier show recov like align sentenc q paramet actual idea model current use machin translat system critic compon allow us recov align train exampl next lectur cours well talk phrasebas system phrasebas system go make direct use align recov use ibm model,[ 3  8  4 14 13]
73,Course1_W6-S1-L1_Introduction_0-41,okay next segment class going look phrasebased translation systems phrasebased translation systems first came around late sense theyre second generation statistical machine translation systems first generation ibm models saw last lecture phrasebased systems build ideas form ibm translation models make direct use alignments saw last lecture represent significant step forward also widely used modern research also industry based systems translation,Course1,W6-S1-L1,W6,S1,L1,Introduction,6,1,1,okay next segment class go look phrasebas translat system phrasebas translat system first came around late sens theyr second gener statist machin translat system first gener ibm model saw last lectur phrasebas system build idea form ibm translat model make direct use align saw last lectur repres signific step forward also wide use modern research also industri base system translat,[ 8  3  4  9 14]
74,Course1_W6-S1-L2_Learning_Phrases_from_Alignments_Part_1_9-18,okay heres road map whats coming next class well first describe learn phrases often called phrase based lexicon translation examples alignments covered using ibm models showed last time well describe basic phrasebased model finally well spend time talking decoding problem phrasebased models describe decoding algorithm phrasebased models critical idea phrasebased models automatically learn large lexicon data lexicon consists phrases one language paired phrases another language example phrases left hand side german right hand side english entries lexicon pairs sequence german words sequence english words example first entry says two words german translated two words english notice general one words german side one words english side number words differ example two words german corresponding three words english lexical entries going form basis translation system importantly go beyond simple word word translations saw ibm models remember ibm models parameters probability word french translation word english going probabilities example entire phrase translation english phrase going translation entries lexicon entries multiword phrases either side associated parameters well also learn data first going describe actually learn lexicons set translation examples ibm model im going assume large set training examples example might german versus english training data example training data consists german sentence paired english sentence might several tens thousands several hundreds thousands example translations like given corpus call parallel corpus sometimes called literature bitext given input going describe methods learning lexicon lexicons often large could often many hundreds thousands even millions entries form heres example illustrates basic idea phrases extracted translation examples taken tut tutorial philip koehn kevin knight training sample instance spanish sentence aligned english sentence two sentences translations run ibm models get kind alignment might example maybe something like okay well talk lot moment use alignments driving phrases weve derived alignment start extracting phrase pairs example examples maria identified translation mary correspondence word corresponding word witch word corresponding green single word phrases might example line spanish phrase english sequence might longer phrases example might fact sequence words seen spanish aligned sequence words slap english heres another example phrase phrasal entry lexical entry going pair subsequence spanish words subsequent sequence english words might everything anything wordtoword correspondences single word side much longer phrases multiple words one language aligned multiple words language basically going run algorithm training examples turn extract set lexical entries form imagine hundred thousand training examples training example consists pair sentences might end quite large set possible phrases heres quick recap use ibm translation models derive alignments training examples specifically ibm model used way remember ibm model defines distribution length french sentence e english sentence e e e l l length english sentence example might english sentence f french sentence consisting words f f f set alignment variables conditioned english sentence french sentence length distribution possible choices alignment variables choices french translations useful byproduct weve trained model right q parameters showed last time calculate likely alignment training example given english sentence french foreign sentence case spanish sentence length foreign sentence search possible alignments find alignment likely model actually foreign word find likely alignment alignment specifies foreign word word english aligns particular example may example recover alignment ill show reasonable alignment two sentences okay every one training examples weve learned q parameters using em algorithm derive q using em go training sentences one one recover alignments notice ibm models alignments following property foreign word alignment single english word foreign word pick single english word aligned going useful represent alignments called alignment matrices heres example english spanish pair showed english words column first column spanish words across top table show dot theres alignmentookay dot means mary marie aligned dot means aligned dot means slap daba aligned similarly okay una aligned slap bof aligned slap place alignment one dots cell matrix two words aligned notice spanish word alignment single english word spanish word see exactly one symbol one dot column one dot one dot dots position one dot one dot one dot okay obeying constraint foreign word aligned single english word,Course1,W6-S1-L2,W6,S1,L2,Learning,6,1,2,okay here road map what come next class well first describ learn phrase often call phrase base lexicon translat exampl align cover use ibm model show last time well describ basic phrasebas model final well spend time talk decod problem phrasebas model describ decod algorithm phrasebas model critic idea phrasebas model automat learn larg lexicon data lexicon consist phrase one languag pair phrase anoth languag exampl phrase left hand side german right hand side english entri lexicon pair sequenc german word sequenc english word exampl first entri say two word german translat two word english notic gener one word german side one word english side number word differ exampl two word german correspond three word english lexic entri go form basi translat system importantli go beyond simpl word word translat saw ibm model rememb ibm model paramet probabl word french translat word english go probabl exampl entir phrase translat english phrase go translat entri lexicon entri multiword phrase either side associ paramet well also learn data first go describ actual learn lexicon set translat exampl ibm model im go assum larg set train exampl exampl might german versu english train data exampl train data consist german sentenc pair english sentenc might sever ten thousand sever hundr thousand exampl translat like given corpu call parallel corpu sometim call literatur bitext given input go describ method learn lexicon lexicon often larg could often mani hundr thousand even million entri form here exampl illustr basic idea phrase extract translat exampl taken tut tutori philip koehn kevin knight train sampl instanc spanish sentenc align english sentenc two sentenc translat run ibm model get kind align might exampl mayb someth like okay well talk lot moment use align drive phrase weve deriv align start extract phrase pair exampl exampl maria identifi translat mari correspond word correspond word witch word correspond green singl word phrase might exampl line spanish phrase english sequenc might longer phrase exampl might fact sequenc word seen spanish align sequenc word slap english here anoth exampl phrase phrasal entri lexic entri go pair subsequ spanish word subsequ sequenc english word might everyth anyth wordtoword correspond singl word side much longer phrase multipl word one languag align multipl word languag basic go run algorithm train exampl turn extract set lexic entri form imagin hundr thousand train exampl train exampl consist pair sentenc might end quit larg set possibl phrase here quick recap use ibm translat model deriv align train exampl specif ibm model use way rememb ibm model defin distribut length french sentenc e english sentenc e e e l l length english sentenc exampl might english sentenc f french sentenc consist word f f f set align variabl condit english sentenc french sentenc length distribut possibl choic align variabl choic french translat use byproduct weve train model right q paramet show last time calcul like align train exampl given english sentenc french foreign sentenc case spanish sentenc length foreign sentenc search possibl align find align like model actual foreign word find like align align specifi foreign word word english align particular exampl may exampl recov align ill show reason align two sentenc okay everi one train exampl weve learn q paramet use em algorithm deriv q use em go train sentenc one one recov align notic ibm model align follow properti foreign word align singl english word foreign word pick singl english word align go use repres align call align matric here exampl english spanish pair show english word column first column spanish word across top tabl show dot there alignmentookay dot mean mari mari align dot mean align dot mean slap daba align similarli okay una align slap bof align slap place align one dot cell matrix two word align notic spanish word align singl english word spanish word see exactli one symbol one dot column one dot one dot dot posit one dot one dot one dot okay obey constraint foreign word align singl english word,[ 3  4  8  0 10]
75,Course1_W6-S1-L3_Learning_Phrases_from_Alignments_Part_2_7-01,recap use ibm models derive alignments training data sentences reality though two problems firstly alignments often rather noisy much alignments look good often contain noise erroneous alignments secondly alignments manytoone mean word foreign language aligned exactly one word english language okay may think english foreign languages word foreign side aligned exactly one word english side many words side may aligned one word side certainly cant example particular english word cant alignments like multiple english words aligned foreign word ruled constrain foreign word like exactly one english word isnt necessarily realistic many examples youll find constraint realistic number heuristics developed try get around kind problems first try make alignments robust somehow reduce noise secondly somehow get around constraint alignments manytoone particular try move two alignments manytomany might example alignments like foreign words aligned multiple english words similarly english words aligned multiple french words heres generally done trick make observation ibm models trained directions train model condition probability french sentence given english weve seen far also reverse two languages train model english given french find likely alignment models two directions going two different alignments training example intersection alignments turns reliable starting point alignment process let illustrate example sentence pair weve seen earlier spanish sentence english sentence f case spanish foreign string e english alignment showed earlier derived ibm model f given e notice satisfies constraint foreign word align single english word single point columns specifying alignment foreign words heres alignment reverse model train ibm model model language pair going opposite direction modeled conditional probability english given spanish derive likely alignment notice alignment constraint english word aligned single spanish word look row matrix theres single dot row specifying spanish word english word aligned two alignments consider intersection two alignments certain alignment points example one seen alignments constitute intersection theres one one look see one also alignments one alignments one one set subset points two alignments going seen subset quotes intersected alignment okay practice found intersected alignments tend quite reliable theyre useful theyre useful starting point process going grow alignments okay well see next well see process considers put cross points seen alignments im going circle okay circled points alignments points neither alignment one two three four points sorry one alignment alignments okay well see methods basically start intersected alignments start adding back points dont want go detail process works let give sketch explore alignment points occur one alignment alignment add one alignment point time add alignment points align word currently alignment first restrict alignment points neighbors theyre adjacent diagonal current alignment points ill try post link papers process want read detail class heres example kind final alignment might get using heuristics start intersect alignments add points union two alignments final alignment matrix notice alignment longer manytoone okay english words aligned multiple spanish words similarly spanish words aligned multiple english words recap motivation two problems one alignments often noisy two manytoone hopefully set manytomany alignments reliable taking alignments simply one direction one model pfe pef alone,Course1,W6-S1-L3,W6,S1,L3,Learning,6,1,3,recap use ibm model deriv align train data sentenc realiti though two problem firstli align often rather noisi much align look good often contain nois erron align secondli align manytoon mean word foreign languag align exactli one word english languag okay may think english foreign languag word foreign side align exactli one word english side mani word side may align one word side certainli cant exampl particular english word cant align like multipl english word align foreign word rule constrain foreign word like exactli one english word isnt necessarili realist mani exampl youll find constraint realist number heurist develop tri get around kind problem first tri make align robust somehow reduc nois secondli somehow get around constraint align manytoon particular tri move two align manytomani might exampl align like foreign word align multipl english word similarli english word align multipl french word here gener done trick make observ ibm model train direct train model condit probabl french sentenc given english weve seen far also revers two languag train model english given french find like align model two direct go two differ align train exampl intersect align turn reliabl start point align process let illustr exampl sentenc pair weve seen earlier spanish sentenc english sentenc f case spanish foreign string e english align show earlier deriv ibm model f given e notic satisfi constraint foreign word align singl english word singl point column specifi align foreign word here align revers model train ibm model model languag pair go opposit direct model condit probabl english given spanish deriv like align notic align constraint english word align singl spanish word look row matrix there singl dot row specifi spanish word english word align two align consid intersect two align certain align point exampl one seen align constitut intersect there one one look see one also align one align one one set subset point two align go seen subset quot intersect align okay practic found intersect align tend quit reliabl theyr use theyr use start point process go grow align okay well see next well see process consid put cross point seen align im go circl okay circl point align point neither align one two three four point sorri one align align okay well see method basic start intersect align start ad back point dont want go detail process work let give sketch explor align point occur one align align add one align point time add align point align word current align first restrict align point neighbor theyr adjac diagon current align point ill tri post link paper process want read detail class here exampl kind final align might get use heurist start intersect align add point union two align final align matrix notic align longer manytoon okay english word align multipl spanish word similarli spanish word align multipl english word recap motiv two problem one align often noisi two manytoon hope set manytomani align reliabl take align simpli one direct one model pfe pef alon,[ 3  4 14 13 12]
76,Course1_W6-S1-L4_Learning_Phrases_from_Alignments_Part_3_8-47,okay derived alignments training examples next step extract phrases training example remember phrase phrasal entry looks like sequence english words paired sequence foreign words would example basic idea going extract possible phrase pairs consistent alignments particular example could potentially take sequence words foreign side maybe words sequence words english side say words extract phrased pair long theyre consistent alignments well talk second means consistent abstractly going consider possible sub sequences words foreign side possible subsequences word name side consider possible parents two choices okay check potential phrasal entry consistent alignment let show one alignment consistent phrase pair pairs maria mary going useful visualize drawing rectangle figure corresponding particular pairing two phrases okay phrase pair consistent three conditions satisfied one least one word english phrase e aligned word f one three words aligned one two foreign words actually multiple alignments okay definitely satisfy first condition actually three cases word english side aligned word foreign side second condition words f aligned words outside e okay lets check one one look maria see alignments words within phrase proposing line actually mary case thats within three words im considering similarly look word aligned english word within phrase second condition satisfied would broken alignment points one two three four five six seven eight cells example also alignment would aligned english word falls outside subsequence mary case second constraint would violated course isnt present fine thirdly similar constraint saying words english side align aligned words outside f could check english words one one mary aligned maria thats within phrase considering aligned within phrase considering aligned within phrase considering phrase pair mary maria certainly consistent would actually extracted phrase pair would extracted training example let show one example phrase pair isnt consistent mary mary thats pair maria mary fill rectangle actually violates constraint foreign wood aligned outside sequence english words mary mary good phrase pair would extracted example notice convenient way visualizing phrase pairs consistent ones correspond rectangles draw surround alignments alignment points rows columns outside rectangles alignment points cells corresponds constrain three alignment points columns corresponds constraint number two actually another perfectly well formed phrase slap daba una bofetada many phrases okay verde aligned green go single words alignments maria mary bruja witch okay single word phrases translations translate la thats another good one larger phrases look rectangle bruja sound verde aligned green witch sound look case la bruja verde im sorry spanish im sure terrible aligned green witch sound get everything single word translations much longer sequences example four spanish words align three english words every training example okay training example going get english sentence paired foreign sentence going extract set phrases training sample see going end really pretty large lexicon phrases phrases consist everything single word correspondences like bruja witch larger strings alignment weve extracted phrases also calculate parameter estimates based phrases weve extracted simple least simplest way pr ph phrase pair fe example f might sequence e might single word slap derive conditional probability f given e ratio two counts looks like maximum likelihood estimates weve seen elsewhere course count number times ive seen sequence corresponding word slap count number times ive seen english word slap take ratio two terms simplest possible way estimating parameters models little dubious probabilistic point view dont want get details fact multiple overlapping phrases training sample makes somewhat suspect empirically actually perform translation parameters useful actually work quite well used translation context heres actually example tutorial phillip kern eacl conference natural language processing heres example gave french string den vorschlag means roughly speaking proposal english im showing sorry p translation parameters list phrases extracted example one phrase german sequence words paired proposal sound probability parameter funnylooking phrase comes something like governments proposal case extracted phrase thats apostrophe comes proposal idea proposal would word proposal proposal see whole bunch phrases varying probabilities probabilities phrases look quite reasonable real example extracted german english data,Course1,W6-S1-L4,W6,S1,L4,Learning,6,1,4,okay deriv align train exampl next step extract phrase train exampl rememb phrase phrasal entri look like sequenc english word pair sequenc foreign word would exampl basic idea go extract possibl phrase pair consist align particular exampl could potenti take sequenc word foreign side mayb word sequenc word english side say word extract phrase pair long theyr consist align well talk second mean consist abstractli go consid possibl sub sequenc word foreign side possibl subsequ word name side consid possibl parent two choic okay check potenti phrasal entri consist align let show one align consist phrase pair pair maria mari go use visual draw rectangl figur correspond particular pair two phrase okay phrase pair consist three condit satisfi one least one word english phrase e align word f one three word align one two foreign word actual multipl align okay definit satisfi first condit actual three case word english side align word foreign side second condit word f align word outsid e okay let check one one look maria see align word within phrase propos line actual mari case that within three word im consid similarli look word align english word within phrase second condit satisfi would broken align point one two three four five six seven eight cell exampl also align would align english word fall outsid subsequ mari case second constraint would violat cours isnt present fine thirdli similar constraint say word english side align align word outsid f could check english word one one mari align maria that within phrase consid align within phrase consid align within phrase consid phrase pair mari maria certainli consist would actual extract phrase pair would extract train exampl let show one exampl phrase pair isnt consist mari mari that pair maria mari fill rectangl actual violat constraint foreign wood align outsid sequenc english word mari mari good phrase pair would extract exampl notic conveni way visual phrase pair consist one correspond rectangl draw surround align align point row column outsid rectangl align point cell correspond constrain three align point column correspond constraint number two actual anoth perfectli well form phrase slap daba una bofetada mani phrase okay verd align green go singl word align maria mari bruja witch okay singl word phrase translat translat la that anoth good one larger phrase look rectangl bruja sound verd align green witch sound look case la bruja verd im sorri spanish im sure terribl align green witch sound get everyth singl word translat much longer sequenc exampl four spanish word align three english word everi train exampl okay train exampl go get english sentenc pair foreign sentenc go extract set phrase train sampl see go end realli pretti larg lexicon phrase phrase consist everyth singl word correspond like bruja witch larger string align weve extract phrase also calcul paramet estim base phrase weve extract simpl least simplest way pr ph phrase pair fe exampl f might sequenc e might singl word slap deriv condit probabl f given e ratio two count look like maximum likelihood estim weve seen elsewher cours count number time ive seen sequenc correspond word slap count number time ive seen english word slap take ratio two term simplest possibl way estim paramet model littl dubiou probabilist point view dont want get detail fact multipl overlap phrase train sampl make somewhat suspect empir actual perform translat paramet use actual work quit well use translat context here actual exampl tutori phillip kern eacl confer natur languag process here exampl gave french string den vorschlag mean roughli speak propos english im show sorri p translat paramet list phrase extract exampl one phrase german sequenc word pair propos sound probabl paramet funnylook phrase come someth like govern propos case extract phrase that apostroph come propos idea propos would word propos propos see whole bunch phrase vari probabl probabl phrase look quit reason real exampl extract german english data,[ 3 10  4  0 13]
77,Course1_W6-S1-L5_A_Sketch_of_Phrase-based_Translation_8-17,next segment class well talk extensively whats called decoding problem phrasebased models first thing want give sketch phrasebased model used translation im going use particular example german string going see produce english translation basically left right building translation left right english side build translation left right going incorporate various schools indicating plausible particular translation particular therere going three types schools firstly language modeling score secondly phrase model score using parameters showed earlier finally distortion model describe detail go basic idea translating german sentence going try find sequence phrases translate point one german words one english words thereby build translation left right okay first step might say oh lets translate word heute today phrase entry says two things possible translation particular move particular choice going associated probability cost going language models scores q language model going allow us give prior probability likely english string building would typically trigram language model going translation model scores going parameters showed earlier theyre going reflect example likely today translated heute finally going unknown parameters going called distortion parameters going penalize us skipping large sequences english german words theyre going try enforce constraint word order two languages somewhat related well talk go forward class could multiply q parameters together could search possible translations try find maximum translation score actually little convenient sum log parameters going take sum log q plus log plus well see unknown term okay particular move going language model score firstly probability today trigram language model q parameter going trigram parameter whats probability today given two star symboles old star symbols weve seen language models first lecture class secondly probability generating word heute given today finally going cost penalizing number words skip phrases start sentence define number words weve skipped unknown going parameter thats typically negative might minus example going penalize cases skip large numbers words going control differing word order two languages extremely crude makes little sense linguistically give control word order said well talk later class improve upon well start thats first step make translation second step translation might choose following phrase might see german sequence words translated shall sorry shall see language model scores see phrase model scores distortion model scores language model score using trigram language model probability given start today two previous words probability shall given today finally probability given shall one q parameter english words weve chosen phrase model side side log german string theyre given english substring given shall finally distortion cost notice havent skipped german words weve weve weve chosen next phrase completely adjacent heute current translation thats right current translation translated heute next phrase adjacent weve skipped words cost heres next step translation weve decided use phrase entry says word german seen right end sentence corresponds word debating english make translation step building another word english translation three scores language model score phrasebased model score distortion model score language model score simply single term single word generated probability debating given previous two words shall phrase model score says whats probability german word conditioned fact english word debating finally something interesting going distortion model remember unknown going negative value say minus used discourage long range reorderings models six thats skipped one two three four five six words word incurs cost minus times would minus thats fairly heavy penalty case skipping several words process continue maybe next step well choose three german words translated reopening going language model score phrasebased model score distortion score finally would translate might translate german sequence sequence mont blanc tunnel final translation today shall debating reopening mont blanc tunnel weve done using sequence phrases first thing today first thing today think shall debating reopening mont blanc tunnel weve basically derived phrases using phrasal lexicon weve ordered phrases ordering left right thats sketch next lecture go considerable depth actually decode translation models basic process translation going involve picking sequence phrases like choice going involve language modeling score phrase score distortion score actually going try search space possible translations going search translation highest score combination three parameter types,Course1,W6-S1-L5,W6,S1,L5,A,6,1,5,next segment class well talk extens what call decod problem phrasebas model first thing want give sketch phrasebas model use translat im go use particular exampl german string go see produc english translat basic left right build translat left right english side build translat left right go incorpor variou school indic plausibl particular translat particular therer go three type school firstli languag model score secondli phrase model score use paramet show earlier final distort model describ detail go basic idea translat german sentenc go tri find sequenc phrase translat point one german word one english word therebi build translat left right okay first step might say oh let translat word heut today phrase entri say two thing possibl translat particular move particular choic go associ probabl cost go languag model score q languag model go allow us give prior probabl like english string build would typic trigram languag model go translat model score go paramet show earlier theyr go reflect exampl like today translat heut final go unknown paramet go call distort paramet go penal us skip larg sequenc english german word theyr go tri enforc constraint word order two languag somewhat relat well talk go forward class could multipli q paramet togeth could search possibl translat tri find maximum translat score actual littl conveni sum log paramet go take sum log q plu log plu well see unknown term okay particular move go languag model score firstli probabl today trigram languag model q paramet go trigram paramet what probabl today given two star symbol old star symbol weve seen languag model first lectur class secondli probabl gener word heut given today final go cost penal number word skip phrase start sentenc defin number word weve skip unknown go paramet that typic neg might minu exampl go penal case skip larg number word go control differ word order two languag extrem crude make littl sens linguist give control word order said well talk later class improv upon well start that first step make translat second step translat might choos follow phrase might see german sequenc word translat shall sorri shall see languag model score see phrase model score distort model score languag model score use trigram languag model probabl given start today two previou word probabl shall given today final probabl given shall one q paramet english word weve chosen phrase model side side log german string theyr given english substr given shall final distort cost notic havent skip german word weve weve weve chosen next phrase complet adjac heut current translat that right current translat translat heut next phrase adjac weve skip word cost here next step translat weve decid use phrase entri say word german seen right end sentenc correspond word debat english make translat step build anoth word english translat three score languag model score phrasebas model score distort model score languag model score simpli singl term singl word gener probabl debat given previou two word shall phrase model score say what probabl german word condit fact english word debat final someth interest go distort model rememb unknown go neg valu say minu use discourag long rang reorder model six that skip one two three four five six word word incur cost minu time would minu that fairli heavi penalti case skip sever word process continu mayb next step well choos three german word translat reopen go languag model score phrasebas model score distort score final would translat might translat german sequenc sequenc mont blanc tunnel final translat today shall debat reopen mont blanc tunnel weve done use sequenc phrase first thing today first thing today think shall debat reopen mont blanc tunnel weve basic deriv phrase use phrasal lexicon weve order phrase order left right that sketch next lectur go consider depth actual decod translat model basic process translat go involv pick sequenc phrase like choic go involv languag model score phrase score distort score actual go tri search space possibl translat go search translat highest score combin three paramet type,[ 8 10  4 14 13]
78,Course1_W6-S2-L1_Definition_of_the_Decoding_Problem_Part_1_9-12,okay next segment class going talk decode phrasebased translation models actually apply phrasebased translation models new sentences produce translations recap critical ideal phasebased model idea phrasebased lexicon phrasebased lexicon contains phrase entries f e pairs f sequence one foreign foreign words e sequence one english words look particular german sentence shall use example throughout slides say want translate english phrasebased lexicon going provide various entries may relevant particular sentence example entries lexicon might say two words german correspond must maybe three words german correspond must also first two words first three words single word german appears might translated seriously three entries phrasedbased lexicon practice lexicon might know millions entries saw last lecture segment extract kinds lexicons large quantities translation examples first running ibm translation models derive alignments based alignments pulling phrase based entries ill use g f e refer score lexicon entry score wouldnt mind minus minus minus example might log ratio accounts similar maximum likelihood estimate could thought f given e conditional probability foreign sequence words f condition english sentence sequence words e ratio two counts going take log saw last lecture convenient take logs well start summing different scores different phrases used translation throughout lecture im going consider phrasebased models consist three things firstly phrasebased lexicon consisting f e pairs exactly showed second thing going make use trigram language model language trying translate throughout lecture ill assume trying translate german english phrase entries one component second component trigram language model english language model example trigram language model language trying translate parameters exactly saw first lecture class example parameters probability also given previous two words must finally going distortion parameter single parameter typically negative well see penalize things moving far translation process start use phrases translate particular examples thats essentially three components phrasebased translation model little bit trigram language model parameters trained large quantities english text trigram language model going invaluable providing prior distribution sentences versus arent likely english heres notation ill use throughout lecture given sentence try trying translate extract set possible phrases applicable particular sentence itll useful use following notation phrases ste going start point phrase going end point phrase e sequence one english words one phrase applicable particular example must lets number words says two words wir mussen translated must okay words inclusive translated must okay im going use capital p script p denote set possible phrases particular sentence okay going derived entries comes fact phrase entry lexicon saying two words translated must two words wir mussen appear apply entry going little bit convenient think phrases particular input sentence start point end point english string p going set possible phrases sentence going set might example include must might include must also saying three words translated must also might include entry saying seriously would come entry saying word ernst sixth translated seriously set might quite large notice many phrases overlap example entry words entry words summary big p set possible phrases input sentence phrase start point end point english string phrase p lets say p equal must ill sometimes use following notation sp tp ep three components p p case e p equal must okay three functions pulling three different components phrase finally g p going score phrase might minus come original lexicon entry entry saying wir mussen must g equal minus remember typically log count whole phrase divided count english saying thats score phrase table okay p going set old phrases sentence phrase going scores probability easily calculate set taking phrasebased lexicon applying particular input sentence trying translate,Course1,W6-S2-L1,W6,S2,L1,Definition,6,2,1,okay next segment class go talk decod phrasebas translat model actual appli phrasebas translat model new sentenc produc translat recap critic ideal phasebas model idea phrasebas lexicon phrasebas lexicon contain phrase entri f e pair f sequenc one foreign foreign word e sequenc one english word look particular german sentenc shall use exampl throughout slide say want translat english phrasebas lexicon go provid variou entri may relev particular sentenc exampl entri lexicon might say two word german correspond must mayb three word german correspond must also first two word first three word singl word german appear might translat serious three entri phrasedbas lexicon practic lexicon might know million entri saw last lectur segment extract kind lexicon larg quantiti translat exampl first run ibm translat model deriv align base align pull phrase base entri ill use g f e refer score lexicon entri score wouldnt mind minu minu minu exampl might log ratio account similar maximum likelihood estim could thought f given e condit probabl foreign sequenc word f condit english sentenc sequenc word e ratio two count go take log saw last lectur conveni take log well start sum differ score differ phrase use translat throughout lectur im go consid phrasebas model consist three thing firstli phrasebas lexicon consist f e pair exactli show second thing go make use trigram languag model languag tri translat throughout lectur ill assum tri translat german english phrase entri one compon second compon trigram languag model english languag model exampl trigram languag model languag tri translat paramet exactli saw first lectur class exampl paramet probabl also given previou two word must final go distort paramet singl paramet typic neg well see penal thing move far translat process start use phrase translat particular exampl that essenti three compon phrasebas translat model littl bit trigram languag model paramet train larg quantiti english text trigram languag model go invalu provid prior distribut sentenc versu arent like english here notat ill use throughout lectur given sentenc tri tri translat extract set possibl phrase applic particular sentenc itll use use follow notat phrase ste go start point phrase go end point phrase e sequenc one english word one phrase applic particular exampl must let number word say two word wir mussen translat must okay word inclus translat must okay im go use capit p script p denot set possibl phrase particular sentenc okay go deriv entri come fact phrase entri lexicon say two word translat must two word wir mussen appear appli entri go littl bit conveni think phrase particular input sentenc start point end point english string p go set possibl phrase sentenc go set might exampl includ must might includ must also say three word translat must also might includ entri say serious would come entri say word ernst sixth translat serious set might quit larg notic mani phrase overlap exampl entri word entri word summari big p set possibl phrase input sentenc phrase start point end point english string phrase p let say p equal must ill sometim use follow notat sp tp ep three compon p p case e p equal must okay three function pull three differ compon phrase final g p go score phrase might minu come origin lexicon entri entri say wir mussen must g equal minu rememb typic log count whole phrase divid count english say that score phrase tabl okay p go set old phrase sentenc phrase go score probabl easili calcul set take phrasebas lexicon appli particular input sentenc tri translat,[10  8  4  0  3]
79,Course1_W6-S2-L2_Definition_of_the_Decoding_Problem_Part_2_13-00,okay particular input sentence trying translate going define idea derivation basically sequence phrases drawn set p okay derivation sequence phrases p p p capital l l constant integer okay sequences length next slide well talk constraints derivations make valid invalid heres one example derivation saying translate example ive shown choose firstly phrase says translate words must also next phrase translating words inclusive take criticism seriously okay derivation going sequence phrases phrases going define translation obvious ways im going use e pull english string translation underlying particular example thats taken concatenating english strings must also followed take followed criticism followed seriously okay short derivation sequence phrases phrases e triple start point end point e english sequence words lets look makes valid derivation input sentence trying translate input sentence x consists n words x x n im going use x refer set valid derivations sentence slide going give abstract definition next slide ill give example illustrating ideas x going set finite length sequences phrases p p p l l length satisfy following constraints firstly say phrase remember phrase triple e sequence english words phrases member set phrases x x n phrase valid phrase base lexicon second constraint kind obvious says word input sentence translated exactly third case little interesting abstract definition well give example next slide illustrating detail phrase based systems make use something called distortion limit sound going hard constraint far phrases move typical value might little equals distortion limit basically saying remember p k end point k phrase p k plus start point next phrase okay might sequence two phrases must might also something going look endpoint thats p k start point make sure two values arent far going hard constraint far phrases move taking absolute value p k plus minus p k case would absolute value plus minus equal case distortion would less equal weve satisfied distortion limit okay intuitively two phrases right adjacent ive translated first words words adjacent distortion particular pair phrases satisfies distortion limit addition constraint start sentence saying start first phrase minus start first phrase less equal okay come example moment reason distortion limit really twofold firstly going reduce search space possible translations make translation efficient second empirically found distortion limit thats formed often improve translation performance proves phrases moving far translations tend rather poor flip side really incredibly crude way control reordering different languages different languages different word orders simply imposing limit saying allowing two languages vary word order hard limit equals end lecture ill talk recent methods alternative approaches try build sophisticated models reordering effect okay lets illustrate definitions example im going show different possible derivations example sentence going check theyre valid invalid heres first possible derivation first know intuitively first translate wir missen auch must also secondly lets number words second phrase says lets translate words inclusive thats single word nehmen well translate word take translate words criticism sound finally well translate word seriously okay final translation actually pretty good translation german input must also take criticism seriously produced derivation sequence phrases actually valid derivation check one need check word translated exactly thats first thing need check okay look see sure enough every one words translated exactly satisfy constraint second constraint distortion limit thats little bit complicated let go detail going assume distortion limit equal goes im going check end point phrase stop point next phrase im going calculate value plus minus remember wanted magnitude p k plus minus p k plus less equal k p k kth phrase p k plus k plus phrase end point phra phrase start point phrase compare calculate equal absolute magnitude absolute magnitude plus minus minus equal thats less equal okay two phrases essentially close enough thats saying go term term next compare position taking end point phrase start point next phrase got plus minus actually equal less equal good thats also valid next one check followed endpoint phrase stop point phrase look plus minus actually magnitude clearly less equal two phrases actually consecutive sentence consecutive finally look first position remember also magnitude minus p less equal minus start point first phrase magnitude less equal okay short derivation satisfies firstly constraint word translated exactly second none phrases move far distortion limit satisfied okay distortion limit calculated looking end point phrase start point next phrase making sure far okay heres second derivation well see rule quickly invalid sequence phrases phrase consists start end point sequence english words problem course two words translated twice lets give count many times ive translated word think zero times weve clearly know failed derivation good derivation doesnt satisfy constraint word translated exactly heres fine final example well check two constraints particular derivation first constraint know exactly word translated exactly particular case see thats case look phrases every word translated exactly example second question distortion limit sound lets go using procedure showed check firstly lets compare im going assume distortion limit equal take magnitude plus minus magnitude minus less equal thats fine first pair phrases satisfy distortion limit lets look second pair though compare plus minus magnitude less equal weve immediately seen pair phrases violated distortion limit theres much dist distance position position definition actually invalid derivation spite satisfying constraint word translated exactly violate distortion limit ruled,Course1,W6-S2-L2,W6,S2,L2,Definition,6,2,2,okay particular input sentenc tri translat go defin idea deriv basic sequenc phrase drawn set p okay deriv sequenc phrase p p p capit l l constant integ okay sequenc length next slide well talk constraint deriv make valid invalid here one exampl deriv say translat exampl ive shown choos firstli phrase say translat word must also next phrase translat word inclus take critic serious okay deriv go sequenc phrase phrase go defin translat obviou way im go use e pull english string translat underli particular exampl that taken concaten english string must also follow take follow critic follow serious okay short deriv sequenc phrase phrase e tripl start point end point e english sequenc word let look make valid deriv input sentenc tri translat input sentenc x consist n word x x n im go use x refer set valid deriv sentenc slide go give abstract definit next slide ill give exampl illustr idea x go set finit length sequenc phrase p p p l l length satisfi follow constraint firstli say phrase rememb phrase tripl e sequenc english word phrase member set phrase x x n phrase valid phrase base lexicon second constraint kind obviou say word input sentenc translat exactli third case littl interest abstract definit well give exampl next slide illustr detail phrase base system make use someth call distort limit sound go hard constraint far phrase move typic valu might littl equal distort limit basic say rememb p k end point k phrase p k plu start point next phrase okay might sequenc two phrase must might also someth go look endpoint that p k start point make sure two valu arent far go hard constraint far phrase move take absolut valu p k plu minu p k case would absolut valu plu minu equal case distort would less equal weve satisfi distort limit okay intuit two phrase right adjac ive translat first word word adjac distort particular pair phrase satisfi distort limit addit constraint start sentenc say start first phrase minu start first phrase less equal okay come exampl moment reason distort limit realli twofold firstli go reduc search space possibl translat make translat effici second empir found distort limit that form often improv translat perform prove phrase move far translat tend rather poor flip side realli incred crude way control reorder differ languag differ languag differ word order simpli impos limit say allow two languag vari word order hard limit equal end lectur ill talk recent method altern approach tri build sophist model reorder effect okay let illustr definit exampl im go show differ possibl deriv exampl sentenc go check theyr valid invalid here first possibl deriv first know intuit first translat wir missen auch must also secondli let number word second phrase say let translat word inclus that singl word nehmen well translat word take translat word critic sound final well translat word serious okay final translat actual pretti good translat german input must also take critic serious produc deriv sequenc phrase actual valid deriv check one need check word translat exactli that first thing need check okay look see sure enough everi one word translat exactli satisfi constraint second constraint distort limit that littl bit complic let go detail go assum distort limit equal goe im go check end point phrase stop point next phrase im go calcul valu plu minu rememb want magnitud p k plu minu p k plu less equal k p k kth phrase p k plu k plu phrase end point phra phrase start point phrase compar calcul equal absolut magnitud absolut magnitud plu minu minu equal that less equal okay two phrase essenti close enough that say go term term next compar posit take end point phrase start point next phrase got plu minu actual equal less equal good that also valid next one check follow endpoint phrase stop point phrase look plu minu actual magnitud clearli less equal two phrase actual consecut sentenc consecut final look first posit rememb also magnitud minu p less equal minu start point first phrase magnitud less equal okay short deriv satisfi firstli constraint word translat exactli second none phrase move far distort limit satisfi okay distort limit calcul look end point phrase start point next phrase make sure far okay here second deriv well see rule quickli invalid sequenc phrase phrase consist start end point sequenc english word problem cours two word translat twice let give count mani time ive translat word think zero time weve clearli know fail deriv good deriv doesnt satisfi constraint word translat exactli here fine final exampl well check two constraint particular deriv first constraint know exactli word translat exactli particular case see that case look phrase everi word translat exactli exampl second question distort limit sound let go use procedur show check firstli let compar im go assum distort limit equal take magnitud plu minu magnitud minu less equal that fine first pair phrase satisfi distort limit let look second pair though compar plu minu magnitud less equal weve immedi seen pair phrase violat distort limit there much dist distanc posit posit definit actual invalid deriv spite satisfi constraint word translat exactli violat distort limit rule,[10  8  4  0 14]
80,Course1_W6-S2-L3_Definition_of_the_Decoding_Problem_Part_3_10-43,final word idea set possible derivations remember x set valid derivations particular input sentence x general exponential size sound little precisely number elements number valid derivations going grow exponentially fast respect sentence length ok thats usual observation going large set possible derivations blankaudio okay almost definition decoding problem phrase based models final definition following given sentence x x going sequence words x xn going german sentence particular examples generate set valid derivations going sequence phrases okay set said could extremely large translation problem going try find derivation set highest score subfunction f okay fy roughly speaking something like log p p x given trigram language model translation model say roughly speaking actually questionable whether phrasebased models well define really rigorously define model form thats roughly intuition okay good translation remember going derivation going good probability trigram language model also good probability translation model lets absolutely explicit f score derivation going take account three terms im going give abstract definition well go considerable detail next slide example three terms following firstly remember e sequence words translation could example must alsosound take criticisms seriously h e going score trigram language model okay score h going score english sentence sorry maybe clear english sequence words going actually log probability trigram language model score okay second term remember derivation consists sequence phrases p p pl phrases going score g p example might score first phrase might minus something going sum scores individual phrases okay thats second part score finally going called distortion score add addition strict limit distortion distances showed previous slides actually going cost associated distortions going take eta times distance calculated basically distance phrase next phrase eta parameter thats typically negative penalizes reorderings penalizes large values difference two phrases eta typically negative might take value like minus minus something like typically chosen optimize translation performance okay various ways choose eta way optimizes translation quality okay lets look expression calculated using particular example definition f decoding problem going try find derivation maximizes sum three different scores okay lets illustrate example input sentence one possible derivation im going calculate f going score particular derivation okay said going consist sum terms language model score first language model sound secondly called phrase scores sound finally distortion scores sound okay lets go turn firstly language model im going sum terms firstly im going log q given star star sound im going log must given star must second word translation log also given must log take given must alsosound derivation translation must also take criticism seriously basically sum terms case take trigram parameter take log sum together logs okay thats language model score going reflect likely final translation sentence english making critical use language model scoring derivation plausible sentence english thats first component lets look phrase scores im going set terms g must also plus g take plus g criticism remember g function takes phrase gives score typically log probability going one g terms phrases phrases going four g terms thats phrase scores finally lets look distortion scores scored particular example okay im going go phrase turn looking far previous ph phrase first thing going look start point first phrase look one minus one thats absolute value times eta corresponds thats first term basically eta times minus p penalizing far first phrase phrase start german sentence actually zero case lets look next term im going compare end phrase start one im going take plus minus take absolute value times eta okay im going look versus im going look plus minus times eta versus plus minus times eta eta might take negative value example eta equals minus typically chosen optimize performance translation model see terms basically going give penalties whenever see distortion thats nonzero zero zero think value value final cost case times eta final distortion cost particular example okay jump back back high level sum three different types scores language model phrase scores distortion scores language model scores likely resulting translation sentence english phrase stores scores thought measure well english phrases match underlying german example well must also matches wir mussen auch finally distortion scores penalize phrases moving long distances translations,Course1,W6-S2-L3,W6,S2,L3,Definition,6,2,3,final word idea set possibl deriv rememb x set valid deriv particular input sentenc x gener exponenti size sound littl precis number element number valid deriv go grow exponenti fast respect sentenc length ok that usual observ go larg set possibl deriv blankaudio okay almost definit decod problem phrase base model final definit follow given sentenc x x go sequenc word x xn go german sentenc particular exampl gener set valid deriv go sequenc phrase okay set said could extrem larg translat problem go tri find deriv set highest score subfunct f okay fy roughli speak someth like log p p x given trigram languag model translat model say roughli speak actual question whether phrasebas model well defin realli rigor defin model form that roughli intuit okay good translat rememb go deriv go good probabl trigram languag model also good probabl translat model let absolut explicit f score deriv go take account three term im go give abstract definit well go consider detail next slide exampl three term follow firstli rememb e sequenc word translat could exampl must alsosound take critic serious h e go score trigram languag model okay score h go score english sentenc sorri mayb clear english sequenc word go actual log probabl trigram languag model score okay second term rememb deriv consist sequenc phrase p p pl phrase go score g p exampl might score first phrase might minu someth go sum score individu phrase okay that second part score final go call distort score add addit strict limit distort distanc show previou slide actual go cost associ distort go take eta time distanc calcul basic distanc phrase next phrase eta paramet that typic neg penal reorder penal larg valu differ two phrase eta typic neg might take valu like minu minu someth like typic chosen optim translat perform okay variou way choos eta way optim translat qualiti okay let look express calcul use particular exampl definit f decod problem go tri find deriv maxim sum three differ score okay let illustr exampl input sentenc one possibl deriv im go calcul f go score particular deriv okay said go consist sum term languag model score first languag model sound secondli call phrase score sound final distort score sound okay let go turn firstli languag model im go sum term firstli im go log q given star star sound im go log must given star must second word translat log also given must log take given must alsosound deriv translat must also take critic serious basic sum term case take trigram paramet take log sum togeth log okay that languag model score go reflect like final translat sentenc english make critic use languag model score deriv plausibl sentenc english that first compon let look phrase score im go set term g must also plu g take plu g critic rememb g function take phrase give score typic log probabl go one g term phrase phrase go four g term that phrase score final let look distort score score particular exampl okay im go go phrase turn look far previou ph phrase first thing go look start point first phrase look one minu one that absolut valu time eta correspond that first term basic eta time minu p penal far first phrase phrase start german sentenc actual zero case let look next term im go compar end phrase start one im go take plu minu take absolut valu time eta okay im go look versu im go look plu minu time eta versu plu minu time eta eta might take neg valu exampl eta equal minu typic chosen optim perform translat model see term basic go give penalti whenev see distort that nonzero zero zero think valu valu final cost case time eta final distort cost particular exampl okay jump back back high level sum three differ type score languag model phrase score distort score languag model score like result translat sentenc english phrase store score thought measur well english phrase match underli german exampl well must also match wir mussen auch final distort score penal phrase move long distanc translat,[10  4  8 14 13]
81,Course1_W6-S2-L4_The_Decoding_Algorithm_Part_1_14-39,go back problem translation problem input sentence x going search possible derivations sentence one score f equal score showed going try find highest scoring derivation score okay set x typically exponential size sound certainly brute force method explicitly enumerating every possible derivation scoring way going feasible next thing going develop algorithm problem worth noting problem actually hard almost certainly nphard problem going mean going develop heuristic method solving problem based beam search well see develop approximate search algorithm problem algorithm really performs quite well practice first key idea algorithm going idea state state tuple heres example might state must say sentence length x x x first two elements tuple two words must essentially corresponding last two words translation well see example next slide works b going bit string length length input sentence might bit string r going integer alpha going score okay states intuitively correspond saying formed translation ending words must translates words sentence none words bit string going record keep track words input sentance actually translated position going endpoint last phase example might used phrase must last phrase building state records going allow us enforce distortion limit also calculate distortion scores finally going score indication score partial translation reached state initial state always going following star star usual start symbols language models reflects start translation always starts star star bit string specifying none input words translated seven case assuming input length seven endpoint previous phrase start sentence score partial translation hasnt used phrases yet score essentially going useful think search space translation problem essentially directed graph states ive shown previous slide modes graph let explain mean lets assume sentence trying translate ive shown start state star star seve specifying seven words none translated point endpoint last phrase score given state choose first phrase translation lets example assume choose phrase translate first two words must ill label arc choice phrase heres state end going must im going following bit string specifying first two words translated ones havent ill thats end point phrase ill score like minus example okay arcs going labeled choices phrases theyre going leave one state another intuitively arc corresponds choice phrase must start sentence let talk little bit score calculated case would following would combination language modeling scores phrase scores also distortion scores wed language modeling scores log q given star star plus log q must given star one log q terms word must g must recall thats score choice phrase cor matching must wir mussen finally ader times distortion case distortion previous phrase ended point usual rules plus minus actually equal theres distortion cost sum terms going give us score basically score sorry plus started score ive added terms corresponding transition might several possibilities heres another one could also say must also auch corresponding choice translating first three words must also would lead state must also last two words bit string specifying first three words translated endpoint phrase ends point gains score would three log q terms one one must one also youll g term must also fact distortion cost heres third example another choice might following might say also corresponds translating word auch also would lead state star also wed specifying third word translated would ends point score minus maybe minus something okay states also outgoing also say point many many essentially one auch every possible phrase could come start sentence phrases also outgoing auchs example might following might say lets lets translate words criticism okay thats another choice phrase would actually lead state criticism would would end point wed new score maybe minus new score calculated score plus various terms language modeling terms two words phrase score phrase distortion score similarly different states also outgoing auchs okay think entire graph state list possible outgoing auchs possible phrases follow state large search space every path directed graph going correspond translation final states translations look like following might something like criticisms seriously choice two words end sentence critically bit string final states going complete specifying weve completed words sentence might end point example minus score okay translation task visualized trying find path one end end states high score trying find end state highest possible score thats useful way visualize search space important realize graph exponential size well take one bit strings n possible bit strings going exponential number notes graph means going able explore graph exhaustively useful picture mind graph directed graph states nodes outgoing auchs labeled choices phrases point phrase combined previous state specifies next state graph little bit notation reflect state q example star star q well define p h q function returns set possible phrases follow state might things like must must also may also phrases satisfy two different constraints okay one thing weve gotta make sure dont translate word twice phrase p p member set p h q p must overlap bitstring b okay case bitstring empty phrases fine example words certainly translated consider different state say take q equals must state reflecting fact first two words translated something like state sorry phrase start phrase maybe one possible phrase definitely p h q phrase follow would mean translate word one see bit strings come play constrain set possible phrases choose point particularly choose phrases translate new words okay second condition distortion limit must violated remember states store endpoint previous phrase example means last phrase chose ended point allows us immediately check phrase distortion example following phrase choose something like also plus minus equal less equal rather lets say distortion limit equal hand take phrase like dont know seriously okay case look distortion plus minus equal magnitude minus definitely violates constraint phrase would allowed follow state okay summarize two constraints phrase set p h q remember set phrases follow state q must overlap bit string also must satisfy distortion limit,Course1,W6-S2-L4,W6,S2,L4,The,6,2,4,go back problem translat problem input sentenc x go search possibl deriv sentenc one score f equal score show go tri find highest score deriv score okay set x typic exponenti size sound certainli brute forc method explicitli enumer everi possibl deriv score way go feasibl next thing go develop algorithm problem worth note problem actual hard almost certainli nphard problem go mean go develop heurist method solv problem base beam search well see develop approxim search algorithm problem algorithm realli perform quit well practic first key idea algorithm go idea state state tupl here exampl might state must say sentenc length x x x first two element tupl two word must essenti correspond last two word translat well see exampl next slide work b go bit string length length input sentenc might bit string r go integ alpha go score okay state intuit correspond say form translat end word must translat word sentenc none word bit string go record keep track word input sentanc actual translat posit go endpoint last phase exampl might use phrase must last phrase build state record go allow us enforc distort limit also calcul distort score final go score indic score partial translat reach state initi state alway go follow star star usual start symbol languag model reflect start translat alway start star star bit string specifi none input word translat seven case assum input length seven endpoint previou phrase start sentenc score partial translat hasnt use phrase yet score essenti go use think search space translat problem essenti direct graph state ive shown previou slide mode graph let explain mean let assum sentenc tri translat ive shown start state star star seve specifi seven word none translat point endpoint last phrase score given state choos first phrase translat let exampl assum choos phrase translat first two word must ill label arc choic phrase here state end go must im go follow bit string specifi first two word translat one havent ill that end point phrase ill score like minu exampl okay arc go label choic phrase theyr go leav one state anoth intuit arc correspond choic phrase must start sentenc let talk littl bit score calcul case would follow would combin languag model score phrase score also distort score wed languag model score log q given star star plu log q must given star one log q term word must g must recal that score choic phrase cor match must wir mussen final ader time distort case distort previou phrase end point usual rule plu minu actual equal there distort cost sum term go give us score basic score sorri plu start score ive ad term correspond transit might sever possibl here anoth one could also say must also auch correspond choic translat first three word must also would lead state must also last two word bit string specifi first three word translat endpoint phrase end point gain score would three log q term one one must one also youll g term must also fact distort cost here third exampl anoth choic might follow might say also correspond translat word auch also would lead state star also wed specifi third word translat would end point score minu mayb minu someth okay state also outgo also say point mani mani essenti one auch everi possibl phrase could come start sentenc phrase also outgo auch exampl might follow might say let let translat word critic okay that anoth choic phrase would actual lead state critic would would end point wed new score mayb minu new score calcul score plu variou term languag model term two word phrase score phrase distort score similarli differ state also outgo auch okay think entir graph state list possibl outgo auch possibl phrase follow state larg search space everi path direct graph go correspond translat final state translat look like follow might someth like critic serious choic two word end sentenc critic bit string final state go complet specifi weve complet word sentenc might end point exampl minu score okay translat task visual tri find path one end end state high score tri find end state highest possibl score that use way visual search space import realiz graph exponenti size well take one bit string n possibl bit string go exponenti number note graph mean go abl explor graph exhaust use pictur mind graph direct graph state node outgo auch label choic phrase point phrase combin previou state specifi next state graph littl bit notat reflect state q exampl star star q well defin p h q function return set possibl phrase follow state might thing like must must also may also phrase satisfi two differ constraint okay one thing weve gotta make sure dont translat word twice phrase p p member set p h q p must overlap bitstr b okay case bitstr empti phrase fine exampl word certainli translat consid differ state say take q equal must state reflect fact first two word translat someth like state sorri phrase start phrase mayb one possibl phrase definit p h q phrase follow would mean translat word one see bit string come play constrain set possibl phrase choos point particularli choos phrase translat new word okay second condit distort limit must violat rememb state store endpoint previou phrase exampl mean last phrase chose end point allow us immedi check phrase distort exampl follow phrase choos someth like also plu minu equal less equal rather let say distort limit equal hand take phrase like dont know serious okay case look distort plu minu equal magnitud minu definit violat constraint phrase would allow follow state okay summar two constraint phrase set p h q rememb set phrase follow state q must overlap bit string also must satisfi distort limit,[10  4  8 14 13]
82,Course1_W6-S2-L5_The_Decoding_Algorithm_Part_2_6-23,okay lets look another example q say state two ends must also end translation words translated ones arent position last endpoint last phrase ph q going set possible phrases could follow state lets see weve translated words natural things would things like criticism okay phrase certainly doesnt overlap bit string addition close enough allowed seriously theres overlap bit string far enough sorry close enough three theres violation might even take theres overlap bit string verify plus minus equal magnitude minus less equal thats fine assuming distortion limit going whole set possible phrases notice looking state calculate phrases actually follow another useful definition going following q phrase p im going define next q p state formed combining state q phrase p mean next lets say take q take one phrases example take okay going return new state formed basically concatenating phrase end state ive shown state case would also take would last two words translation would following one two three take position one two three four five six seven update seventh bit reflect one record endpoint phrase seven update score would minus something calculated using combination trigram language parameters phrase score distortion score okay basically implementing graph definition showed earlier transition take particular phrase exactly ive shown formal details actually define next function ill go quickly think youve gotten intuition last slide next function takes state q phrase p state q consists e e bit stream b position r score alpha phrase consists start point endpoint sequence capital words cause epsilon epsilon next state state q primed new elements e prime e prime b primed r primed alpha primed calculated follows let briefly give sketch e primed e epsilon minus e primed epsilon take last two words phrase weve got slightly careful case phrase one word long definitions capture update bit string say new bit string value range start endpoint phrase otherwise copy across bit string previous bit string define endpoint r prime endpoint new phrase thats r primed alpha primed score new state going alpha score old state plus score phrase g p language model scores trigram language model scores distortion parameter distortion cost notice r endpoint last phrase stop point new phrase calculate distortion cost straightforward manner okay thats formal definition next function almost terms defining beam search heres one definition useful simple function return true false depending whether two states q q prime equal particular definition equality say two states q q primed equal function going return true e equal e primed e equal e primed b equal b primed r equal r primed actually going ignore scores going come take account four examples going return true four things equal well see definition important particular dont check alpha alpha primed scores kay may example two different ways reaching particular setting say must also maybe something like maybe another state looks like actually components first entries different score say q q primed case eq q q primed equal true,Course1,W6-S2-L5,W6,S2,L5,The,6,2,5,okay let look anoth exampl q say state two end must also end translat word translat one arent posit last endpoint last phrase ph q go set possibl phrase could follow state let see weve translat word natur thing would thing like critic okay phrase certainli doesnt overlap bit string addit close enough allow serious there overlap bit string far enough sorri close enough three there violat might even take there overlap bit string verifi plu minu equal magnitud minu less equal that fine assum distort limit go whole set possibl phrase notic look state calcul phrase actual follow anoth use definit go follow q phrase p im go defin next q p state form combin state q phrase p mean next let say take q take one phrase exampl take okay go return new state form basic concaten phrase end state ive shown state case would also take would last two word translat would follow one two three take posit one two three four five six seven updat seventh bit reflect one record endpoint phrase seven updat score would minu someth calcul use combin trigram languag paramet phrase score distort score okay basic implement graph definit show earlier transit take particular phrase exactli ive shown formal detail actual defin next function ill go quickli think youv gotten intuit last slide next function take state q phrase p state q consist e e bit stream b posit r score alpha phrase consist start point endpoint sequenc capit word caus epsilon epsilon next state state q prime new element e prime e prime b prime r prime alpha prime calcul follow let briefli give sketch e prime e epsilon minu e prime epsilon take last two word phrase weve got slightli care case phrase one word long definit captur updat bit string say new bit string valu rang start endpoint phrase otherwis copi across bit string previou bit string defin endpoint r prime endpoint new phrase that r prime alpha prime score new state go alpha score old state plu score phrase g p languag model score trigram languag model score distort paramet distort cost notic r endpoint last phrase stop point new phrase calcul distort cost straightforward manner okay that formal definit next function almost term defin beam search here one definit use simpl function return true fals depend whether two state q q prime equal particular definit equal say two state q q prime equal function go return true e equal e prime e equal e prime b equal b prime r equal r prime actual go ignor score go come take account four exampl go return true four thing equal well see definit import particular dont check alpha alpha prime score kay may exampl two differ way reach particular set say must also mayb someth like mayb anoth state look like actual compon first entri differ score say q q prime case eq q q prime equal true,[10  4 14 13 12]
83,Course1_W6-S2-L6_The_Decoding_Algorithm_Part_3_12-29,put ideas together final decoding algorithm beam search algorithm phrasebased models input algorithm sentence x xn example might sentence german addition phrasebased model phrasebased model consists lexicon l remember consists large set german phrases paired english phrases example language model ive called h distortion limit little example equal distortion parameter given input phrasebased model define functions ph q next q p remember ph function takes state input returns phrases follow state next q p takes state together phrase returns next state state formed concatenating phrase state q lets concrete assume put sentence length x x upto x input key data structure going set qs ill use capital q q beam q q q right q q well see qs going contain states created search process initialization step set first q contain single element little q remember little q initial state going something like star star one two three four five six seven position score okay initial state okay next thing going going loop equals n minus initially going start q going move qs turn value first thing look every possible state q sub case would look initial state explore every possible phrase follow initial state going look phrases p ph q might things like must basically corresponds outset outgoing arcs sorry must must also maybe also okay going set phrases could follow initial state going explore one term one possible phrases calculate next state call q primed state appropriate q example look must get following state must five score minus state going get added one qs one get added actually gets added q number words translated state q primed equal q going store whole bunch states one restriction bit string states exactly words translated idea beam search algorithms sense group together states number words translated thats q words translated q one q two irrespective position grouping together states number words translated look three phrases phrase going lead towards member q going add state ones going add q first step considered equals considered single state q considered ways extending thats going lead bunch new states q primed added later qs notice add function well describe moment intuitively going add phrase sorry state q primed qi critically length q primed number words translated q primed add also going take q p previous state phrase thatll additional bookkeeping okay actually move qs one one q going take one state expand various ways im going get q im going find one states q one im going go process im going find phrases follow state im going concatenate im going add entries later go youre going populate states various qs okay finally well hit q well take states populate case populate q q always pick state earlier q extend populating state later q final thing return highest scoring state q sub n q case would set states one score simply return highest scoring state set q correspond highest scoring translation found beam search process similar way dynamic programming backpointers actually used trace back steps constructing particular translation thereby get final translation noise lets look two functions play critical role one add function well see simple secondly critically beam function okay actually said take q state q take q q equal set states q equal set states careful sets quickly grow exponentially size youre going end huge number states weve said number possible states models exponential size beam function actually going consider small subset items particular q expand small subset thats going make whole thing efficient okay lets look two functions beam add heres definition add add takes q capital q state q primed adds q prime q remember q primed equal next q p record history used create q primed first thing check see theres something already set equal current state okay remember equal function example something like following must also minus okay maybe q primed thing adding existing state looks extremely similar fact equal definition equal gave earlier add position well okay two states equal definition showed earlier two words bit string last position final phrase equal thing differs score step going similar dynamic programming two things equal simply keep highest scoring element records highest scoring path particular state theres reason keep lowest scoring path okay two things check score new thing added created old score move old state add new state record backpointers backpointers going say reached state q primed extending state q phrase p okay otherwise dont find anything q equal current state added following simply add add q primed set backpointer set record okay thats add function basically adding q primed q recording backpointer little careful multiple states equal different scores simply take highest scoring states final function idea beam beam q going basically subset highest scoring items q define well say b set q number possible states score lets write scores first find highest scoring state set minus example maybe thats highest score sorry max arg max okay alpha star highest score state set beta beamwidth parameter says well far example might take beta beta equal subtract beam minus minus equal minus basically keep states score greater equal throw states away example state minus get thrown away state minus get thrown away okay beam going keep items within certain tolerance highest scoring element q intuition sort throw away translations even theyre complete suboptimal low scores particular state lets go back algorithm see two functions used remember go equals right way n minus remove states satisfy beam condition okay remove high scoring states point extend state call add function add function actually throw away terms two states equal one higher score,Course1,W6-S2-L6,W6,S2,L6,The,6,2,6,put idea togeth final decod algorithm beam search algorithm phrasebas model input algorithm sentenc x xn exampl might sentenc german addit phrasebas model phrasebas model consist lexicon l rememb consist larg set german phrase pair english phrase exampl languag model ive call h distort limit littl exampl equal distort paramet given input phrasebas model defin function ph q next q p rememb ph function take state input return phrase follow state next q p take state togeth phrase return next state state form concaten phrase state q let concret assum put sentenc length x x upto x input key data structur go set qs ill use capit q q beam q q q right q q well see qs go contain state creat search process initi step set first q contain singl element littl q rememb littl q initi state go someth like star star one two three four five six seven posit score okay initi state okay next thing go go loop equal n minu initi go start q go move qs turn valu first thing look everi possibl state q sub case would look initi state explor everi possibl phrase follow initi state go look phrase p ph q might thing like must basic correspond outset outgo arc sorri must must also mayb also okay go set phrase could follow initi state go explor one term one possibl phrase calcul next state call q prime state appropri q exampl look must get follow state must five score minu state go get ad one qs one get ad actual get ad q number word translat state q prime equal q go store whole bunch state one restrict bit string state exactli word translat idea beam search algorithm sens group togeth state number word translat that q word translat q one q two irrespect posit group togeth state number word translat look three phrase phrase go lead toward member q go add state one go add q first step consid equal consid singl state q consid way extend that go lead bunch new state q prime ad later qs notic add function well describ moment intuit go add phrase sorri state q prime qi critic length q prime number word translat q prime add also go take q p previou state phrase thatll addit bookkeep okay actual move qs one one q go take one state expand variou way im go get q im go find one state q one im go go process im go find phrase follow state im go concaten im go add entri later go your go popul state variou qs okay final well hit q well take state popul case popul q q alway pick state earlier q extend popul state later q final thing return highest score state q sub n q case would set state one score simpli return highest score state set q correspond highest score translat found beam search process similar way dynam program backpoint actual use trace back step construct particular translat therebi get final translat nois let look two function play critic role one add function well see simpl secondli critic beam function okay actual said take q state q take q q equal set state q equal set state care set quickli grow exponenti size your go end huge number state weve said number possibl state model exponenti size beam function actual go consid small subset item particular q expand small subset that go make whole thing effici okay let look two function beam add here definit add add take q capit q state q prime add q prime q rememb q prime equal next q p record histori use creat q prime first thing check see there someth alreadi set equal current state okay rememb equal function exampl someth like follow must also minu okay mayb q prime thing ad exist state look extrem similar fact equal definit equal gave earlier add posit well okay two state equal definit show earlier two word bit string last posit final phrase equal thing differ score step go similar dynam program two thing equal simpli keep highest score element record highest score path particular state there reason keep lowest score path okay two thing check score new thing ad creat old score move old state add new state record backpoint backpoint go say reach state q prime extend state q phrase p okay otherwis dont find anyth q equal current state ad follow simpli add add q prime set backpoint set record okay that add function basic ad q prime q record backpoint littl care multipl state equal differ score simpli take highest score state final function idea beam beam q go basic subset highest score item q defin well say b set q number possibl state score let write score first find highest score state set minu exampl mayb that highest score sorri max arg max okay alpha star highest score state set beta beamwidth paramet say well far exampl might take beta beta equal subtract beam minu minu equal minu basic keep state score greater equal throw state away exampl state minu get thrown away state minu get thrown away okay beam go keep item within certain toler highest score element q intuit sort throw away translat even theyr complet suboptim low score particular state let go back algorithm see two function use rememb go equal right way n minu remov state satisfi beam condit okay remov high score state point extend state call add function add function actual throw away term two state equal one higher score,[10  4 14 13 12]
84,Course1_W7-S1-L1_Introduction_0-47,next segment class going look loglinear models loglinear models offer powerful set techniques alternative smooth estimation techniques weve seen earlier class sense simple way estimating parameters models defining models well see loglinear models allow us make use much representation many natural language processing problems well see use rather interesting powerful ideas estimate parameters models well describing basic framework loglinear models well looking applications number problems natural language processing,Course1,W7-S1-L1,W7,S1,L1,Introduction,7,1,1,next segment class go look loglinear model loglinear model offer power set techniqu altern smooth estim techniqu weve seen earlier class sens simpl way estim paramet model defin model well see loglinear model allow us make use much represent mani natur languag process problem well see use rather interest power idea estim paramet model well describ basic framework loglinear model well look applic number problem natur languag process,[ 4  1  5  8 14]
85,Course1_W7-S1-L2_Two_Example_Problems_11-19,first example problem im going use motivate log linear models language modeling problem saw right start class recap quickly problem follows define w sub ith word document task estimate distribution conditional distribution wi given context history previous minus words heres one example passage taken taken chomsky say sequence minus words task estimate distribution word appears ith position wi course one model studied extensively schools idea trigram language model quick recap trigram estimate defined follows define estimate probability particular word say model given previous context word trying predict wi defined combination three maximum likelihood estimates assuming using smoothing maximum likelihood estimate model given previous two words statistical maximum likelihood estimate model given word statistical finally socalled unigram estimate probability seeing model conditioned context course maximum likelihood estimates defined ratios counts weve seen want estimate probability given context x take ratio two counts lambdas three values lambda lambda lambda dictate relevant relative weight three estimates lambdas positive sum thats trigram language model familiar point class clear deficiencies okay models make use bigram trigram unigram estimates okay basically restrict small window conditioning previous two words context think back passage could kinds quote features context could useful predicting distribution next word example might want come estimate conditions fact word two backs word two positions back word notice weve sense skipped wi minus case looked word back might condition fact previous word adjective gives us coarser estimate ignores exact identity previous word estimate might able make roughly reliably given amount data might condition previous word ending particular suffix prefix example ical might condition author entire article thats likely influence distribution might condition long range features might example condition fact word model fact doesnt occur somewhere previous context might condition fact word example grammatical occurs somewhere previous context notice critically features actually also feature sense go beyond two previous words context either consider entire document meter information document example documents author point estimates could provide useful distribution useful information distribution next word document features context could useful estimating distribution course trigram model ignores information ive shown lets imagine trying define estimate probability particular word model given previous minus words document want incorporate pieces information showed one natural first attempt would define smoothed model thats similar trigram models wed seen earlier class rather similar smoothing methods wed seen models could take nine different maximum likelihood estimates ive shown regular trigram bigram unigram estimate condition word condition last word adjective condition author presence absence modeling context presence absence grammatical context take estimates simply take linear interpolation estimates parameters smoothing parameters lambda lambda lambda lambda great link sum okay wanted go thought experiment first way might try build model makes use different information practice kind approach quickly becomes extremely unwieldy fact beset kinds practical problems really makes nonstarter method trigram language model may use three estimates three definitions contexts works quite well try extend models include different types feature become extremely unwieldy well see log linear models clean think elegant solution problem incorporating multiple sources information estimates second important motivating example log linear models thats problem tagging specifically example going look part speech tagging recap problem take sentence sentence sequence words input map representation word associated tag example n noun v verb p preposition natural estimation problem associated particular problem okay take particular word sentence task going estimate distribution possible tags position might possible part speech tags ti going ith tag sequence well use wi refer ith word sequence okay going try estimate distribution potential tags ith position thats definition problem im going give going condition two things firstly potentially condition information entire sentence w wn heres input sentence secondly condition information previous minus tags particular case ti minus equal tag sequence nnp v vb dt unknown jj thats actually sequence length five trying estimate probability given previous sequence tags entire sentence input okay looks slightly different kind parameters saw hidden markov models tagging well see little later come accurate estimates conditional probabilities lead direct powerful tagging model alternative problem well alternative hidden markov models weve seen earlier course thing realize coming estimate could look kinds features history context okay look particular example trying tag word base trying estimate distribution tags position look various things say trying estimate probability seeing word base tagged nn thats common noun singular noun english condition fact current word tagged word base could condition fact previous tag ti minus tag jj could condition prefix suffix information word tagged could condition fact wi ends single letter e fact wi ends pair letters followed e could condition surrounding words case could condition fact previous word important could condition fact next word word continue example anyone features previous context could useful predicting distribution tag ith position could come method based linear interpolation old method smoothing saw within context trigram language models would quickly become really unwieldy incorporate sources information,Course1,W7-S1-L2,W7,S1,L2,Two,7,1,2,first exampl problem im go use motiv log linear model languag model problem saw right start class recap quickli problem follow defin w sub ith word document task estim distribut condit distribut wi given context histori previou minu word here one exampl passag taken taken chomski say sequenc minu word task estim distribut word appear ith posit wi cours one model studi extens school idea trigram languag model quick recap trigram estim defin follow defin estim probabl particular word say model given previou context word tri predict wi defin combin three maximum likelihood estim assum use smooth maximum likelihood estim model given previou two word statist maximum likelihood estim model given word statist final socal unigram estim probabl see model condit context cours maximum likelihood estim defin ratio count weve seen want estim probabl given context x take ratio two count lambda three valu lambda lambda lambda dictat relev rel weight three estim lambda posit sum that trigram languag model familiar point class clear defici okay model make use bigram trigram unigram estim okay basic restrict small window condit previou two word context think back passag could kind quot featur context could use predict distribut next word exampl might want come estim condit fact word two back word two posit back word notic weve sens skip wi minu case look word back might condit fact previou word adject give us coarser estim ignor exact ident previou word estim might abl make roughli reliabl given amount data might condit previou word end particular suffix prefix exampl ical might condit author entir articl that like influenc distribut might condit long rang featur might exampl condit fact word model fact doesnt occur somewher previou context might condit fact word exampl grammat occur somewher previou context notic critic featur actual also featur sens go beyond two previou word context either consid entir document meter inform document exampl document author point estim could provid use distribut use inform distribut next word document featur context could use estim distribut cours trigram model ignor inform ive shown let imagin tri defin estim probabl particular word model given previou minu word document want incorpor piec inform show one natur first attempt would defin smooth model that similar trigram model wed seen earlier class rather similar smooth method wed seen model could take nine differ maximum likelihood estim ive shown regular trigram bigram unigram estim condit word condit last word adject condit author presenc absenc model context presenc absenc grammat context take estim simpli take linear interpol estim paramet smooth paramet lambda lambda lambda lambda great link sum okay want go thought experi first way might tri build model make use differ inform practic kind approach quickli becom extrem unwieldi fact beset kind practic problem realli make nonstart method trigram languag model may use three estim three definit context work quit well tri extend model includ differ type featur becom extrem unwieldi well see log linear model clean think eleg solut problem incorpor multipl sourc inform estim second import motiv exampl log linear model that problem tag specif exampl go look part speech tag recap problem take sentenc sentenc sequenc word input map represent word associ tag exampl n noun v verb p preposit natur estim problem associ particular problem okay take particular word sentenc task go estim distribut possibl tag posit might possibl part speech tag ti go ith tag sequenc well use wi refer ith word sequenc okay go tri estim distribut potenti tag ith posit that definit problem im go give go condit two thing firstli potenti condit inform entir sentenc w wn here input sentenc secondli condit inform previou minu tag particular case ti minu equal tag sequenc nnp v vb dt unknown jj that actual sequenc length five tri estim probabl given previou sequenc tag entir sentenc input okay look slightli differ kind paramet saw hidden markov model tag well see littl later come accur estim condit probabl lead direct power tag model altern problem well altern hidden markov model weve seen earlier cours thing realiz come estim could look kind featur histori context okay look particular exampl tri tag word base tri estim distribut tag posit look variou thing say tri estim probabl see word base tag nn that common noun singular noun english condit fact current word tag word base could condit fact previou tag ti minu tag jj could condit prefix suffix inform word tag could condit fact wi end singl letter e fact wi end pair letter follow e could condit surround word case could condit fact previou word import could condit fact next word word continu exampl anyon featur previou context could use predict distribut tag ith posit could come method base linear interpol old method smooth saw within context trigram languag model would quickli becom realli unwieldi incorpor sourc inform,[ 4  1  5 14 13]
86,Course1_W7-S1-L3_Features_in_Log-Linear_Models_Part_1_13-56,remainder lecture going describe loglinear models solve many problems weve seen ill first give basic definitions loglinear models well talk estimate parameters loglinear models finally well talk smoothing often called regularization loglinear models way dealing parameter estimation problems high dimensional spaces cases large numbers parameters well see employ simple highly effective method smoothing socalled regularization models folks first lets give definition problem trying solve going assume set possible inputs call script x example could set possible document histories w w wi minus vary going sequence words previous words document set inputs set possible document prefixes thats clearly infinite set case finite okay well assume label set finite set possible call labels particular context example could set possible values wi would typically unknown v v vocabulary looking maybe v union stop addition stop symbol aim provide conditional probability p given x x pair x set possible inputs set possible labels going see loglinear models build model precisely form lets see definitions fit first example problem language modelling problem case x sequence minus words history previous words document word wi trying predict course given definitions showed previous slide trying estimate probability given x case p wi given w wi minus script x set possible document prefixes put infinite set script set possible outcomes position role possible labels position set words vocabulary first key idea loglinear models going idea features well call feature vector representations heres works ill first give abstract definition well go particular example general feature going function going features f xy f xy fm xy little features feature fk function takes x pair inputs x set possible inputs set possible labels remember trying model probability given x one features fk k equals simply going take x pairs input map positive negative value real value im going use r symbol mean set possible real values everything negative values like minus minus positive values value could potentially map x pair real value practice least natural language processing features often see called binary features indicator functions features going map x pair either often interpreted true interpreted false consider feature asking particular question x pair well see many examples moment features f fm often convenient concatenate vector okay im going use angle brackets enclose vector one possible vector dimension feature vector f x vector formed taking output feature output feature right away output feature dimensional representation x pair often referred feature vector thats rather abstract lets go particular example illustrating kind definitions okay back language modelling x history w wy minus example portion outcome example wy might model okay could word vocabulary example features actually capture kind features saw considered example start lecture okay f xy might defined indicator function takes value either either takes value equal model otherwise feature could potentially look information x information case simple feature simply looks identity label case might referred unigram feature looks single word ignores contexts bit like unigram estimates saw trigram language models heres second feature f x equal model wi minus equal word statistical okay might think bigram feature im really replicating kind information see trigram language models well soon see go trigram language models approach third feature f x equals model wi minus wi minus statistical otherwise basically called trigram feature okay features looks x pair return real value indicator functions binary features simply returns depending whether question x pair true false lets define features okay others could define f x equal model wi minus equals word often referred skip bigram feature bigram looks two words skip skips wi minus looked immediately word two words back ignoring identity one word back features useful contexts heres another feature word predicted model previous word adjective adjective assume sake argument deterministic function looks wi minus figures whether adjective another feature looks fact equal model previous word ends four letters ical c l might strong indicator type previous work might feature looking fact equals model fact author document chomsky feature conditioning fact equal word model fact word model seen previous minus words feature looks fact equals model fact grammatical seen previous minus words see could define many many features types every case look label okay theyre fact feature see reasons well see shortly always sense take label account addition consider context okay feature previous minus words method feature data example author important think little actually define features practice example showed feature f particular trigram statistical model formed look x pair otherwise practice course captures information single trigram would really want features consider sense possible trigrams possible bigrams possible unigrams thats exactly one important caveat would probably practice following would introduce one trigram feature every unique trigram scene training data triples words u v w seen training data create feature trigram u v w otherwise rather trigram feature noise might easily million trigram features actually theres real harm models large number features done ive slightly careful number feature going defined n u v w n u v w hash function maps trigram different unique integer okay weve basically indexed distinct trigrams see training data included one trigrams feature model notice key include trigrams seen training data one key reason simply many would lead huge number features basically feat number features basically v cubed v size vocabulary thats way many thing unseen trigrams well see dont really evidence estimate parameters every unknown including model okay see parameters loglinear model well see different trigrams parameter see estimate parameters data estimate parameters trigrams actually seen training data,Course1,W7-S1-L3,W7,S1,L3,Features,7,1,3,remaind lectur go describ loglinear model solv mani problem weve seen ill first give basic definit loglinear model well talk estim paramet loglinear model final well talk smooth often call regular loglinear model way deal paramet estim problem high dimension space case larg number paramet well see employ simpl highli effect method smooth socal regular model folk first let give definit problem tri solv go assum set possibl input call script x exampl could set possibl document histori w w wi minu vari go sequenc word previou word document set input set possibl document prefix that clearli infinit set case finit okay well assum label set finit set possibl call label particular context exampl could set possibl valu wi would typic unknown v v vocabulari look mayb v union stop addit stop symbol aim provid condit probabl p given x x pair x set possibl input set possibl label go see loglinear model build model precis form let see definit fit first exampl problem languag model problem case x sequenc minu word histori previou word document word wi tri predict cours given definit show previou slide tri estim probabl given x case p wi given w wi minu script x set possibl document prefix put infinit set script set possibl outcom posit role possibl label posit set word vocabulari first key idea loglinear model go idea featur well call featur vector represent here work ill first give abstract definit well go particular exampl gener featur go function go featur f xy f xy fm xy littl featur featur fk function take x pair input x set possibl input set possibl label rememb tri model probabl given x one featur fk k equal simpli go take x pair input map posit neg valu real valu im go use r symbol mean set possibl real valu everyth neg valu like minu minu posit valu valu could potenti map x pair real valu practic least natur languag process featur often see call binari featur indic function featur go map x pair either often interpret true interpret fals consid featur ask particular question x pair well see mani exampl moment featur f fm often conveni concaten vector okay im go use angl bracket enclos vector one possibl vector dimens featur vector f x vector form take output featur output featur right away output featur dimension represent x pair often refer featur vector that rather abstract let go particular exampl illustr kind definit okay back languag model x histori w wy minu exampl portion outcom exampl wy might model okay could word vocabulari exampl featur actual captur kind featur saw consid exampl start lectur okay f xy might defin indic function take valu either either take valu equal model otherwis featur could potenti look inform x inform case simpl featur simpli look ident label case might refer unigram featur look singl word ignor context bit like unigram estim saw trigram languag model here second featur f x equal model wi minu equal word statist okay might think bigram featur im realli replic kind inform see trigram languag model well soon see go trigram languag model approach third featur f x equal model wi minu wi minu statist otherwis basic call trigram featur okay featur look x pair return real valu indic function binari featur simpli return depend whether question x pair true fals let defin featur okay other could defin f x equal model wi minu equal word often refer skip bigram featur bigram look two word skip skip wi minu look immedi word two word back ignor ident one word back featur use context here anoth featur word predict model previou word adject adject assum sake argument determinist function look wi minu figur whether adject anoth featur look fact equal model previou word end four letter ical c l might strong indic type previou work might featur look fact equal model fact author document chomski featur condit fact equal word model fact word model seen previou minu word featur look fact equal model fact grammat seen previou minu word see could defin mani mani featur type everi case look label okay theyr fact featur see reason well see shortli alway sens take label account addit consid context okay featur previou minu word method featur data exampl author import think littl actual defin featur practic exampl show featur f particular trigram statist model form look x pair otherwis practic cours captur inform singl trigram would realli want featur consid sens possibl trigram possibl bigram possibl unigram that exactli one import caveat would probabl practic follow would introduc one trigram featur everi uniqu trigram scene train data tripl word u v w seen train data creat featur trigram u v w otherwis rather trigram featur nois might easili million trigram featur actual there real harm model larg number featur done ive slightli care number featur go defin n u v w n u v w hash function map trigram differ uniqu integ okay weve basic index distinct trigram see train data includ one trigram featur model notic key includ trigram seen train data one key reason simpli mani would lead huge number featur basic feat number featur basic v cube v size vocabulari that way mani thing unseen trigram well see dont realli evid estim paramet everi unknown includ model okay see paramet loglinear model well see differ trigram paramet see estim paramet data estim paramet trigram actual seen train data,[ 5  4 14 13 12]
87,Course1_W7-S1-L4_Features_in_Log-Linear_Models_Part_2_10-13,lets consider second example problem part speech tagging remember case x history consists sentence example might dog saw cat position position tagged lets say sake argument position case trying estimate distribution potential tags word sequence previous tags minus might example n v context previous tags sentence fourth position x encapsulates contextual information entire sentence previous sequence tags little bit bookkeeping position actually tagging label part speech tag going one say tags typically might see given language going assume features f k x features going take entire x pair return real value make heavy use indicator functions return depending whether property x paired true example features actually taken part speech tagger developed adwate radner parky mid made direct use loglinear models elegant effective way tagging problems example features f x current word w word base tag predicting v looks word tag pair particular feature look word base conjunction tag v practice would introduce large set features form basically every possible combination word tag least every possible combination word tag youve actually seen training data assuming tagged sentences train parameters model one clearly important type feature feature basically captures tendency particular word take particular tag think think back hidden markov models anal analogous e base given v least see parameters associated feature parameter going play similar role submission parameter saw hidden markov models heres interesting feature though f ive defined word youre predicting ends ing tag v b g van trebach tag reserved verbs gerunds things like liking talking verbs english typically end ing feature going capture tendency words ending ing take particular tag might design large number features like say possible prefixes suffixes combined possible tags least combinations youve seen training data actually essentially give full set feature types used ratnaparkhis tagger fairly simple set definitions let go used features looked word tag exactly ive shown previous slide used spelling features considered prefixes suffix suffixes length conjunction tags examples know feature saw conjoining ing suffix together v b g heres another feature one current word starts three letter prefix pre tag nn imagine two features large number looking prefixes suffixes seen training data conjunction possible tags features ratnaparkhi used used contextual features features actually analogous trigram tag parameters saw hmm remember parameters probability seeing transitive verb given previous two tags determiner adjective analogous feature feature minus minus form particular triagram okay remember sentence w w n example dog saw cat noise position tagging example equals previous tags example equal n v basically look previous tags conjunction label predicted create feature like tracking trigram tags bigram feature looks fact minus one j j equal v finally unigram feature looks label basically allow us basically capture relative frequency different tags maybe v frequent tag frequent frequent tag naive feature conditions context course useful provide evidence thats robust dont need estimate dont need much data estimate parameters associated feature features ratnaparkhi used feature looks label fact equal v looks previous word going one feature many look previous word conjunction current tag different anything saw hmm didnt see hidden markov models making use features conjoining previous word current tag indeed would quite quite difficult extend hidden markov models take account information another feature looks identity next word current tag one many features look next word current tag summary final result following come practically questions weve called features look history tag pairs basically given history x conjoined label get feature vector features indicator functions binary functions return could sort picture follows f going take account entire history x takes account sentence position previous tags considers particular tag example v sixth position returns binary vector summarizing results questions weve asked x together get different binary vector every possible tag sixth position dont want go much detail practice feature vectors often sparse fact theyre almost always sparse relatively sound versus might large number features models uncommon hundred thousand million features typically much smaller number features equal particular x pair might dont know order tens features example equal questions ask false going true see notes posted along lecture much discussion particular issue important leads models rather efficient might think given large number features particular models,Course1,W7-S1-L4,W7,S1,L4,Features,7,1,4,let consid second exampl problem part speech tag rememb case x histori consist sentenc exampl might dog saw cat posit posit tag let say sake argument posit case tri estim distribut potenti tag word sequenc previou tag minu might exampl n v context previou tag sentenc fourth posit x encapsul contextu inform entir sentenc previou sequenc tag littl bit bookkeep posit actual tag label part speech tag go one say tag typic might see given languag go assum featur f k x featur go take entir x pair return real valu make heavi use indic function return depend whether properti x pair true exampl featur actual taken part speech tagger develop adwat radner parki mid made direct use loglinear model eleg effect way tag problem exampl featur f x current word w word base tag predict v look word tag pair particular featur look word base conjunct tag v practic would introduc larg set featur form basic everi possibl combin word tag least everi possibl combin word tag youv actual seen train data assum tag sentenc train paramet model one clearli import type featur featur basic captur tendenc particular word take particular tag think think back hidden markov model anal analog e base given v least see paramet associ featur paramet go play similar role submiss paramet saw hidden markov model here interest featur though f ive defin word your predict end ing tag v b g van trebach tag reserv verb gerund thing like like talk verb english typic end ing featur go captur tendenc word end ing take particular tag might design larg number featur like say possibl prefix suffix combin possibl tag least combin youv seen train data actual essenti give full set featur type use ratnaparkhi tagger fairli simpl set definit let go use featur look word tag exactli ive shown previou slide use spell featur consid prefix suffix suffix length conjunct tag exampl know featur saw conjoin ing suffix togeth v b g here anoth featur one current word start three letter prefix pre tag nn imagin two featur larg number look prefix suffix seen train data conjunct possibl tag featur ratnaparkhi use use contextu featur featur actual analog trigram tag paramet saw hmm rememb paramet probabl see transit verb given previou two tag determin adject analog featur featur minu minu form particular triagram okay rememb sentenc w w n exampl dog saw cat nois posit tag exampl equal previou tag exampl equal n v basic look previou tag conjunct label predict creat featur like track trigram tag bigram featur look fact minu one j j equal v final unigram featur look label basic allow us basic captur rel frequenc differ tag mayb v frequent tag frequent frequent tag naiv featur condit context cours use provid evid that robust dont need estim dont need much data estim paramet associ featur featur ratnaparkhi use featur look label fact equal v look previou word go one featur mani look previou word conjunct current tag differ anyth saw hmm didnt see hidden markov model make use featur conjoin previou word current tag inde would quit quit difficult extend hidden markov model take account inform anoth featur look ident next word current tag one mani featur look next word current tag summari final result follow come practic question weve call featur look histori tag pair basic given histori x conjoin label get featur vector featur indic function binari function return could sort pictur follow f go take account entir histori x take account sentenc posit previou tag consid particular tag exampl v sixth posit return binari vector summar result question weve ask x togeth get differ binari vector everi possibl tag sixth posit dont want go much detail practic featur vector often spars fact theyr almost alway spars rel sound versu might larg number featur model uncommon hundr thousand million featur typic much smaller number featur equal particular x pair might dont know order ten featur exampl equal question ask fals go true see note post along lectur much discuss particular issu import lead model rather effici might think given larg number featur particular model,[ 5  1  4 14 13]
88,Course1_W7-S1-L5_Definition_of_Log-linear_Models_Part_1_11-50,thats definition features loglinear models assuming features f sub k k equals feat feature maps x pair real value value often context natural language processing problems addition features going define parameter vectors second key component loglinear models features going assume parameter vector also dimension use r refer space possible mdimensional real valued vectors okay example features sorry example equals features parameter vector vector consisting real values values might value reals positive negative value given feature vector f parameter vector v map x pair well call score well call score value could positive negative simply computing inner product v feature vector f x used dot refer inner product dot product two vectors little explicitly means sum k equals multiply v sub k times f sub k need sum terms okay hand x pair compute score first computing values features features computing inner product simply looking parameters multiplying various feature values key question follows definitions go scores probabilities really want probability given x could positive negative value certainly necessarily doesnt necessarily form wellformed distribution example want p sum want sum want p given x greater equal clearly still bit work going scores arbitrary positive negative values probabilities well see appealing way used within loglinear models lets continue language modelling example illustrate idea practice going happen following history x case sequence previous words want go x distribution p given x label set first thing realize definitions ive given possible value compute score taking inner product v f applied context x conjoined label think know trying predict distribution next word particular context enumerate possibilities labels set case thats going vocabulary particular language model interested labels compute value score well see moment go scores distribution p given x intuitively well see higher scores mean end higher probabilities items low scores end lower probabilities key definition going recap input domain x set possible inputs label set aim provide conditional probability given x x input set label set feature function sorry f sub k takes next way parent return real value often binary features returns feature vector f x going dimensional vector might example x pair parameter vector vector reals sound might something like sound course two vectors dimension many parameters features define probability model follows read probability given x parameter values v particular setting parameters f well distribution given x course little later well see actually estimate parameter values v actual training examples lets look definition well particular interested calculate v dot f x going score could positive negative okay positive negative clearly sense probability well see transformation show turn values probabilities well firstly exponentiate take value e raise power e dot f thing greater strictly positive value sense weve gotten little closer probabilities weve least gotten rid negative values importantly v dot f large value large positive value large large negative value thisll close fact get arbitrarily close set negative lets look denominator definition normalization term sum possible labels sum possible values primed label set calculate score labels sound denominator ratio two terms normalization constant sorry normalization term ensure wellformed distribution okay easy enough see p given x v greater numerator denominator strictly positive terms lets look happens sum possible values want ensure expression sums okay lets look back say sum set possible labels ratio two terms easy enough see equal bring sum ratio two terms little bit algebra verify two conditions satisfied weve gone scores vf value positive negative probabilities satisfy criteria theyre greater sum probabilities valid lets trace works previous example definition works say particular case want calculate probability word model given current history x sentence parameter values v first thing ill ill calculate score every possible label scores im going transform using definition showed previous slide first thing take score thats going numerator least e score going numerator e denominator sum terms follows e plus e plus e plus e minus give another example say want calculate probability word given context x parameters v thats going e minus actually ill sum nominator normalization term sound okay sum several terms every possible term vocabulary see immediately score particular word large probability close least much larger scores similarly score strongly negative general going mean probability associated word going close,Course1,W7-S1-L5,W7,S1,L5,Definition,7,1,5,that definit featur loglinear model assum featur f sub k k equal feat featur map x pair real valu valu often context natur languag process problem addit featur go defin paramet vector second key compon loglinear model featur go assum paramet vector also dimens use r refer space possibl mdimension real valu vector okay exampl featur sorri exampl equal featur paramet vector vector consist real valu valu might valu real posit neg valu given featur vector f paramet vector v map x pair well call score well call score valu could posit neg simpli comput inner product v featur vector f x use dot refer inner product dot product two vector littl explicitli mean sum k equal multipli v sub k time f sub k need sum term okay hand x pair comput score first comput valu featur featur comput inner product simpli look paramet multipli variou featur valu key question follow definit go score probabl realli want probabl given x could posit neg valu certainli necessarili doesnt necessarili form wellform distribut exampl want p sum want sum want p given x greater equal clearli still bit work go score arbitrari posit neg valu probabl well see appeal way use within loglinear model let continu languag model exampl illustr idea practic go happen follow histori x case sequenc previou word want go x distribut p given x label set first thing realiz definit ive given possibl valu comput score take inner product v f appli context x conjoin label think know tri predict distribut next word particular context enumer possibl label set case that go vocabulari particular languag model interest label comput valu score well see moment go score distribut p given x intuit well see higher score mean end higher probabl item low score end lower probabl key definit go recap input domain x set possibl input label set aim provid condit probabl given x x input set label set featur function sorri f sub k take next way parent return real valu often binari featur return featur vector f x go dimension vector might exampl x pair paramet vector vector real sound might someth like sound cours two vector dimens mani paramet featur defin probabl model follow read probabl given x paramet valu v particular set paramet f well distribut given x cours littl later well see actual estim paramet valu v actual train exampl let look definit well particular interest calcul v dot f x go score could posit neg okay posit neg clearli sens probabl well see transform show turn valu probabl well firstli exponenti take valu e rais power e dot f thing greater strictli posit valu sens weve gotten littl closer probabl weve least gotten rid neg valu importantli v dot f larg valu larg posit valu larg larg neg valu thisll close fact get arbitrarili close set neg let look denomin definit normal term sum possibl label sum possibl valu prime label set calcul score label sound denomin ratio two term normal constant sorri normal term ensur wellform distribut okay easi enough see p given x v greater numer denomin strictli posit term let look happen sum possibl valu want ensur express sum okay let look back say sum set possibl label ratio two term easi enough see equal bring sum ratio two term littl bit algebra verifi two condit satisfi weve gone score vf valu posit neg probabl satisfi criteria theyr greater sum probabl valid let trace work previou exampl definit work say particular case want calcul probabl word model given current histori x sentenc paramet valu v first thing ill ill calcul score everi possibl label score im go transform use definit show previou slide first thing take score that go numer least e score go numer e denomin sum term follow e plu e plu e plu e minu give anoth exampl say want calcul probabl word given context x paramet v that go e minu actual ill sum nomin normal term sound okay sum sever term everi possibl term vocabulari see immedi score particular word larg probabl close least much larger score similarli score strongli neg gener go mean probabl associ word go close,[ 5  4  7 10  2]
89,Course1_W7-S1-L6_Definition_of_Log-linear_Models_Part_2_3-45,basically complete definition model form loglinear models conditional probability given x parameter values v calculate compute inner product f remember equal sum k equals v sub k times f sub k x compute normalization term summing possible labels primed taking e e power v dot f x primed transmission basically takes us scores well formed distribution possible values next thing want talk actually estimate parameters training examples general going assume set features fixed thats defined coming problem parameter estimation problem going estimate parameters v training examples talk parameter estimation want talk one thing loglinear models get name things called loglinear models reason following name recall p given x v equal e vf x sum primed e vf x primed lets look happens take log okay take log p im assuming log natural log log going log e f v dot f x minus log sum primed e v dot f x primed using usual rules taking log ratio two terms course equal v dot f x log assuming natural log exponent cancel essentially end log probability two terms inner product v f applied x elements interested log normalization constant linear term thats call thing log linear models end something linear term first term normalization term function x doesnt end thats second term expression okay thats reason name see derive parameter estimates going useful bear mind take log p end two terms itll easy manipulate things form,Course1,W7-S1-L6,W7,S1,L6,Definition,7,1,6,basic complet definit model form loglinear model condit probabl given x paramet valu v calcul comput inner product f rememb equal sum k equal v sub k time f sub k x comput normal term sum possibl label prime take e e power v dot f x prime transmiss basic take us score well form distribut possibl valu next thing want talk actual estim paramet train exampl gener go assum set featur fix that defin come problem paramet estim problem go estim paramet v train exampl talk paramet estim want talk one thing loglinear model get name thing call loglinear model reason follow name recal p given x v equal e vf x sum prime e vf x prime let look happen take log okay take log p im assum log natur log log go log e f v dot f x minu log sum prime e v dot f x prime use usual rule take log ratio two term cours equal v dot f x log assum natur log expon cancel essenti end log probabl two term inner product v f appli x element interest log normal constant linear term that call thing log linear model end someth linear term first term normal term function x doesnt end that second term express okay that reason name see deriv paramet estim go use bear mind take log p end two term itll easi manipul thing form,[ 4 10  5 14 13]
90,Course1_W7-S1-L7_Parameter_Estimation_in_Log-linear_Models_Part_1_12-44,okay going talk parameter estimation loglinear models problem taking set training examples input producing output setting v parameters showed previous part lecture first key idea parameter estimation going following im going assume set training examples training sample consists xy pair ill write xi yi ith example training set going assume little n training examples xy pair xi member set inputs yi member set possible labels one example language modeling problem xi would sequence words dog saw three words case yi would single word seen following sequence words okay see easy enough given large amount text gather training examples form basically consist contexts together word appearing next word particular context going employ maximum likelihood estimation scenario least first passed problem little later see smooth estimates regularize lets consider maximumlikelihood estimation means maximumlikelihood parameters v sub ml going parameter values space possible ndimensional vectors maximize function l v l v going function takes parameter vector input returns value basically going measure well parameters fit data precisely going log likelihood data parameters mean l v defined follows sum n training examples sum equals n log probability ith training example thats log p yi given xi parameters v okay training sample fixed vary parameters v probabilities change theyll become higher lower sum log probabilities one log probability item training set thats define l v intuitively would like probabilities high possible reflecting fact parameters v fit data well maximize function l v well put high probability training samples actually see formally many nice properties derive maximumlikelihood estimation apply quite generally certainly apply particular case okay remind p takes full length form ev dot f dot dot dot normalization term takes kind form take log whole thing end showed previously lecture e v dot f minus log something okay expression explicit form sum equals n v dot f xi yi actually feature vector ith training sample kind log normalization terms sum equals n log second sum possible labels primed e v dot f xi conjoined primed primed okay basic definition maximumlikelihood estimation problem going try choose parameters v make function l v large possible big remaining question actually optimize l v find maximumlikelihood estimates get want talk one critical property l v following lv concave means essentially nicely behaved function although general case finding closed form solution arg max going possible lv concave fairly easy optimize whats mean concave imagine single parameter v case onedimensional parameter vector case graph v versus lv concave function looks like following basically take two points two points line goes underneath function particular means find local optimum particular function lv also going global optimum intuitively means use kind hillclimbing technique going reach global maximum function concave function sense relatively easy optimize many results optimization showing efficiently find maximum kind function heres function concave function multiple local optima actually global global optimum local optima globally optimal hard nonconcave roughly speaking generally much harder optimization problem beauty one beautiful thing function lv actually concave function course going generalize multidimensional case similar definitions apply concave function means use kind hillclimbing technique general converge global optimum function trying optimize recap unfortunately difficult find closed form solutions maximization problem however fact function concave means use hillclimbing methods example well see moment gradient ascent way ultimizing function fact use gradientbased methods optimize function mean lets imagine l v v actually two parameters okay want sketch set contour lines hypothetical version l v contour lines exactly youd see see map okay peak function right lines equal value l v gradientbased method looks like following start point maybe wed start origin someone might start parameters equal calculate gradient means calculate direction steepest direction point might move far possible direction might example case move optimal point restrict move direction basically think climbing hill start point look steepest way hill walk distance direction takes maximum particular line new point calculate gradient move direction point might fairly close optimal point okay thats basic idea behind gradientbased methods gradient ascent point calculate gradients move direction move steepest direction weve sufficiently close optimum function gradients actually take fairly convenient form remember l v following sum equals n v dot f sum equals n log function lets see happens differentiate respect particular parameter v sub k okay differentiate term simply get sum equals n fk okay easy calculate first term simply sum training samples calculating value k feature case first component derivative derivative respect k log term takes little bit work differentiate wont take steps gory details refer notes provided class detailed explanation end actually fairly simple expression sum equals n sum labels primed feature value fk applied xi summing conjunction prime rules summing remarkably fact conditional probability primed current model terms often referred empirical counts fks indicator functions zero one basically number times particular particular question kth feature true training sample often referred expected counts kind expected number times feature fired current model importantly terms fairly easy calculate calculating empirical counts sum training samples calculating fks expected counts little bit work sum training samples calculate probability distribution labels current parameters particular training sample multiply feature vector definition fk xi primed nevertheless fairly straightforward calculate given definitions calculate derivative respective v v right vm particular example calculate derivative respective v v give direction move gradient im trying move,Course1,W7-S1-L7,W7,S1,L7,Parameter,7,1,7,okay go talk paramet estim loglinear model problem take set train exampl input produc output set v paramet show previou part lectur first key idea paramet estim go follow im go assum set train exampl train sampl consist xy pair ill write xi yi ith exampl train set go assum littl n train exampl xy pair xi member set input yi member set possibl label one exampl languag model problem xi would sequenc word dog saw three word case yi would singl word seen follow sequenc word okay see easi enough given larg amount text gather train exampl form basic consist context togeth word appear next word particular context go employ maximum likelihood estim scenario least first pass problem littl later see smooth estim regular let consid maximumlikelihood estim mean maximumlikelihood paramet v sub ml go paramet valu space possibl ndimension vector maxim function l v l v go function take paramet vector input return valu basic go measur well paramet fit data precis go log likelihood data paramet mean l v defin follow sum n train exampl sum equal n log probabl ith train exampl that log p yi given xi paramet v okay train sampl fix vari paramet v probabl chang theyll becom higher lower sum log probabl one log probabl item train set that defin l v intuit would like probabl high possibl reflect fact paramet v fit data well maxim function l v well put high probabl train sampl actual see formal mani nice properti deriv maximumlikelihood estim appli quit gener certainli appli particular case okay remind p take full length form ev dot f dot dot dot normal term take kind form take log whole thing end show previous lectur e v dot f minu log someth okay express explicit form sum equal n v dot f xi yi actual featur vector ith train sampl kind log normal term sum equal n log second sum possibl label prime e v dot f xi conjoin prime prime okay basic definit maximumlikelihood estim problem go tri choos paramet v make function l v larg possibl big remain question actual optim l v find maximumlikelihood estim get want talk one critic properti l v follow lv concav mean essenti nice behav function although gener case find close form solut arg max go possibl lv concav fairli easi optim what mean concav imagin singl paramet v case onedimension paramet vector case graph v versu lv concav function look like follow basic take two point two point line goe underneath function particular mean find local optimum particular function lv also go global optimum intuit mean use kind hillclimb techniqu go reach global maximum function concav function sens rel easi optim mani result optim show effici find maximum kind function here function concav function multipl local optima actual global global optimum local optima global optim hard nonconcav roughli speak gener much harder optim problem beauti one beauti thing function lv actual concav function cours go gener multidimension case similar definit appli concav function mean use kind hillclimb techniqu gener converg global optimum function tri optim recap unfortun difficult find close form solut maxim problem howev fact function concav mean use hillclimb method exampl well see moment gradient ascent way ultim function fact use gradientbas method optim function mean let imagin l v v actual two paramet okay want sketch set contour line hypothet version l v contour line exactli youd see see map okay peak function right line equal valu l v gradientbas method look like follow start point mayb wed start origin someon might start paramet equal calcul gradient mean calcul direct steepest direct point might move far possibl direct might exampl case move optim point restrict move direct basic think climb hill start point look steepest way hill walk distanc direct take maximum particular line new point calcul gradient move direct point might fairli close optim point okay that basic idea behind gradientbas method gradient ascent point calcul gradient move direct move steepest direct weve suffici close optimum function gradient actual take fairli conveni form rememb l v follow sum equal n v dot f sum equal n log function let see happen differenti respect particular paramet v sub k okay differenti term simpli get sum equal n fk okay easi calcul first term simpli sum train sampl calcul valu k featur case first compon deriv deriv respect k log term take littl bit work differenti wont take step gori detail refer note provid class detail explan end actual fairli simpl express sum equal n sum label prime featur valu fk appli xi sum conjunct prime rule sum remark fact condit probabl prime current model term often refer empir count fk indic function zero one basic number time particular particular question kth featur true train sampl often refer expect count kind expect number time featur fire current model importantli term fairli easi calcul calcul empir count sum train sampl calcul fk expect count littl bit work sum train sampl calcul probabl distribut label current paramet particular train sampl multipli featur vector definit fk xi prime nevertheless fairli straightforward calcul given definit calcul deriv respect v v right vm particular exampl calcul deriv respect v v give direct move gradient im tri move,[ 4  5 10 14 13]
91,Course1_W7-S1-L8_Parameter_Estimation_in_Log-linear_Models_Part_2_4-13,heres first sketch gradient ascent method works follows goal maximize function l v derivatives take following form ive actually done used usual vector calculus definition derivatives l v v going vector whose first component derivative respect first parameter whose second component derivative respect second parameter derived way showed previous slide thing n dimensional vector basically giving gradient easily calculated using definitions showed naive gradient based method would form follows would initialize v vector zeroes point would calculate gradient capital delta basically search talking okay would start point wed calculate gradient wed line search find optimal distance move would move point beta star basically far move calculating optimal value function move one direction beta distance move direction capital delta reset v new value iterate convergence calculate steepness sort steepest line ascent either gradient move optimal distance direction basically reached optimal value value think optimal good news general implement kind gradient ascent methods theyve developed years actually decades quite sophisticated methods derived practice work extremely well one thing note vanilla gradient ascent method ive showed rather slow tend sort get stuck long valleys take quite iterations sophisticated method use youre interested looking conjugate gradient methods one one commonly used method loglinear models algorithm called lbfgs good news many implementations algorithm conjugate gradient style algorithms available general assume rather simple interface okay theyre basically going assume given parameter vector v calculate value l v youll calculate gradient okay objective value one thing calculate gradient software packages optimize function theyll basically start initial point example zeros theyll move direction takes account gradient also previous directions algorithm moved efficient optimizing function l v thats implement functions l v l v gradient function given one existing packages optimize l v quite efficiently find maximum likely distance,Course1,W7-S1-L8,W7,S1,L8,Parameter,7,1,8,here first sketch gradient ascent method work follow goal maxim function l v deriv take follow form ive actual done use usual vector calculu definit deriv l v v go vector whose first compon deriv respect first paramet whose second compon deriv respect second paramet deriv way show previou slide thing n dimension vector basic give gradient easili calcul use definit show naiv gradient base method would form follow would initi v vector zero point would calcul gradient capit delta basic search talk okay would start point wed calcul gradient wed line search find optim distanc move would move point beta star basic far move calcul optim valu function move one direct beta distanc move direct capit delta reset v new valu iter converg calcul steep sort steepest line ascent either gradient move optim distanc direct basic reach optim valu valu think optim good news gener implement kind gradient ascent method theyv develop year actual decad quit sophist method deriv practic work extrem well one thing note vanilla gradient ascent method ive show rather slow tend sort get stuck long valley take quit iter sophist method use your interest look conjug gradient method one one commonli use method loglinear model algorithm call lbfg good news mani implement algorithm conjug gradient style algorithm avail gener assum rather simpl interfac okay theyr basic go assum given paramet vector v calcul valu l v youll calcul gradient okay object valu one thing calcul gradient softwar packag optim function theyll basic start initi point exampl zero theyll move direct take account gradient also previou direct algorithm move effici optim function l v that implement function l v l v gradient function given one exist packag optim l v quit effici find maximum like distanc,[ 4  5 14 13 12]
92,Course1_W7-S1-L9_Smoothing-Regularization_in_Log-linear_Models_15-12,weve described basic form loglinear models weve talked parameter estimation models final piece puzzle going talk smoothing regularization going slight modification important modification parameter estimation methods ive described okay lets look particular problem maximum likelihood estimates ive described well looking particular example lets return problem part speech tagging remember case history x consists sentence w wn sequence previous tags ti minus also position three things conditioning label tag example vt okay lets say feature definition say th feature going word tagging base tag choosing vt okay lets assume sake argument training data see word base three times see tag vt every single time unrealistic situation many cases see words tag every time look gradients maximum going dlv dl dv equal hill climb maximum function talking global maximum gradients going zero reason extent final parameters using property gradients equal zero means look back definition gradients equation satisfied sum f xi yi sum py model times feature value im going go tremendous detail derivation important point following ensure case way gradients going satisfied conditional probability vt given xi parameters v equal whenever see word base history training data essentially going mean parameter v going tend towards infinity optimize solution thats going happen almost cases probabilities one going set parameters tends towards infinity means test data example whenever see word base tagged going estimate probability well model essentially smoothed looks much like know think estimating trigram parameters language model dog given saw lets say equal count saw dog count saw lets sake argument assume count equal sorry denominator count equal say numerator count equal estimate going going estimate probability seeing dog given content saw probability one every time ive seen saw see dog next word sense extreme estimate doesnt seem really justified basis three examples estimate one similar particular example basis basically three examples estimating probability one practice use large numbers features model actually severe problem end many parameters diverging large values lead really rather bad generalizations test data examples way maximum likelihood estimate trigram language models generalized poly new sentences maximum likelihood estimates loglinear models generalize general generalize poly problem probability go parameters diverge infinity going show simple effective solution problem called regularization basic idea going modify loss function slightly im going take optimal parameters v star parameters v maximizes function l v exactly saw showed equals n log p yi given xi parameters v new term showing remember maximizing function l v minus lambda lambda parameter greater lambda usually chosen validation development set usually try several different values lambda see model performs better held sort beta similar lambda saw language models sense sum k equals vk squared essentially length vk squared euclidian norm length vk square going practice think function lv two terms first term thought measure well v fits data measure likelihood beta higher value higher probabilities weve weve given actual training examples term going drive us towards fitting data well possible second term going penalize large values vk sorry v squared v squared length vector going encourage us keep parameter values relatively small least keep sum squares parameter parameter values relatively small thought penalty complexity model think simpler models smaller parameter values encourage models smaller parameter values two parts subjective function also play fit data something tries keep parameter value relatively small second term justified various ways cut long story short great deal evidence empirically also theoretical analyses including kind penalties primitive values learn effectively high dimensional spaces actually number features extremely large long penalize large parameter values still get models generalize well new test examples fact kind strategy loglinear models highly effective models many scenarios optimize function l v using gradientbased methods gradients look similar showed differentiate respect parameter vk two terms exactly showed resolved differentiating term finally differentiate expression respect vk simply get lambda times vk theres simple change gradient run conjugate gradient methods lbfgs often method choice models new objective requires small modification method simple change gradients saw heres one example application loglinear models well see several others next lectures class complete story language modeling lets talk early paper chen rosenfeld tried simple experiment sort validate loglinear models kind regularization showed case replicated trigram language model used loglinear methods estimate parameters models case features look history x conditioning pair previous words single word language modeling problem go back scenario condition previous two words introduce trigram bigram unigram features example trigram feature might might look particular trigram bigram unigram described basically included trigrams bigrams unigrams seen trending data rather included features every trigram bigram unigram seen trending data define locally loglinear model estimate wi given previous two words gain exponentiate product fv normalize summing possible words vocabulary could estimate parameters v using gradient ascent regularized loglikelihood function showed previous slides couple interesting things note loglinear method language modeling one following use plain maximumlikelihood estimation regularization use maximumlikelihood estimation showed first attempt parameterization method simple methods q parameters end regular old maximumlikelihood estimates trigram models simply ratio two counts would referred qml wi given minus minus sort emphasizes models really unsmoothed pure maximumlikelihood case deficiencies saw maximumlikelihood estimation trigram language models however chen rosenfeld showed regularization method showed actually get good results look plexity performs least well discounting methods new interpolation methods saw way back first lecture two class thats reassuring box loglinear models get highly competitive results particular problem thats appealing many ways method ive described rather cleaner elegant principled estimation methods saw using discounting linear interpolation example downside particular case recall calculate probability w given context previous two words calculate e vf denominator normalization term involves summing words vocabulary computing sum slow literally going sum possible words vocabulary downside particular context language modeling said loglinear models incredibly effective many domains may see resurgence use language models people start come approximations methods deal kinds terms efficiently certainly benefit able incorporate features much flexible clean way methods saw linear interpolation,Course1,W7-S1-L9,W7,S1,L9,Smoothing-Regularization,7,1,9,weve describ basic form loglinear model weve talk paramet estim model final piec puzzl go talk smooth regular go slight modif import modif paramet estim method ive describ okay let look particular problem maximum likelihood estim ive describ well look particular exampl let return problem part speech tag rememb case histori x consist sentenc w wn sequenc previou tag ti minu also posit three thing condit label tag exampl vt okay let say featur definit say th featur go word tag base tag choos vt okay let assum sake argument train data see word base three time see tag vt everi singl time unrealist situat mani case see word tag everi time look gradient maximum go dlv dl dv equal hill climb maximum function talk global maximum gradient go zero reason extent final paramet use properti gradient equal zero mean look back definit gradient equat satisfi sum f xi yi sum py model time featur valu im go go tremend detail deriv import point follow ensur case way gradient go satisfi condit probabl vt given xi paramet v equal whenev see word base histori train data essenti go mean paramet v go tend toward infin optim solut that go happen almost case probabl one go set paramet tend toward infin mean test data exampl whenev see word base tag go estim probabl well model essenti smooth look much like know think estim trigram paramet languag model dog given saw let say equal count saw dog count saw let sake argument assum count equal sorri denomin count equal say numer count equal estim go go estim probabl see dog given content saw probabl one everi time ive seen saw see dog next word sens extrem estim doesnt seem realli justifi basi three exampl estim one similar particular exampl basi basic three exampl estim probabl one practic use larg number featur model actual sever problem end mani paramet diverg larg valu lead realli rather bad gener test data exampl way maximum likelihood estim trigram languag model gener poli new sentenc maximum likelihood estim loglinear model gener gener gener poli problem probabl go paramet diverg infin go show simpl effect solut problem call regular basic idea go modifi loss function slightli im go take optim paramet v star paramet v maxim function l v exactli saw show equal n log p yi given xi paramet v new term show rememb maxim function l v minu lambda lambda paramet greater lambda usual chosen valid develop set usual tri sever differ valu lambda see model perform better held sort beta similar lambda saw languag model sens sum k equal vk squar essenti length vk squar euclidian norm length vk squar go practic think function lv two term first term thought measur well v fit data measur likelihood beta higher valu higher probabl weve weve given actual train exampl term go drive us toward fit data well possibl second term go penal larg valu vk sorri v squar v squar length vector go encourag us keep paramet valu rel small least keep sum squar paramet paramet valu rel small thought penalti complex model think simpler model smaller paramet valu encourag model smaller paramet valu two part subject function also play fit data someth tri keep paramet valu rel small second term justifi variou way cut long stori short great deal evid empir also theoret analys includ kind penalti primit valu learn effect high dimension space actual number featur extrem larg long penal larg paramet valu still get model gener well new test exampl fact kind strategi loglinear model highli effect model mani scenario optim function l v use gradientbas method gradient look similar show differenti respect paramet vk two term exactli show resolv differenti term final differenti express respect vk simpli get lambda time vk there simpl chang gradient run conjug gradient method lbfg often method choic model new object requir small modif method simpl chang gradient saw here one exampl applic loglinear model well see sever other next lectur class complet stori languag model let talk earli paper chen rosenfeld tri simpl experi sort valid loglinear model kind regular show case replic trigram languag model use loglinear method estim paramet model case featur look histori x condit pair previou word singl word languag model problem go back scenario condit previou two word introduc trigram bigram unigram featur exampl trigram featur might might look particular trigram bigram unigram describ basic includ trigram bigram unigram seen trend data rather includ featur everi trigram bigram unigram seen trend data defin local loglinear model estim wi given previou two word gain exponenti product fv normal sum possibl word vocabulari could estim paramet v use gradient ascent regular loglikelihood function show previou slide coupl interest thing note loglinear method languag model one follow use plain maximumlikelihood estim regular use maximumlikelihood estim show first attempt parameter method simpl method q paramet end regular old maximumlikelihood estim trigram model simpli ratio two count would refer qml wi given minu minu sort emphas model realli unsmooth pure maximumlikelihood case defici saw maximumlikelihood estim trigram languag model howev chen rosenfeld show regular method show actual get good result look plexiti perform least well discount method new interpol method saw way back first lectur two class that reassur box loglinear model get highli competit result particular problem that appeal mani way method ive describ rather cleaner eleg principl estim method saw use discount linear interpol exampl downsid particular case recal calcul probabl w given context previou two word calcul e vf denomin normal term involv sum word vocabulari comput sum slow liter go sum possibl word vocabulari downsid particular context languag model said loglinear model incred effect mani domain may see resurg use languag model peopl start come approxim method deal kind term effici certainli benefit abl incorpor featur much flexibl clean way method saw linear interpol,[ 4  5  1 14 13]
93,Course1_W8-S1-L1_Introduction_1-41,last weeks lectures developed loglinear models new way looking modelling natural image processing also parameter estimation natural image processing current weeks lectures going look various applications loglinear models two problems natural language processing first problem going cover segment going application loglinear models tagging problems well see important example loglinear models used lp well see develop powerful alternatives hidden mark model taggers saw earlier class historically loglinear models tagging often also referred maximumentropy markov models well loglinear models sometimes referred particularly earlier literature maximumentropy models wont really go reason naming worth bearing mind sometimes see term maximumentropy models used youll know basically loglinear model markov comes essentailly going models tagging share many characteristics hidden markov models although well see provide useful alternative hidden markov models,Course1,W8-S1-L1,W8,S1,L1,Introduction,8,1,1,last week lectur develop loglinear model new way look model natur imag process also paramet estim natur imag process current week lectur go look variou applic loglinear model two problem natur languag process first problem go cover segment go applic loglinear model tag problem well see import exampl loglinear model use lp well see develop power altern hidden mark model tagger saw earlier class histor loglinear model tag often also refer maximumentropi markov model well loglinear model sometim refer particularli earlier literatur maximumentropi model wont realli go reason name worth bear mind sometim see term maximumentropi model use youll know basic loglinear model markov come essentailli go model tag share mani characterist hidden markov model although well see provid use altern hidden markov model,[ 1  4  8 14 13]
94,Course1_W8-S1-L2_Recap_of_the_Tagging_Problem_3-15,first lets give quick recap tagging problem remember considered couple important examples tagging problems much earlier class first one looked partofspeech tagging problem take sentence input provide output word sentence associated tag example profits noun sword verb preposition extracting problem taking sequence words input produce sequence tags output model word gets single tag important tagging problem looked problem named entity recognition case input sentence output model segmentation sentence identify important segments corresponding example companies boeing locations wall street people alan mulally particular example described named entity recognition problem also framed tagging problem let remind word tagged sound one number possible tags listed example words tagged na meaning theyre part entity whereas words tagged start company continuation company start location continuation location start person continuation person thats reminder frame named entity problem named entity recognition problem segmentation problem actually tagging problem going treat problem machine learning problem goal following training set consists perhaps quite large quantity example sentences sentence underlying tags marked like training set need induce function algorithm maps new sentences underlying tag sequences think second week class saw hidden markov models viterbi algorithm one way achieving goal recall hidden markov models would read parameter estimates taking counts training samples new test example would use viterbi algorithm find likely text sequence underlying model,Course1,W8-S1-L2,W8,S1,L2,Recap,8,1,2,first let give quick recap tag problem rememb consid coupl import exampl tag problem much earlier class first one look partofspeech tag problem take sentenc input provid output word sentenc associ tag exampl profit noun sword verb preposit extract problem take sequenc word input produc sequenc tag output model word get singl tag import tag problem look problem name entiti recognit case input sentenc output model segment sentenc identifi import segment correspond exampl compani boe locat wall street peopl alan mulal particular exampl describ name entiti recognit problem also frame tag problem let remind word tag sound one number possibl tag list exampl word tag na mean theyr part entiti wherea word tag start compani continu compani start locat continu locat start person continu person that remind frame name entiti problem name entiti recognit problem segment problem actual tag problem go treat problem machin learn problem goal follow train set consist perhap quit larg quantiti exampl sentenc sentenc underli tag mark like train set need induc function algorithm map new sentenc underli tag sequenc think second week class saw hidden markov model viterbi algorithm one way achiev goal recal hidden markov model would read paramet estim take count train sampl new test exampl would use viterbi algorithm find like text sequenc underli model,[ 1 13  4 14 12]
95,Course1_W8-S1-L3_Independence_Assumptions_in_Log-linear_Taggers_8-32,okay lets consider develop log linear model tagging problem general well assume input sentence w wn example might something like dog barks ill sometimes use following notation use w sub colon n refer entire sequence okay wi ith word particular sentence addition tag sequence tn might example sequence n v equal tn similarly ill use notation sub colon n shorthand entire sequence key idea loglinear models well see construct model conditional probability tag sequence given underlying word sequence going many possible tag sequences given input sentence lets say example know many possible tag sequences im writing going assign tag sequences probability might sum probabilities sum okay well defined conditional distribution set possible tag sequences particular input sentence one important thing realize quite stark contrast case saw hidden markov models hmms remember hidden markov models actually defined joint probability distribution tag sequences paired word sequences okay first interesting thing see loglinear models directly model conditional distribution wont actually attempt model joint distribution thats first key difference hidden markov models weve derived model particular form apply new test sentence test sentence w wn take likely text sequence star tag sequence tn maximizes conditional probability remeber hmms would something similar fact would star lin n equal augmax n joint probability n w n really similar except using conditional probability loglinear case hidden markov model joint probability going key questions well answer lecture know firstly define conditional distribution estimate parameters model underlies definition finally well answer question actually efficiently find likely tag sequence given input sentence critical question going model conditional distribution probability given tag sequence one tn given particular word sequence w wn well see method used first use chain rule secondly use independence assumptions simplify model first application chain rule im going write probability product terms one term position j equals n conditional probability jth tag value conditioned full sequence words w one wn addition conditioned previous j minus one tags step usual chain rules exact distribution conditional distribution tags given words decomposed product terms way thats step second step make independence assumption actually looks similar markov independence assumptions seen trigram language models trigram hmms done basically replaced sequence tags tj minus last two tags sequence tj minus tj minus ive little careful make sure define minus special star symbol basically symbol start sentence im saying tag informally depends previous two tags formally means value random variable tj conditionally independent tags preceding previous two tags condition entire sentence also previous two tags looks much like log assumptions wed seen earlier class real difference conditioning entire sentence throughout decomposition sort carrying along entire sentence independence assumption thus made model let give particular example say want write conditional probability tag sequence dnv given three words dog barks going decomposed product three terms firstly probability given sentence dog barks given previous two text star start symbols sequence essentially looks like star star n v going probability term tags second term going p n given dog barks star third term going p v given dog barks end n tag probability term p j thats conditioned previous two tags entire input sentence critical question going estimate probabilities define model probabilities estimate parameters model training examples well see going direct application ideas log linear models saw lectures last week class,Course1,W8-S1-L3,W8,S1,L3,Independence,8,1,3,okay let consid develop log linear model tag problem gener well assum input sentenc w wn exampl might someth like dog bark ill sometim use follow notat use w sub colon n refer entir sequenc okay wi ith word particular sentenc addit tag sequenc tn might exampl sequenc n v equal tn similarli ill use notat sub colon n shorthand entir sequenc key idea loglinear model well see construct model condit probabl tag sequenc given underli word sequenc go mani possibl tag sequenc given input sentenc let say exampl know mani possibl tag sequenc im write go assign tag sequenc probabl might sum probabl sum okay well defin condit distribut set possibl tag sequenc particular input sentenc one import thing realiz quit stark contrast case saw hidden markov model hmm rememb hidden markov model actual defin joint probabl distribut tag sequenc pair word sequenc okay first interest thing see loglinear model directli model condit distribut wont actual attempt model joint distribut that first key differ hidden markov model weve deriv model particular form appli new test sentenc test sentenc w wn take like text sequenc star tag sequenc tn maxim condit probabl remeb hmm would someth similar fact would star lin n equal augmax n joint probabl n w n realli similar except use condit probabl loglinear case hidden markov model joint probabl go key question well answer lectur know firstli defin condit distribut estim paramet model underli definit final well answer question actual effici find like tag sequenc given input sentenc critic question go model condit distribut probabl given tag sequenc one tn given particular word sequenc w wn well see method use first use chain rule secondli use independ assumpt simplifi model first applic chain rule im go write probabl product term one term posit j equal n condit probabl jth tag valu condit full sequenc word w one wn addit condit previou j minu one tag step usual chain rule exact distribut condit distribut tag given word decompos product term way that step second step make independ assumpt actual look similar markov independ assumpt seen trigram languag model trigram hmm done basic replac sequenc tag tj minu last two tag sequenc tj minu tj minu ive littl care make sure defin minu special star symbol basic symbol start sentenc im say tag inform depend previou two tag formal mean valu random variabl tj condit independ tag preced previou two tag condit entir sentenc also previou two tag look much like log assumpt wed seen earlier class real differ condit entir sentenc throughout decomposit sort carri along entir sentenc independ assumpt thu made model let give particular exampl say want write condit probabl tag sequenc dnv given three word dog bark go decompos product three term firstli probabl given sentenc dog bark given previou two text star start symbol sequenc essenti look like star star n v go probabl term tag second term go p n given dog bark star third term go p v given dog bark end n tag probabl term p j that condit previou two tag entir input sentenc critic question go estim probabl defin model probabl estim paramet model train exampl well see go direct applic idea log linear model saw lectur last week class,[ 1  4 14 13 12]
96,Course1_W8-S1-L4_Features_in_Log-Linear_Taggers_13-21,lets consider modeling problem left little bit closely im going use following example consider word base many possible tags particular position word base well use script refer set possible tags example singular nouns plural nouns transitive verbs intransitive verbs prepositions determiners task going estimate probability one possibilities given context context context going previous tags case determiner adjective potentially features sentence remember p j given w w n sentence tj minus tj minus one condition previous two tags condition information entire sentence course complex object conditioning many many possible features context could useful thats really motivation using loglinear models context apply loglinear models going develop little bit notation critical idea going idea history basically captures information context contextual information condition history going tuple considering consisting following previous two tags minus minus entire sequence words input sentence w wn also need little bit careful make sure condition position sentence tagging going index word tagged position center lets see tuple substantiated particular example case minus minus equal tags dt followed jj w n consisting entire input sentence entire sequence words position equal count words sentence one two three four five position sentence history basically captures information context previous two tags entire sentence particular position sentence well use script x refer set possible histories thats going large set actually potentially infinite set countably infinite set allow sentences possible lengths given definitions apply log linear model direct way gain cap idea feature vector representations used directly log linear models general log linear model going assume sets possible inputs histories tagging case going tuple minus two minus one w one w n position well also assume finite label set use denote script example might set possible part speech tags maybe n v aim provide conditional probability py given x history x combined label key representational trick loglinear models make use features feature function takes history paired label maps value saw last week considered loglinear models many features used nlp rather simply binary features result function either zero one features typically ask questions input x conjunction label return zero question false one answer question true given set n features f sub k equals one define feature vector vector consisting values different features model basically end mapping particular history input paired particular tag feature vector represents outputs sorry results features questions history conjunction label heres examples might instantiate features tagging problem x instead possible histories form set tags various features first example featured following f going current word wi word base tag equal vt intuitively going capture affinity particular word base considering transitive verb tag one feature course would practice define many potential features quite likely corresponding every possible word paired every possible part speech tag heres second type feature f two says following going one current wi ends suffix ing tag vbg gerund verb tag gerund pen tree bank example words ending ing english often tagged vbg youll start see something interesting happening sense feature captured parameter hmm e base given vt natural parameter hmm captures affinity word base trasitive verb really new type feature looks spelling feature word tagged case suffix ing kind features much difficult hmms fact saw programming assignment course actually lecture slides hmms know one common way dealing kind spelling features introduce mappings rare infrequent words maybe different word classes really rather crude way things much convenient able define features like look spelling conjunction tag actually two feature types roadg adwait ratnaparkhi one earliest applications loglinear models tagging actually probably earliest actually describe full set features used model theres pretty simple set features yet still close state art use features part speech tagger example get state art model introduced wordtag features features like one ive shown pair particular word identity wi particular word particular tag included one feature like every wordtag pair seen training sample quite large number features pair words underlying tags also considered spelling features looked prefixes suffixes length less equal four okay example feature looks suffix g particular tag feature looks prefix fires particular word wi starts three letters pre tag case common rather singular noun going consider prefixes suffixes length less equal four potentially combined possible tags possible quite large number features features invaluable youre dealing infrequent unknown words kind spelling futures useful dis disambiguating words ratnaparkhi also looked contextual features first three features look much like trigram bigram unigram features tags first feature one tag sequence minus minus equal dt jj vt zero otherwise captures particular trigram tags would likely one feature like every possible trigram tags least one feature like every trigram tags seen training sample heres bigram feature face previous tag jj current tag proposed vt thats bigram text finally feature looks like unigram looks like tag lone flies fact vt course going features sorry parameters model v v v basically going sense correlated probability likelihood trigram bigram unigram tags ratnaparkhi also made use features addition looking current word wi tagged generally might example might word base also look words surrounding see left immediate right one feature one previous word wi equals tag equals vt well might feature looking next word fact wi plus equals equals vt see flexibility loglinear models allows us define kinds features look current tag proposed well features context whether features previous two tags context surrounding words particular context,Course1,W8-S1-L4,W8,S1,L4,Features,8,1,4,let consid model problem left littl bit close im go use follow exampl consid word base mani possibl tag particular posit word base well use script refer set possibl tag exampl singular noun plural noun transit verb intransit verb preposit determin task go estim probabl one possibl given context context context go previou tag case determin adject potenti featur sentenc rememb p j given w w n sentenc tj minu tj minu one condit previou two tag condit inform entir sentenc cours complex object condit mani mani possibl featur context could use that realli motiv use loglinear model context appli loglinear model go develop littl bit notat critic idea go idea histori basic captur inform context contextu inform condit histori go tupl consid consist follow previou two tag minu minu entir sequenc word input sentenc w wn also need littl bit care make sure condit posit sentenc tag go index word tag posit center let see tupl substanti particular exampl case minu minu equal tag dt follow jj w n consist entir input sentenc entir sequenc word posit equal count word sentenc one two three four five posit sentenc histori basic captur inform context previou two tag entir sentenc particular posit sentenc well use script x refer set possibl histori that go larg set actual potenti infinit set countabl infinit set allow sentenc possibl length given definit appli log linear model direct way gain cap idea featur vector represent use directli log linear model gener log linear model go assum set possibl input histori tag case go tupl minu two minu one w one w n posit well also assum finit label set use denot script exampl might set possibl part speech tag mayb n v aim provid condit probabl py given x histori x combin label key represent trick loglinear model make use featur featur function take histori pair label map valu saw last week consid loglinear model mani featur use nlp rather simpli binari featur result function either zero one featur typic ask question input x conjunct label return zero question fals one answer question true given set n featur f sub k equal one defin featur vector vector consist valu differ featur model basic end map particular histori input pair particular tag featur vector repres output sorri result featur question histori conjunct label here exampl might instanti featur tag problem x instead possibl histori form set tag variou featur first exampl featur follow f go current word wi word base tag equal vt intuit go captur affin particular word base consid transit verb tag one featur cours would practic defin mani potenti featur quit like correspond everi possibl word pair everi possibl part speech tag here second type featur f two say follow go one current wi end suffix ing tag vbg gerund verb tag gerund pen tree bank exampl word end ing english often tag vbg youll start see someth interest happen sens featur captur paramet hmm e base given vt natur paramet hmm captur affin word base trasit verb realli new type featur look spell featur word tag case suffix ing kind featur much difficult hmm fact saw program assign cours actual lectur slide hmm know one common way deal kind spell featur introduc map rare infrequ word mayb differ word class realli rather crude way thing much conveni abl defin featur like look spell conjunct tag actual two featur type roadg adwait ratnaparkhi one earliest applic loglinear model tag actual probabl earliest actual describ full set featur use model there pretti simpl set featur yet still close state art use featur part speech tagger exampl get state art model introduc wordtag featur featur like one ive shown pair particular word ident wi particular word particular tag includ one featur like everi wordtag pair seen train sampl quit larg number featur pair word underli tag also consid spell featur look prefix suffix length less equal four okay exampl featur look suffix g particular tag featur look prefix fire particular word wi start three letter pre tag case common rather singular noun go consid prefix suffix length less equal four potenti combin possibl tag possibl quit larg number featur featur invalu your deal infrequ unknown word kind spell futur use di disambigu word ratnaparkhi also look contextu featur first three featur look much like trigram bigram unigram featur tag first featur one tag sequenc minu minu equal dt jj vt zero otherwis captur particular trigram tag would like one featur like everi possibl trigram tag least one featur like everi trigram tag seen train sampl here bigram featur face previou tag jj current tag propos vt that bigram text final featur look like unigram look like tag lone fli fact vt cours go featur sorri paramet model v v v basic go sens correl probabl likelihood trigram bigram unigram tag ratnaparkhi also made use featur addit look current word wi tag gener might exampl might word base also look word surround see left immedi right one featur one previou word wi equal tag equal vt well might featur look next word fact wi plu equal equal vt see flexibl loglinear model allow us defin kind featur look current tag propos well featur context whether featur previou two tag context surround word particular context,[ 5  1  4 14 13]
97,Course1_W8-S1-L5_Parameters_in_Log-linear_Models_3-59,description set potential features loglinear attacker recap local linear models take features produce conditional probability distributions form py given x parameter vector v weve already said features f sub k k equals gives us feature vector recall loglinear models also assume parameter vector v r equals example might f xy equal might v equal take inner product two things v f xy equal times plus times plus times equal compute inner product every possible label conjunction x conditional probability described last week lectures loglinear models find e power v f divide normalization term involves sum possible labels e v f particular label estimate parameters models v values come typically come learning parameters set training examples particular using regularized log likelihood methods described last week class abstractly assume training set consisting xi yi pairs equals n choose parameter values v star vs maximize sum two terms firstly sum log probabilities training examples secondly negative term lambda constant greater zero dictating relative weight term basically length squared parameter vector v directly written sum squared parameter values regularize going keep parameter values small whereas term going encourage parameter values fit training sample well said regularize term generally leads much better generalization cases could potentially large numbers features concretely loglinear taggers training set xi yi pairs going set examples wwhere xi consists history example dt jj red dog remember history consists pair tags sentence position tagged yi going consist part speech tag take training examples tag sentences every position training samples extract history extract tag use history tag pairs training examples training parameters log linear model,Course1,W8-S1-L5,W8,S1,L5,Parameters,8,1,5,descript set potenti featur loglinear attack recap local linear model take featur produc condit probabl distribut form py given x paramet vector v weve alreadi said featur f sub k k equal give us featur vector recal loglinear model also assum paramet vector v r equal exampl might f xy equal might v equal take inner product two thing v f xy equal time plu time plu time equal comput inner product everi possibl label conjunct x condit probabl describ last week lectur loglinear model find e power v f divid normal term involv sum possibl label e v f particular label estim paramet model v valu come typic come learn paramet set train exampl particular use regular log likelihood method describ last week class abstractli assum train set consist xi yi pair equal n choos paramet valu v star vs maxim sum two term firstli sum log probabl train exampl secondli neg term lambda constant greater zero dictat rel weight term basic length squar paramet vector v directli written sum squar paramet valu regular go keep paramet valu small wherea term go encourag paramet valu fit train sampl well said regular term gener lead much better gener case could potenti larg number featur concret loglinear tagger train set xi yi pair go set exampl wwhere xi consist histori exampl dt jj red dog rememb histori consist pair tag sentenc posit tag yi go consist part speech tag take train exampl tag sentenc everi posit train sampl extract histori extract tag use histori tag pair train exampl train paramet log linear model,[ 5  4  1 14 13]
98,Course1_W8-S1-L6_The_Viterbi_Algorithm_for_Log-linear_Taggers_9-37,weve described key steps constructing loglinear tagger firstly define features take account features context history conjuction current tag predicted secondly weve described estimate parameters model essentially optimizing regularized log likelihood exactly way saw last weeks lecture loglinear models final piece puzzle question apply loglinear model new test sentence means going take test sentences input example dog barks going search possible tag sequences tag sequence maximizes conditional probability might consider tag sequence ddd ddn dnd going return tag sequence highest value conditional probability usual argued earlier course brute force search isnt going scale appreciable sentence length theres way explicitly enumerate possible tag sequences calculate probability subsequents return highest one well see use viterbi algorithm actually minor variant viterbi algorithm seen hidden markov models find highest scoring highest probability tag sequence efficiently critical observation going make assumption conditional probability takes following form notice product equals one n q given previous two tags entire sentence position key property definition point condition previous two tags markov like assumption allow us apply dynamic programming viterbi algorithm problem finding highest scoring tag sequence happens case q estimate output loglinear model makes direct use parameters features saw earlier lets build viterbi algorithm case similar way way solve hidden markov models key idea define dynamic programming table pi kuv going maximum probability tag sequence ending tags u v position k little bit formally well define function r takes sequence k tags tk calcs calculates probability onto model notice basically truncated form probability entire tag sequence multiplying articles k q ti given previous two tags define pi k u v simply maximum probability tag sequence ending u v positions k thats definition saying thats similar say hidden markov models hidden markov models also saw dynamic programming table form pi values correspond maximum probabilities prefixes tag sequences recursive definition firstly say pi star star equals thats base case recursion identical hmm algorithm changes recursive definition looks like following define sub k set possible tags position k sentence thats exactly definition algorithm hidden markov models k u sk minus v sk define pi k u v following take max tags position k minus multiply n pi k minus tu thats pi value position k minus note position k conditional probability v given previous two tags u given entire sentence w given position k would output loglinear model im going go justification recursion great detail extremely similar justification saw recursion applied hidden markov models fact whats happened essentially hidden markov models instead term would something like following would q v given u secondly e wk k word given tag v wed product two terms trigram term tag emission term probability w k emitted v loglinear model new term conditional probability v given u entire sentence w n position k otherwise recursive definition almost exactly saw viterbi algorithm puts ideas together want stress familiar viterbi algorithm saw hidden markov models input algorithm test sentence w wn wed like tag addition well assume provided loglinear model provides conditional probability v given tags u given input sentence position two inputs model model proceeds follows initialization step set pi star star equal following every position k range n consider possible tags u position k minus v position k going fill value pi k u v searching tag position k minus maximizes product two terms pi k minus u q term refecting reflecting conditional probability tag v particular context particular history keep track back pointers every ku v triple record arg max tag actually achieves maximum thats main body algorithm ive completed steps find highest scoring tag sequence following back pointers backwards sequence first set tn minus tn uv pair maximizes pi n u v work backwards sequence position n minus point saying case tag equal back pointer position k plus tk plus tk plus im essentially working backwards sequence first recovering last two tags say n n filling previous tags one one backwards sequence ive done finally return tag sequence tn highest probability sequence loglinear modal,Course1,W8-S1-L6,W8,S1,L6,The,8,1,6,weve describ key step construct loglinear tagger firstli defin featur take account featur context histori conjuct current tag predict secondli weve describ estim paramet model essenti optim regular log likelihood exactli way saw last week lectur loglinear model final piec puzzl question appli loglinear model new test sentenc mean go take test sentenc input exampl dog bark go search possibl tag sequenc tag sequenc maxim condit probabl might consid tag sequenc ddd ddn dnd go return tag sequenc highest valu condit probabl usual argu earlier cours brute forc search isnt go scale appreci sentenc length there way explicitli enumer possibl tag sequenc calcul probabl subsequ return highest one well see use viterbi algorithm actual minor variant viterbi algorithm seen hidden markov model find highest score highest probabl tag sequenc effici critic observ go make assumpt condit probabl take follow form notic product equal one n q given previou two tag entir sentenc posit key properti definit point condit previou two tag markov like assumpt allow us appli dynam program viterbi algorithm problem find highest score tag sequenc happen case q estim output loglinear model make direct use paramet featur saw earlier let build viterbi algorithm case similar way way solv hidden markov model key idea defin dynam program tabl pi kuv go maximum probabl tag sequenc end tag u v posit k littl bit formal well defin function r take sequenc k tag tk calc calcul probabl onto model notic basic truncat form probabl entir tag sequenc multipli articl k q ti given previou two tag defin pi k u v simpli maximum probabl tag sequenc end u v posit k that definit say that similar say hidden markov model hidden markov model also saw dynam program tabl form pi valu correspond maximum probabl prefix tag sequenc recurs definit firstli say pi star star equal that base case recurs ident hmm algorithm chang recurs definit look like follow defin sub k set possibl tag posit k sentenc that exactli definit algorithm hidden markov model k u sk minu v sk defin pi k u v follow take max tag posit k minu multipli n pi k minu tu that pi valu posit k minu note posit k condit probabl v given previou two tag u given entir sentenc w given posit k would output loglinear model im go go justif recurs great detail extrem similar justif saw recurs appli hidden markov model fact what happen essenti hidden markov model instead term would someth like follow would q v given u secondli e wk k word given tag v wed product two term trigram term tag emiss term probabl w k emit v loglinear model new term condit probabl v given u entir sentenc w n posit k otherwis recurs definit almost exactli saw viterbi algorithm put idea togeth want stress familiar viterbi algorithm saw hidden markov model input algorithm test sentenc w wn wed like tag addit well assum provid loglinear model provid condit probabl v given tag u given input sentenc posit two input model model proce follow initi step set pi star star equal follow everi posit k rang n consid possibl tag u posit k minu v posit k go fill valu pi k u v search tag posit k minu maxim product two term pi k minu u q term refect reflect condit probabl tag v particular context particular histori keep track back pointer everi ku v tripl record arg max tag actual achiev maximum that main bodi algorithm ive complet step find highest score tag sequenc follow back pointer backward sequenc first set tn minu tn uv pair maxim pi n u v work backward sequenc posit n minu point say case tag equal back pointer posit k plu tk plu tk plu im essenti work backward sequenc first recov last two tag say n n fill previou tag one one backward sequenc ive done final return tag sequenc tn highest probabl sequenc loglinear modal,[ 7  1  4  5 14]
99,Course1_W8-S1-L7_An_Example_Application_9-28,want finish segment loglinear taggers example particular application illustrates real strengths paper andrew mccallum others another absolutely seminal reference use loglinear moles mccallum compared hidden markov model loglinear tagger fact segmentation test well see second main point particular domain modeling conditional probability word given tag difficult remember hmms two types parameters example trigram tag parameter probability v u admission parameters word given tag difficult distribution model really hampers ability hmms take account compec complex features build tagging models faq segmentation task mccallum collaborators looked fact theyre going tagging entire lines faq faq going sequence lines weve seen one two three four five six close lines blank notice blank lines goal assign one three possible tags line line either part header head tags header faq part question tag question saying question finally tag answer answer question see useful task perform kind tagging accurately could automatically pull question answer pairs rather unstructured data would see least type things one thing thats worth noting word tagger entire sentence okay tagging sentence sentence items im actually tagging quite complex objects theyre entire sentences itll critical model make use various features entire sentences entire lines rather features used sort mccallum others general binary features true false particular line could ask whether line begins number whether begins automal whether begins punctuation whether begins question word sort closed set words like woo look whether line blank whether contains alphanumeric whether contains number imagine making whole bunch features individual lines might give strong evidence content line importantly would give strong evidence whether falls one three categories header question answer describe features used within log linear tagger work theyre really rather intuitive going look properties line example fact line begins number conjunction current tag say question also previous tag case going look bigram tags remember always ability look previous one two tags context addition look feature line example fact begins number contains alphanumeric contains nonspace contains number basic set features used loglinear tagger challenging task actually coming hmm tagging model particular problem quote words particular application entire lines estimate probability entire line given fact tag question first method mccallum others used break sequence terms one word wed mission parameters condition word term identity tag question case e given question first word times e given question second word one e term word sentence looks much like language model tag example question fact essentially unigram language model trying define conditional probability particular entire sentence entire line conditioned particular tag thats first method guys looked distributions apply hidden maskov model tagger usual way heres second method used method took line training test data immediately transformed sequence features case wed sequence saying begins number contains alphanumeric contains nonspace contains number previous blank means previous line blank example use basically kind unigram language model one term features probability begins number given question times probability contains alphanumeric given question looks much like model previous slide go transformation step first extracts important features individual line rather simply staying words lets look results different methods going terms precision recall recovering entire segments segment might correspond header question answer first result called stateless stands maximum entropy stateless remember maximum entropy model basically loglinear model loglinear model conditioning previous two tags context conditioned case features individual line tagged model performs quite poorly thats really evidence surrounding context form previous two tags actually useful tokenhmm first approach building hidden markov model showed previous couple slides simply calculated probability entire line basically creating unigram language model single term word line featurehmm second hmm solution described first step took line transformed sequence features representing line rather representing features line see get slight improvement numbers getting better better course memm loglinear trigram tagger things sometimes referred maximum entropy markov models memm performed considerably better methods theres clear advantage hidden markov models,Course1,W8-S1-L7,W8,S1,L7,An,8,1,7,want finish segment loglinear tagger exampl particular applic illustr real strength paper andrew mccallum other anoth absolut semin refer use loglinear mole mccallum compar hidden markov model loglinear tagger fact segment test well see second main point particular domain model condit probabl word given tag difficult rememb hmm two type paramet exampl trigram tag paramet probabl v u admiss paramet word given tag difficult distribut model realli hamper abil hmm take account compec complex featur build tag model faq segment task mccallum collabor look fact theyr go tag entir line faq faq go sequenc line weve seen one two three four five six close line blank notic blank line goal assign one three possibl tag line line either part header head tag header faq part question tag question say question final tag answer answer question see use task perform kind tag accur could automat pull question answer pair rather unstructur data would see least type thing one thing that worth note word tagger entir sentenc okay tag sentenc sentenc item im actual tag quit complex object theyr entir sentenc itll critic model make use variou featur entir sentenc entir line rather featur use sort mccallum other gener binari featur true fals particular line could ask whether line begin number whether begin autom whether begin punctuat whether begin question word sort close set word like woo look whether line blank whether contain alphanumer whether contain number imagin make whole bunch featur individu line might give strong evid content line importantli would give strong evid whether fall one three categori header question answer describ featur use within log linear tagger work theyr realli rather intuit go look properti line exampl fact line begin number conjunct current tag say question also previou tag case go look bigram tag rememb alway abil look previou one two tag context addit look featur line exampl fact begin number contain alphanumer contain nonspac contain number basic set featur use loglinear tagger challeng task actual come hmm tag model particular problem quot word particular applic entir line estim probabl entir line given fact tag question first method mccallum other use break sequenc term one word wed mission paramet condit word term ident tag question case e given question first word time e given question second word one e term word sentenc look much like languag model tag exampl question fact essenti unigram languag model tri defin condit probabl particular entir sentenc entir line condit particular tag that first method guy look distribut appli hidden maskov model tagger usual way here second method use method took line train test data immedi transform sequenc featur case wed sequenc say begin number contain alphanumer contain nonspac contain number previou blank mean previou line blank exampl use basic kind unigram languag model one term featur probabl begin number given question time probabl contain alphanumer given question look much like model previou slide go transform step first extract import featur individu line rather simpli stay word let look result differ method go term precis recal recov entir segment segment might correspond header question answer first result call stateless stand maximum entropi stateless rememb maximum entropi model basic loglinear model loglinear model condit previou two tag context condit case featur individu line tag model perform quit poorli that realli evid surround context form previou two tag actual use tokenhmm first approach build hidden markov model show previou coupl slide simpli calcul probabl entir line basic creat unigram languag model singl term word line featurehmm second hmm solut describ first step took line transform sequenc featur repres line rather repres featur line see get slight improv number get better better cours memm loglinear trigram tagger thing sometim refer maximum entropi markov model memm perform consider better method there clear advantag hidden markov model,[ 1  4  9  5 14]
100,Course1_W8-S1-L8_Summary_2-45,okay summarize segment class log linear taggers key ideas first key idea directly model conditional probability tag sequence conditioned word sequence using decomposition product terms product positions equals n point condition value ith tag previous two tags entire sentence tagged left left us modeling problem question actually model conditional probabilities conditioning lot potential context many potential features might useful second key idea loglinear model estimate terms using loglinear models may quite rich use features looking context previews text potentially really arbitrary context surrounding words final insight loglinear models given test sentence w wn use viterbi algorithm find highest scoring highest probability tag sequence use viterbi algorithm markov style assumption models ith tag depends previous two tags context finally key advantage hidden markov models really flexibility feature definitions loglinear taggers use empowered make kinds features look current tag previous history history consists information example easy incorporate prefix suffix features words information previous word next word tagging current word kinds features input sentence surrounding context really key reason preferring loglinear taggers markov models downside course theyre slightly complicated terms estimation parameter estimation method loglinear taggers little bit complex little bit computationally expensive general worth extra effort,Course1,W8-S1-L8,W8,S1,L8,Summary,8,1,8,okay summar segment class log linear tagger key idea first key idea directli model condit probabl tag sequenc condit word sequenc use decomposit product term product posit equal n point condit valu ith tag previou two tag entir sentenc tag left left us model problem question actual model condit probabl condit lot potenti context mani potenti featur might use second key idea loglinear model estim term use loglinear model may quit rich use featur look context preview text potenti realli arbitrari context surround word final insight loglinear model given test sentenc w wn use viterbi algorithm find highest score highest probabl tag sequenc use viterbi algorithm markov style assumpt model ith tag depend previou two tag context final key advantag hidden markov model realli flexibl featur definit loglinear tagger use empow make kind featur look current tag previou histori histori consist inform exampl easi incorpor prefix suffix featur word inform previou word next word tag current word kind featur input sentenc surround context realli key reason prefer loglinear tagger markov model downsid cours theyr slightli complic term estim paramet estim method loglinear tagger littl bit complex littl bit comput expens gener worth extra effort,[ 1  4  5 14 13]
101,Course1_W8-S2-L1_Introduction_0-47,last segment class saw loglinear taggers showed loglinear models could applied effective way tagging problem next segment want describe generalize general approach much wider class problems going consider parsing problem using method called historybased parsing making extensive use loglinear models hopefully youll general methods ill describe segment class could potentially applied large class problems nlp matter fields,Course1,W8-S2-L1,W8,S2,L1,Introduction,8,2,1,last segment class saw loglinear tagger show loglinear model could appli effect way tag problem next segment want describ gener gener approach much wider class problem go consid pars problem use method call historybas pars make extens use loglinear model hope youll gener method ill describ segment class could potenti appli larg class problem nlp matter field,[ 1  4  0 14 13]
102,Course1_W8-S2-L2_Conditional_History-based_Models_7-14,heres quick recap loglinear models applied tagging problem remember use notation w colon n refer input sentence w wn similarly colon n tag sequence cri first critical idea define conditional probability tag sequence n conditioned word sequence w wn first using chain rule secondly using independence assumptions result end conditional probability product terms product j equals n point jth tag sequence sub j thats conditioned entire input sentence previous two tags sequence mock style assumption j tag depends previous two tags addition condition entire sentence every point left problem estimating p terms thats complex problem conditioning large amount information large number possible features might want incorporate model predicts conditional distribution use loglinear models task argued quite flexible terms types features included finally use viterbi algorithm compute highest probability tag sequence input sentence given test sentence input sentence w wn search tag sequence highest value conditional probability achieved using veterbi algorithm going consider generalize approach much wider class problems well focus pausing problem youll see well see general principles emerge apply loglinear models complex problems class models well describe often called historybased models theyre actually conditional history based models well modeling conditional distributions p w critical question following im using notation capital equal input sentence ill use throughout slides define conditional probability big given big tag sequence rather structure example parse structure generalize ideas showed loglinear taggers case structure might parse tree objects maybe even translation example sentence sound heres high level outline approach well use throughout rest segment class well instantiate various choices shown outline essentially three steps rather four steps conditional historybased models first one going take structure example parse tree represent sequence decisions dm well see soon sequence decisions basically corresponds us building tree kind bottom left right order input notice value number decisions necessarily length sentence tagging models saw sequence decisions essentially n tagging decision corresponding n word sentence go complex structures going see number decisions actually different length sentence second step use chain rule decompose probability follows conditional probability tree given sentence going product terms point conditional probability ith decision sequence condition minus one previous decisions addition sentence input model notice havent actually made markup assumptions tagging case replace di minus di minus reflecting fact make kind mark assumption condition previous two decisions fact historybased parsing models see going make assumption okay could potentially condition information context information sentence paused information previous sequence decisions third step going use loglinear model estimate cost conditional probability decision given previous minus decisions input sentence well make use flexibility loglinear models terms features include theyll another setting really see power loglinear models terms representations use final question search general going take sentence input going try find parse tree maximizes conditional probability given course defined expression crucially going make markup assumption case means general case dynamic programming algorithms wont available wont able make use tricks saw dynamic programming give us exact efficient algorithm search models well see use something similar kind beam search algorithms saw phrasedbased decoding use similar kind algorithm particular context,Course1,W8-S2-L2,W8,S2,L2,Conditional,8,2,2,here quick recap loglinear model appli tag problem rememb use notat w colon n refer input sentenc w wn similarli colon n tag sequenc cri first critic idea defin condit probabl tag sequenc n condit word sequenc w wn first use chain rule secondli use independ assumpt result end condit probabl product term product j equal n point jth tag sequenc sub j that condit entir input sentenc previou two tag sequenc mock style assumpt j tag depend previou two tag addit condit entir sentenc everi point left problem estim p term that complex problem condit larg amount inform larg number possibl featur might want incorpor model predict condit distribut use loglinear model task argu quit flexibl term type featur includ final use viterbi algorithm comput highest probabl tag sequenc input sentenc given test sentenc input sentenc w wn search tag sequenc highest valu condit probabl achiev use veterbi algorithm go consid gener approach much wider class problem well focu paus problem youll see well see gener principl emerg appli loglinear model complex problem class model well describ often call historybas model theyr actual condit histori base model well model condit distribut p w critic question follow im use notat capit equal input sentenc ill use throughout slide defin condit probabl big given big tag sequenc rather structur exampl pars structur gener idea show loglinear tagger case structur might pars tree object mayb even translat exampl sentenc sound here high level outlin approach well use throughout rest segment class well instanti variou choic shown outlin essenti three step rather four step condit historybas model first one go take structur exampl pars tree repres sequenc decis dm well see soon sequenc decis basic correspond us build tree kind bottom left right order input notic valu number decis necessarili length sentenc tag model saw sequenc decis essenti n tag decis correspond n word sentenc go complex structur go see number decis actual differ length sentenc second step use chain rule decompos probabl follow condit probabl tree given sentenc go product term point condit probabl ith decis sequenc condit minu one previou decis addit sentenc input model notic havent actual made markup assumpt tag case replac di minu di minu reflect fact make kind mark assumpt condit previou two decis fact historybas pars model see go make assumpt okay could potenti condit inform context inform sentenc paus inform previou sequenc decis third step go use loglinear model estim cost condit probabl decis given previou minu decis input sentenc well make use flexibl loglinear model term featur includ theyll anoth set realli see power loglinear model term represent use final question search gener go take sentenc input go tri find pars tree maxim condit probabl given cours defin express crucial go make markup assumpt case mean gener case dynam program algorithm wont avail wont abl make use trick saw dynam program give us exact effici algorithm search model well see use someth similar kind beam search algorithm saw phrasedbas decod use similar kind algorithm particular context,[ 1  4  0  5 14]
103,Course1_W8-S2-L3_Representing_Trees_as_Decision_Sequences_Part_1_7-23,first important question kind approach implement step one represent tree sequence decisions next several slides segment go detail example tree im going use throughout part lecture sentence lawyer questioned witness revolver standard pen tree bank style tree parse tree sentence youll notice head words tree way saw lexicalized context free grammars im going focus approach developed adwait ratnaparkhi phd thesis approach parsing one way representing tree three sorry representing tree sequence decisions actually used three layers structure decisions well go bit bit first set decisions correspond partofspeech tagging decisions actually first level parsing model looks much like regular log linear tagging model build called chunks finally well build remaining structure parse trees heres layer recap trying represent tree sequence decisions dn going represented sequence decisions input sentence w w wn little n number words input ratnaparkhis parserg first n decisions sequence tagging decisions dn sequence determine noun verb determine noun seen left write particular input sentence okay first n decisions simply tagging decisions seen loglinear tagging model thats first layer simply part speech tags second layer corresponds called chunks second layer going sequence decisions recover chunks within parse tree chunk ratnaparkhis definition defined phrase children part speech tags particular parse tree showed example working verify actually three chunks lawyer np chunk witness np chunk revolver np chunk see constituants three nps satisfies condition children part speech tags actually cases two children firstly determiner secondly noun parts speech phrases one common type chunk common chunks adjp title phrases qps actually essentially numeric phrases things like might qp ratnaparkhi introduced level think found beneficial first recover lowlevel chunks recover high levels parse tree chunks turns recovered quite high accuracy theyve built build higher levels representation top recall whole game define mapping parse trees decision sequences dn second layer layer chunks also going encoded sequence decisions first n decisions parsing process going tagging decisions next n decisions going called chunk tagging decisions means im going actually tag word next level tree tag indicating whether part chunk specifically ill tags startnp start noun phrase chunk joinnp basically saying continuation noun phrase chunk words part chunks notice saw previous slide three np chunks particular example theyre marked one two three see start join annotations encode chunking decisions decisions total first two n covered first n part ofspeech tagging decisions next n chunking decisions start join start join start join recall whole game define mapping parse trees decision sequences dm second layer layer chunks also going encoded sequence decisions first n decisions parsing process tagging decisions next n decisions going called chunk tagging decisions means im going actually tag word next level tree tag indicating whether part chunk specifically ill tags startnp start noun phrase chunk joinnp basically saying continuation noun phrase chunk words part chunks notice saw previous slide three np chunks particular example theyre marked one two three see start join annotations encode chunky decisions decisions total first two n covered first n partofspeech tagging decisions next n chunking decisions start join start join start join,Course1,W8-S2-L3,W8,S2,L3,Representing,8,2,3,first import question kind approach implement step one repres tree sequenc decis next sever slide segment go detail exampl tree im go use throughout part lectur sentenc lawyer question wit revolv standard pen tree bank style tree pars tree sentenc youll notic head word tree way saw lexic context free grammar im go focu approach develop adwait ratnaparkhi phd thesi approach pars one way repres tree three sorri repres tree sequenc decis actual use three layer structur decis well go bit bit first set decis correspond partofspeech tag decis actual first level pars model look much like regular log linear tag model build call chunk final well build remain structur pars tree here layer recap tri repres tree sequenc decis dn go repres sequenc decis input sentenc w w wn littl n number word input ratnaparkhi parserg first n decis sequenc tag decis dn sequenc determin noun verb determin noun seen left write particular input sentenc okay first n decis simpli tag decis seen loglinear tag model that first layer simpli part speech tag second layer correspond call chunk second layer go sequenc decis recov chunk within pars tree chunk ratnaparkhi definit defin phrase children part speech tag particular pars tree show exampl work verifi actual three chunk lawyer np chunk wit np chunk revolv np chunk see constitu three np satisfi condit children part speech tag actual case two children firstli determin secondli noun part speech phrase one common type chunk common chunk adjp titl phrase qp actual essenti numer phrase thing like might qp ratnaparkhi introduc level think found benefici first recov lowlevel chunk recov high level pars tree chunk turn recov quit high accuraci theyv built build higher level represent top recal whole game defin map pars tree decis sequenc dn second layer layer chunk also go encod sequenc decis first n decis pars process go tag decis next n decis go call chunk tag decis mean im go actual tag word next level tree tag indic whether part chunk specif ill tag startnp start noun phrase chunk joinnp basic say continu noun phrase chunk word part chunk notic saw previou slide three np chunk particular exampl theyr mark one two three see start join annot encod chunk decis decis total first two n cover first n part ofspeech tag decis next n chunk decis start join start join start join recal whole game defin map pars tree decis sequenc dm second layer layer chunk also go encod sequenc decis first n decis pars process tag decis next n decis go call chunk tag decis mean im go actual tag word next level tree tag indic whether part chunk specif ill tag startnp start noun phrase chunk joinnp basic say continu noun phrase chunk word part chunk notic saw previou slide three np chunk particular exampl theyr mark one two three see start join annot encod chunki decis decis total first two n cover first n partofspeech tag decis next n chunk decis start join start join start join,[ 1  0  4 14 13]
104,Course1_W8-S2-L4_Representing_Trees_as_Decision_Sequences_Part_2_10-20,weve seen first two layers structure firstly part speech tags sentence secondly sequence chunking decisions going see third layer structure remaining parts parse tree built game encode remaining parts parse tree sequence decisions decisions well see join start check going alternate two classes actions first well choose either join x start x x label example n p v p point weve chosen join start choose check equals yes check equals im going illustrate example tree next slides first appear rather abstract hopefully go example things become clear high level heres meaning actions start x going action starts new constituent label x example start new np start new start new vp always acts leftmost constituent start join label join x similar except continues consitiuent label x always acts leftmost contituent start join label inuitively first thing well choose either start new constiuent continue existing constituent welll apply check operation ccheck equals means nothing essentially say check equals yes well see take previous drawing start action convert completed constituent okay thats abstract description lets see pans particular example heres state weve reached first two levels decisions namely part speech tagging decisions layer secondly chunking decisions well choose chunk chunk chunk structure weve built first two n decisions n length sentence next thing im going pick left constituent join start annotation thats np case im going choose either start join constituent fact start really option cant continue constituent constituants built point fact parse tree saw sentence decision start remember actually something like structure tree lawyer nonterminal np basically making choice stop next thing check operation two possibilities check equals yes check equals choose check equal yes means would actually said okay constituents complete want start would build said okay thats complete constituent past three ive showed sentence one child definitely dont want say check equals yes complete constuent term point check equals basically means leave structure unchanged next operation look look left non terminal doesnt structure rather doesnt start join rules structure look first part speech could either choose start new constituent could choose join would mean wed continue constituent case choose start vp think structure tree correct parse tree case vp started point thats decision reflects next thing check operation say check equals yes wouldve completed vp would structure actually incorrect sentence parse tree ive shown check equal leave vp uncompleted next step go process finding leftmost nonterminal doesnt structure thats np case structure structure choose start new constituent could choose join existing constituent particular case going choose join vp means noun phrase actually going second constituent particular vp go check operation could say check equals check equals yes particular parse tree actually three children vt np prepositional phrase coming going built case going say check equals otherwise would immediately completed constituent vp two children underneath would correct particular parse tree next step youre hopefully getting idea pick less sub tree doesnt structure thats preposition could choose join previous constituent start new one say start pp case prepositional phrase isnt complete say check equals go back left constituent isnt completed doesnt structure thats np could start new constituent could choose join previous one case choose join prepositional phrase finally something interesting happens respect check operation let go back previous state position ive reached im going check basically going say constituent prepositional phrase recent constituent ive built complete case complete theres nothing right theres way build additional structure case say check equals yes notice weve built prepositional phrase okay weve converted start joint structure prepositional phrase structure continue going pick leftmost structure join start label thats prepositional phrase choose start new contradiction join previous one choose join vp case means going extend verb phrase weve building going follow check fact think thing really structures right say check equals yes complete verb phrase indeed happens check equals yes also vp three children underneath pick leftmost structure choose case join decision two structures sequence im going check operation check case equals yes point ive actually completed tree thats sketch sequence decisions particular parse tree lets go back trying acheive remember game try come sequence decisions describes parse tree constructed first end descisions level one part speech tagging decisions individual word second end decisions level two correspond chunking decisions particular parse tree four remaining decisions level three start check operations build remaining structure notice alternate always start check join check final structure complete parse tree built anyway summarize way ratna parkey chose represent parse trees sequences decisions firstly part speech tags secondly chunks finally start join check operations build high levels structure,Course1,W8-S2-L4,W8,S2,L4,Representing,8,2,4,weve seen first two layer structur firstli part speech tag sentenc secondli sequenc chunk decis go see third layer structur remain part pars tree built game encod remain part pars tree sequenc decis decis well see join start check go altern two class action first well choos either join x start x x label exampl n p v p point weve chosen join start choos check equal ye check equal im go illustr exampl tree next slide first appear rather abstract hope go exampl thing becom clear high level here mean action start x go action start new constitu label x exampl start new np start new start new vp alway act leftmost constitu start join label join x similar except continu consitiu label x alway act leftmost contitu start join label inuit first thing well choos either start new constiuent continu exist constitu welll appli check oper ccheck equal mean noth essenti say check equal ye well see take previou draw start action convert complet constitu okay that abstract descript let see pan particular exampl here state weve reach first two level decis name part speech tag decis layer secondli chunk decis well choos chunk chunk chunk structur weve built first two n decis n length sentenc next thing im go pick left constitu join start annot that np case im go choos either start join constitu fact start realli option cant continu constitu constitu built point fact pars tree saw sentenc decis start rememb actual someth like structur tree lawyer nontermin np basic make choic stop next thing check oper two possibl check equal ye check equal choos check equal ye mean would actual said okay constitu complet want start would build said okay that complet constitu past three ive show sentenc one child definit dont want say check equal ye complet constuent term point check equal basic mean leav structur unchang next oper look look left non termin doesnt structur rather doesnt start join rule structur look first part speech could either choos start new constitu could choos join would mean wed continu constitu case choos start vp think structur tree correct pars tree case vp start point that decis reflect next thing check oper say check equal ye wouldv complet vp would structur actual incorrect sentenc pars tree ive shown check equal leav vp uncomplet next step go process find leftmost nontermin doesnt structur that np case structur structur choos start new constitu could choos join exist constitu particular case go choos join vp mean noun phrase actual go second constitu particular vp go check oper could say check equal check equal ye particular pars tree actual three children vt np preposit phrase come go built case go say check equal otherwis would immedi complet constitu vp two children underneath would correct particular pars tree next step your hope get idea pick less sub tree doesnt structur that preposit could choos join previou constitu start new one say start pp case preposit phrase isnt complet say check equal go back left constitu isnt complet doesnt structur that np could start new constitu could choos join previou one case choos join preposit phrase final someth interest happen respect check oper let go back previou state posit ive reach im go check basic go say constitu preposit phrase recent constitu ive built complet case complet there noth right there way build addit structur case say check equal ye notic weve built preposit phrase okay weve convert start joint structur preposit phrase structur continu go pick leftmost structur join start label that preposit phrase choos start new contradict join previou one choos join vp case mean go extend verb phrase weve build go follow check fact think thing realli structur right say check equal ye complet verb phrase inde happen check equal ye also vp three children underneath pick leftmost structur choos case join decis two structur sequenc im go check oper check case equal ye point ive actual complet tree that sketch sequenc decis particular pars tree let go back tri acheiv rememb game tri come sequenc decis describ pars tree construct first end descis level one part speech tag decis individu word second end decis level two correspond chunk decis particular pars tree four remain decis level three start check oper build remain structur notic altern alway start check join check final structur complet pars tree built anyway summar way ratna parkey chose repres pars tree sequenc decis firstli part speech tag secondli chunk final start join check oper build high level structur,[ 0  4  1 14 13]
105,Course1_W8-S2-L5_Features_and_Beam_Search_12-10,okay going go back fill rest steps history based approach pausing remember first step somehow represent trees sequences decisions thats ive shown previous slides second step say probability tree chain rule product terms point condition ice decision entire sentences input previous minus decisions remaining steps approach going derive loglinear model estimates conditional distributions finally somehow implement search highest probability parse tree lets talk next use loglinear model define conditional probability basic idea rather simple actually following usual definition loglinear models condition probability going e raised power fv f feature vector v parameter vector divided normalization constant sum l decisions notice f feature vector looks history combination outcome di history case considers entire sentence previous minus decisions outcome course di th decision similarly look denominator sum possible decisions point parse tree use script refer set possible decisions point would considering f going function takes history compare maps feature vector v going parameter vector course going take f context product v exponentiate normalized usual way weve seen loglinear model models lets think little bit feature vector could potentially look information history combination decision outcome point remember history consists sentence sequence minus decisions fact going correspond partially built structure okay may structure like maybe starts various pieces structure point slides showed previously decisions decision need made lets say sake argument trying make decision constituent could potentially condition information surrounding sentence previously built structure part speech tanks chunks built going leverage flexibility loglinear models defining quite rich features look decision made conjunction contextual information big question define feature f im going give brief sketch kind features ratnaparkhi used particular parse tree feature vector definition vary depending whether next decision falls four different classes ive basically shown previous slides vary depending whether tagging decision next chunking decision one stop join decisions stage chunking check equals versus check equals yes decision actually develops different feature vector definitions four cases briefly tagging decisions used exactly features weve seen partofspeech tagging basically used exactly approach partofspeech tagging case chunking decisions used similar decisions sorry similar features considered word chunked spelling features word chunked previous word next word previous two chunking decisions really features mirrored features used partofspeech tagging subtlety features look back tagging decisions could ask questions partofspeech tag current word im trying chunk obviously useful feature let give sketch features used join start decisions look head words look constituent labels look start join annotations considering nth tree relative decision made n constitutes previous two constituents thats choice ratnaparkhi made head words covered using head finding rules saw right start class lexicalized pcfgs well assume lexical items available useful addition look constituents labels partsofspeech type labels similarly look ahead look head word constituent label relative decision position actually current tree built upon also one position ahead two position ahead positions ahead give sketch set sub trees join start annotations im trying make decision look terminal label partofspeech tag table terminal label head words constituents similarly previous two constituents also looked bigram features might conjoin information two constituents two constituents trigram features would look three sequences label three might look three nonterminal labels along three nonterminals three nonterminal labels finally might look various punctuation features punctuation often useful pausing things like commas colons semicolons presence neighborhood decision quite useful disambiguating parse trees deciding position take really sketch main point im trying get accross make decision join start particular constituent employ quite rich representations context surrounding decision check decision two choices either check equals check equals yes might example check equals yes right start parse tree showed earlier might constructed parse tree actually rather bad parse tree fragment whereas check equals yes id chosen check equals wouldve left start wouldve wouldve kind structure check decisions ratnaparkhi used set features asked variety questions constituent would built check equals yes might example feature saying well case ive constructed rule goes n goes np presumably way bad indicator somethings gone wrong feature get negative weight might ask variety questions constituent ive constructed check check operation thats basically feature effected definitions ratnaparkhis model thats fairly high level sketch type features used feature vectors defined train parameters loglinear model usual way using typically kind regularized maximum likelihood approach basically using identical algorithm one saw training parameters general log linear models specifically loglinear models tagging ill stress one nice thing models make use quite rich representations quite rich feature vector definitions remember partofspeech tagging made markov assumption j th tag depended previous two tags addition entire sentence parsing problem ive general made use features might sensitive kinds decisions made past thats making decision wanted condition kinds context past future partofspeech tagging chunking decisions kind markov assumption doesnt really make sense least would limiting case parsing decision di could depend arbitrary decisions past make independent assumption like one really means dynamic programming going question doesnt problem decoding problem required structure dynamic programming ie kind markov structure instead ratnaparkhi used beam search method similar beam search method saw phrasebased translation coding wont go details intuitively going build several possible parse trees left right using kind decisions described parse trees going probability point well keep around say top probable parse trees model partial set decisions ive made constructing one parse trees,Course1,W8-S2-L5,W8,S2,L5,Features,8,2,5,okay go go back fill rest step histori base approach paus rememb first step somehow repres tree sequenc decis that ive shown previou slide second step say probabl tree chain rule product term point condit ice decis entir sentenc input previou minu decis remain step approach go deriv loglinear model estim condit distribut final somehow implement search highest probabl pars tree let talk next use loglinear model defin condit probabl basic idea rather simpl actual follow usual definit loglinear model condit probabl go e rais power fv f featur vector v paramet vector divid normal constant sum l decis notic f featur vector look histori combin outcom di histori case consid entir sentenc previou minu decis outcom cours di th decis similarli look denomin sum possibl decis point pars tree use script refer set possibl decis point would consid f go function take histori compar map featur vector v go paramet vector cours go take f context product v exponenti normal usual way weve seen loglinear model model let think littl bit featur vector could potenti look inform histori combin decis outcom point rememb histori consist sentenc sequenc minu decis fact go correspond partial built structur okay may structur like mayb start variou piec structur point slide show previous decis decis need made let say sake argument tri make decis constitu could potenti condit inform surround sentenc previous built structur part speech tank chunk built go leverag flexibl loglinear model defin quit rich featur look decis made conjunct contextu inform big question defin featur f im go give brief sketch kind featur ratnaparkhi use particular pars tree featur vector definit vari depend whether next decis fall four differ class ive basic shown previou slide vari depend whether tag decis next chunk decis one stop join decis stage chunk check equal versu check equal ye decis actual develop differ featur vector definit four case briefli tag decis use exactli featur weve seen partofspeech tag basic use exactli approach partofspeech tag case chunk decis use similar decis sorri similar featur consid word chunk spell featur word chunk previou word next word previou two chunk decis realli featur mirror featur use partofspeech tag subtleti featur look back tag decis could ask question partofspeech tag current word im tri chunk obvious use featur let give sketch featur use join start decis look head word look constitu label look start join annot consid nth tree rel decis made n constitut previou two constitu that choic ratnaparkhi made head word cover use head find rule saw right start class lexic pcfg well assum lexic item avail use addit look constitu label partsofspeech type label similarli look ahead look head word constitu label rel decis posit actual current tree built upon also one posit ahead two posit ahead posit ahead give sketch set sub tree join start annot im tri make decis look termin label partofspeech tag tabl termin label head word constitu similarli previou two constitu also look bigram featur might conjoin inform two constitu two constitu trigram featur would look three sequenc label three might look three nontermin label along three nontermin three nontermin label final might look variou punctuat featur punctuat often use paus thing like comma colon semicolon presenc neighborhood decis quit use disambigu pars tree decid posit take realli sketch main point im tri get accross make decis join start particular constitu employ quit rich represent context surround decis check decis two choic either check equal check equal ye might exampl check equal ye right start pars tree show earlier might construct pars tree actual rather bad pars tree fragment wherea check equal ye id chosen check equal wouldv left start wouldv wouldv kind structur check decis ratnaparkhi use set featur ask varieti question constitu would built check equal ye might exampl featur say well case ive construct rule goe n goe np presum way bad indic someth gone wrong featur get neg weight might ask varieti question constitu ive construct check check oper that basic featur effect definit ratnaparkhi model that fairli high level sketch type featur use featur vector defin train paramet loglinear model usual way use typic kind regular maximum likelihood approach basic use ident algorithm one saw train paramet gener log linear model specif loglinear model tag ill stress one nice thing model make use quit rich represent quit rich featur vector definit rememb partofspeech tag made markov assumpt j th tag depend previou two tag addit entir sentenc pars problem ive gener made use featur might sensit kind decis made past that make decis want condit kind context past futur partofspeech tag chunk decis kind markov assumpt doesnt realli make sens least would limit case pars decis di could depend arbitrari decis past make independ assumpt like one realli mean dynam program go question doesnt problem decod problem requir structur dynam program ie kind markov structur instead ratnaparkhi use beam search method similar beam search method saw phrasebas translat code wont go detail intuit go build sever possibl pars tree left right use kind decis describ pars tree go probabl point well keep around say top probabl pars tree model partial set decis ive made construct one pars tree,[ 5  1  4  0 14]
106,Course1_W8-S2-L6_Summary_1-12,thats basically weve seen way using loglinear models construct different type parsing model problistic contextfree grammar style models saw earlier class either approaches strengths weaknesses ratnaparkhis method sense rather simple need define feature vector definitions approach parser also quite powerful incorporate quite rich features within feature vector definitions ratnaparkhis parser recently successful accuracy quite close accuracy lexia piece cheese showed earlier class recently kind models applied problem called dependency parsing theyre extremely effective produce efficient accurate dependency parsers hopefully well see dependency parsing models next week youll find little bit next week class,Course1,W8-S2-L6,W8,S2,L6,Summary,8,2,6,that basic weve seen way use loglinear model construct differ type pars model problist contextfre grammar style model saw earlier class either approach strength weak ratnaparkhi method sens rather simpl need defin featur vector definit approach parser also quit power incorpor quit rich featur within featur vector definit ratnaparkhi parser recent success accuraci quit close accuraci lexia piec chees show earlier class recent kind model appli problem call depend pars theyr extrem effect produc effici accur depend parser hope well see depend pars model next week youll find littl bit next week class,[14  5  4  0 13]
107,Course1_W9-S1-L1_Introduction_0-36,next segment class going describe algorithm called brown et al word clustering algorithm rather remarkable algorithm takes input large quantity unlabeled raw text output produces useful representations individual words really first example well see course socalled unsupervised learning algorithm doesnt require annotated data rather requires raw unlabeled text input,Course1,W9-S1-L1,W9,S1,L1,Introduction,9,1,1,next segment class go describ algorithm call brown et al word cluster algorithm rather remark algorithm take input larg quantiti unlabel raw text output produc use represent individu word realli first exampl well see cours socal unsupervis learn algorithm doesnt requir annot data rather requir raw unlabel text input,[11  4 14 13 12]
108,Course1_W9-S1-L2_Word_Cluster_Representations_8-36,input brown clustering algorithm corpus words rather sentences could potentially quite large might use million tens millions even hundred hundreds millions sentences said one advantage brown method doesnt require annotated data use raw unannotated text might find web news wire data various sources produce two types output first partition words word clusters second actually generalization first hierarchical clustering words let actually give examples two types outputs firstly example clusters actually original paper brown colleagues remember input algorithm unlabeled text theyve shown different clusters algorithm recovered basically partition words vocabulary clusters way similar words appear similar clusters really rather striking first one look words friday monday thursday wednesday days week addition weekends case stress derived completely automatically unlabeled text well soon second cluster seems consist month names another cluster words like people guys folks fellows ceos chaps doubters commies unfortunates blokes seem mineral cluster water glass coal man woman boy first names imagine kind word clusters useful pretty wide range natural language applications additional knowledge identity particular word also class words falls actually final part segment ill show kind cluster representations used directly problem named entity recognition give big improvements particular task example set word clusters lets look second kind representation brown clustering produce hierarchical representation words illustrate let give simple hierarchy say six words vocabulary say apple pear boy girl maybe said reported might case hierarchical clustering looks like take node tree drawn drawn end cluster words node example corresponds two words boy girl node corresponds two words apple pear node corresponds two words said reported go higher tree node would correspond cluster four words apple pair boy girl finally top node entire tree entire vocabulary least interesting clustering think see kind hierarchical clustering reflects fact boy girl similar apple pear sense four words similar theyre nouns two words verbs hierarchical representation allows clustering different levels granularity itll useful think hierarchies assigning bit strings word vocabulary whenever branch tree left branch right branch similarly versus versus think words apple corresponds bit string pear corresponds bit string boy girl said think reported notice bit strings different lengths example two versus three two lengths different words may different depths tree think prefix prefix bit strings defines clustering cluster apple pear cluster boy girl actually four words apple pear girl said reported case going useful think representation hierarchical clusterings bit strings ive shown actual example hierarchical representation type derived using brown clusters paper scott miller others conference called naacl well look closely paper later lecture used kind clusters within context named entity recognition lets take look see brown algorithm really remarkably effective deriving useful hierarchical representations representations derived tens millions words think news wire text see words bit string think common right way first several bits theyre deeply nested hierarchical tree path node words like lawyer newspaperman stewardess toxicologist clearly people general course occasionally errors heres one word slang reason general representations look pretty good similarly company names share pretty long bit string way theyre pretty deep hierarchy finally first names okay share pretty deep bit stream imagine think named entity recognition problem example imagine useful representations infrequent rare words words never seen training data knowing word like consuelo appears class first names extremely useful information trying build named entity detector,Course1,W9-S1-L2,W9,S1,L2,Word,9,1,2,input brown cluster algorithm corpu word rather sentenc could potenti quit larg might use million ten million even hundr hundr million sentenc said one advantag brown method doesnt requir annot data use raw unannot text might find web news wire data variou sourc produc two type output first partit word word cluster second actual gener first hierarch cluster word let actual give exampl two type output firstli exampl cluster actual origin paper brown colleagu rememb input algorithm unlabel text theyv shown differ cluster algorithm recov basic partit word vocabulari cluster way similar word appear similar cluster realli rather strike first one look word friday monday thursday wednesday day week addit weekend case stress deriv complet automat unlabel text well soon second cluster seem consist month name anoth cluster word like peopl guy folk fellow ceo chap doubter commi unfortun bloke seem miner cluster water glass coal man woman boy first name imagin kind word cluster use pretti wide rang natur languag applic addit knowledg ident particular word also class word fall actual final part segment ill show kind cluster represent use directli problem name entiti recognit give big improv particular task exampl set word cluster let look second kind represent brown cluster produc hierarch represent word illustr let give simpl hierarchi say six word vocabulari say appl pear boy girl mayb said report might case hierarch cluster look like take node tree drawn drawn end cluster word node exampl correspond two word boy girl node correspond two word appl pear node correspond two word said report go higher tree node would correspond cluster four word appl pair boy girl final top node entir tree entir vocabulari least interest cluster think see kind hierarch cluster reflect fact boy girl similar appl pear sens four word similar theyr noun two word verb hierarch represent allow cluster differ level granular itll use think hierarchi assign bit string word vocabulari whenev branch tree left branch right branch similarli versu versu think word appl correspond bit string pear correspond bit string boy girl said think report notic bit string differ length exampl two versu three two length differ word may differ depth tree think prefix prefix bit string defin cluster cluster appl pear cluster boy girl actual four word appl pear girl said report case go use think represent hierarch cluster bit string ive shown actual exampl hierarch represent type deriv use brown cluster paper scott miller other confer call naacl well look close paper later lectur use kind cluster within context name entiti recognit let take look see brown algorithm realli remark effect deriv use hierarch represent represent deriv ten million word think news wire text see word bit string think common right way first sever bit theyr deepli nest hierarch tree path node word like lawyer newspaperman stewardess toxicologist clearli peopl gener cours occasion error here one word slang reason gener represent look pretti good similarli compani name share pretti long bit string way theyr pretti deep hierarchi final first name okay share pretti deep bit stream imagin think name entiti recognit problem exampl imagin use represent infrequ rare word word never seen train data know word like consuelo appear class first name extrem use inform tri build name entiti detector,[11  4 13 14 12]
109,Course1_W9-S1-L3_The_Brown_Clustering_Algorithm_Part_1_11-50,recover representations derive representations large quantities unlabeled dater moment im going describe mechanics brown clustering algorithm firstly want talk intuition underlies algorithm intuition large amount unlabeled dater leverage fact similar words tend appear similar contexts little precisely similar words similar distributions words immediate left right example take word word similar words theyre determiners look frequency different words immediate left right might find prepositions like frequently seen left word nouns like dog park share frequently seen right given large conceivable dater basically look distribution words immediate left word distribution words immediate right similar thing word two words similar im going see similar distribution least somewhat similar distribution see frequent words nouns like dog park share right prepositions like left take another example take monday versus tuesday look frequent words left theyre going things like last monday last monday various things going see words like last frequently context similarly therell set words tend appear days week well see similar right context monday tuesday days week thats basic intuition brown clustering method one clustering algorithm theyre actually several possible algorithms going focus one different ways deriving word clusters basically leverage intuition similar words tend similar distributions words left right lets look formulation underlying brown algorithm well use script v refer set words seen corpus going vocabulary well assume corpus consists sequence words w w capital convenience ill assume dont individual sentences one long sequence words think entire corpus concatenated minor detail whether split sentences output brown clustering model going clustering clustering function c maps vocabulary set one k every word vocabulary going sign assigned integer say k equals clusters might assign word maybe monday sound said function going map word integer say basically youre defining mapping words underlying classes brown model basically two types parameters look similar types parameters would see bigram hmm firstly mission parameters might example e given intuitively probability cluster one omitting word okay cluster k going distribution different words vocabulary second type parameter looks much like kind transition parameter saw hmms example q given would probability class following class brown clustering model going consist function c maps vocabulary v set k together parameters e q parameters really similar hmm main difference function c deterministic word gets mapped single state going allow ambiguity different words rather word belong different states hmm given settings parameters e q given definition function c write probability corpus follows going product terms equals n basically going bigram model wi given wi minus thats term calculate calculate rather different way weve seen two terms firstly q c wi given c wi minus intuitively probability given previous cluster c wi minus choosing next cluster cwi e wi given c wi minus c wi sorry thats think follows say previous word say wi minus want calculate probability next word dog lets say sake argument c equal one c dog equal falls first cluster dog fall falls th cluster sense im im first mapping word cluster one conditioned im mapping class next one im generating word dog cluster result p dog given equal e dog given sorry times q given thats basic model thing really bear mind though machinery really derive clustering well see learning model actually going method takes corpus input choose parameters e q clustering c clustering really looking output model want division words vocabulary set clusters similar words appear cluster little later well talk actually optimize choose values e q c attempt maximize function let give one example model works slightly complicated example full distribution definition showed previous slide lets assume vocabulary v consists words dog cat saw least four words might many others c going c dog equal c cat equal c saw equal thats reflecting fact word appears cluster one two cluster word word cluster lets assume mission probabilities e given e cat given equal e dog given e saw given lets assume transition prob probabilities lets see calculate probability dogs cat well first thing notice words falls cluster falls cluster one dog two saw three one cat two write precise form equation particular example firstly im going q terms corresponding basic mark sequence five words q given im assuming class class sort star word start sentence equal q given times q given times q given times q given looks much like standard hmm e given times e dog given times e saw given one emission term words similar calculations see hmms wrinkle really function c maps word deterministic choice underlying state calculate probability well first look word cluster falls calculate probability sentence product q e terms,Course1,W9-S1-L3,W9,S1,L3,The,9,1,3,recov represent deriv represent larg quantiti unlabel dater moment im go describ mechan brown cluster algorithm firstli want talk intuit underli algorithm intuit larg amount unlabel dater leverag fact similar word tend appear similar context littl precis similar word similar distribut word immedi left right exampl take word word similar word theyr determin look frequenc differ word immedi left right might find preposit like frequent seen left word noun like dog park share frequent seen right given larg conceiv dater basic look distribut word immedi left word distribut word immedi right similar thing word two word similar im go see similar distribut least somewhat similar distribut see frequent word noun like dog park share right preposit like left take anoth exampl take monday versu tuesday look frequent word left theyr go thing like last monday last monday variou thing go see word like last frequent context similarli therel set word tend appear day week well see similar right context monday tuesday day week that basic intuit brown cluster method one cluster algorithm theyr actual sever possibl algorithm go focu one differ way deriv word cluster basic leverag intuit similar word tend similar distribut word left right let look formul underli brown algorithm well use script v refer set word seen corpu go vocabulari well assum corpu consist sequenc word w w capit conveni ill assum dont individu sentenc one long sequenc word think entir corpu concaten minor detail whether split sentenc output brown cluster model go cluster cluster function c map vocabulari set one k everi word vocabulari go sign assign integ say k equal cluster might assign word mayb monday sound said function go map word integ say basic your defin map word underli class brown model basic two type paramet look similar type paramet would see bigram hmm firstli mission paramet might exampl e given intuit probabl cluster one omit word okay cluster k go distribut differ word vocabulari second type paramet look much like kind transit paramet saw hmm exampl q given would probabl class follow class brown cluster model go consist function c map vocabulari v set k togeth paramet e q paramet realli similar hmm main differ function c determinist word get map singl state go allow ambigu differ word rather word belong differ state hmm given set paramet e q given definit function c write probabl corpu follow go product term equal n basic go bigram model wi given wi minu that term calcul calcul rather differ way weve seen two term firstli q c wi given c wi minu intuit probabl given previou cluster c wi minu choos next cluster cwi e wi given c wi minu c wi sorri that think follow say previou word say wi minu want calcul probabl next word dog let say sake argument c equal one c dog equal fall first cluster dog fall fall th cluster sens im im first map word cluster one condit im map class next one im gener word dog cluster result p dog given equal e dog given sorri time q given that basic model thing realli bear mind though machineri realli deriv cluster well see learn model actual go method take corpu input choos paramet e q cluster c cluster realli look output model want divis word vocabulari set cluster similar word appear cluster littl later well talk actual optim choos valu e q c attempt maxim function let give one exampl model work slightli complic exampl full distribut definit show previou slide let assum vocabulari v consist word dog cat saw least four word might mani other c go c dog equal c cat equal c saw equal that reflect fact word appear cluster one two cluster word word cluster let assum mission probabl e given e cat given equal e dog given e saw given let assum transit prob probabl let see calcul probabl dog cat well first thing notic word fall cluster fall cluster one dog two saw three one cat two write precis form equat particular exampl firstli im go q term correspond basic mark sequenc five word q given im assum class class sort star word start sentenc equal q given time q given time q given time q given look much like standard hmm e given time e dog given time e saw given one emiss term word similar calcul see hmm wrinkl realli function c map word determinist choic underli state calcul probabl well first look word cluster fall calcul probabl sentenc product q e term,[11  4 14 13 12]
110,Course1_W9-S1-L4_The_Brown_Clustering_Algorithm_Part_2_8-30,summarize brown clustering model consists following vocabulary typically set words seen training course training course might costed cost us million tens millions even hundreds millions sentences words function c defines partition vocabulary k different clusters typically mid k might around thousand example parameter e v given c every v paired every c parameter q c prime given c every pair clusters c prime c intuitively probability next cluster c primed given previous cluster c parameter saying whats probability admitting word v given cost c current cluster critical question going take training corpus input output produce three things function c parameters e q thats going come next going describe derive partition c training corpus training corpus consists sequence words w w wn think concatenation sentences training data might easily millions words im going describe measure well particular partition c fits particular training corpus function called quality takes input training corpus partition c returns value reflecting well partition c fits particular set training samples nothing log likelihood data well see moment function quality going drive algorithm actually picks partition c well see soon attempt maximize quality want describe actual definition term sum equals n log probability ith word corpus whereas log probability consists two terms product two terms firstly probability omitting wi cluster wi secondly probability given previous word cluster w minus one next word cluster w actually going see second simplify considerably inaudible theres number steps im going go detail ill post note describing im going give sketch slide end end really rather beautiful expression derived brownatail paper lets talk little bit criterion though actually function three things function partition c function cubed parameters function e parameters fact going maximize function respect three things function thought measure well three things fit training data theres bit cheat quality function depends c actually e q modified critical observation function c values e q parameters derived usual maximum likelihood way example e v given one cgoing count state one emitting divided number number times weve seen state one q given say going count one followed two divided count one fix value c partition read counts like read number times see cluster one number times see word cluster count number times see cluster followed cluster relationships hold fix c still want maximize function respect emq values emq maximizes function insight critical leads lines algebra following form two terms g constant ignore going maximizing function constant insensitive partition c end term sum pairs clusters c c primed look probability seeing c followed c primed corpus look log p c followed c primed divided p c p c primed ps actually derived well read counts corpus see example dog cat lets say function capital c im trying evaluate say maps word one dog two saw cat performed mapping calculate number times see c followed c prime example n equal case compute number times ive seen particular class example n equal read counts directly myinaudible h function capital c estimate probabilities im describing mechanical way go partition c value actually equal likelyhood involves computing probabilities joint probability seeing c followed c primed marginal probability seeing cluster c computing expression identity follows lines algebra thats important point really simple way evaluating clustering relatively simple form actually briefly might seen information theory actually mutual information stress main result slide way taking partition c assignment words clusters evaluating quality clustering function take clustering measure well fits training corpus derived likelihood basically going measure likelihood corpus particular classroom game derivation line takes times algebra im going go details well post original paper notes explaining detail,Course1,W9-S1-L4,W9,S1,L4,The,9,1,4,summar brown cluster model consist follow vocabulari typic set word seen train cours train cours might cost cost us million ten million even hundr million sentenc word function c defin partit vocabulari k differ cluster typic mid k might around thousand exampl paramet e v given c everi v pair everi c paramet q c prime given c everi pair cluster c prime c intuit probabl next cluster c prime given previou cluster c paramet say what probabl admit word v given cost c current cluster critic question go take train corpu input output produc three thing function c paramet e q that go come next go describ deriv partit c train corpu train corpu consist sequenc word w w wn think concaten sentenc train data might easili million word im go describ measur well particular partit c fit particular train corpu function call qualiti take input train corpu partit c return valu reflect well partit c fit particular set train sampl noth log likelihood data well see moment function qualiti go drive algorithm actual pick partit c well see soon attempt maxim qualiti want describ actual definit term sum equal n log probabl ith word corpu wherea log probabl consist two term product two term firstli probabl omit wi cluster wi secondli probabl given previou word cluster w minu one next word cluster w actual go see second simplifi consider inaud there number step im go go detail ill post note describ im go give sketch slide end end realli rather beauti express deriv brownatail paper let talk littl bit criterion though actual function three thing function partit c function cube paramet function e paramet fact go maxim function respect three thing function thought measur well three thing fit train data there bit cheat qualiti function depend c actual e q modifi critic observ function c valu e q paramet deriv usual maximum likelihood way exampl e v given one cgo count state one emit divid number number time weve seen state one q given say go count one follow two divid count one fix valu c partit read count like read number time see cluster one number time see word cluster count number time see cluster follow cluster relationship hold fix c still want maxim function respect emq valu emq maxim function insight critic lead line algebra follow form two term g constant ignor go maxim function constant insensit partit c end term sum pair cluster c c prime look probabl see c follow c prime corpu look log p c follow c prime divid p c p c prime ps actual deriv well read count corpu see exampl dog cat let say function capit c im tri evalu say map word one dog two saw cat perform map calcul number time see c follow c prime exampl n equal case comput number time ive seen particular class exampl n equal read count directli myinaud h function capit c estim probabl im describ mechan way go partit c valu actual equal likelyhood involv comput probabl joint probabl see c follow c prime margin probabl see cluster c comput express ident follow line algebra that import point realli simpl way evalu cluster rel simpl form actual briefli might seen inform theori actual mutual inform stress main result slide way take partit c assign word cluster evalu qualiti cluster function take cluster measur well fit train corpu deriv likelihood basic go measur likelihood corpu particular classroom game deriv line take time algebra im go go detail well post origin paper note explain detail,[11  4 10 14 13]
111,Course1_W9-S1-L5_The_Brown_Clustering_Algorithm_Part_3_9-18,lets think maximize function quality c im going describe couple methods first one useful thought experiment thats slide inefficient well see modifications actually make efficient basically kind greedy bottom approach start every word cluster might dog cat red blue aim going find say k final clusters maybe example might find k equal case following going run series called merge steps really simple idea time start partition consider possible pairs merges could consider merging single cluster dog cat red blue dog cat red blue consider possible pairwise mergers example say choose merge end new partition c equals c equal say c dog equal c cat equal every word still cluster except two words single cluster time consider poten potential mode step form new clustering capital c calculate quality clustering mean criterion showed previous slide im basically going consider possible merge steps like find merge step maximizes quality resulting clustering say case maybe would find actually going merge step maximizes value quality c thats first merge go second merge step merged two words single cluster exactly thing consider pairwise mergers treat cluster single unit could choose merge two words dog two words cat two words red two words blue could choose merge dog cat could choose merge dog red going pairwise comparisons well one fewer elements two considered unit say sake argument choose merge two third step might something similar maybe find two words merged keep end target number clusters say k equals clusters case thats kind heuristic greedy method step pick merge step maximizes measure quality naively going expensive actually show would take order cabbage size power ibm folks brown et al paper gives slightly efficient algorithm cubic vocabulary thats improvement sure actually still slow realistic values v first algorithm thought experiment well see second algorithm give much practical thought experiment useful well use basic kind greedy bottom method basis algorithm okay actually algorithm people run practice additional parameter approach called typical value equals another property algorithm well see rather producing partition clustering k classes itll actually produce hierarchy might something like red blue dog cat know hierarchical clustering exactly way showed earlier lecture heres works initially take top frequent words example top thousand frequent words put words cluster might initially start say cat dog listing top frequent words corpus sort seed starting point algorithm iterate remaining words go th word right way last word corpus im going consider words turn maybe th word word sheep example okay ive added know words ive added thousand first frequent word im going consider possible merge steps im going consider merging cat dog right sheep im going consider merging cat dog right way sheep im going pairwise comparison look pairs words choose merge would result two words merged lets say sake argument choose merge cat dog case even though sheep choosing thousand first word important realize wont necessarily part merge step may choose merge pair words within initial okay thats first merge step add word maybe word dont know something like notice thousand thousand examples merge step basically different clusters cluster might single word might pair words like cat dog consider pairwise mergers maybe example would take sheep merge cat dog case thats going reduce number clusters one go go next frequent word maybe red thousandth third frequent word im going consider possible pairs mergers possible ways merging either single word entire cluster another single word another entire cluster way process im using quality criterion showed earlier kind oracle given clustering evaluate quality clustering im picking merges maximize measure quality ive added words vocabulary basically clustering show thousand different separate clusters last thing carry minus final merges example final merge steps result going full hierarchy entire set words vocabulary exactly process brown et al researchers used exactly process resulted bitstring representations showed earlier slides running time following show us vocabulary size times squared value say plus linear term size corpus still expensive nevertheless feasible quite large corpora training examples indeed algorithm applied large training corpora,Course1,W9-S1-L5,W9,S1,L5,The,9,1,5,let think maxim function qualiti c im go describ coupl method first one use thought experi that slide ineffici well see modif actual make effici basic kind greedi bottom approach start everi word cluster might dog cat red blue aim go find say k final cluster mayb exampl might find k equal case follow go run seri call merg step realli simpl idea time start partit consid possibl pair merg could consid merg singl cluster dog cat red blue dog cat red blue consid possibl pairwis merger exampl say choos merg end new partit c equal c equal say c dog equal c cat equal everi word still cluster except two word singl cluster time consid poten potenti mode step form new cluster capit c calcul qualiti cluster mean criterion show previou slide im basic go consid possibl merg step like find merg step maxim qualiti result cluster say case mayb would find actual go merg step maxim valu qualiti c that first merg go second merg step merg two word singl cluster exactli thing consid pairwis merger treat cluster singl unit could choos merg two word dog two word cat two word red two word blue could choos merg dog cat could choos merg dog red go pairwis comparison well one fewer element two consid unit say sake argument choos merg two third step might someth similar mayb find two word merg keep end target number cluster say k equal cluster case that kind heurist greedi method step pick merg step maxim measur qualiti naiv go expens actual show would take order cabbag size power ibm folk brown et al paper give slightli effici algorithm cubic vocabulari that improv sure actual still slow realist valu v first algorithm thought experi well see second algorithm give much practic thought experi use well use basic kind greedi bottom method basi algorithm okay actual algorithm peopl run practic addit paramet approach call typic valu equal anoth properti algorithm well see rather produc partit cluster k class itll actual produc hierarchi might someth like red blue dog cat know hierarch cluster exactli way show earlier lectur here work initi take top frequent word exampl top thousand frequent word put word cluster might initi start say cat dog list top frequent word corpu sort seed start point algorithm iter remain word go th word right way last word corpu im go consid word turn mayb th word word sheep exampl okay ive ad know word ive ad thousand first frequent word im go consid possibl merg step im go consid merg cat dog right sheep im go consid merg cat dog right way sheep im go pairwis comparison look pair word choos merg would result two word merg let say sake argument choos merg cat dog case even though sheep choos thousand first word import realiz wont necessarili part merg step may choos merg pair word within initi okay that first merg step add word mayb word dont know someth like notic thousand thousand exampl merg step basic differ cluster cluster might singl word might pair word like cat dog consid pairwis merger mayb exampl would take sheep merg cat dog case that go reduc number cluster one go go next frequent word mayb red thousandth third frequent word im go consid possibl pair merger possibl way merg either singl word entir cluster anoth singl word anoth entir cluster way process im use qualiti criterion show earlier kind oracl given cluster evalu qualiti cluster im pick merg maxim measur qualiti ive ad word vocabulari basic cluster show thousand differ separ cluster last thing carri minu final merg exampl final merg step result go full hierarchi entir set word vocabulari exactli process brown et al research use exactli process result bitstr represent show earlier slide run time follow show us vocabulari size time squar valu say plu linear term size corpu still expens nevertheless feasibl quit larg corpora train exampl inde algorithm appli larg train corpora,[11  4 14 13 12]
112,Course1_W9-S1-L6_Clusters_in_NE_Recognition_Part_1_11-33,recap output whole algorithm output process ahierarchical clustering showed earlier kind treelike structure words leaves tree node corresponds different cluster example node would correspond two words red blue described earlier hierarchical constraints described bit strings mapped mapping word bit string case example red would get bit string blue would get bit string kind representations extremely useful useful know example words sense similar theyre cluster least look first several bits im going talk remainder lecture effective application representations within context named identity recognition work miller others published naacl lets describe works paper called name tagging word clusters discriminative training really wonderful paper think bringing together ideas brown cluster representations combination actually log linear tagging models saw last week class miller et al motivate work following way actually excerpt directly taken paper say relate experience actually built name identity tagger demonstrating potential user actually work done bbn company developed best named entity developed years best name entity recognition methods considerable experience domain saying know knew technology performed well former evaluations applied successfully several research groups needed annotated training examples trained new domain nevertheless wasnt quite user wanted critical problem need annotated training examples youve probably realized want apply rather want develop named entity recognizer new set entities say going beyond people locations organizations going least use methods ive described earlier class well collect labeled examples examples sentences underlying segmentations underlying markings entities course gathering kind annotated training data expensive proposition true pausing problems weve looked also part speech tagging problems weve looked actually many problems natural language processing miller et al go emphasis point actually hmmbased technology similar saw second week class required roughly words annotated examples get peak accuracy maybe million words okay fairly considerable amount training data thats large number parameters models theres always sparse data always words little information getting training data always going help reality need quite large amounts training data models work well annotators actually employed might annotate data pretty fast rate say around words per hour thats impressive nevertheless annotating kind quantity data might take several days annotation might feasible cases cases may simply slow may expensive key idea work use brown cluster information significantly reduce number labeled required build named entity classifier miller et al used something similar lock linear taggers weve seen last weeks lectures remember long linear tagger history going tuple minus minus rather minus previous two tags sentence w wn position example could history h equal say determiner nn dog laughs signifying attacking word three sentence previous two tags would determine noun label going tag example equals verb maybe feature definitions set feautres fk hy k equals number features generally questions history conjunction label key insight miller et al paper include features log linear tagger looked identity particular words context also looked cluster identities words current position previous position next position let go actually list features used work used features like tag previous tag would fairly standard feature something like following fkhy equals minus equals nn equals vb many features form otherwise looking possible pairs tags feauters look tags current word would feature fkhy equals wi equals less equals vb otherwise would likely feature like large number combinations words together possbile tags feature looked current tag example vb spelling features word tagged maybe whether capitalized whether numbers essentially going look much like mapping words rare word feature capture spelling features particular word tagged features look tag previous word tag next word pretty standard features exactly kind thing saw previous lectures lock linear taggers nothing new whats new new features look brown clustering information describing mean says feature looking tag pref eight current word essentially going look first eight bits bit screen representations showed brown clusters might example say fkhy label equal vb first bits wy thats word tag equal know bit string would features like bit strings actually seen training data conjunction tags remember given way brown clusters work look set words particular prefix going get cluster words certain level granularity bit level granularity maybe youd find example red blue green basically related set words cluster feature fire words features generalize across multiple words moreover capture membership words different clusters work looked eh bit prefix bit bit bit looks everything fairly coarse level granularity level equal possible clusters right way bit string length possible clusters enormous number point youve probably almost gotten individual words addition looking current word look bit strings previous word maybe length look bit strings next word length high level critical idea features look bit strings corresponding words context thereby generalize across words occur cluster,Course1,W9-S1-L6,W9,S1,L6,Clusters,9,1,6,recap output whole algorithm output process ahierarch cluster show earlier kind treelik structur word leav tree node correspond differ cluster exampl node would correspond two word red blue describ earlier hierarch constraint describ bit string map map word bit string case exampl red would get bit string blue would get bit string kind represent extrem use use know exampl word sens similar theyr cluster least look first sever bit im go talk remaind lectur effect applic represent within context name ident recognit work miller other publish naacl let describ work paper call name tag word cluster discrimin train realli wonder paper think bring togeth idea brown cluster represent combin actual log linear tag model saw last week class miller et al motiv work follow way actual excerpt directli taken paper say relat experi actual built name ident tagger demonstr potenti user actual work done bbn compani develop best name entiti develop year best name entiti recognit method consider experi domain say know knew technolog perform well former evalu appli success sever research group need annot train exampl train new domain nevertheless wasnt quit user want critic problem need annot train exampl youv probabl realiz want appli rather want develop name entiti recogn new set entiti say go beyond peopl locat organ go least use method ive describ earlier class well collect label exampl exampl sentenc underli segment underli mark entiti cours gather kind annot train data expens proposit true paus problem weve look also part speech tag problem weve look actual mani problem natur languag process miller et al go emphasi point actual hmmbase technolog similar saw second week class requir roughli word annot exampl get peak accuraci mayb million word okay fairli consider amount train data that larg number paramet model there alway spars data alway word littl inform get train data alway go help realiti need quit larg amount train data model work well annot actual employ might annot data pretti fast rate say around word per hour that impress nevertheless annot kind quantiti data might take sever day annot might feasibl case case may simpli slow may expens key idea work use brown cluster inform significantli reduc number label requir build name entiti classifi miller et al use someth similar lock linear tagger weve seen last week lectur rememb long linear tagger histori go tupl minu minu rather minu previou two tag sentenc w wn posit exampl could histori h equal say determin nn dog laugh signifi attack word three sentenc previou two tag would determin noun label go tag exampl equal verb mayb featur definit set feautr fk hy k equal number featur gener question histori conjunct label key insight miller et al paper includ featur log linear tagger look ident particular word context also look cluster ident word current posit previou posit next posit let go actual list featur use work use featur like tag previou tag would fairli standard featur someth like follow fkhi equal minu equal nn equal vb mani featur form otherwis look possibl pair tag feauter look tag current word would featur fkhi equal wi equal less equal vb otherwis would like featur like larg number combin word togeth possbil tag featur look current tag exampl vb spell featur word tag mayb whether capit whether number essenti go look much like map word rare word featur captur spell featur particular word tag featur look tag previou word tag next word pretti standard featur exactli kind thing saw previou lectur lock linear tagger noth new what new new featur look brown cluster inform describ mean say featur look tag pref eight current word essenti go look first eight bit bit screen represent show brown cluster might exampl say fkhi label equal vb first bit wy that word tag equal know bit string would featur like bit string actual seen train data conjunct tag rememb given way brown cluster work look set word particular prefix go get cluster word certain level granular bit level granular mayb youd find exampl red blue green basic relat set word cluster featur fire word featur gener across multipl word moreov captur membership word differ cluster work look eh bit prefix bit bit bit look everyth fairli coars level granular level equal possibl cluster right way bit string length possibl cluster enorm number point youv probabl almost gotten individu word addit look current word look bit string previou word mayb length look bit string next word length high level critic idea featur look bit string correspond word context therebi gener across word occur cluster,[11  4  5  1 13]
113,Course1_W9-S1-L7_Clusters_in_NE_Recognition_Part_2_7-28,heres results paper first set results axis number training samples believe terms number words go million words actually log scale youll notice axis fmeasure accuracy named entity recognizer measure actually combination precision recall actually fmeasure equal times precision times recoil precision plus recall basically think kind averaging precision recall perfect performance example means roughly accuracy really way taking two numbers precision recall forming single number measuring measuring accuracy lets see curve firstly hmm basically best model point serious model hmm model developed several years good name entity recognizer pink see model showed loglinear tagger combination brown clusters worth noting without clusters loglinear tagger similar performance hmm havent showed slide task loglinear model hmm roughly comparable performance see theres pretty substantial difference two curves cluster information really adds quite bit terms accuracy thats words training think think another way want achieve level performance would need much data hmm would need roughly much data discriminative model rather cluster base model notice log scale pretty pretty substantial difference quite impressive result final result sure actually throws another useful technique problem problem called active learning methods weve seen class given fixed set training examples train model examples test test data active learning attempt reduce number training examples need dynamically choosing examples labeled particular choose label useful examples stage would typically mean would train loglinear tagger small number examples maybe would large pool additional examples could send annotated labeling would choose la label examples difficult loglinear tagger maybe loglinear tagger confusion measure another way end rather repeatedly labeling examples already good idea whats going choose examples challenging essentially contain information current learning algorithm thats active learning idea dynamically select examples based informative well known give pretty significant reductions amount training data thats required different curves baseline model loglinear tagger active learning use word clusters ive showed use active learning get pink curve see theres fairly particular improvements yellow curve result brown clusters see good improvement method doesnt use clusters finally impressively result makes use brown clusters also active learning look performance really rather remarkable get money percent performance sorry thats straight line order roughly ten thousand words training data look level performance would needed words training data old method would combination brown clustering information active learning vastly reduced amount label data need saw motivation work certainly useful thing many context ive spoken results named entity recognition kind technique apply many problems another problem brown cost us showed useful pausing problem severe problems unknown brown cost really improve things final thing ill say note use modeling model critical enabled us use representations brown clustering representations really seamless straightforward way would much challenging used representations within hidden markov model said selling point loglinear models flexibility terms representation use weve used flexibility directly leverage information brown clusters derived large quantities unlabeled data maybe millions words unlabeled data,Course1,W9-S1-L7,W9,S1,L7,Clusters,9,1,7,here result paper first set result axi number train sampl believ term number word go million word actual log scale youll notic axi fmeasur accuraci name entiti recogn measur actual combin precis recal actual fmeasur equal time precis time recoil precis plu recal basic think kind averag precis recal perfect perform exampl mean roughli accuraci realli way take two number precis recal form singl number measur measur accuraci let see curv firstli hmm basic best model point seriou model hmm model develop sever year good name entiti recogn pink see model show loglinear tagger combin brown cluster worth note without cluster loglinear tagger similar perform hmm havent show slide task loglinear model hmm roughli compar perform see there pretti substanti differ two curv cluster inform realli add quit bit term accuraci that word train think think anoth way want achiev level perform would need much data hmm would need roughli much data discrimin model rather cluster base model notic log scale pretti pretti substanti differ quit impress result final result sure actual throw anoth use techniqu problem problem call activ learn method weve seen class given fix set train exampl train model exampl test test data activ learn attempt reduc number train exampl need dynam choos exampl label particular choos label use exampl stage would typic mean would train loglinear tagger small number exampl mayb would larg pool addit exampl could send annot label would choos la label exampl difficult loglinear tagger mayb loglinear tagger confus measur anoth way end rather repeatedli label exampl alreadi good idea what go choos exampl challeng essenti contain inform current learn algorithm that activ learn idea dynam select exampl base inform well known give pretti signific reduct amount train data that requir differ curv baselin model loglinear tagger activ learn use word cluster ive show use activ learn get pink curv see there fairli particular improv yellow curv result brown cluster see good improv method doesnt use cluster final impress result make use brown cluster also activ learn look perform realli rather remark get money percent perform sorri that straight line order roughli ten thousand word train data look level perform would need word train data old method would combin brown cluster inform activ learn vastli reduc amount label data need saw motiv work certainli use thing mani context ive spoken result name entiti recognit kind techniqu appli mani problem anoth problem brown cost us show use paus problem sever problem unknown brown cost realli improv thing final thing ill say note use model model critic enabl us use represent brown cluster represent realli seamless straightforward way would much challeng use represent within hidden markov model said sell point loglinear model flexibl term represent use weve use flexibl directli leverag inform brown cluster deriv larg quantiti unlabel data mayb million word unlabel data,[11  4 13  1 14]
114,Course1_W9-S2-L1_Introduction_0-30,going start final topic class framework called global linear models youll see global linear models take many ideas log linear models saw week two ago class extend fairly radical ways general framework solve many natural language processing problems weve seen far tagging parsing potentially also problems like translation,Course1,W9-S2-L1,W9,S2,L1,Introduction,9,2,1,go start final topic class framework call global linear model youll see global linear model take mani idea log linear model saw week two ago class extend fairli radic way gener framework solv mani natur languag process problem weve seen far tag pars potenti also problem like translat,[ 1  4  8  5 14]
115,Course1_W9-S2-L2_Recap_of_History-based_Models_7-11,lets first give brief review weve seen far class many problems weve considered involve supervised learning supervised learning problems general task learn function capital f maps members sum set capital x set possible inputs members sum set set possible outputs going sum function capital f maps sum x sum example parsing problem input x sentence output parse tree machine translation input might sentence one language say french output sentence another language say english language translating part speech tagging input sentence output sequence tags particular part speech tags particular sentence sequence words supervised learning problems weve assumed training set consists xi yi pairs equals n example would n examples consisting sentence parse tree might n translation examples example consists french sentence paired english sentence might tagging case n examples example consists sentence paired sequence tags going somehow learn mapping f training set going algorithm takes training set input somehow produces function capital f output almost models weve seen bar far ill refer history based models high level models take following approach break structure example parse tree tag sequence might call derivation basically sequence decisions example might sequence tagging decisions sequence rule applications context free grammar ill give examples moment decision associated conditional probability might example case hmm conditional probabilities form ti given ti minus ti minus e xi given ti decisions choose tag given previous two tags generate ith word given ith tag multiply decisions get probability entire structure probability structure product decision probabilities typically estimate parameters using variants maximum likelihood estimation typically kind maximum likelihood estimation form smoothing regularization finally function capital f maps input x labels defined label maximizes either joint probability x think hmns pcfgs one example case conditional probability given x saw loglinear taggers basically going use history based models define either joint distribution inputs outputs conditional distribution outputs given inputs models followed methodology weve broken structures sequence decisions associated decision probability multiplied probabilities together get probability entire structure briefly heres first example pcfgs sequence decisions pfcg talked lot left derivations top derivations point derivation nonterminal example decision expand rule example goes np vp decisions associated conditional probability q goes np vp theres parameters basically specifies conditional probability seeing rule goes npvp given expanding probability structure product terms rule n rules sorry tree n rules alpha goes beta goes n multiply together q parameters individual rules parameter values usually estimated using variants maximumlikelihood estimation case means parameter estimate ratio two counts number times weve seen rule divided number times weve seen terminal alpha finally said last slide final function sentence parse tree find follows sentence x search parse trees return parse tree highest probability probability joint probability x heres second example log linear taggers case sentence length n n tagging decisions left right order example might decision determiner noun verb sentence length three example decision associated conditional probability going probability ith tag given previous two tags given entire sentence probability tag sequence conditioned word sequence product terms probability ti given previous two tags entire word sequence case estimate define model conditional model tag given previous two tags sentence using loglinear model saw define models define parameter estimation models finally input x define function f x maximizes conditional probability given x,Course1,W9-S2-L2,W9,S2,L2,Recap,9,2,2,let first give brief review weve seen far class mani problem weve consid involv supervis learn supervis learn problem gener task learn function capit f map member sum set capit x set possibl input member sum set set possibl output go sum function capit f map sum x sum exampl pars problem input x sentenc output pars tree machin translat input might sentenc one languag say french output sentenc anoth languag say english languag translat part speech tag input sentenc output sequenc tag particular part speech tag particular sentenc sequenc word supervis learn problem weve assum train set consist xi yi pair equal n exampl would n exampl consist sentenc pars tree might n translat exampl exampl consist french sentenc pair english sentenc might tag case n exampl exampl consist sentenc pair sequenc tag go somehow learn map f train set go algorithm take train set input somehow produc function capit f output almost model weve seen bar far ill refer histori base model high level model take follow approach break structur exampl pars tree tag sequenc might call deriv basic sequenc decis exampl might sequenc tag decis sequenc rule applic context free grammar ill give exampl moment decis associ condit probabl might exampl case hmm condit probabl form ti given ti minu ti minu e xi given ti decis choos tag given previou two tag gener ith word given ith tag multipli decis get probabl entir structur probabl structur product decis probabl typic estim paramet use variant maximum likelihood estim typic kind maximum likelihood estim form smooth regular final function capit f map input x label defin label maxim either joint probabl x think hmn pcfg one exampl case condit probabl given x saw loglinear tagger basic go use histori base model defin either joint distribut input output condit distribut output given input model follow methodolog weve broken structur sequenc decis associ decis probabl multipli probabl togeth get probabl entir structur briefli here first exampl pcfg sequenc decis pfcg talk lot left deriv top deriv point deriv nontermin exampl decis expand rule exampl goe np vp decis associ condit probabl q goe np vp there paramet basic specifi condit probabl see rule goe npvp given expand probabl structur product term rule n rule sorri tree n rule alpha goe beta goe n multipli togeth q paramet individu rule paramet valu usual estim use variant maximumlikelihood estim case mean paramet estim ratio two count number time weve seen rule divid number time weve seen termin alpha final said last slide final function sentenc pars tree find follow sentenc x search pars tree return pars tree highest probabl probabl joint probabl x here second exampl log linear tagger case sentenc length n n tag decis left right order exampl might decis determin noun verb sentenc length three exampl decis associ condit probabl go probabl ith tag given previou two tag given entir sentenc probabl tag sequenc condit word sequenc product term probabl ti given previou two tag entir word sequenc case estim defin model condit model tag given previou two tag sentenc use loglinear model saw defin model defin paramet estim model final input x defin function f x maxim condit probabl given x,[1 4 0 8 5]
116,Course1_W9-S2-L3_Motivation_for_GLMs_6-34,heres overview remain remainder todays lecture going des describe basic framework global linear models well see offer different way thinking supervised learning problems history based models discussed well talk parsing problems framework particular reranking approaches finally well talk variant perceptron algorithm used parameter estimation global linear models one key idea following well move away idea derivations particular well move away idea attaching probabilities individual decisions go building entire structure instead going talk feature vectors entire structures well call global features really name global linear models comes going define functions example look entire parse tree match feature vector one sense feature vector mappings highly closely related feature vector mappings saw log linear models another sense theres rather radical move situation entire structures getting mapped feature vectors several reasons thinking models terms several reasons introducing global features first piece motivation offer considerable freedom defining features allow us incorporate kinds features kinds information within supervised learning problems really challenging include historybased models weve seen point course blankaudio let give couple examples features parsing problem well see quite easily incorporated within global linear model much difficult incorporate within example probabilistic context free grammar one observation mark johnson others theres definite tendency natural languages something called parallelism coordination essentially following look two phrases bars new york pubs london second phrase bars new york pubs one instance parallelism sound thats bars new york similar structure syntactically speaking pubs london two things coordinated basically similar structures look second example parallelism coordinating bars new york pubs two constituents internal syntactic structure statistically speaking seem see preference kind structures opposed kind preference come useful trying disambiguate structures say example visited bars new york pubs london going pauses going go going include sort parallel structure two things noun phrases coordinated structures include two parallel structures coordinated knowing theres preference parallelism coordination help give preference parse trees kind parallel structures would challenge go back lectures probabilistic context free grammars try figure incorporate preference within probabilistic context free grammar really entirely straight forward one motivations global linear models well see actually easy incorporate kind feature within model blankaudio heres second example kind features might useful rather difficult incorporate within historybased model probabilistic context free grammar semantic features imagine ontology kind lexicon gives properties different nouns verbs one example ontology might resource called wordnet famous resource information large category nouns verbs english ontology might state example word cappuccino plus liquid feature whereas word book feature okay cappuccino liquid thats important thinking word like pour general nouns plus liquid feature likely appear objects whereas nouns liquids much less likely objects kind kind ontological information different nouns different properties quite useful modeling probability different path structures disambiguating different path structures say wed like build parser preference verb poor taking nouns plus liquid feature would challenge think would incorporate kind features within probabilistic contextfree grammar somewhat challenging whereas well see global linear models relatively easy incorporate kind features,Course1,W9-S2-L3,W9,S2,L3,Motivation,9,2,3,here overview remain remaind today lectur go de describ basic framework global linear model well see offer differ way think supervis learn problem histori base model discuss well talk pars problem framework particular rerank approach final well talk variant perceptron algorithm use paramet estim global linear model one key idea follow well move away idea deriv particular well move away idea attach probabl individu decis go build entir structur instead go talk featur vector entir structur well call global featur realli name global linear model come go defin function exampl look entir pars tree match featur vector one sens featur vector map highli close relat featur vector map saw log linear model anoth sens there rather radic move situat entir structur get map featur vector sever reason think model term sever reason introduc global featur first piec motiv offer consider freedom defin featur allow us incorpor kind featur kind inform within supervis learn problem realli challeng includ historybas model weve seen point cours blankaudio let give coupl exampl featur pars problem well see quit easili incorpor within global linear model much difficult incorpor within exampl probabilist context free grammar one observ mark johnson other there definit tendenc natur languag someth call parallel coordin essenti follow look two phrase bar new york pub london second phrase bar new york pub one instanc parallel sound that bar new york similar structur syntact speak pub london two thing coordin basic similar structur look second exampl parallel coordin bar new york pub two constitu intern syntact structur statist speak seem see prefer kind structur oppos kind prefer come use tri disambigu structur say exampl visit bar new york pub london go paus go go go includ sort parallel structur two thing noun phrase coordin structur includ two parallel structur coordin know there prefer parallel coordin help give prefer pars tree kind parallel structur would challeng go back lectur probabilist context free grammar tri figur incorpor prefer within probabilist context free grammar realli entir straight forward one motiv global linear model well see actual easi incorpor kind featur within model blankaudio here second exampl kind featur might use rather difficult incorpor within historybas model probabilist context free grammar semant featur imagin ontolog kind lexicon give properti differ noun verb one exampl ontolog might resourc call wordnet famou resourc inform larg categori noun verb english ontolog might state exampl word cappuccino plu liquid featur wherea word book featur okay cappuccino liquid that import think word like pour gener noun plu liquid featur like appear object wherea noun liquid much less like object kind kind ontolog inform differ noun differ properti quit use model probabl differ path structur disambigu differ path structur say wed like build parser prefer verb poor take noun plu liquid featur would challeng think would incorpor kind featur within probabilist contextfre grammar somewhat challeng wherea well see global linear model rel easi incorpor kind featur,[ 5  0  4 14 13]
117,Course1_W9-S2-L4_Three_Components_of_GLMs_14-39,next going describe global linear models well focus three components models firstly well see feature vectors f going function actually maps entire xy pair feature vector dimensional space x might example sentence might parse tree example f function maps input output pair feature vector feature vectors going somewhat similar feature vectors youve seen log linear models difference take entire structures input second component function called gen takes input x maps input set candidates example might take sentence function maps sentence set candidate possible policies sentence well see several ways essentially finally parameter vector v thats third component also going dimensions number parameters features general functions f gen fixed theyll defined ahead time training data used set parameter values v parameters v similar kind parameters saw log linear models let talk three components turn little detail heres f sticking pausing problem f takes entire xy pairs input pausing k would sentence see fringe tree paired past tree going define function takes parse trees input returns feature vector power put case dimensionality number features equal seven dimensional representation practice many models large often tens thousands hundreds thousands features models particularly parsing case features start take account lexical information identify individual words f going play critical role defines candidate structure represented going information goes learning algorithm going information learning algorithm leverages choosing good bad structures particular input general well construct feature vector defining number individual features feature going function takes entire structure input returns real value output common type feature feature counts kind substructure within structure looking parsing case example itd natural define feature counts number times particular rule say goes bc goes np vp vp goes vt np okay could whole number features track counts different rules grammar particular example define hxry encumber times rule goes bc seen structure particular tree well return value one tree well return value two two instances rule one instance rule general define feature vector defining whole set functions well actually little functions h hd concatenate get feature vector representation parse string one might count number times rule goes bc seen within tree h might count number times rule seen within tree fact simple representation parse trees would simply one feature every possible context free rule underlying grammar see one parse tree might get mapped one feature vector another parse tree might get mapped another feature vector like ill stress feature vector mappings input learning algorithm really important come good definition features second component global linear models function gen essintially takes x input enumerates set candidate structures learning model actually going choose one structures gen critical thats going enumerate full set candidates going considered particular input pausing case gen would take sentences input produce set candidate pause trees sentence well see use feature vectors combination parameter vector choose different pause trees number different ways defining gen parsing could define gen x set possible parses x grammar example context free grammar sound notice case gen large general number parse trees sentence easily grow exponentially respect size sentence actually later segment global linear models probably next weeks lectures see ways dealing cases genx large set like one another scenario quite common within models define genx top n probable parses historybased model means think historybased model baseline approach given sentence produce may different parses case size gen fairly manageable relatively rather small motivation kind setting history based model pretty good job generating reasonably good parses particular one parses may correct one global linear model used select within different parses using powerful features seen underlying history based model tacking could define gen x syllable possible tax sequences protectance say dog barks sort text dnv basically every possible tax sequence gen set quickly going get large going grow exponentially quickly respective length sentence finally translation setting gen x might set possible english translations friend sentence maybe possible translations example phrase based lexicon particular distortion limit abstractly important thing remember gen general way new rating set candidates input sentence x final component global linear models paramiter vector going actually learned training examples ill use little v refer parameter vector also dimensions remember feature vector features well use f v together map candidate structure realvalued score take xy pair case sentence paired pause tree first map structure feature vector function f secondly take inner product f v dot product exactly way saw log linear models get overall score parse tree feature vector f perimeter says example first feature get weigh second weight minus third take inner product two vectors get score example particular example score going interpretation measure plausibility particular structure paired x likely structures going higher scores particular well see soon output global linear model structure maximizes inner product want stress weve made rather radical move weve basically taken technology saw log linear models applied entire structures entire pause structure gets mapped feature vector feature vector representation entire paul structure gets score inner product f v score essentially going replace wed seen models tehis class either joint probability x condition probability given x well see school product plays rather similar role probabilities ive given three parts model put things together see global linear model defines function set inputs set outputs whats function capitol f going take input x going return structure calculation actually simple simply search set candidates gen x maximizes score show showed previous slide really simple idea simply choose highest scoring candidate plausible structure critically function inputs outputs defined see depends three components gen f v said gen f general fixed theyll chosen see training examples critical question well address given training examples xiyi goes n given examples xy mapping set parameters v something well come shortly line may help us visualize entire process little bit better input sentence want map parse tree function gen first enumerates set candidate parse trees input sentence case one two three four five six possible parse trees pause trees gets mapped different feature vector function f one two three four five six feature vectors one pause trees given parameter vector v take inner product f v give score different postries one two three four five six scores tree gets score tree gets scored finally find highest scoring postries highest scores parse tree actually selected output models arg max operation picks highest scoring tree see vary parameter values v youre going affect scores given different parse trees youre going going affect ranking given different parse trees importantly effect output model arg max training procedure essentially going somehow manipulate parameters way well recovering structures training test examples,Course1,W9-S2-L4,W9,S2,L4,Three,9,2,4,next go describ global linear model well focu three compon model firstli well see featur vector f go function actual map entir xy pair featur vector dimension space x might exampl sentenc might pars tree exampl f function map input output pair featur vector featur vector go somewhat similar featur vector youv seen log linear model differ take entir structur input second compon function call gen take input x map input set candid exampl might take sentenc function map sentenc set candid possibl polici sentenc well see sever way essenti final paramet vector v that third compon also go dimens number paramet featur gener function f gen fix theyll defin ahead time train data use set paramet valu v paramet v similar kind paramet saw log linear model let talk three compon turn littl detail here f stick paus problem f take entir xy pair input paus k would sentenc see fring tree pair past tree go defin function take pars tree input return featur vector power put case dimension number featur equal seven dimension represent practic mani model larg often ten thousand hundr thousand featur model particularli pars case featur start take account lexic inform identifi individu word f go play critic role defin candid structur repres go inform goe learn algorithm go inform learn algorithm leverag choos good bad structur particular input gener well construct featur vector defin number individu featur featur go function take entir structur input return real valu output common type featur featur count kind substructur within structur look pars case exampl itd natur defin featur count number time particular rule say goe bc goe np vp vp goe vt np okay could whole number featur track count differ rule grammar particular exampl defin hxri encumb time rule goe bc seen structur particular tree well return valu one tree well return valu two two instanc rule one instanc rule gener defin featur vector defin whole set function well actual littl function h hd concaten get featur vector represent pars string one might count number time rule goe bc seen within tree h might count number time rule seen within tree fact simpl represent pars tree would simpli one featur everi possibl context free rule underli grammar see one pars tree might get map one featur vector anoth pars tree might get map anoth featur vector like ill stress featur vector map input learn algorithm realli import come good definit featur second compon global linear model function gen essinti take x input enumer set candid structur learn model actual go choos one structur gen critic that go enumer full set candid go consid particular input paus case gen would take sentenc input produc set candid paus tree sentenc well see use featur vector combin paramet vector choos differ paus tree number differ way defin gen pars could defin gen x set possibl pars x grammar exampl context free grammar sound notic case gen larg gener number pars tree sentenc easili grow exponenti respect size sentenc actual later segment global linear model probabl next week lectur see way deal case genx larg set like one anoth scenario quit common within model defin genx top n probabl pars historybas model mean think historybas model baselin approach given sentenc produc may differ pars case size gen fairli manag rel rather small motiv kind set histori base model pretti good job gener reason good pars particular one pars may correct one global linear model use select within differ pars use power featur seen underli histori base model tack could defin gen x syllabl possibl tax sequenc protect say dog bark sort text dnv basic everi possibl tax sequenc gen set quickli go get larg go grow exponenti quickli respect length sentenc final translat set gen x might set possibl english translat friend sentenc mayb possibl translat exampl phrase base lexicon particular distort limit abstractli import thing rememb gen gener way new rate set candid input sentenc x final compon global linear model paramit vector go actual learn train exampl ill use littl v refer paramet vector also dimens rememb featur vector featur well use f v togeth map candid structur realvalu score take xy pair case sentenc pair paus tree first map structur featur vector function f secondli take inner product f v dot product exactli way saw log linear model get overal score pars tree featur vector f perimet say exampl first featur get weigh second weight minu third take inner product two vector get score exampl particular exampl score go interpret measur plausibl particular structur pair x like structur go higher score particular well see soon output global linear model structur maxim inner product want stress weve made rather radic move weve basic taken technolog saw log linear model appli entir structur entir paus structur get map featur vector featur vector represent entir paul structur get score inner product f v score essenti go replac wed seen model tehi class either joint probabl x condit probabl given x well see school product play rather similar role probabl ive given three part model put thing togeth see global linear model defin function set input set output what function capitol f go take input x go return structur calcul actual simpl simpli search set candid gen x maxim score show show previou slide realli simpl idea simpli choos highest score candid plausibl structur critic function input output defin see depend three compon gen f v said gen f gener fix theyll chosen see train exampl critic question well address given train exampl xiyi goe n given exampl xy map set paramet v someth well come shortli line may help us visual entir process littl bit better input sentenc want map pars tree function gen first enumer set candid pars tree input sentenc case one two three four five six possibl pars tree paus tree get map differ featur vector function f one two three four five six featur vector one paus tree given paramet vector v take inner product f v give score differ postri one two three four five six score tree get score tree get score final find highest score postri highest score pars tree actual select output model arg max oper pick highest score tree see vari paramet valu v your go affect score given differ pars tree your go go affect rank given differ pars tree importantli effect output model arg max train procedur essenti go somehow manipul paramet way well recov structur train test exampl,[ 5  0  4 14 13]
118,Course1_W9-S2-L5_GLMs_for_Parse_Reranking_10-36,let talk first application models detail parsing problem particular approach called reranking well talk rerank parses rather end best parses existing historybased probalistic parsing reranking approaches basically going following well use baseline parser particular might lexicalized pcfg example know quite good model parsing going use baseline parser produce top n parses sentence training test data n example might im assuming way finding top top top probable parses lexicalized pcfg model fact theyre variance kind dynamic programming algorithms saw earlier class precisely return top n likely parses think visualize following sentence generate bunch parses say top probable parses onto lexicalized pcfg even first likely parse pcfg isnt correct one theres good chance one hundred parses correct theres even better chance one parses actually much much better single best parse particular experiments ill describe little later actually generated parses average training sentences gave us million training parses consider possible members gen possible sentences seen training data supervised data going look like sentence tree pairs couple things either take yi true tree bank parse sentence could take yi member function gen closest tree bank parse example precision recall thats minor detail assume way tree bank choosing one members genx label correct parse particular input lets talk representation f feature vector mapping used critical idea reranking approaches function f takes entire tree input returns feature vector maybe sequence features features going function mapping tree real value critically really position define features could think think back examples parallelism coordination things like bars new york pubs london could example feature identifies fact particular structure parallel coordination structure could example feature counts number parallel coordinator structures within parse tree feature counts number non parallel features structures within parse tree introducing features track kind frequencies parallel verses non parallel structures model learn prefer parallel versus non parallel structures features actually features used another useful feature actually log probability xy baseline model remember lexicalized pcfg take probability often slightly better take log probability might first feature basically gives us default ranking least reranking model well lexicalized pcfg information probabilities pcfg heres another potential feature zero one feature indicator function similar indicator functions weve seen log linear models earlier one x pair contained particular context free rule example entire context free rule let describe features used paper wrote terri koo back throughout ill use quite complicated rule vp goes prepositional phrase vbd np np sbar example rule features well see generally oriented sense around context free rules like one first set features included reranking model features considered individual entire rules example might one feature counts number times ive seen particular rule within tree interestingly lexicalized pcfg using broke larger rules smaller ones roughly speaking binarization process take rule might somehow binarize introducing intermediate nonterminals ok underline less close pcfg something close chomsky novel form didnt really parameters corresponding entire large rules like one reranking model actually added features corresponding entire rules motivation rules important chomsky normal form grammar useful reducing number parameters model make assumptions miss important properties considering frequencies entire entire contextfree rules added features like theyre quite useful heres another feature centered around particular rule example captures something lexicalized pcfg missing sort bigram features within rules adjacent pairs non terminals left right head particular rule going see following bigrams going see one two three right case nonterminal vp pair consecutive nonterminals np np thats particular feature comes another feature would look fact something right head rule vbd way head rule would look fact vp np sbar right finally right words feature vp sbar stop acknowledging stop final component rule left word bigram left word vp prepositional phrase stop features going capturing bigrams within rules lexicalized pcfgs looking really missed features introducing new features useful disambiguating good bad parse trees heres another example type feature might include call grandparent rules features going sensitive rule example ive shown also non terminal directly case weve essentially introduced feature corresponding larger fragment tree features concluded within last class pcfg useful cases minor variant idea actually include two levels context free rules fact goes np vp followed vp rewritten entire rule could introduce similar features many two level rules seen training set blankaudio gives basic idea thats examples features used model see basic idea start introduce features look larger fragments syntactic structures go beyond kind features saw simple lexcalized pcfg,Course1,W9-S2-L5,W9,S2,L5,GLMs,9,2,5,let talk first applic model detail pars problem particular approach call rerank well talk rerank pars rather end best pars exist historybas probalist pars rerank approach basic go follow well use baselin parser particular might lexic pcfg exampl know quit good model pars go use baselin parser produc top n pars sentenc train test data n exampl might im assum way find top top top probabl pars lexic pcfg model fact theyr varianc kind dynam program algorithm saw earlier class precis return top n like pars think visual follow sentenc gener bunch pars say top probabl pars onto lexic pcfg even first like pars pcfg isnt correct one there good chanc one hundr pars correct there even better chanc one pars actual much much better singl best pars particular experi ill describ littl later actual gener pars averag train sentenc gave us million train pars consid possibl member gen possibl sentenc seen train data supervis data go look like sentenc tree pair coupl thing either take yi true tree bank pars sentenc could take yi member function gen closest tree bank pars exampl precis recal that minor detail assum way tree bank choos one member genx label correct pars particular input let talk represent f featur vector map use critic idea rerank approach function f take entir tree input return featur vector mayb sequenc featur featur go function map tree real valu critic realli posit defin featur could think think back exampl parallel coordin thing like bar new york pub london could exampl featur identifi fact particular structur parallel coordin structur could exampl featur count number parallel coordin structur within pars tree featur count number non parallel featur structur within pars tree introduc featur track kind frequenc parallel vers non parallel structur model learn prefer parallel versu non parallel structur featur actual featur use anoth use featur actual log probabl xy baselin model rememb lexic pcfg take probabl often slightli better take log probabl might first featur basic give us default rank least rerank model well lexic pcfg inform probabl pcfg here anoth potenti featur zero one featur indic function similar indic function weve seen log linear model earlier one x pair contain particular context free rule exampl entir context free rule let describ featur use paper wrote terri koo back throughout ill use quit complic rule vp goe preposit phrase vbd np np sbar exampl rule featur well see gener orient sens around context free rule like one first set featur includ rerank model featur consid individu entir rule exampl might one featur count number time ive seen particular rule within tree interestingli lexic pcfg use broke larger rule smaller one roughli speak binar process take rule might somehow binar introduc intermedi nontermin ok underlin less close pcfg someth close chomski novel form didnt realli paramet correspond entir larg rule like one rerank model actual ad featur correspond entir rule motiv rule import chomski normal form grammar use reduc number paramet model make assumpt miss import properti consid frequenc entir entir contextfre rule ad featur like theyr quit use here anoth featur center around particular rule exampl captur someth lexic pcfg miss sort bigram featur within rule adjac pair non termin left right head particular rule go see follow bigram go see one two three right case nontermin vp pair consecut nontermin np np that particular featur come anoth featur would look fact someth right head rule vbd way head rule would look fact vp np sbar right final right word featur vp sbar stop acknowledg stop final compon rule left word bigram left word vp preposit phrase stop featur go captur bigram within rule lexic pcfg look realli miss featur introduc new featur use disambigu good bad pars tree here anoth exampl type featur might includ call grandpar rule featur go sensit rule exampl ive shown also non termin directli case weve essenti introduc featur correspond larger fragment tree featur conclud within last class pcfg use case minor variant idea actual includ two level context free rule fact goe np vp follow vp rewritten entir rule could introduc similar featur mani two level rule seen train set blankaudio give basic idea that exampl featur use model see basic idea start introduc featur look larger fragment syntact structur go beyond kind featur saw simpl lexcal pcfg,[ 5  0  4 14 13]
119,Course1_W9-S2-L6_Parameter_Estimation_with_the_Perceptron_Algorithm_6-11,soon ill talk results using reranking approach using kind features described first want talk first method parameter estimation namely method finding v parameters variant perceptron algorithm well see simple algorithm empirically turns work really quite well wide range natural language processing problems algorithm going assume input training set set examples xi yi equals one n functions gen f described gen enumerates candidates input f defines feature vector mapping well assume components fixed task going learn learn parameters v using training examples information initialize parameters v set v equals vector zeros thats starting state ill define capital f input x exactly way enumerate possible structures set gen x structure calculate feature vector f x take inner product v calculate score structure take highest scoring member gen final output model initially parameters structures going score going tie lets assume case arbitrary way choosing tie members argmax thats minor detail input algorithm capital number iterations algorithm run one nice thing perceptron often quite small maybe okay algorithm proceeds follows make big passes data thats loop loop data pass examples equals one n go single training example time training example things firstly calculate current output model z sub going output f xi going highest scoring structure current parameters zi equal yi means ive correctly recovered correct structure particular example case nothing parameters leave parameters phi unchanged idea broken dont try fix made mistake example lets leave parameters unchanged however zi equal yi structure weve produced somehow different target structure yii case make update parameters v updates look extremely simple simply take v new value v old value v add feature vector xi yi subtract feature vector xi zi remember vectors dimensional space parameter x dimensional phi x also dimensional theres clash computations make sense im adding subtracting dimensional vectors intuitively going features seen trueunknown structure parameter value increased whereas features seen incorrectly proposed structure feature values sorry parameter values decreased kind reinforcing step pushes parameter values things seen truth pushes parameter values features seen zi incorrectly produced structure perceptron algorithm things stress inputs algorithm set training examples definitions gen f also specification number passes training set output said parameters v hope youll see simple algorithm relies decoding step finding highest con structure current model make mistake making simple update parameters v theres actually quite rich theory underlying perceptron justifying algorithmic sense defining converges also statistical sense describing generalize well new test examples least specifying conditions guaranteed generalize well new test examples well try post pointers papers topics course im going detail theory underlying peceptron,Course1,W9-S2-L6,W9,S2,L6,Parameter,9,2,6,soon ill talk result use rerank approach use kind featur describ first want talk first method paramet estim name method find v paramet variant perceptron algorithm well see simpl algorithm empir turn work realli quit well wide rang natur languag process problem algorithm go assum input train set set exampl xi yi equal one n function gen f describ gen enumer candid input f defin featur vector map well assum compon fix task go learn learn paramet v use train exampl inform initi paramet v set v equal vector zero that start state ill defin capit f input x exactli way enumer possibl structur set gen x structur calcul featur vector f x take inner product v calcul score structur take highest score member gen final output model initi paramet structur go score go tie let assum case arbitrari way choos tie member argmax that minor detail input algorithm capit number iter algorithm run one nice thing perceptron often quit small mayb okay algorithm proce follow make big pass data that loop loop data pass exampl equal one n go singl train exampl time train exampl thing firstli calcul current output model z sub go output f xi go highest score structur current paramet zi equal yi mean ive correctli recov correct structur particular exampl case noth paramet leav paramet phi unchang idea broken dont tri fix made mistak exampl let leav paramet unchang howev zi equal yi structur weve produc somehow differ target structur yii case make updat paramet v updat look extrem simpl simpli take v new valu v old valu v add featur vector xi yi subtract featur vector xi zi rememb vector dimension space paramet x dimension phi x also dimension there clash comput make sens im ad subtract dimension vector intuit go featur seen trueunknown structur paramet valu increas wherea featur seen incorrectli propos structur featur valu sorri paramet valu decreas kind reinforc step push paramet valu thing seen truth push paramet valu featur seen zi incorrectli produc structur perceptron algorithm thing stress input algorithm set train exampl definit gen f also specif number pass train set output said paramet v hope youll see simpl algorithm reli decod step find highest con structur current model make mistak make simpl updat paramet v there actual quit rich theori underli perceptron justifi algorithm sens defin converg also statist sens describ gener well new test exampl least specifi condit guarante gener well new test exampl well tri post pointer paper topic cours im go detail theori underli peceptron,[ 5  4  7 14 13]
120,Course1_W9-S2-L7_Summary_3-01,let conclude lecture results parsing problem using reranking models described conjunction perception one set experiments terry koon started baseline model electrolyzed pcfg least time close state art parsing scores around precision recall remember f measure kind average precision recall recovering sub constituents within parse true reranked model described scores fmeasure relative error reduction errors corrected reranking model thats fairly significant improvement given models starting reach quite high levels accuracy actually pretty significant improvement indeed developed lexicalized p pcfgs phd thesis hard push measure see results recently eugene charniak mark johnson employed similar approach better best lists better features also importantly better baseline model model ive shown pushed accuracy accuracy actually close state art parsing performance reranking model gives pretty significant gain actually produced one best results weve seen parsing ive shown though lecture quite new way thinking supervised learning problems see natural language processing idea global linear models defined gen f v finally perception algorithm one way training parameters v give significant improvements reranking problems perhaps importantly theyre going open whole new way thinking algorithms problems translation tagging parsing well see apply models several contexts final week lectures next week class,Course1,W9-S2-L7,W9,S2,L7,Summary,9,2,7,let conclud lectur result pars problem use rerank model describ conjunct percept one set experi terri koon start baselin model electrolyz pcfg least time close state art pars score around precis recal rememb f measur kind averag precis recal recov sub constitu within pars true rerank model describ score fmeasur rel error reduct error correct rerank model that fairli signific improv given model start reach quit high level accuraci actual pretti signific improv inde develop lexic p pcfg phd thesi hard push measur see result recent eugen charniak mark johnson employ similar approach better best list better featur also importantli better baselin model model ive shown push accuraci accuraci actual close state art pars perform rerank model give pretti signific gain actual produc one best result weve seen pars ive shown though lectur quit new way think supervis learn problem see natur languag process idea global linear model defin gen f v final percept algorithm one way train paramet v give signific improv rerank problem perhap importantli theyr go open whole new way think algorithm problem translat tag pars well see appli model sever context final week lectur next week class,[4 0 5 1 8]
121,Course2_W1-S1-L1_Course_Introduction_14-11,hi im dan jurafsky chris manning happy welcome course natural language processing particularly exciting time working natural language processing vast amount data web social media made possible build fantastic new applications lets look one question answering may know ibms watson jeopardy challenge february sixteen answering questions like william wilkinsons book inspired authors famous novel may know answer bram stoker famously wrote sound dracula another important task information extraction example imagine following email colleague chris scheduling meeting wed like software automatically notice dates like tomorrow times like ten eleven room like gates extract information create new calendar entry populate calendar kind structured information event date start end calendar program modern email calendar programs capable text another application kind information extraction involves sentiment analysis imagine youre interested cameras youre reading lot reviews cameras web heres bunch bunch reviews wed like automatically determine reviews people care cameras particular attributes theyre buying camera want know good zoom affordability size weight want automatically determine attributes wed like automatically particular attribute determine reviewers felt attributes example reviewer said nice compact carry thats positive sentiment heres another positive example phrase like flimsy negative sentiment wed like automatically detect sentence sentiment aggregate feature say presume affordability might decide camera reviewers really like flash werent happy ease use might measure positive negative sentiment attribute aggregate machine translation another important new application machine translation fully automatic example might source sentence chinese heres stanfords phrasal mt system translating english mt also used help human translators might arabic text human translator translating english might need help mt system example collection possible next words mt system build automatically help human translator lets look state art language technology like every field nlps divided specialties subspecialties number problems pretty close solved example spam detection hard completely detect spam email boxes dont percent spam thats spam detection relatively easy classification task couple important component tasks part speech tagging named entity tagging well talk later course work pretty high accuracies gonna get percent accuracy part speech tagging well see thats important parsing tasks making good progress commercial completely solved systems used talked sentiment analysis task deciding thumbs thumbs sentence product component technologies like word sense disambiguation deciding talking rodent computer mouse people talk mouses search well talk parcing good enough used lots applications machine translation usable web number applications however still quite hard example answering hard questions like effective medicine treating disease looking web summarizing information know quite hard similarly made progress deciding sentence xyz company acquired abc company yesterday means something similar abc taken xyz general problem detecting two phrases sentences mean thing paraphrase tasks still quite hard even harder task summarization reading number lets say news articles say oh dow jones sp jumped housing prices rose aggregating give user information like summary economy good finally one hardest tasks natural language processing carrying complete humanmachine communication dialogue heres simple example asking movie playing buy movie tickets get applications today general problem understanding everything user might ask returning sensible response quite difficult natural language processing difficult one cute example kinds ambiguity problems called crash blossoms ambiguity case surface form might multiple interpretations crash blossom name kind headline two meanings ambiguity causes humorous interpretation reading first headline violinist linked jal crash blossoms might think main verb linked violinist linked hes linked japan airlines crash blossoms well crash blossoms well headline gave name phenomenon actual interpretation headline writer intended main verb blossoms blossoming violinist fact linked ja crash modifier violinist similar kinds syntactic ambiguities teacher strikes idle kids writer intended main verb idle strikes caused kids idle course humorous interpretation teacher striking strike verb teacher striking idle kids another important kind ambiguity word sense ambiguity third example red tape holds new bridges writer intended holds mean something like delay call sense one holds amusing interpretation second sense holds might write support get interpretation literal redtape opposed bureaucratic redtape actually supporting bridge see lots kinds ambiguities actual headlines turns amusing headlines ambiguity ambiguity pervasive throughout natural language text lets look sensible nonambiguouslooking headline new york times headline shortened bit fed raises interest rates buy seems unambiguous verb vital parser inaudible raises gets raised noun phrase vital role announce interest rates verb phase raising interest rates fed make little noun phrase well say sentence noun phrase fed verb phrase raises gets raised interest rates called phrase structure parse well talk later course phrase structure could also write dependency parse say head verb raises argument fed another dependent rates rates another dependent interest see main verb raising well another interpretation sentence one people dont see parsers see right away raises thats main verb sentence interest somebody interests something something gets interested rates interesting rates well fed raises raises fed completely different sentence different interpretation something interesting rates whatever could mean seems unlikely interpretation people course parser perfectly reasonable interpretation learn rule fact sentence get even difficult actual headline somewhat longer fed raises interest rates half percent could imagine rates verb reading fed raises interest interest federal raises rating half percent might dependency structure like interest rates raises interesting fed modifier raises whether phrase structure parse dependency parse even add words get ambiguity solved order build parse sentence format course youre going video quizzes lectures include little quiz theyre check basic understanding theyre simple multiple choice questions retake get wrong lets see one right number things make natural language understanding difficult one non standard english frequently see text like twitter feeds capitalization unusual spelling words hash tags user ids parsers part speech taggers gonna make use often trained clean newspaper text english actual english wild cause us lot problems well lot segmentation problems example see string r k dash w part new york new know correct segmentation new york new new york new railroad something like yorkdashnew word word like indashlaw solve segmentation problem correctly problems idioms new words havent seen well also problems entity names like movie bugs life english words often difficult know movie name starts ends comes often biology genes proteins named english words task natural understanding difficult tools need well need knowledge language knowledge world way combine knowledge sources generally way use probabilistic models built language data example see word maison french likely translate word house english hand see word avocation french unlikely translate general avocado training probabilistic models general hard turns approximate job probabilistic models rough text features well introduce rough text features go goal class teaching key theory methods statistical natural language processing well talk viterbi algorithm nieve base maxen classifiers well introduce n gram language modeling statistical parcing well talk inverted index tfidf vector models meaning important information retrieval well practical robust real world applications well talk information extraction spelling correction information retrieval skills youll need task youll need simple linear algebra know factor matrix basic probability theory need know program either job python therell weekly programming assignments know choice languages happy welcome course natural language processing look forward seeing following lectures,Course2,W1-S1-L1,W1,S1,L1,Course,1,1,1,hi im dan jurafski chri man happi welcom cours natur languag process particularli excit time work natur languag process vast amount data web social media made possibl build fantast new applic let look one question answer may know ibm watson jeopardi challeng februari sixteen answer question like william wilkinson book inspir author famou novel may know answer bram stoker famous wrote sound dracula anoth import task inform extract exampl imagin follow email colleagu chri schedul meet wed like softwar automat notic date like tomorrow time like ten eleven room like gate extract inform creat new calendar entri popul calendar kind structur inform event date start end calendar program modern email calendar program capabl text anoth applic kind inform extract involv sentiment analysi imagin your interest camera your read lot review camera web here bunch bunch review wed like automat determin review peopl care camera particular attribut theyr buy camera want know good zoom afford size weight want automat determin attribut wed like automat particular attribut determin review felt attribut exampl review said nice compact carri that posit sentiment here anoth posit exampl phrase like flimsi neg sentiment wed like automat detect sentenc sentiment aggreg featur say presum afford might decid camera review realli like flash werent happi eas use might measur posit neg sentiment attribut aggreg machin translat anoth import new applic machin translat fulli automat exampl might sourc sentenc chines here stanford phrasal mt system translat english mt also use help human translat might arab text human translat translat english might need help mt system exampl collect possibl next word mt system build automat help human translat let look state art languag technolog like everi field nlp divid specialti subspecialti number problem pretti close solv exampl spam detect hard complet detect spam email box dont percent spam that spam detect rel easi classif task coupl import compon task part speech tag name entiti tag well talk later cours work pretti high accuraci gonna get percent accuraci part speech tag well see that import pars task make good progress commerci complet solv system use talk sentiment analysi task decid thumb thumb sentenc product compon technolog like word sens disambigu decid talk rodent comput mous peopl talk mous search well talk parc good enough use lot applic machin translat usabl web number applic howev still quit hard exampl answer hard question like effect medicin treat diseas look web summar inform know quit hard similarli made progress decid sentenc xyz compani acquir abc compani yesterday mean someth similar abc taken xyz gener problem detect two phrase sentenc mean thing paraphras task still quit hard even harder task summar read number let say news articl say oh dow jone sp jump hous price rose aggreg give user inform like summari economi good final one hardest task natur languag process carri complet humanmachin commun dialogu here simpl exampl ask movi play buy movi ticket get applic today gener problem understand everyth user might ask return sensibl respons quit difficult natur languag process difficult one cute exampl kind ambigu problem call crash blossom ambigu case surfac form might multipl interpret crash blossom name kind headlin two mean ambigu caus humor interpret read first headlin violinist link jal crash blossom might think main verb link violinist link he link japan airlin crash blossom well crash blossom well headlin gave name phenomenon actual interpret headlin writer intend main verb blossom blossom violinist fact link ja crash modifi violinist similar kind syntact ambigu teacher strike idl kid writer intend main verb idl strike caus kid idl cours humor interpret teacher strike strike verb teacher strike idl kid anoth import kind ambigu word sens ambigu third exampl red tape hold new bridg writer intend hold mean someth like delay call sens one hold amus interpret second sens hold might write support get interpret liter redtap oppos bureaucrat redtap actual support bridg see lot kind ambigu actual headlin turn amus headlin ambigu ambigu pervas throughout natur languag text let look sensibl nonambiguouslook headlin new york time headlin shorten bit fed rais interest rate buy seem unambigu verb vital parser inaud rais get rais noun phrase vital role announc interest rate verb phase rais interest rate fed make littl noun phrase well say sentenc noun phrase fed verb phrase rais get rais interest rate call phrase structur pars well talk later cours phrase structur could also write depend pars say head verb rais argument fed anoth depend rate rate anoth depend interest see main verb rais well anoth interpret sentenc one peopl dont see parser see right away rais that main verb sentenc interest somebodi interest someth someth get interest rate interest rate well fed rais rais fed complet differ sentenc differ interpret someth interest rate whatev could mean seem unlik interpret peopl cours parser perfectli reason interpret learn rule fact sentenc get even difficult actual headlin somewhat longer fed rais interest rate half percent could imagin rate verb read fed rais interest interest feder rais rate half percent might depend structur like interest rate rais interest fed modifi rais whether phrase structur pars depend pars even add word get ambigu solv order build pars sentenc format cours your go video quizz lectur includ littl quiz theyr check basic understand theyr simpl multipl choic question retak get wrong let see one right number thing make natur languag understand difficult one non standard english frequent see text like twitter feed capit unusu spell word hash tag user id parser part speech tagger gonna make use often train clean newspap text english actual english wild caus us lot problem well lot segment problem exampl see string r k dash w part new york new know correct segment new york new new york new railroad someth like yorkdashnew word word like indashlaw solv segment problem correctli problem idiom new word havent seen well also problem entiti name like movi bug life english word often difficult know movi name start end come often biolog gene protein name english word task natur understand difficult tool need well need knowledg languag knowledg world way combin knowledg sourc gener way use probabilist model built languag data exampl see word maison french like translat word hous english hand see word avoc french unlik translat gener avocado train probabilist model gener hard turn approxim job probabilist model rough text featur well introduc rough text featur go goal class teach key theori method statist natur languag process well talk viterbi algorithm niev base maxen classifi well introduc n gram languag model statist parc well talk invert index tfidf vector model mean import inform retriev well practic robust real world applic well talk inform extract spell correct inform retriev skill youll need task youll need simpl linear algebra know factor matrix basic probabl theori need know program either job python therel weekli program assign know choic languag happi welcom cours natur languag process look forward see follow lectur,[ 4  8  0 13  9]
122,Course2_W1-S2-L1_Regular_Expressions_11-25,gonna talk text processing basic fundamental tool text processing regular expression regular expression formal language specifying text strings lets suppose looking woodchucks text document woodchucks expressed number ways could singular woodchuck could plural end could capital letter beginning lower case combination gonna need tools deal problem simplest fundamental tool regular expression disjunction square brackets regular expression pattern mean letter inside square brackets lowercase w capital w square bracket means either lowercase w capital w combine woodchuck match lowercase uppercase woodchuck similarly digits one two three four zero matches digit kinda annoying write wed like instead little ranges range zero dash nine square bracket zero dash nine means character inside range range az means character capital letter z lets see see works heres example red x inaudible little tool going use regular expressions searching little text dr seuss looked saw stepping mat looked saw cat hat lets try disjunctions capital w lower case w x excuse capital w lower case w thats gonna match see capital ws lower case ws fine could es ms thats gonna match es ms ranges capital letters heres capital letters matched lower case letters theres lot lower case letters match alphanumeric characters think second match alphanumeric characters simply match nonalphanumeric characters space exclamation point square brackets gonna match see nonalphabetic characters okay lets go another kind thing might wanna regular expressions negation disjunction might wanna say dont want kind set letters example might wanna say capital letter saying carrot z square brackets carrot occurs right square brackets means carrot z capital letter caret little means neither capital little carrot e carrot means e carrot see carrot occurs right square bracket means later means simply carrot lets take look sound try finding noncapital letters heres non capital letters bout non exclamation points sound things nonalpha numerics sound sorry didnt nonalphabetics theres spaces exclamation points see bout looking carrot carrots none carrots inaudible nothing matches another type disjunction used longer strings pipe symbol sometimes called pipe disjunction groundhog woodchuck mean either string groundhog string wood woodchuck use pipe symbol sometimes thing square bracket pipe b pipe c square bracket abc combine things combine square brackets pipe groundhogger woodchuck use square bracket expressing capitalization beginning see little example looked step sure enough words looked step highlighted distinction random things dont words ats excuse ats inaudible random string fine finally theres sets special characters important regular expressions question mark means previous character optional question mark u mean match word color without u without u u two cleaning operators named steven cleaning inaudible star matches zero previous characters star matches zero os one followed zero os theres initial zero os h heres initial followed one h two three sometimes simple clean plus means one previous characters theres followed plus meaning one theres one theres two os three os dot special character meaning character begn match begin begun begn matches anything sound finally two special characters caret matches beginning line caret capital z matches capital letter beginning line dollar sign matches end line z dollar matches end line like capital letter end line want talk period since periods special character escape back slash period means period period means character back slash period means real period lets go look heres letter heres zero like make lets make one first heres one theres two os sound lets look beginnings ends lines capital letters beginning line heres capital letters end line oh arent punctuation end line theres exclamation points end line sound heres periods remember backslash periods didnt backslash period would get characters cause period matches everything right lets one example lets look little sentence one blithe one lets lets walk search words let find word word little passage think would well simplest thing might type good job finding lets find misses two thes also finds things lets fix first problem get thes middle capitalized thes beginning well going use disjunction sure enough correctly matches two beginning line thes notice pattern although captures something missed still captures things shouldnt capturing blithe need augment patterns going augment pattern really want theres alphabetic character around need space punctuation something nonalphabetic lets say nonalphabetic afterwards great gets rid doesnt solve blithe blithe alphabetic character lets go fix blithe saying non alphabetic ad ether go weve found thes looked noticed missed capitalized examples added made pattern ris expansive increased yield pattern incorrectly returns things inaudible need make pattern precise specifying thing process went based fixing two kinds errors one matching strings shouldnt matched matched matched thats trying thats solving problem false positives theyre called type one errors maxing things shouldnt match thing went solve problem matching things matched missed capital thes thats dealing problem false negatives type two errors turns nat natural language processing constantly dealing two classes errors reducing error rate application gonna see course involves two antagonistic efforts increasing accuracy precision helps us minimize false positives increasing coverage technically called recall minimizing false negatives summary regular expressions play surprisingly large role text processing sophisticated sequences regular expressions weve seen simple versions often first model almost textprocessing task harder tasks often going using well introduce machine learning classifiers much powerful turns even regular expressions used features classifiers useful capturing generalizations youre going returning regular expressions,Course2,W1-S2-L1,W1,S2,L1,Regular,1,2,1,gonna talk text process basic fundament tool text process regular express regular express formal languag specifi text string let suppos look woodchuck text document woodchuck express number way could singular woodchuck could plural end could capit letter begin lower case combin gonna need tool deal problem simplest fundament tool regular express disjunct squar bracket regular express pattern mean letter insid squar bracket lowercas w capit w squar bracket mean either lowercas w capit w combin woodchuck match lowercas uppercas woodchuck similarli digit one two three four zero match digit kinda annoy write wed like instead littl rang rang zero dash nine squar bracket zero dash nine mean charact insid rang rang az mean charact capit letter z let see see work here exampl red x inaud littl tool go use regular express search littl text dr seuss look saw step mat look saw cat hat let tri disjunct capit w lower case w x excus capit w lower case w that gonna match see capit ws lower case ws fine could es ms that gonna match es ms rang capit letter here capit letter match lower case letter there lot lower case letter match alphanumer charact think second match alphanumer charact simpli match nonalphanumer charact space exclam point squar bracket gonna match see nonalphabet charact okay let go anoth kind thing might wanna regular express negat disjunct might wanna say dont want kind set letter exampl might wanna say capit letter say carrot z squar bracket carrot occur right squar bracket mean carrot z capit letter caret littl mean neither capit littl carrot e carrot mean e carrot see carrot occur right squar bracket mean later mean simpli carrot let take look sound tri find noncapit letter here non capit letter bout non exclam point sound thing nonalpha numer sound sorri didnt nonalphabet there space exclam point see bout look carrot carrot none carrot inaud noth match anoth type disjunct use longer string pipe symbol sometim call pipe disjunct groundhog woodchuck mean either string groundhog string wood woodchuck use pipe symbol sometim thing squar bracket pipe b pipe c squar bracket abc combin thing combin squar bracket pipe groundhogg woodchuck use squar bracket express capit begin see littl exampl look step sure enough word look step highlight distinct random thing dont word at excus at inaud random string fine final there set special charact import regular express question mark mean previou charact option question mark u mean match word color without u without u u two clean oper name steven clean inaud star match zero previou charact star match zero os one follow zero os there initi zero os h here initi follow one h two three sometim simpl clean plu mean one previou charact there follow plu mean one there one there two os three os dot special charact mean charact begn match begin begun begn match anyth sound final two special charact caret match begin line caret capit z match capit letter begin line dollar sign match end line z dollar match end line like capit letter end line want talk period sinc period special charact escap back slash period mean period period mean charact back slash period mean real period let go look here letter here zero like make let make one first here one there two os sound let look begin end line capit letter begin line here capit letter end line oh arent punctuat end line there exclam point end line sound here period rememb backslash period didnt backslash period would get charact caus period match everyth right let one exampl let look littl sentenc one blith one let let walk search word let find word word littl passag think would well simplest thing might type good job find let find miss two the also find thing let fix first problem get the middl capit the begin well go use disjunct sure enough correctli match two begin line the notic pattern although captur someth miss still captur thing shouldnt captur blith need augment pattern go augment pattern realli want there alphabet charact around need space punctuat someth nonalphabet let say nonalphabet afterward great get rid doesnt solv blith blith alphabet charact let go fix blith say non alphabet ad ether go weve found the look notic miss capit exampl ad made pattern ri expans increas yield pattern incorrectli return thing inaud need make pattern precis specifi thing process went base fix two kind error one match string shouldnt match match match that tri that solv problem fals posit theyr call type one error max thing shouldnt match thing went solv problem match thing match miss capit the that deal problem fals neg type two error turn nat natur languag process constantli deal two class error reduc error rate applic gonna see cours involv two antagonist effort increas accuraci precis help us minim fals posit increas coverag technic call recal minim fals neg summari regular express play surprisingli larg role text process sophist sequenc regular express weve seen simpl version often first model almost textprocess task harder task often go use well introduc machin learn classifi much power turn even regular express use featur classifi use captur gener your go return regular express,[ 4 12 14 13 11]
123,Course2_W1-S2-L2_Regular_Expressions_in_Practical_NLP_6-04,recent inaudible days theres always lot talk probabilistic models machine mining actually look large systems hood youll almost always find also make quite bit use regular expressions various places many tasks turns regular expressions practical capable way specifying various kinds natural language patterns im gonna show one example showing use regular expressions english tokenizer inside stanford op tools passiron part speech tagger coranal p suite overall okay code stanford english inaudible large determinate stick regular expression written tool called jflex jflex belongs family commonly called computer science lexors another word tokenizers take sequence characters cut pieces one token time front original lexor part unix flex jflex java compatable version lets scroll regular expressions used define character classes often find many regular expressions arent actually complicated theyre really nothing lists put regular expressions putting verticle bars alternation example see several places one abbreviated months one abbreviated days week continues ones like american states various kinds person name title acronym inaudible lets go little bit one thats bit interesting okay heres one phone numbers kind illdocumented regular expression thats little bit hard actually get head around much used practice top level regular expression things divided alternation right right hand side ordination theres inaudible separator used dots ones separated consistent use dots cuz otherwise easy regular expression go wrong also recognize various kinds inaudible numbers patterns part actually easier part beginning optionally use plus signs used europe rest world international prefix county codes country code numbers range two four optional weve got first set numbers area code dot second set numbers guess historically exchange finally third set numbers sets numbers given length three four numbers three five numbers area code two four numbers ranges chosen theyll work phone numbers bunch countries around world know well international phone youll realize actually cases wont still recognized go lefthand side regular expression effectively thing complex first part going recognize things like optional country codes see piece sound inaudible country code allowing possibilities weve escaped inaudible actually sort numbers put inside parentheses got character class allowing variety separators apart period dash needs escaped space non breaking space inaudible non braking space overall allow recognize bunch formats phone numbers itll recognize almost american phone numbers generally pretty well things like uk australian phone numbers want example doesnt work normal phone number format france pairs digits spaces thats included difficulty isnt sort writing regular expression matches context inaudible make expression match managing write one wanted alo wrongly match various things numbers appearing sequence numbers reason well hope thats given idea use regular expressions nlp systems polk around another nlp system im sure youll find lots examples commonly people want match particular patterns whether patterns level words patterns level parts speech convenient practical methods solve many practical tasks,Course2,W1-S2-L2,W1,S2,L2,Regular,1,2,2,recent inaud day there alway lot talk probabilist model machin mine actual look larg system hood youll almost alway find also make quit bit use regular express variou place mani task turn regular express practic capabl way specifi variou kind natur languag pattern im gonna show one exampl show use regular express english token insid stanford op tool passiron part speech tagger coran p suit overal okay code stanford english inaud larg determin stick regular express written tool call jflex jflex belong famili commonli call comput scienc lexor anoth word token take sequenc charact cut piec one token time front origin lexor part unix flex jflex java compat version let scroll regular express use defin charact class often find mani regular express arent actual complic theyr realli noth list put regular express put verticl bar altern exampl see sever place one abbrevi month one abbrevi day week continu one like american state variou kind person name titl acronym inaud let go littl bit one that bit interest okay here one phone number kind illdocu regular express that littl bit hard actual get head around much use practic top level regular express thing divid altern right right hand side ordin there inaud separ use dot one separ consist use dot cuz otherwis easi regular express go wrong also recogn variou kind inaud number pattern part actual easier part begin option use plu sign use europ rest world intern prefix counti code countri code number rang two four option weve got first set number area code dot second set number guess histor exchang final third set number set number given length three four number three five number area code two four number rang chosen theyll work phone number bunch countri around world know well intern phone youll realiz actual case wont still recogn go lefthand side regular express effect thing complex first part go recogn thing like option countri code see piec sound inaud countri code allow possibl weve escap inaud actual sort number put insid parenthes got charact class allow varieti separ apart period dash need escap space non break space inaud non brake space overal allow recogn bunch format phone number itll recogn almost american phone number gener pretti well thing like uk australian phone number want exampl doesnt work normal phone number format franc pair digit space that includ difficulti isnt sort write regular express match context inaud make express match manag write one want alo wrongli match variou thing number appear sequenc number reason well hope that given idea use regular express nlp system polk around anoth nlp system im sure youll find lot exampl commonli peopl want match particular pattern whether pattern level word pattern level part speech conveni practic method solv mani practic task,[ 4 13 14 12 11]
124,Course2_W1-S2-L3_Word_Tokenization_14-26,word tokenization important part text processing every natural language processing text normalize text way start segmenting tokenizing words often normalize format words part process gonna break sentences text lets start talking kind word tokenization many words sentence heres sentence main mainly business data processing many words sentence complicated question theres word like word cut main mainly call things like main fragment call things like uh filled pause certain applications might want counting dealing speech synthesis speech recognition correcting things cat cats talked cat hat define term lemma two words lemma stem part speech roughly word sense cat cats nouns similar meaning could say cat cats lemma word sense define term word form mean fullinflected surface form cat cats definition word different words theyre different word forms gonna use different definitions depending goals lets look example sentence lay back san francisco grass looked stars lets ask many words sentence count define words couple ways word types many vocabulary elements many unique words word tokens many instances particular type running text many tokens well easy count one two three four five count san francisco separately end fifteen count san francisco one token end fourteen even definition word depends little bit gonna spaces types count well thirteen types depending count multiple copies word theres depends count san francisco one word two remember lemmas might decide since lemma although different word forms might want count type depending goal general gonna referring number tokens comes whenever counting things capital n well use capital v mean vocabulary set different types well use set notation cardinality set v size vocabulary although sometimes simplification well use capital v mean vocabulary size ambiguous many words tokens types kind data sets look natural language processing well lets look couple da data sets text called corpora heres three important corpora switchboard corpus phone conversations million word tokens theres word types millions words shakespeare million word tokens shakespeare quite small corpus wrote words lifetime less million words actually used distinct words broad vocabulary famously look huge corpus google ngrams corpus trillion different tokens large number words theres thirteen million types many words english well look conversation different words look shakespeare words combine two probably somewhere quite sum two larger number look google engrams thirteen million course probably urls email addresses even eliminate number words language large maybe theres million words english fact church gale suggested size vocabulary grows greater square root number tokens get n tokens square root n vocabulary vocabulary keeps growing growing names kind things contribute growing vocabulary gonna introduce standard unix tools used text processing corpus shakespeare shakespeares complete works see heres sonnet goes onto place lets start extracting words corpus gonna using tr program sound right tr program takes character maps every instance character another character specify trc means compliment means take every character thats characters turn character case take every nonalphabetic character turn carriage return gonna replace periods commas spaces shakespeare new lines gonna create one line one word per line way lets look theres weve know turned sonnets one word per line sound gonna sort words let us look unique word types lets see heres theres lot occurs lot shakespeare thats boring way look shakespeare dont wanna lets instead use program uniq program uniq take sorted file tell us unique type count times occurs lets try words shakespeare count along left product unique program walk know shakespeare word achievement capitol occurs word achilles appears times word acquaint six times thats interesting would nice didnt look words alphabetical order could look frequency order lets take list words resort frequency frequent word shakespeare word followed word followed word actual accounts shakespeare lexicon shakespeare sorted frequency order problems one word occurs twice didnt map uppercase words lowercase words lets lets fix mapping case first lets try gonna map uppercase letters lower case letters shakespeare gonna pipe another instance r program replaces non alphabetics new lines gonna sorting going unique find individual type uniq c tells us actual account gonna sort n means numerically r means start highest one well look lets alright weve solved problem ands lowercase dont uppercase appearing another problem word word frequent shakespeare also decide standard gonna need words example input finland apostrophe capital gonna tokenize finlands depends goal might choose keep apostrophes finland apostrophe might choose replace apostrophes nothing might choose eliminate apostrophe ses similarly might choose expand whatres ims ams example looking cases sentiment analysis task looking cases negation task might want turn isnt hewlett packard decide whether word like hewlett packard going represented n space true phrases like state art well decide words like lowercase dash dash space talked issue san francisco issues periods become huge issue decide gonna represent mph leave periods algorithms use periods splitting things gonna sensitive issue tokenization becomes even complicated languages french phrase lensemble l apostrophe separate word turn full article le keep l apostrophe l wed like match word ensemble even different article occurs going want break reasons stuck sort nonwords another issue deal german long nouns segmented english word like life insurance company employee english would segmented german gonna get long phrase spelled single word german tasks like information retrieval gonna need like compound splitting chinese japanese different problem theres spaces words heres inaudible weve shown original chinese sentence heres sentence segmented sharapova lives us english segment chinese dont wanna natural language processing chinese applications need break things words well need way similarly japanese problem theres spaces words problem multiple alphabets intermingled katakana alphabet theres hiragana alphabet kanji like chinese characters theres romaji roman letters another complicating issue dealt tokenizing japanese sound word tokenization chinese common research problem addressed kind chinese natural language processing characters chinese represent single syllable often single morpheme average word characters long word broken approximately two three characters lots complicated algorithms standard baseline segmentation algorithm called max match maximum matching algorithm also called greedy algorithm lets look max match algorithm given word list chinese vocabulary chinese dictionary string well start pointer beginning string well find longest word dictionary matches string far starting pointer well move pointer word string well go back move next words lets see example working im gonna pick english example easier think well take phrase imagine english written like chinese spaces well phrase like cat hat ran bun together dictionary words like cat look say whats longest word dictionary matches beginning longest word dictionary thec word theca word well start weve gotten say whats longest word starting c longest word cat say whats longest word starting good job phrase table weve taken spaces table whats segmentation inaudible segmentation algorithm gonna table think little may think gonna produce table theres problem english lot long words english word theta variable instead table gonna get theta right bled gonna get theta bled max match fact generally good algorithm kind pseudo english english without spaces english long words short words mixed together since chinese general relatively consistent word length works well chinese turns modern problemistic segmentation algorithms work even better thats end section word tokenization,Course2,W1-S2-L3,W1,S2,L3,Word,1,2,3,word token import part text process everi natur languag process text normal text way start segment token word often normal format word part process gonna break sentenc text let start talk kind word token mani word sentenc here sentenc main mainli busi data process mani word sentenc complic question there word like word cut main mainli call thing like main fragment call thing like uh fill paus certain applic might want count deal speech synthesi speech recognit correct thing cat cat talk cat hat defin term lemma two word lemma stem part speech roughli word sens cat cat noun similar mean could say cat cat lemma word sens defin term word form mean fullinflect surfac form cat cat definit word differ word theyr differ word form gonna use differ definit depend goal let look exampl sentenc lay back san francisco grass look star let ask mani word sentenc count defin word coupl way word type mani vocabulari element mani uniqu word word token mani instanc particular type run text mani token well easi count one two three four five count san francisco separ end fifteen count san francisco one token end fourteen even definit word depend littl bit gonna space type count well thirteen type depend count multipl copi word there depend count san francisco one word two rememb lemma might decid sinc lemma although differ word form might want count type depend goal gener gonna refer number token come whenev count thing capit n well use capit v mean vocabulari set differ type well use set notat cardin set v size vocabulari although sometim simplif well use capit v mean vocabulari size ambigu mani word token type kind data set look natur languag process well let look coupl da data set text call corpora here three import corpora switchboard corpu phone convers million word token there word type million word shakespear million word token shakespear quit small corpu wrote word lifetim less million word actual use distinct word broad vocabulari famous look huge corpu googl ngram corpu trillion differ token larg number word there thirteen million type mani word english well look convers differ word look shakespear word combin two probabl somewher quit sum two larger number look googl engram thirteen million cours probabl url email address even elimin number word languag larg mayb there million word english fact church gale suggest size vocabulari grow greater squar root number token get n token squar root n vocabulari vocabulari keep grow grow name kind thing contribut grow vocabulari gonna introduc standard unix tool use text process corpu shakespear shakespear complet work see here sonnet goe onto place let start extract word corpu gonna use tr program sound right tr program take charact map everi instanc charact anoth charact specifi trc mean compliment mean take everi charact that charact turn charact case take everi nonalphabet charact turn carriag return gonna replac period comma space shakespear new line gonna creat one line one word per line way let look there weve know turn sonnet one word per line sound gonna sort word let us look uniqu word type let see here there lot occur lot shakespear that bore way look shakespear dont wanna let instead use program uniq program uniq take sort file tell us uniqu type count time occur let tri word shakespear count along left product uniqu program walk know shakespear word achiev capitol occur word achil appear time word acquaint six time that interest would nice didnt look word alphabet order could look frequenc order let take list word resort frequenc frequent word shakespear word follow word follow word actual account shakespear lexicon shakespear sort frequenc order problem one word occur twice didnt map uppercas word lowercas word let let fix map case first let tri gonna map uppercas letter lower case letter shakespear gonna pipe anoth instanc r program replac non alphabet new line gonna sort go uniqu find individu type uniq c tell us actual account gonna sort n mean numer r mean start highest one well look let alright weve solv problem and lowercas dont uppercas appear anoth problem word word frequent shakespear also decid standard gonna need word exampl input finland apostroph capit gonna token finland depend goal might choos keep apostroph finland apostroph might choos replac apostroph noth might choos elimin apostroph se similarli might choos expand whatr im am exampl look case sentiment analysi task look case negat task might want turn isnt hewlett packard decid whether word like hewlett packard go repres n space true phrase like state art well decid word like lowercas dash dash space talk issu san francisco issu period becom huge issu decid gonna repres mph leav period algorithm use period split thing gonna sensit issu token becom even complic languag french phrase lensembl l apostroph separ word turn full articl le keep l apostroph l wed like match word ensembl even differ articl occur go want break reason stuck sort nonword anoth issu deal german long noun segment english word like life insur compani employe english would segment german gonna get long phrase spell singl word german task like inform retriev gonna need like compound split chines japanes differ problem there space word here inaud weve shown origin chines sentenc here sentenc segment sharapova live us english segment chines dont wanna natur languag process chines applic need break thing word well need way similarli japanes problem there space word problem multipl alphabet intermingl katakana alphabet there hiragana alphabet kanji like chines charact there romaji roman letter anoth complic issu dealt token japanes sound word token chines common research problem address kind chines natur languag process charact chines repres singl syllabl often singl morphem averag word charact long word broken approxim two three charact lot complic algorithm standard baselin segment algorithm call max match maximum match algorithm also call greedi algorithm let look max match algorithm given word list chines vocabulari chines dictionari string well start pointer begin string well find longest word dictionari match string far start pointer well move pointer word string well go back move next word let see exampl work im gonna pick english exampl easier think well take phrase imagin english written like chines space well phrase like cat hat ran bun togeth dictionari word like cat look say what longest word dictionari match begin longest word dictionari thec word theca word well start weve gotten say what longest word start c longest word cat say what longest word start good job phrase tabl weve taken space tabl what segment inaud segment algorithm gonna tabl think littl may think gonna produc tabl there problem english lot long word english word theta variabl instead tabl gonna get theta right bled gonna get theta bled max match fact gener good algorithm kind pseudo english english without space english long word short word mix togeth sinc chines gener rel consist word length work well chines turn modern problemist segment algorithm work even better that end section word token,[ 4 14 13 12 11]
125,Course2_W1-S2-L4_Word_Normalization_and_Stemming_11-47,weve segmented words tokenized need normalize stem normalizing means different things information retrieval example require index text query terms form wanna match u usa somebody asks question query one answer want match like implicitly defining kind equivalent class terms might always deleting periods example might take rule takes u usa alternative kind asymmetric expansion example lets say information retrieval enter term window might wanna search window windows morphological variant word window enter capital w windows might wanna search capital w windows cause persons presumably looking product part house mean potentially powerful algorithm less efficient much complicated general use symmetric relatively simple expansions example information retrieval generally remove reduce letters lowercase since users tend use lowercase small exceptions example see uppercase middle sentence like general motors might want keep case matters distinguishing verb fed federal reserve bank capital f group like sail stanford artificial intelligence lab verb sail turns sentiment analysis machine translation information extraction case fact helpful big difference u us also often want lemmatization reducing inflections variant forms base form words like get lemmatized car cars car cars get lemmatized car phrase like boys cars different colors get lemmatized boy car different color general task limatization finding correct dictionary head word form word form given course important sorts applications particularly machine translations example spanish verb like quiero want quieres want important know lemma querer verb want general topic looking parts words leads us morphology morphology study morpheme morpheme smallest unit makes word usually distinguish two kinds morphemes stems thats core meaning bearing units word affixes affix bits pieces adhere stem often grammatical functions particular slide fact stem stem affix word affix confuse us stem es affix theres affix theres theres meaningful theres another affix stemming task taking affixes reduce terms stem particularly historically derived information retrieval although used sorts applications use word stemming specifically mean kind crude chopping affixes course languagedependent kind process english word automate automates automatic automation wed like refer stem automat stemming like simplified version lemmatization pick prefix word use represent word chop suffixes relevant leading stem heres example little text example compressed compression accepted equivalent compress thats text stemmed text heres resulting output see weve lost e example compressed compressing turned compress used ar could used representation ar particular example used ar simplest algorithm commonly used one simple stemming english porter algorithm porters algorithm series iterated series simple rules simple replace rules example one set rules rule step takes strings like sses replaces ss word like caresses chops es caresses rule takes ies chops ies ponies levy pony gonna use pony representation porter stemming pony rules operate order point theres sss left stay sss get deleted point cats deleted ss caress kept similarly step b might remove ings eds want cross ing walk ed plaster specify rule carefully porters stemmer words vowel get ings removed thats world like sing words theres vow additional vowel ing world like sing extra vowels sing one vowel vowel ing stays sing walking vowel addition vowel ing allowed delete suffix word vowel followed ing ing deleted theres lots rules ational turns ate cross relational tion end relate izer ize cross r roads get even complicated get long stems going remove al revival able sound lets look rule strips aim practice using unix tools saw last section look morphology corpus remember stripping ings theres vowel preceding ing rule remember said word like walking vowel ing okay remove ing word like sing vowel theres letters theres previous vowels sing rule doesnt apply sound lets little search four words ending ing shakespeare gonna first take shakespeare turn nonalphabetic characters sound oops sound turn inaudible characters new lines gonna get one word per line gonna translate upper case lower case dealing combining upper case lower case words lets grep grep program finding line contains regular expression file useful unix program gonna look regular expression ing well find words ending ing lets sort well take one copy count sort counts lets see words find ending ing shakespeare see lots words words fact would like remove ing words like king nothing thing ring something sing anything spring lot words lot frequent words fact would bad idea remove ing remove ing king wed get k remove ing spring get sp lets modify rule instead saying grepping words ending ing lets go back change grep words vowel well make e u simulate vowels vowel letters need way say theres vowel anything happen followed ing well say anything dot means character star meaning zero lets look words get back pattern since specified word start vowel weve done much better job finding two syllable words ing fact supposed stripped still problematic words like nothing something dont wanna noth someth anything otherwise maybe cunning otherwise weve done pretty good job making rule little bit better theres little explanation porter stemmer works actually use unix tools little corpus linguistics help write rules kind thats simple example morphology turns languages much complex morphology necessary turkish famous example heres word turkish wont able pronounce means behaving among could civilized see kind thing mother says youve particularly naughty turkish one word long word lot stems civilized stem affix meaning become affix meaning cause affix meaning able languages like turkish saw earlier long nouns german gonna richer complex morpheme segmentation weve seen word tokenization weve seen words normalized stems map normal form,Course2,W1-S2-L4,W1,S2,L4,Word,1,2,4,weve segment word token need normal stem normal mean differ thing inform retriev exampl requir index text queri term form wanna match u usa somebodi ask question queri one answer want match like implicitli defin kind equival class term might alway delet period exampl might take rule take u usa altern kind asymmetr expans exampl let say inform retriev enter term window might wanna search window window morpholog variant word window enter capit w window might wanna search capit w window caus person presum look product part hous mean potenti power algorithm less effici much complic gener use symmetr rel simpl expans exampl inform retriev gener remov reduc letter lowercas sinc user tend use lowercas small except exampl see uppercas middl sentenc like gener motor might want keep case matter distinguish verb fed feder reserv bank capit f group like sail stanford artifici intellig lab verb sail turn sentiment analysi machin translat inform extract case fact help big differ u us also often want lemmat reduc inflect variant form base form word like get lemmat car car car car get lemmat car phrase like boy car differ color get lemmat boy car differ color gener task limat find correct dictionari head word form word form given cours import sort applic particularli machin translat exampl spanish verb like quiero want quier want import know lemma querer verb want gener topic look part word lead us morpholog morpholog studi morphem morphem smallest unit make word usual distinguish two kind morphem stem that core mean bear unit word affix affix bit piec adher stem often grammat function particular slide fact stem stem affix word affix confus us stem es affix there affix there there meaning there anoth affix stem task take affix reduc term stem particularli histor deriv inform retriev although use sort applic use word stem specif mean kind crude chop affix cours languagedepend kind process english word autom autom automat autom wed like refer stem automat stem like simplifi version lemmat pick prefix word use repres word chop suffix relev lead stem here exampl littl text exampl compress compress accept equival compress that text stem text here result output see weve lost e exampl compress compress turn compress use ar could use represent ar particular exampl use ar simplest algorithm commonli use one simpl stem english porter algorithm porter algorithm seri iter seri simpl rule simpl replac rule exampl one set rule rule step take string like ss replac ss word like caress chop es caress rule take i chop i poni levi poni gonna use poni represent porter stem poni rule oper order point there sss left stay sss get delet point cat delet ss caress kept similarli step b might remov ing ed want cross ing walk ed plaster specifi rule care porter stemmer word vowel get ing remov that world like sing word there vow addit vowel ing world like sing extra vowel sing one vowel vowel ing stay sing walk vowel addit vowel ing allow delet suffix word vowel follow ing ing delet there lot rule ation turn ate cross relat tion end relat izer ize cross r road get even complic get long stem go remov al reviv abl sound let look rule strip aim practic use unix tool saw last section look morpholog corpu rememb strip ing there vowel preced ing rule rememb said word like walk vowel ing okay remov ing word like sing vowel there letter there previou vowel sing rule doesnt appli sound let littl search four word end ing shakespear gonna first take shakespear turn nonalphabet charact sound oop sound turn inaud charact new line gonna get one word per line gonna translat upper case lower case deal combin upper case lower case word let grep grep program find line contain regular express file use unix program gonna look regular express ing well find word end ing let sort well take one copi count sort count let see word find end ing shakespear see lot word word fact would like remov ing word like king noth thing ring someth sing anyth spring lot word lot frequent word fact would bad idea remov ing remov ing king wed get k remov ing spring get sp let modifi rule instead say grep word end ing let go back chang grep word vowel well make e u simul vowel vowel letter need way say there vowel anyth happen follow ing well say anyth dot mean charact star mean zero let look word get back pattern sinc specifi word start vowel weve done much better job find two syllabl word ing fact suppos strip still problemat word like noth someth dont wanna noth someth anyth otherwis mayb cun otherwis weve done pretti good job make rule littl bit better there littl explan porter stemmer work actual use unix tool littl corpu linguist help write rule kind that simpl exampl morpholog turn languag much complex morpholog necessari turkish famou exampl here word turkish wont abl pronounc mean behav among could civil see kind thing mother say youv particularli naughti turkish one word long word lot stem civil stem affix mean becom affix mean caus affix mean abl languag like turkish saw earlier long noun german gonna richer complex morphem segment weve seen word token weve seen word normal stem map normal form,[ 4  0 14 13 12]
126,Course2_W1-S2-L5_Sentence_Segmentation_5-31,final discussion basic text processing segmenting sentences running text going segment sentences things end exclamation points question marks thats really great relatively unambiguous clues weve gotten end sentence periods unfortunately quite ambiguous think period sentence boundary periods also used abbreviations like inc dr theyre used numbers like point four point three cant assume period end sentence need solve period problem build classifier going build binary classifier looks period simply makes binary yes decision end sentence end sentence make classifier could use handwritten rules could use regular expressions could build machine learning classifiers simplest kind classifier decision tree heres simple decision tree deciding whether word end sentence decision tree simple procedure asks question branches based answer question say piece text lot blank lines well im probably end sentence theres blank lines well final punctuation question mark exclamation point well im still probably end sentence well final punctuation period well im im end sentence period well depends im long list abbreviations like word etc im probably end sentence im period marking abbreviation like doctor etc im abbreviation im end sentence heres decision tree could imagine arbitrarily sophisticated decision tree features could use one thing could use case called word shape word period uppercase word lowercase word caps uppercase meaning first letter uppercase lower meaning lowercase cap meaning caps number kind word shape features give us information caps word likely abbreviation look word abbrevi period look word period next word starts capital letter im likely beginning period ends sentence next word starts capital letter look lots numeric features look long word short word abbreviations tend relatively short acronyms tend short use sophisticated features say lets look word im looking right take word ask corpus already know sentence boundaries often word occur period end sentence kind word ends sentence kind word example tends start sentence word phrase example word period likely capital period likely start sentence space high probability start sentence use kind features depending condition words help us deciding isnt end sentence period decision tree else statement thats definition decision tree interesting research choosing features weve seen number features might pick particular task general structure decision tree often hard build hand general hand building decision trees possible simple features simple domains might build simple decision tree six seven rules like simple tasks hard numeric features pick thresh hold numeric feature picking probability one features ive gotta question decision tree probability greater thresh hold data ive got set datas generally use machine learning learns structure tree learns things like thresh hold questions asking nonetheless questions decision tree think kind features could exploited classifier whether linguistic regression svms neural nets classifiers well talk later intuition build classifier derive features good predictors whether period acting end sentence put features kind classifier hold whatever classifier going using,Course2,W1-S2-L5,W1,S2,L5,Sentence,1,2,5,final discuss basic text process segment sentenc run text go segment sentenc thing end exclam point question mark that realli great rel unambigu clue weve gotten end sentenc period unfortun quit ambigu think period sentenc boundari period also use abbrevi like inc dr theyr use number like point four point three cant assum period end sentenc need solv period problem build classifi go build binari classifi look period simpli make binari ye decis end sentenc end sentenc make classifi could use handwritten rule could use regular express could build machin learn classifi simplest kind classifi decis tree here simpl decis tree decid whether word end sentenc decis tree simpl procedur ask question branch base answer question say piec text lot blank line well im probabl end sentenc there blank line well final punctuat question mark exclam point well im still probabl end sentenc well final punctuat period well im im end sentenc period well depend im long list abbrevi like word etc im probabl end sentenc im period mark abbrevi like doctor etc im abbrevi im end sentenc here decis tree could imagin arbitrarili sophist decis tree featur could use one thing could use case call word shape word period uppercas word lowercas word cap uppercas mean first letter uppercas lower mean lowercas cap mean cap number kind word shape featur give us inform cap word like abbrevi look word abbrevi period look word period next word start capit letter im like begin period end sentenc next word start capit letter look lot numer featur look long word short word abbrevi tend rel short acronym tend short use sophist featur say let look word im look right take word ask corpu alreadi know sentenc boundari often word occur period end sentenc kind word end sentenc kind word exampl tend start sentenc word phrase exampl word period like capit period like start sentenc space high probabl start sentenc use kind featur depend condit word help us decid isnt end sentenc period decis tree els statement that definit decis tree interest research choos featur weve seen number featur might pick particular task gener structur decis tree often hard build hand gener hand build decis tree possibl simpl featur simpl domain might build simpl decis tree six seven rule like simpl task hard numer featur pick thresh hold numer featur pick probabl one featur ive gotta question decis tree probabl greater thresh hold data ive got set data gener use machin learn learn structur tree learn thing like thresh hold question ask nonetheless question decis tree think kind featur could exploit classifi whether linguist regress svm neural net classifi well talk later intuit build classifi deriv featur good predictor whether period act end sentenc put featur kind classifi hold whatev classifi go use,[4 5 0 9 6]
127,Course2_W1-S3-L1_Defining_Minimum_Edit_Distance_7-04,lets begin discussion minimum edit distance defining minimum edit distance minimum distance way solving problem string similarity similar two strings lets pick particular example spell correction user typed graffe really mean one way operationalizing question asking following words closer letters typed graph graft grail giraffe problem string similarity comes also computational biology sequences nucleotides c g inaudible trying align good alignment able tell us two particular sequences perhaps two samples line certain way amount error idea string similarity sequence similarity comes machine translation information extraction speech recognition comes everywhere lets define edit distance minimum edit distance two strings minimum number editing operations insertion deletion substitution needed transform one generally use three editing operations insertion deletion substitution imagine complicated transpositions long distance movements tend avoid example string intention string execution heres alignment showing many letters line substitutions gaps gap lines letter c execution gap execution lines letter intention think alignment set operations generated alignment turn intention execution delete delete substitute n e substitute x insert c substitute n u rest letters e n edit distance operation one five theres five five things turn intention execution substitutions cost two called levenshtein distance levenshtein distance insertions deletions cost one substitutions cost two distance two strings eight computational biology weve seen sequences bases job figure aligns g g maybe c c c see theres kind insertion represent alignment characters showing align string symbols task given two sequences align letter letter gap thats task biology edit distance comes place machine translation example wed like measure well machine translation system lets suppose machine translation system represented sentence maybe translated chinese spokesman said senior advisor shot dead human expert translator said spokesman confirms senior government advisor shot measure difference two saying many words changed confirms substituted said words inserted words dead words deleted government measuring good machine translation comparing humans similarly tasks like entity extraction know ibm inc ibm entity stanford university president john hennessy entity stanford president john hennessy using distance notice similar one word different one word different measuring number words different improve accuracy invented extraction kinds tasks alright going find minimum distance intuition algorithm search path path mean sequence edits start string final string well start initial state word transforming well set operators insertion deletion substitution well goal state thats word trying get finally well cost path getting thing trying minimize thats number x thats path cost example intention heres piece path intention could delete letter end ntention could insert letter end e eintention could substitute letter end entention thats pieces along path would go intention way screen possible ways transform intention something space possible sequences enormous cant afford navigate naively sequence intuition solving problem lots possible sequences paths wind state dont keep track every way transforming one string another second pieces set identical keep shortest path every revisited state lets look example works gonna define minimum edit distance formally two strings string x length n string length well define distance matrix di j edit distance first characters one x first j characters one j string thats thats whats thats whats defined di j distance entire two strings gonna dn strings length n thats definition minimum added distance,Course2,W1-S3-L1,W1,S3,L1,Defining,1,3,1,let begin discuss minimum edit distanc defin minimum edit distanc minimum distanc way solv problem string similar similar two string let pick particular exampl spell correct user type graff realli mean one way operation question ask follow word closer letter type graph graft grail giraff problem string similar come also comput biolog sequenc nucleotid c g inaud tri align good align abl tell us two particular sequenc perhap two sampl line certain way amount error idea string similar sequenc similar come machin translat inform extract speech recognit come everywher let defin edit distanc minimum edit distanc two string minimum number edit oper insert delet substitut need transform one gener use three edit oper insert delet substitut imagin complic transposit long distanc movement tend avoid exampl string intent string execut here align show mani letter line substitut gap gap line letter c execut gap execut line letter intent think align set oper gener align turn intent execut delet delet substitut n e substitut x insert c substitut n u rest letter e n edit distanc oper one five there five five thing turn intent execut substitut cost two call levenshtein distanc levenshtein distanc insert delet cost one substitut cost two distanc two string eight comput biolog weve seen sequenc base job figur align g g mayb c c c see there kind insert repres align charact show align string symbol task given two sequenc align letter letter gap that task biolog edit distanc come place machin translat exampl wed like measur well machin translat system let suppos machin translat system repres sentenc mayb translat chines spokesman said senior advisor shot dead human expert translat said spokesman confirm senior govern advisor shot measur differ two say mani word chang confirm substitut said word insert word dead word delet govern measur good machin translat compar human similarli task like entiti extract know ibm inc ibm entiti stanford univers presid john hennessi entiti stanford presid john hennessi use distanc notic similar one word differ one word differ measur number word differ improv accuraci invent extract kind task alright go find minimum distanc intuit algorithm search path path mean sequenc edit start string final string well start initi state word transform well set oper insert delet substitut well goal state that word tri get final well cost path get thing tri minim that number x that path cost exampl intent here piec path intent could delet letter end ntention could insert letter end e eintent could substitut letter end entent that piec along path would go intent way screen possibl way transform intent someth space possibl sequenc enorm cant afford navig naiv sequenc intuit solv problem lot possibl sequenc path wind state dont keep track everi way transform one string anoth second piec set ident keep shortest path everi revisit state let look exampl work gonna defin minimum edit distanc formal two string string x length n string length well defin distanc matrix di j edit distanc first charact one x first j charact one j string that that what that what defin di j distanc entir two string gonna dn string length n that definit minimum ad distanc,[12  3  4  8 13]
128,Course2_W1-S3-L2_Computing_Minimum_Edit_Distance_5-54,going compute interim distance standard algorithm dynamic programing programming tabular method computation gonna gonna compute distance two strings x x length n length combining solutions subproblems combining solutions subproblems intuition dynamic programming algorithms intuition simple gonna small prefixes length string x j string well compute difference strings well compute larger distances larger strings based previously computed smaller values words going compute distance ij prefixes string x length prefixes string length j j well end end di distance lets look actual equation heres equation defining minimum distance ive given levenshtein distance distance cost one insertion one deletions two substitutions lets look initialization condition first say characters x x string character x string distance null string cost deleting characters cost length string deleting character similarly inserting characters create string distance null string x string length insertion cost well recurrence relation walking string x walking string well distance particular cell matrix going minimum way getting cell three previous cells go string thats one shorter deleting one thing make j inserting one thing j make longer substituting previous string length x length minus one length j minus one adding new character strings cost zero different substitution cost two end distance two strings simply simply dfnm upper right corner matrix heres table fill element table using equation tells us deletion cost insertion cost substitution cost lets put equation corner want know whats distance null string intention null string execution obviously zero null string string nothing still cost deleting thats one lets try compute whats cost converting e intuitively expect going deletion substitution lets see works alright element cell minimum three values distance plus one distance plus one distance plus either two zero different zero well theyre different minimum two one two zero two two two gonna write two inaudible similarly wanna know distance n e minimum distance nothing plus one two three different distance e plus cost adding n three cost adding n e substitution two three three two three continue along manner case looking previous cells using equation well slowly end sound continue along manner gonna end following complete table every cell table lets take cell tells cost sub ca distance editing string inte turning string exe means value upper right corner cost distance intention execution cost turning intention execution see value eight earlier said levenstein distance levenstein distance equals eight thats algorithm computing inaudible,Course2,W1-S3-L2,W1,S3,L2,Computing,1,3,2,go comput interim distanc standard algorithm dynam program program tabular method comput gonna gonna comput distanc two string x x length n length combin solut subproblem combin solut subproblem intuit dynam program algorithm intuit simpl gonna small prefix length string x j string well comput differ string well comput larger distanc larger string base previous comput smaller valu word go comput distanc ij prefix string x length prefix string length j j well end end di distanc let look actual equat here equat defin minimum distanc ive given levenshtein distanc distanc cost one insert one delet two substitut let look initi condit first say charact x x string charact x string distanc null string cost delet charact cost length string delet charact similarli insert charact creat string distanc null string x string length insert cost well recurr relat walk string x walk string well distanc particular cell matrix go minimum way get cell three previou cell go string that one shorter delet one thing make j insert one thing j make longer substitut previou string length x length minu one length j minu one ad new charact string cost zero differ substitut cost two end distanc two string simpli simpli dfnm upper right corner matrix here tabl fill element tabl use equat tell us delet cost insert cost substitut cost let put equat corner want know what distanc null string intent null string execut obvious zero null string string noth still cost delet that one let tri comput what cost convert e intuit expect go delet substitut let see work alright element cell minimum three valu distanc plu one distanc plu one distanc plu either two zero differ zero well theyr differ minimum two one two zero two two two gonna write two inaud similarli wanna know distanc n e minimum distanc noth plu one two three differ distanc e plu cost ad n three cost ad n e substitut two three three two three continu along manner case look previou cell use equat well slowli end sound continu along manner gonna end follow complet tabl everi cell tabl let take cell tell cost sub ca distanc edit string int turn string exe mean valu upper right corner cost distanc intent execut cost turn intent execut see valu eight earlier said levenstein distanc levenstein distanc equal eight that algorithm comput inaud,[12  4 14 13 11]
129,Course2_W1-S3-L3_Backtrace_for_Computing_Alignments_5-55,knowing edit distance two strings important turns sufficient often need something alignment two strings wanna know symbol string x corresponds symbol string gonna important application inaudible often spell checking machine translation even computational biology way compute alignment keep back trace back trace simply pointer enter cell matrix tells us came reach upper right corner matrix use pointer trace back pointers read alignment lets see works practice ive given equation cell edit distance put values saw earlier ill start putting values sound alright ask get value two two pick minimum three values could either take two distance two distance string string e got saying either alignment nothing e plus insertion extra thats distance one plus one two zero plus two two one plus one two three different values asking minimum path come really theyre could come thats going true value three well yeah computed minimum two plus one one plus two two plus one could come similarly thats going true didnt work arithmetic going true cell work distant distant difference distance inte e could compute taking distance cost us convert n e nothing add another insertion e would would silly four plus one five theres cheaper way get n e e costs us nothing match e e previous alignment n nothing add zero three get three minimum path three came three cases cell came many places case inaudible came previous three going every cell array result look something like every cell every place could come youll see lot cases path could worked six could come place crucially final alignment eight tells us final edit distance intention execution traceback tells us came best alignment intentio executio came best alignment intensi executi trace back alignment get alignment tells us n match n match match maybe insertion rather clean lining computing back trace simple take minimum edit algorithm seen labelled cases looking cell either deleting inserting substituting simply add pointers case inserting point left case deleting point case substituting point diagonally shown arrow previous slide sound look distance matrix think paths origin end matrix nondecreasing path goes origin point nm corresponds alignment two sequences optimal alignment composed optimal subsequences thats idea makes possible use dynamic programming task resulting back trace two strings alignment well know things line exactly things line substitutions insertions deletions whats performance algorithm time order nm back distance matrix size nm filling cell one time true space backtrace worst case go n deletions insertions wed go n plus wed touch n plus cells thats backtrace algorithm computing alignments,Course2,W1-S3-L3,W1,S3,L3,Backtrace,1,3,3,know edit distanc two string import turn suffici often need someth align two string wanna know symbol string x correspond symbol string gonna import applic inaud often spell check machin translat even comput biolog way comput align keep back trace back trace simpli pointer enter cell matrix tell us came reach upper right corner matrix use pointer trace back pointer read align let see work practic ive given equat cell edit distanc put valu saw earlier ill start put valu sound alright ask get valu two two pick minimum three valu could either take two distanc two distanc string string e got say either align noth e plu insert extra that distanc one plu one two zero plu two two one plu one two three differ valu ask minimum path come realli theyr could come that go true valu three well yeah comput minimum two plu one one plu two two plu one could come similarli that go true didnt work arithmet go true cell work distant distant differ distanc int e could comput take distanc cost us convert n e noth add anoth insert e would would silli four plu one five there cheaper way get n e e cost us noth match e e previou align n noth add zero three get three minimum path three came three case cell came mani place case inaud came previou three go everi cell array result look someth like everi cell everi place could come youll see lot case path could work six could come place crucial final align eight tell us final edit distanc intent execut traceback tell us came best align intentio executio came best align intensi executi trace back align get align tell us n match n match match mayb insert rather clean line comput back trace simpl take minimum edit algorithm seen label case look cell either delet insert substitut simpli add pointer case insert point left case delet point case substitut point diagon shown arrow previou slide sound look distanc matrix think path origin end matrix nondecreas path goe origin point nm correspond align two sequenc optim align compos optim subsequ that idea make possibl use dynam program task result back trace two string align well know thing line exactli thing line substitut insert delet what perform algorithm time order nm back distanc matrix size nm fill cell one time true space backtrac worst case go n delet insert wed go n plu wed touch n plu cell that backtrac algorithm comput align,[12  3  4 14 13]
130,Course2_W1-S3-L4_Weighted_Minimum_Edit_Distance_2-47,distance also waited would add waits computation distance think particular applications spell correction obvious letters likely mistyped others biology constraints subject matter kinds deletions insertions likely others example spelling heres confusion matrix spelling errors look confusion matrix see e likely confused e vowels tend confused unlikely con confuse b confused es os us theyre theyre kinding spelling errors people make systematicity confusing vowels vowels also fact keyboards means youre likely make errors either using homologous finger hand using nearby keystrokes constraints domain case talking spelling maybe talking biology gonna make editive edits likely others going represent modifying algorithm slightly add weights levenshtein distance cost one insertion one deletion two substitution weighted minimum edit distance simply add special cost look initialization instead adding one deletion actual cost deletion instead adding one insertion add cost insertion recurrence relation gonna add special cost delete x much cost delete particular character much cost insert character much cost substitute character gonna end termination condition gonna add separate little tables look tables tell us deletion insertion substitution costs way name dynamic programming come heres quotes richard bellmans autobiography bellman one invented dynamic programming amusingly tells us came name dynamic programming really public relations move make algorithm sound exciting maybe one first algorithms named branding way make algorithm sound exciting theres summary algorithm weighted minimum distance,Course2,W1-S3-L4,W1,S3,L4,Weighted,1,3,4,distanc also wait would add wait comput distanc think particular applic spell correct obviou letter like mistyp other biolog constraint subject matter kind delet insert like other exampl spell here confus matrix spell error look confus matrix see e like confus e vowel tend confus unlik con confus b confus es os us theyr theyr kind spell error peopl make systemat confus vowel vowel also fact keyboard mean your like make error either use homolog finger hand use nearbi keystrok constraint domain case talk spell mayb talk biolog gonna make edit edit like other go repres modifi algorithm slightli add weight levenshtein distanc cost one insert one delet two substitut weight minimum edit distanc simpli add special cost look initi instead ad one delet actual cost delet instead ad one insert add cost insert recurr relat gonna add special cost delet x much cost delet particular charact much cost insert charact much cost substitut charact gonna end termin condit gonna add separ littl tabl look tabl tell us delet insert substitut cost way name dynam program come here quot richard bellman autobiographi bellman one invent dynam program amusingli tell us came name dynam program realli public relat move make algorithm sound excit mayb one first algorithm name brand way make algorithm sound excit there summari algorithm weight minimum distanc,[12  4 14 13 11]
131,Course2_W1-S3-L5_Minimum_Edit_Distance_in_Computational_Biology_9-29,number advanced variants minimum edit distance play special role computational biology called computational biology aligning sequence nucleotides sometimes proteins job take two strings like produce alignment like biology important number reasons finding regions genome could discovering functions genes could looking evolutionary things comparing different species also important assembling fragments dna sequencing going trying assemble fragments want look overlapping pieces well talk overlapping pieces find matches find two pieces match comparing individuals looking mutations finding places similarities differences general natural language processing talk distance string edit distance minimum edit distance minimizing distance computing weights things computational biology talking similarities maximizing similarities asking similar two things trying maximize something generally talk scores rather weights competition biology standard minimum distance algorithm looked called needlemanwunsch ive shown algorithm thing saw although general going keep well use mean cost insertions deletions well little value substitution positive negative value substituting things general biology well talk positive costs things match positive value things match cost things deletions insertions heres needlemanwunsch matrix notice opposed natural language processing general computational biology put origin upper left sound lets lets first look variants important computational biology one cases possible unlimited gaps beginning end string happens exactly two little snips know end points one might overlap ends another might something else going places one long sequence heres another long sequence piece sequence piece might overlap dont wanna penalize fact theres things going wed like modify algorithm doesnt penalize gaps end fact various different kinds overlapping sort might happen sequencing overlapping reads might looking piece gene inside another piece subset piece inside larger piece variant dynamic programing algorithm use overlap detection overlap detection variant well make small changes algorithm first changed initialization doesnt cost us anything start long string delete everything insert everything use star j star weve gotten rid allowing start path random point intersection allowing start zero cost penalized matching things looking edge overlaps termination condition gonna look start upper right corner allowing match go way edge well find place along final column final row maximum value well trace back case maximum value column well trace back similar extension needlemanwunsch standard dynamic programming algorithm string distance local alignment problem heres local alignment problem two strings x length length n want find two sub strings whos similarity maximum imagine x would like add two stings would like find two sub strings c c c g g g thats largest similar sub strings similar overlap detection variant saw except allow ignore previously unaligned sequences beginning end also anywhere basically maximum alignment somewhere middle sound sound order order modify needlemanwunsch algorithm allow kind local alignments new version called smithwaterman algorithm first going allow overlap detection variant allow initialization conditions zero x dont penalize initial strings gonna make one modification cell looking possible places could come choose alignment gonna pick maximum three previous cells also going add maximum zero going let sense biology talking maximizing similarity things get different get negative score going start zero allow throw away regions dont align termination condition smithwaterman algorithm depends looking want best local alignment well pick place thats thats maximum entire array well trace back want local alignments score greater threshold maybe well find place thats greater find places trace back gets complicated fact overlapping local alignments might two alignments like might actually overlap tracing back complications want best local alignment thats actually much easier heres example local alignment lets lets imagine getting one positive point every time two symbols match negative point deletion insertion substitution lets look local alignments two strings c c fill matrix theyre gonna start zeroes everywhere local alignment see two look regions cells maximum distance trace back see two cells one corresponds alignment atcat attat four strings match one mismatch thats gonna distance three one corresponds alignment atc atc three matching symbols advanced variance edit distance see computational biology,Course2,W1-S3-L5,W1,S3,L5,Minimum,1,3,5,number advanc variant minimum edit distanc play special role comput biolog call comput biolog align sequenc nucleotid sometim protein job take two string like produc align like biolog import number reason find region genom could discov function gene could look evolutionari thing compar differ speci also import assembl fragment dna sequenc go tri assembl fragment want look overlap piec well talk overlap piec find match find two piec match compar individu look mutat find place similar differ gener natur languag process talk distanc string edit distanc minimum edit distanc minim distanc comput weight thing comput biolog talk similar maxim similar ask similar two thing tri maxim someth gener talk score rather weight competit biolog standard minimum distanc algorithm look call needlemanwunsch ive shown algorithm thing saw although gener go keep well use mean cost insert delet well littl valu substitut posit neg valu substitut thing gener biolog well talk posit cost thing match posit valu thing match cost thing delet insert here needlemanwunsch matrix notic oppos natur languag process gener comput biolog put origin upper left sound let let first look variant import comput biolog one case possibl unlimit gap begin end string happen exactli two littl snip know end point one might overlap end anoth might someth els go place one long sequenc here anoth long sequenc piec sequenc piec might overlap dont wanna penal fact there thing go wed like modifi algorithm doesnt penal gap end fact variou differ kind overlap sort might happen sequenc overlap read might look piec gene insid anoth piec subset piec insid larger piec variant dynam program algorithm use overlap detect overlap detect variant well make small chang algorithm first chang initi doesnt cost us anyth start long string delet everyth insert everyth use star j star weve gotten rid allow start path random point intersect allow start zero cost penal match thing look edg overlap termin condit gonna look start upper right corner allow match go way edg well find place along final column final row maximum valu well trace back case maximum valu column well trace back similar extens needlemanwunsch standard dynam program algorithm string distanc local align problem here local align problem two string x length length n want find two sub string who similar maximum imagin x would like add two sting would like find two sub string c c c g g g that largest similar sub string similar overlap detect variant saw except allow ignor previous unalign sequenc begin end also anywher basic maximum align somewher middl sound sound order order modifi needlemanwunsch algorithm allow kind local align new version call smithwaterman algorithm first go allow overlap detect variant allow initi condit zero x dont penal initi string gonna make one modif cell look possibl place could come choos align gonna pick maximum three previou cell also go add maximum zero go let sens biolog talk maxim similar thing get differ get neg score go start zero allow throw away region dont align termin condit smithwaterman algorithm depend look want best local align well pick place that that maximum entir array well trace back want local align score greater threshold mayb well find place that greater find place trace back get complic fact overlap local align might two align like might actual overlap trace back complic want best local align that actual much easier here exampl local align let let imagin get one posit point everi time two symbol match neg point delet insert substitut let look local align two string c c fill matrix theyr gonna start zero everywher local align see two look region cell maximum distanc trace back see two cell one correspond align atcat attat four string match one mismatch that gonna distanc three one correspond align atc atc three match symbol advanc varianc edit distanc see comput biolog,[12  3  4 14 13]
132,Course2_W2-S1-L1_Introduction_to_N-grams_8-41,today gonna introduce topic language modeling one important topics natural language processing goal language modeling assign probability sentence would want assign probability sentence comes sorts applications machine translation example wed like able distinguish good bad translations probabilities high winds tonight might better translation large winds tonight high winds go together well spelling correction see phrase like fifteen minuets house thats likely mistake minutes one piece information lets us decide fifteen minutes much likely phrase fifteen minuets speech recognition phrase like saw van much likely phrase sounds phonetically similar eyes awe much less likely sequence words turns language modelings play role summarization question answering really everywhere goal language model compute probability sentence sequence words given sequence words w wn gonna compute probability p w well use capital w mean sequence w wn related task computing probability upcoming word p w given w w related task computing pw w w w w model computes either things either pw capital w meaning string joint probability whole string conditional probability last word given previous words either call language model might better call grammar mean technically telling us something good words fit together normally use word grammar turns word language model often well see acronym lm standard gonna go going compute joint probability want compute lets say probability phrase water transparent little part sentence intuition language modeling works youre going rely chain rule probability remind chain rule probability lets think definition conditional probability p given b equals p comma b p b rewrite p given b times p b equals p comma b turning around p comma b equals p given b ill make sure given times p b could generalize variables joint probability whole sequence b c probability times b given times c conditioned b times conditioned b c chain rule general form chain rule probability joint probability sequence variables first times condition second first times third conditioned first two last conditioned first n minus one alright chain rule chain rule applied compute joint probability words sentence lets suppose phrase water transparent chain rule probability sequence probability times probability water given times probability given water times probability given water finally times probability transparent given water formally probability joint probability sequence words product probability word times prefix word gonna estimate probabilities could count divide often compute probabilities counting dividing probability given water transparent could count many times water transparent occurs divide number times water transparent occurs could divide get probability cant reason cant theres far many possible sentences us ever estimate theres way could get enough data see counts possible sentences english instead apply simplifying assumption called markov assumption named andrei markov markov assumption suggest estimate probability given water transparent computing instead probability given word last meaning last word sequence maybe compute probability given water transparent given last two words given transparent thats markov assumption lets look previous maybe couple previous words rather entire context formally markov assumption says probability sequence words product word conditional probability word given prefix last words words chain rule product probabilities multiplying together estimate probability wᵢ given entire prefix one simpler compute probability wᵢ given last words simplest case markov model called unigram model unigram model simply estimate probability whole sequence words product probabilities individual words unigrams generated sentences randomly picking words see would look like word salad heres automatically generated sentences generated dan klein see word fifth word word doesnt look like sentence random sequence words thrift eighty said thats properties unigram model words independent model slightly intelligent bigram model condition single previous word estimate probability word given entire prefix beginning previous word previous word use generate random sentences bigram model sentences look little bit like english still somethings wrong clearly outside new car well new car looks pretty good car parking pretty good parking lot together outside new car parking lot agreement reached thats english even bigram model giving conditioning english simplifying ability model model whats going language extend ngram model trigrams thats grams grams grams general clear ngram modeling insufficient model language reason language longdistance dependencies want say predict computer put machine room fifth floor hadnt seen next word want say whats likelihood next word conditioned previous word floor id unlucky guess crashed really crashed main verb sentence computer subject head subject noun phrase know computer subject much likely guess crashed kind longdistance dependencies mean limit really good model predicting english words well take account lots longdistance information turns practice often get away ngram models local information especially get trigrams grams turn constraining enough cases itll solve problems us,Course2,W2-S1-L1,W2,S1,L1,Introduction,2,1,1,today gonna introduc topic languag model one import topic natur languag process goal languag model assign probabl sentenc would want assign probabl sentenc come sort applic machin translat exampl wed like abl distinguish good bad translat probabl high wind tonight might better translat larg wind tonight high wind go togeth well spell correct see phrase like fifteen minuet hous that like mistak minut one piec inform let us decid fifteen minut much like phrase fifteen minuet speech recognit phrase like saw van much like phrase sound phonet similar eye awe much less like sequenc word turn languag model play role summar question answer realli everywher goal languag model comput probabl sentenc sequenc word given sequenc word w wn gonna comput probabl p w well use capit w mean sequenc w wn relat task comput probabl upcom word p w given w w relat task comput pw w w w w model comput either thing either pw capit w mean string joint probabl whole string condit probabl last word given previou word either call languag model might better call grammar mean technic tell us someth good word fit togeth normal use word grammar turn word languag model often well see acronym lm standard gonna go go comput joint probabl want comput let say probabl phrase water transpar littl part sentenc intuit languag model work your go reli chain rule probabl remind chain rule probabl let think definit condit probabl p given b equal p comma b p b rewrit p given b time p b equal p comma b turn around p comma b equal p given b ill make sure given time p b could gener variabl joint probabl whole sequenc b c probabl time b given time c condit b time condit b c chain rule gener form chain rule probabl joint probabl sequenc variabl first time condit second first time third condit first two last condit first n minu one alright chain rule chain rule appli comput joint probabl word sentenc let suppos phrase water transpar chain rule probabl sequenc probabl time probabl water given time probabl given water time probabl given water final time probabl transpar given water formal probabl joint probabl sequenc word product probabl word time prefix word gonna estim probabl could count divid often comput probabl count divid probabl given water transpar could count mani time water transpar occur divid number time water transpar occur could divid get probabl cant reason cant there far mani possibl sentenc us ever estim there way could get enough data see count possibl sentenc english instead appli simplifi assumpt call markov assumpt name andrei markov markov assumpt suggest estim probabl given water transpar comput instead probabl given word last mean last word sequenc mayb comput probabl given water transpar given last two word given transpar that markov assumpt let look previou mayb coupl previou word rather entir context formal markov assumpt say probabl sequenc word product word condit probabl word given prefix last word word chain rule product probabl multipli togeth estim probabl wᵢ given entir prefix one simpler comput probabl wᵢ given last word simplest case markov model call unigram model unigram model simpli estim probabl whole sequenc word product probabl individu word unigram gener sentenc randomli pick word see would look like word salad here automat gener sentenc gener dan klein see word fifth word word doesnt look like sentenc random sequenc word thrift eighti said that properti unigram model word independ model slightli intellig bigram model condit singl previou word estim probabl word given entir prefix begin previou word previou word use gener random sentenc bigram model sentenc look littl bit like english still someth wrong clearli outsid new car well new car look pretti good car park pretti good park lot togeth outsid new car park lot agreement reach that english even bigram model give condit english simplifi abil model model what go languag extend ngram model trigram that gram gram gram gener clear ngram model insuffici model languag reason languag longdist depend want say predict comput put machin room fifth floor hadnt seen next word want say what likelihood next word condit previou word floor id unlucki guess crash realli crash main verb sentenc comput subject head subject noun phrase know comput subject much like guess crash kind longdist depend mean limit realli good model predict english word well take account lot longdist inform turn practic often get away ngram model local inform especi get trigram gram turn constrain enough case itll solv problem us,[ 4  1  8  0 14]
133,Course2_W2-S1-L2_Estimating_N-gram_Probabilities_9-38,estimate ngram probabilities lets look bigram probabilities maximum likelihood estimate bigram probability probability word given previous word estimate counting count many times word occur together divide many times word occurs like saying times saw word many times followed word well use notation count sometimes sometimes well simplification well refer c well use c count joint count word divided count word lets walk example equation probability word given word maximum like estimate well write ill write mle estimated maximum likelihood estimator count count lets look lets look lets make corpus heres simple corpus borrowed dr seuss three sentences one starts special token start sentence end special token end sentence theyre short sentences sam sam like green eggs ham lets compute language model probabilities small corpus first probability given start symbol thats computed count start symbol comma big count start symbol follows start symbol twice one two start symbol occurs three times one two three probability twothirds thats probability given start see common examples lots different probabilities example lets pick another one random probability sam given word many times sam occur occurs thats one denominator many times occur occurs twice thats gonna one two heres probability sam given lets look larger corpus order get realistic counts corpus using collected dialog system answered questions restaurants city berkeley california heres kind sentences corpus tell good cantonese restaurants close midprice thai food im looking tell chez panisse hear sentences lets compute ngrams based sentences first lets start raw bigram counts small corpus sentences im showing bigram count table word followed word five times word followed word want times word want followed word times word followed word eat times weve put sample word picked want eat chinese food lunch spend show words might occur together sentence word see lot words lot probabilities zero lot counts im sorry zero happened small data set want never followed want thats zero chinese never followed word okay order turn counts probabilities normalize unigram count remember probability word given word minus one count word word count word need divide joint counts two words count previous word heres unigram counts going need compute probabilities heres count heres count eat using equation compute bigram probability probability example given want likely given previous word want next word pretty likely example notice things counts zero still probabilities zero lots things zeros weve computed bigram capabilities estimate probability sentence thats goal language modeling simply multiplying together component probabilities probability want english food probability given start times probability want given times probability english given want food given english start end sentence given food kinds knowledge expressed bigram probabilities example probability english given want lower probability chinese given want well probably thats chinese food popular people gonna ask wanting chinese likely wanting english thats fact world fact cuisines much fact english probability given want high thats fact grammar thats fact verb want english requires infinitive want infinitives thats grammatical fact thats grammar well put grammar probability want given previous word spend zero zero seems caused grammatical fact spend want two verbs row kind verb doesnt seem grammatically possible english zero caused grammatical disallowing zero whats probability food following zero zero contingent zero could imagine sentence food id like stop think sentence like good happens sentence never occurred training data contingency row structural zero right lets move practice dont keep probabilities form probabilities fact keep form log probabilities two reasons one avoid underflow think long sentence youre multiplying together little tiny probabilities small number less zero multiply many small numbers get small number often ends arithmetic underflow computation want avoid kind underflow turns adding faster multiplying anyway instead multiplying four probabilities well general add four log probabilities going store language models logs number publicly available language modeling toolkits one srilm download srilm another publicly available resource google ngrams release five years google released trillion word corpus trillion five grams thirteen million unique words huge data set download use kind ngram applications youd like use heres example data google ngrams release fourgram counts fourgrams beginning serve words beginning big corpus see serve indication occurred times web corpus information google web corpus another publicly available corpus google book ngram corpus lets look corpus lets plot counts words google books number corpora available american english british english chinese french german various kinds corpora downloaded,Course2,W2-S1-L2,W2,S1,L2,Estimating,2,1,2,estim ngram probabl let look bigram probabl maximum likelihood estim bigram probabl probabl word given previou word estim count count mani time word occur togeth divid mani time word occur like say time saw word mani time follow word well use notat count sometim sometim well simplif well refer c well use c count joint count word divid count word let walk exampl equat probabl word given word maximum like estim well write ill write mle estim maximum likelihood estim count count let look let look let make corpu here simpl corpu borrow dr seuss three sentenc one start special token start sentenc end special token end sentenc theyr short sentenc sam sam like green egg ham let comput languag model probabl small corpu first probabl given start symbol that comput count start symbol comma big count start symbol follow start symbol twice one two start symbol occur three time one two three probabl twothird that probabl given start see common exampl lot differ probabl exampl let pick anoth one random probabl sam given word mani time sam occur occur that one denomin mani time occur occur twice that gonna one two here probabl sam given let look larger corpu order get realist count corpu use collect dialog system answer question restaur citi berkeley california here kind sentenc corpu tell good cantones restaur close midpric thai food im look tell chez paniss hear sentenc let comput ngram base sentenc first let start raw bigram count small corpu sentenc im show bigram count tabl word follow word five time word follow word want time word want follow word time word follow word eat time weve put sampl word pick want eat chines food lunch spend show word might occur togeth sentenc word see lot word lot probabl zero lot count im sorri zero happen small data set want never follow want that zero chines never follow word okay order turn count probabl normal unigram count rememb probabl word given word minu one count word word count word need divid joint count two word count previou word here unigram count go need comput probabl here count here count eat use equat comput bigram probabl probabl exampl given want like given previou word want next word pretti like exampl notic thing count zero still probabl zero lot thing zero weve comput bigram capabl estim probabl sentenc that goal languag model simpli multipli togeth compon probabl probabl want english food probabl given start time probabl want given time probabl english given want food given english start end sentenc given food kind knowledg express bigram probabl exampl probabl english given want lower probabl chines given want well probabl that chines food popular peopl gonna ask want chines like want english that fact world fact cuisin much fact english probabl given want high that fact grammar that fact verb want english requir infinit want infinit that grammat fact that grammar well put grammar probabl want given previou word spend zero zero seem caus grammat fact spend want two verb row kind verb doesnt seem grammat possibl english zero caus grammat disallow zero what probabl food follow zero zero conting zero could imagin sentenc food id like stop think sentenc like good happen sentenc never occur train data conting row structur zero right let move practic dont keep probabl form probabl fact keep form log probabl two reason one avoid underflow think long sentenc your multipli togeth littl tini probabl small number less zero multipli mani small number get small number often end arithmet underflow comput want avoid kind underflow turn ad faster multipli anyway instead multipli four probabl well gener add four log probabl go store languag model log number publicli avail languag model toolkit one srilm download srilm anoth publicli avail resourc googl ngram releas five year googl releas trillion word corpu trillion five gram thirteen million uniqu word huge data set download use kind ngram applic youd like use here exampl data googl ngram releas fourgram count fourgram begin serv word begin big corpu see serv indic occur time web corpu inform googl web corpu anoth publicli avail corpu googl book ngram corpu let look corpu let plot count word googl book number corpora avail american english british english chines french german variou kind corpora download,[ 4 14 13 12 11]
134,Course2_W2-S1-L3_Evaluation_and_Perplexity_11-09,every natural language processing tool evaluated language models evaluated well mean tore language model good language model general say good language model one better finding good sentences predicting liking bad sentences specifically sound want assign higher probability real perhaps frequently observed sentences ungrammatical impossible least rarely observed sentences thats goal evaluating language model train parameters language model training set test models performance data havent seen training data unseen data unseen data called test set want something thats training set totally unused weve never looked fair evaluation model well need evaluation metric tells well model unseen test set evaluation models best evaluation best way comparing two models two language models b put model task gonna build spelling corrector speech recognizer mt system whatever application uses language models well put language model well run task well get accuracy system running model system running model b perhaps thats many misspelled words corrected properly spelling correction many words translated correctly translation compare accuracy two models whichever model higher accuracy better language model called extrinsic evaluation using something external ngram model looking performance external task problem kind extrinsic evaluation also called invivo evaluation time consuming many cases could take days weeks modern machine translation system modern speech recognition system running evaluations often extremely slow instead sometimes use intrinsic evaluation something thats intrinsically language models particular application common intrinsic evaluation called perplexity perplexity happens bad approximation extrinsic evaluation unless turns test data looks lot like training data generally perplexity useful pilot experiments help think problem useful tool long also use extrinsic evaluation well lets think intuition perplexity like many ideas language modeling dates back claude shannon shannon proposed among many things game word prediction well predict next word example weve seen sentences like always order pizza cheese job predict next word first sentence might say well good language model might guess likely mushrooms likely pepperoni maybe less likely anchovies anchovies somewhat less popular mushrooms unlikely put fried rice pizza extremely unlikely lets say although people guess say word well model predicts actual words occur intui good model model sentence sentence like thirty third president us know next word likely jfk john kennedy word like predictable case saw anything could come next cases gonna much better predicting next word cases much worse good language model average better bad language model turns unigrams bad game think second youll realize summary better model text better language model one assigns higher probability assigns higher probability whatever word actually occurs guess right next word good language model best language model one best predicts unseen test set assigns average highest probability sentence sentences sees ive seen see test set assign give new test set assign probability sentences better language model one says oh knew sentence coming assigns high probability perplexity new metric going using probability test set normalized number words well take lets say test set long sentence n n words long well take n word sentence well take probability well take one inaudible well take inaudible rou inaudible route well take inverse way normalizing length probability take long sentence take probably whole sentence normalize number words obviously long sentences longer sentence less probable going want normalizing factor compare test sets different lengths thats enthru one perplexity string words w enthru one probability string words parentheses chain rule thats probability string words one n probability overall sorry im sorry product overall probability word given entire prefix hand weve chain rule replaced probability long sequence product probabilities word given prefix bigrams mark approximation chain rule say probability weve replaced probability sequence words product bunch bigrams perplexity string words inaudible route product n gram probabilities multiplied together inverted function probability sentence inversion minimizing perplexity maximizing probability another intuition perplexity also based shannon example comes josh goodman inaudible second intuition perplexity relies idea perplexity average related average branching factor perplexity point sentence average many things occur next well see later related probability upcoming things related entropy upcoming things roughly speaking ten possible word come next equal probability perplexity ten example im recognizing ten digits perplexity task ten theres ten possible things could come next cant decide repre recognize building speech recognizer switchboard phone service recognize names perplexity names theyre equally likely suppose system represent recognize lets say phone switchboard phone operator automatic phone operator recognize word operator occurs onefourth time word sales occurs quarter time word technical support occurs onefourth time one times another names occur take weighted average possibilities could occur compute average likely one word occur perplexity perplexity weighted equivalent branching factor lets examine new kind perplexity weighted equivalent branching factor show inverted normalize probability metric lets take sentence containing random digits whats perplexity sentence according model assign equal probability digit well see perplexity sentence string digits lets make lets make inaudible doesnt matter long bunch digits probability bunch digits well call digit one digit two digit n perplexity first metric negative one probability sequence negative one n since weve said words probability onetenth assuming unigram probability thats probability onetenth x onetenth x onetenth x onetenth thats prob thats onetenth n theres n words negative one n see thats equal ns canceled get onetenth minus one get ten thinking perplexity normalized probability long string sort see intuition average branching factor normalizing length sort asking many things occur time waited probability alright perplexity general lower perplexity better model example heres heres training set trained million words tested million words newspaper wall street journal unigram model perplexity bigram model much lower much accurate perplexity trigram model even lower per perplexity perplexity since modeling something like average branching factor average predictability lower get better predicting model actual data occurs,Course2,W2-S1-L3,W2,S1,L3,Evaluation,2,1,3,everi natur languag process tool evalu languag model evalu well mean tore languag model good languag model gener say good languag model one better find good sentenc predict like bad sentenc specif sound want assign higher probabl real perhap frequent observ sentenc ungrammat imposs least rare observ sentenc that goal evalu languag model train paramet languag model train set test model perform data havent seen train data unseen data unseen data call test set want someth that train set total unus weve never look fair evalu model well need evalu metric tell well model unseen test set evalu model best evalu best way compar two model two languag model b put model task gonna build spell corrector speech recogn mt system whatev applic use languag model well put languag model well run task well get accuraci system run model system run model b perhap that mani misspel word correct properli spell correct mani word translat correctli translat compar accuraci two model whichev model higher accuraci better languag model call extrins evalu use someth extern ngram model look perform extern task problem kind extrins evalu also call invivo evalu time consum mani case could take day week modern machin translat system modern speech recognit system run evalu often extrem slow instead sometim use intrins evalu someth that intrins languag model particular applic common intrins evalu call perplex perplex happen bad approxim extrins evalu unless turn test data look lot like train data gener perplex use pilot experi help think problem use tool long also use extrins evalu well let think intuit perplex like mani idea languag model date back claud shannon shannon propos among mani thing game word predict well predict next word exampl weve seen sentenc like alway order pizza chees job predict next word first sentenc might say well good languag model might guess like mushroom like pepperoni mayb less like anchovi anchovi somewhat less popular mushroom unlik put fri rice pizza extrem unlik let say although peopl guess say word well model predict actual word occur intui good model model sentenc sentenc like thirti third presid us know next word like jfk john kennedi word like predict case saw anyth could come next case gonna much better predict next word case much wors good languag model averag better bad languag model turn unigram bad game think second youll realiz summari better model text better languag model one assign higher probabl assign higher probabl whatev word actual occur guess right next word good languag model best languag model one best predict unseen test set assign averag highest probabl sentenc sentenc see ive seen see test set assign give new test set assign probabl sentenc better languag model one say oh knew sentenc come assign high probabl perplex new metric go use probabl test set normal number word well take let say test set long sentenc n n word long well take n word sentenc well take probabl well take one inaud well take inaud rou inaud rout well take invers way normal length probabl take long sentenc take probabl whole sentenc normal number word obvious long sentenc longer sentenc less probabl go want normal factor compar test set differ length that enthru one perplex string word w enthru one probabl string word parenthes chain rule that probabl string word one n probabl overal sorri im sorri product overal probabl word given entir prefix hand weve chain rule replac probabl long sequenc product probabl word given prefix bigram mark approxim chain rule say probabl weve replac probabl sequenc word product bunch bigram perplex string word inaud rout product n gram probabl multipli togeth invert function probabl sentenc invers minim perplex maxim probabl anoth intuit perplex also base shannon exampl come josh goodman inaud second intuit perplex reli idea perplex averag relat averag branch factor perplex point sentenc averag mani thing occur next well see later relat probabl upcom thing relat entropi upcom thing roughli speak ten possibl word come next equal probabl perplex ten exampl im recogn ten digit perplex task ten there ten possibl thing could come next cant decid repr recogn build speech recogn switchboard phone servic recogn name perplex name theyr equal like suppos system repres recogn let say phone switchboard phone oper automat phone oper recogn word oper occur onefourth time word sale occur quarter time word technic support occur onefourth time one time anoth name occur take weight averag possibl could occur comput averag like one word occur perplex perplex weight equival branch factor let examin new kind perplex weight equival branch factor show invert normal probabl metric let take sentenc contain random digit what perplex sentenc accord model assign equal probabl digit well see perplex sentenc string digit let make let make inaud doesnt matter long bunch digit probabl bunch digit well call digit one digit two digit n perplex first metric neg one probabl sequenc neg one n sinc weve said word probabl onetenth assum unigram probabl that probabl onetenth x onetenth x onetenth x onetenth that prob that onetenth n there n word neg one n see that equal ns cancel get onetenth minu one get ten think perplex normal probabl long string sort see intuit averag branch factor normal length sort ask mani thing occur time wait probabl alright perplex gener lower perplex better model exampl here here train set train million word test million word newspap wall street journal unigram model perplex bigram model much lower much accur perplex trigram model even lower per perplex perplex sinc model someth like averag branch factor averag predict lower get better predict model actual data occur,[ 4  8 14 13 12]
135,Course2_W2-S1-L4_Generalization_and_Zeros_5-15,saw earlier lots times probabilities counts bigrams trigrams would zero cases lets think starting whats called shannon visualization method shannon proposed visualize actual endgram youve built maximized likely estimation heres method choose random bigram according probability heres bigram start first word word according probability roll die pick whichever inaudible comes lets say picked likely first word pick star first biogram another random biogram starts word w generated whose next word chosen according probability pick want pick want go happen chose end sentence want eat eat chinese chinese food food inaudible go bring words together weve generated sentence shannon visualization method show us lot things inaudible weve built example heres grammar language model trained shakespeare generating random sentences heres unigram sentences every enter severally let hill late speaks good sentences bigrams stand forth thy canopy forsooth inaudible hit king henry live king follow oh better beginning sound like shakespeare one indeed duke good friend well sounds pretty good sweet prince falstaff shall die lets look quadro inaudible cannot tell sounds good shakespeare produced words vocabulary turns word produced words produced different bigram types different word unique pair words thats squared million possible bigram multiply percent possible bigrams never seen also gonna zero entries bigram table vast numbers thats bigrams quadrigrams even worse reason quadrigrams look like shakespeare actual shakespeare sentences following particular quadrigram really one possible word could occur small corpus shakespeare see look different corpus like wall street journal shakespeare example heres trigrams sentences wall street journal also point billion three percent rates interest stores mexico brazil market conditions eh sounds like wall street journal heres two corpora english rea know reasonable size corpora millions words least million words overlap shakespeare sentences wall street journal sentences whats lesson one lesson parallels overfitting ngrams work well word prediction test corpus looks like training corpus test shakespeare trained wall street journal youre going predict words well real life doesnt happen wed like train robust models better job generalizing wanna talk one kind generalization dealing zeros zeroes mean things never occurred training set occur test set lets look zeros imagine training set phrases like denied allegations denied reports denied claims denied request never saw denied offer probability based maximum likelihood estimation offer given denied zero go test set see sentence denied offer denied loan whats probability sequences denied offer denied loan gonna well probability gonna weve trained probabilities training set gonna bad job speech recognizer well never recognize phrase machine translator well refuse translate phrase gonna claim phrase good english big problem need solve bi grams zero probability value mean gonna assign zero probability test set never compute perplexity cant divide zero need gonna need find way dealing biograms zero,Course2,W2-S1-L4,W2,S1,L4,Generalization,2,1,4,saw earlier lot time probabl count bigram trigram would zero case let think start what call shannon visual method shannon propos visual actual endgram youv built maxim like estim here method choos random bigram accord probabl here bigram start first word word accord probabl roll die pick whichev inaud come let say pick like first word pick star first biogram anoth random biogram start word w gener whose next word chosen accord probabl pick want pick want go happen chose end sentenc want eat eat chines chines food food inaud go bring word togeth weve gener sentenc shannon visual method show us lot thing inaud weve built exampl here grammar languag model train shakespear gener random sentenc here unigram sentenc everi enter sever let hill late speak good sentenc bigram stand forth thi canopi forsooth inaud hit king henri live king follow oh better begin sound like shakespear one inde duke good friend well sound pretti good sweet princ falstaff shall die let look quadro inaud cannot tell sound good shakespear produc word vocabulari turn word produc word produc differ bigram type differ word uniqu pair word that squar million possibl bigram multipli percent possibl bigram never seen also gonna zero entri bigram tabl vast number that bigram quadrigram even wors reason quadrigram look like shakespear actual shakespear sentenc follow particular quadrigram realli one possibl word could occur small corpu shakespear see look differ corpu like wall street journal shakespear exampl here trigram sentenc wall street journal also point billion three percent rate interest store mexico brazil market condit eh sound like wall street journal here two corpora english rea know reason size corpora million word least million word overlap shakespear sentenc wall street journal sentenc what lesson one lesson parallel overfit ngram work well word predict test corpu look like train corpu test shakespear train wall street journal your go predict word well real life doesnt happen wed like train robust model better job gener wanna talk one kind gener deal zero zero mean thing never occur train set occur test set let look zero imagin train set phrase like deni alleg deni report deni claim deni request never saw deni offer probabl base maximum likelihood estim offer given deni zero go test set see sentenc deni offer deni loan what probabl sequenc deni offer deni loan gonna well probabl gonna weve train probabl train set gonna bad job speech recogn well never recogn phrase machin translat well refus translat phrase gonna claim phrase good english big problem need solv bi gram zero probabl valu mean gonna assign zero probabl test set never comput perplex cant divid zero need gonna need find way deal biogram zero,[ 4 14 13 12 11]
136,Course2_W2-S1-L5_Smoothing-_Add-One_6-30,deal bigrams zero probability simplest idea called addone smoothing lets look picture gives us intuition smoothing general dan klein suppose training data saw denied allegations denied reports denied claims denied request weve computed probabilities seven total things following denied get probabilities everything things would like say denied effort might occur denied outcome might occur wed like steal probability mass save things might see later training data maximum likelihood count things occurred inaudible never occurred wed like steal little little probability mask words put probability mask possible words set words zeros go away simplest way called add one estimation leplas smoothing idea simple pretend saw word one time actually add one counts maximum likelihood estimate count bigram divided count count unigram add one estimate count bigram plus one count unigram plus v add v denominator adding one every word follows word minus one denominator increased total count times something happened minus one wasnt previous things followed one got incremented one v add v denominator add one estimator probability estimator keep using term maximum likelihood estimate lets remind means maximum likelihood estimate parameter model training set one maximizes likelihood training set given model training set gonna maximum likelihood estimator lets us learn model training set one makes training set likely mean suppose word bagel occurs times corpus million words ask whats probability random word text bagels well maximum inaudible estimator corpus could bad estimate corpus knows corpus bagel occurs times per probability estimate one makes likely bagel occur times word corpus occur training corpus maximizing likelihood training data add one smoothing kind smoothing nonmaximum likelihood estimator changing counts occurred training data hope generalize better go back berkley restaurant project add one accounts heres la plaz smooth bigram count become everything else one added compute bigram probabilities counts using laplace add one smoothing equation saw earlier got laplace add one smooth bigrams probability two given one zeros turned iii also take probabilities reconstitute counts seen things number times would see get add one probabilities naturally take probabilities reestimate original counts numbers would given us probabilities ask reconstituted counts look like much add one smoothing changed probabilities heres reconstituted counts wa followed want times chinese followed food times reconstituted counts lets compare original counts top original counts reconstituted counts want notice theres huge change original count two followed want times smoothed counts two follows one times almost third sma third si th smaller three times smaller chinese food occurs times original counts reconstituted counts add one smoothing made massive changes accounts sometimes changing factor ten original counts order steal original probability mass give massive number zeros assigned probabilities words add one estimation blunt instrument makes big changes counts order get probability mast assign massive number practice dont actually use addone smoothing n grams better methods use addone smoothings kinds natural language processing models addone smoothing example used text classification similar kinds domain number isnt enormous,Course2,W2-S1-L5,W2,S1,L5,Smoothing-,2,1,5,deal bigram zero probabl simplest idea call addon smooth let look pictur give us intuit smooth gener dan klein suppos train data saw deni alleg deni report deni claim deni request weve comput probabl seven total thing follow deni get probabl everyth thing would like say deni effort might occur deni outcom might occur wed like steal probabl mass save thing might see later train data maximum likelihood count thing occur inaud never occur wed like steal littl littl probabl mask word put probabl mask possibl word set word zero go away simplest way call add one estim lepla smooth idea simpl pretend saw word one time actual add one count maximum likelihood estim count bigram divid count count unigram add one estim count bigram plu one count unigram plu v add v denomin ad one everi word follow word minu one denomin increas total count time someth happen minu one wasnt previou thing follow one got increment one v add v denomin add one estim probabl estim keep use term maximum likelihood estim let remind mean maximum likelihood estim paramet model train set one maxim likelihood train set given model train set gonna maximum likelihood estim let us learn model train set one make train set like mean suppos word bagel occur time corpu million word ask what probabl random word text bagel well maximum inaud estim corpu could bad estim corpu know corpu bagel occur time per probabl estim one make like bagel occur time word corpu occur train corpu maxim likelihood train data add one smooth kind smooth nonmaximum likelihood estim chang count occur train data hope gener better go back berkley restaur project add one account here la plaz smooth bigram count becom everyth els one ad comput bigram probabl count use laplac add one smooth equat saw earlier got laplac add one smooth bigram probabl two given one zero turn iii also take probabl reconstitut count seen thing number time would see get add one probabl natur take probabl reestim origin count number would given us probabl ask reconstitut count look like much add one smooth chang probabl here reconstitut count wa follow want time chines follow food time reconstitut count let compar origin count top origin count reconstitut count want notic there huge chang origin count two follow want time smooth count two follow one time almost third sma third si th smaller three time smaller chines food occur time origin count reconstitut count add one smooth made massiv chang account sometim chang factor ten origin count order steal origin probabl mass give massiv number zero assign probabl word add one estim blunt instrument make big chang count order get probabl mast assign massiv number practic dont actual use addon smooth n gram better method use addon smooth kind natur languag process model addon smooth exampl use text classif similar kind domain number isnt enorm,[ 4 14 13 12 11]
137,Course2_W2-S1-L6_Interpolation_10-25,description language code en dir ltr name english metadata note resourceuri apipartnersvideosxarksnczroxlanguagesensubtitles siteurl httpwwwamaraorgvideosxarksnczroxen subformat json subtitles end meta newparagraph true position start text lets talk cases need tobrinterpolate back end meta newparagraph false position start text one language model another one andbrwell also touch web today end meta newparagraph false position start text cases helps use lessbrcontext rather end meta newparagraph false position start text intuition suppose abrvery confident trigram youve seen end meta newparagraph false position start text trigram large number timesbryoure confident trigram end meta newparagraph false position start text good estimator well use thebrtrigram suppose saw end meta newparagraph false position start text well maybe dont really trust thatbrtrigram might want back end meta newparagraph false position start text use bigram instead maybe youbrhavent seen bigram either might end meta newparagraph false position start text back unigram idea ofbrback sometimes dont end meta newparagraph false position start text large count trustworthy evidencebrfor larger order enneagram might end meta newparagraph false position start text back smaller one related ideabris interpolation interpolation says well end meta newparagraph false position start text sometimes trigram may usefulbrand case mix trigrams end meta newparagraph false position start text bigrams unigrams well may getbrmore information unigrams end meta newparagraph false position start text bigrams times trigrams bebrmore useful interpolation suggests end meta newparagraph false position start text mix three timebrand get benefits end meta newparagraph false position start text turns practice thatbrinterpolation works better back end meta newparagraph false position start text time language modelingbrwell dealing interpolation end meta newparagraph false position start text two kinds interpolation simplebrlinear interpolation end meta newparagraph false position start text unigram bigram trigram andbrwe simply add together three end meta newparagraph false position start text weights lambda one lambda two andbrlambda three lambdas sum one end meta newparagraph false position start text make probability andbrand compute new probability end meta newparagraph false position start text well call p hat word given thebrprevious two words interpolating end meta newparagraph false position start text three language models dobrsomething slightly complicated end meta newparagraph false position start text condition lambdas contextbrso say still mix trigram end meta newparagraph false position start text bigram unigram lambdasbrare dependent previous two end meta newparagraph false position start text words train even richerbrand complex context conditioning end meta newparagraph false position start text deciding mix trigrams andbrour bigrams unigrams end meta newparagraph false position start text lambdas come normal way tobrset lambdas use held corpus end meta newparagraph false position start text weve talked abrtraining corpus heres training end meta newparagraph false position start text corpus test corpus held outbrcorpus yet another piece set end meta newparagraph false position start text set aside data use abrheld corpus sometimes use end meta newparagraph false position start text held corpus called dev set abrdevelopment set kinds held end meta newparagraph false position start text data use setbrmetaparameters check things end meta newparagraph false position start text use held corpus tobrset lambdas idea end meta newparagraph false position start text gonna choose lambdas maximize thebrlikelihood held data end meta newparagraph false position start text heres take trainingbrdata train enneagrams end meta newparagraph false position start text say lambdas would use tobrinterpolate enneagrams end meta newparagraph false position start text gives highest probability ofbrthis held data ask find end meta newparagraph false position start text set probabilities thebrlog probability actual words end meta newparagraph false position start text occur held data highestbrnow weve talked cases end meta newparagraph false position start text zeros havent seen bigrambrbefore replace zero end meta newparagraph false position start text count count thatsbrsmoothing actual end meta newparagraph false position start text word never seen beforebrnow sometimes doesnt happen end meta newparagraph false position start text tasks lets say menu based taskbrwhere thick set end meta newparagraph false position start text commands words ever bebrsaid vocabulary fixed end meta newparagraph false position start text whats called closed vocabulary taskbrbut lots times language modeling end meta newparagraph false position start text applied cases dont know anybrword could used could words end meta newparagraph false position start text weve never seen training set sobrwe call words oov end meta newparagraph false position start text vocabulary words one way dealingbrwith vocabulary words end meta newparagraph false position start text follows create special token calledbrunk way train unk end meta newparagraph false position start text probabilities create fixedbrlexicon take training data end meta newparagraph false position start text first decide hold fewbrwords rare words end meta newparagraph false position start text unimportant words take thosebrwords change words unk end meta newparagraph false position start text train probabilities unk likebra normal normal word end meta newparagraph false position start text corpus training corpus wordbrword word really low end meta newparagraph false position start text probability word word word word andbrwell take word well change end meta newparagraph false position start text unk train bigram wordbrword word unk word word word end meta newparagraph false position start text unk word atbrdecoding time see new word end meta newparagraph false position start text havent seen replace word withbrunk treat like get bigram end meta newparagraph false position start text probabilities trigrambrprobabilities unk word end meta newparagraph false position start text training set another important issue mbrgrams web scale end meta newparagraph false position start text large grams introduced googlebrm grams corpus earlier deal end meta newparagraph false position start text computing probabilities largebrspaces one answer pruning end meta newparagraph false position start text store ngrams largebrcount example high order end meta newparagraph false position start text n grams might want remove ofbrthose singletons things end meta newparagraph false position start text count one ziffs law theresbrgonna lot singleton counts end meta newparagraph false position start text also use kinds morebrsophisticated versions dont end meta newparagraph false position start text remove things counts actuallybruse compute inaudible perplexities end meta newparagraph false position start text test set remove counts arebrcontributing less probability end meta newparagraph false position start text particular held set thatsbrpruning number end meta newparagraph false position start text efficiency thing use efficientbrdata structures like tries use end meta newparagraph false position start text approximate language models verybrefficient guaranteed give end meta newparagraph false position start text exact probability webrhave efficient things like dont end meta newparagraph false position start text store actual strings storebrindexes use huffman coding end meta newparagraph false position start text often instead storing probabilitiesbras big byte floats might end meta newparagraph false position start text kind quantization justbrstore small number bits end meta newparagraph false position start text probabilities sound aboutbrsmoothing web scale enneagrams end meta newparagraph false position start text popular smoothing methods verybrlarge enneagrams algorithm called end meta newparagraph false position start text stupid back stupid back calledbrstupid simple end meta newparagraph false position start text works well large scale andbrthe fact shown work well end meta newparagraph false position start text complicated algorithm youbrhave large amounts data mean end meta newparagraph false position start text intuition stupid back wannabrcompute stupid back probability end meta newparagraph false position start text word given previous set words ibruse maximum likelihood estimator end meta newparagraph false position start text count words divided bybrthe count prefix count end meta newparagraph false position start text greater zero backbroff probability end meta newparagraph false position start text previous lower order n gram prefixbrwith constant weight end meta newparagraph false position start text trigram would say occurs usebrthe count trigram doesnt end meta newparagraph false position start text take bigram probability multiplybrit point four use end meta newparagraph false position start text get unigrams ibrdont anything use end meta newparagraph false position start text unigram use unigrambrprobability call instead end meta newparagraph false position start text p stupid back doesntbrproduce probabilities produce end meta newparagraph false position start text probabilities would actually tobruse various clever kinds waiting back end meta newparagraph false position start text algorithm discount thisbrprobability leave mass left end meta newparagraph false position start text use bigram probabilitiesbrotherwise gonna end numbers end meta newparagraph false position start text greater one wontbrhave probabilities stupid end meta newparagraph false position start text back produces something like scoresbror rather probabilities end meta newparagraph false position start text turns worksbrquite well summary smoothing end meta newparagraph false position start text far add one smoothing okay textbrcategorization recommended end meta newparagraph false position start text language modeling commonlybrused method well discuss advanced end meta newparagraph false position start text section week thebrinaudible nye algorithm extended end meta newparagraph false position start text interpolated inaudible nye algorithmbrbut large enneagrams like end meta newparagraph false position start text situations youre using webbrsimplistic algorithms like stupid back end meta newparagraph false position start text actually work quite well aboutbradvanced language modeling issues recent end meta newparagraph false position start text research focused things likebrdiscriminative models idea end meta newparagraph false position start text pick enneagram weights instead ofbrpicking fit training data end meta newparagraph false position start text whether maximum likelihood estimatebror smooth instead choose enneagram end meta newparagraph false position start text weights improve task wellbrpick whatever task machine end meta newparagraph false position start text translation speech recognition andbrchoose whatever enneagram weights make end meta newparagraph false position start text task likely another thing webrcan instead using anagrams end meta newparagraph false position start text use parsers well see usebrof parsers statistical parsers later end meta newparagraph false position start text course use cachingbrmodels caching model assume end meta newparagraph false position start text word thats used recently morebrlikely appear end meta newparagraph false position start text probability cache probability abrword given history mix end meta newparagraph false position start text probability word somebrfunction history like end meta newparagraph false position start text much often word occurred thebrhistory weight two end meta newparagraph false position start text probabilities together turns thatbrcache models dont work certain end meta newparagraph false position start text situations particularly theybrperform poorly speech recognition end meta newparagraph false position start text think might thatbra cache model performs poorly speech title interpolation versionno versionnumber video interpolation videodescription videotitle interpolation,Course2,W2-S1-L6,W2,S1,L6,Interpolation,2,1,6,descript languag code en dir ltr name english metadata note resourceuri apipartnersvideosxarksnczroxlanguagesensubtitl siteurl httpwwwamaraorgvideosxarksnczroxen subformat json subtitl end meta newparagraph true posit start text let talk case need tobrinterpol back end meta newparagraph fals posit start text one languag model anoth one andbrwel also touch web today end meta newparagraph fals posit start text case help use lessbrcontext rather end meta newparagraph fals posit start text intuit suppos abrveri confid trigram youv seen end meta newparagraph fals posit start text trigram larg number timesbryour confid trigram end meta newparagraph fals posit start text good estim well use thebrtrigram suppos saw end meta newparagraph fals posit start text well mayb dont realli trust thatbrtrigram might want back end meta newparagraph fals posit start text use bigram instead mayb youbrhav seen bigram either might end meta newparagraph fals posit start text back unigram idea ofbrback sometim dont end meta newparagraph fals posit start text larg count trustworthi evidencebrfor larger order enneagram might end meta newparagraph fals posit start text back smaller one relat ideabri interpol interpol say well end meta newparagraph fals posit start text sometim trigram may usefulbrand case mix trigram end meta newparagraph fals posit start text bigram unigram well may getbrmor inform unigram end meta newparagraph fals posit start text bigram time trigram bebrmor use interpol suggest end meta newparagraph fals posit start text mix three timebrand get benefit end meta newparagraph fals posit start text turn practic thatbrinterpol work better back end meta newparagraph fals posit start text time languag modelingbrwel deal interpol end meta newparagraph fals posit start text two kind interpol simplebrlinear interpol end meta newparagraph fals posit start text unigram bigram trigram andbrw simpli add togeth three end meta newparagraph fals posit start text weight lambda one lambda two andbrlambda three lambda sum one end meta newparagraph fals posit start text make probabl andbrand comput new probabl end meta newparagraph fals posit start text well call p hat word given thebrprevi two word interpol end meta newparagraph fals posit start text three languag model dobrsometh slightli complic end meta newparagraph fals posit start text condit lambda contextbrso say still mix trigram end meta newparagraph fals posit start text bigram unigram lambdasbrar depend previou two end meta newparagraph fals posit start text word train even richerbrand complex context condit end meta newparagraph fals posit start text decid mix trigram andbrour bigram unigram end meta newparagraph fals posit start text lambda come normal way tobrset lambda use held corpu end meta newparagraph fals posit start text weve talk abrtrain corpu here train end meta newparagraph fals posit start text corpu test corpu held outbrcorpu yet anoth piec set end meta newparagraph fals posit start text set asid data use abrheld corpu sometim use end meta newparagraph fals posit start text held corpu call dev set abrdevelop set kind held end meta newparagraph fals posit start text data use setbrmetaparamet check thing end meta newparagraph fals posit start text use held corpu tobrset lambda idea end meta newparagraph fals posit start text gonna choos lambda maxim thebrlikelihood held data end meta newparagraph fals posit start text here take trainingbrdata train enneagram end meta newparagraph fals posit start text say lambda would use tobrinterpol enneagram end meta newparagraph fals posit start text give highest probabl ofbrthi held data ask find end meta newparagraph fals posit start text set probabl thebrlog probabl actual word end meta newparagraph fals posit start text occur held data highestbrnow weve talk case end meta newparagraph fals posit start text zero havent seen bigrambrbefor replac zero end meta newparagraph fals posit start text count count thatsbrsmooth actual end meta newparagraph fals posit start text word never seen beforebrnow sometim doesnt happen end meta newparagraph fals posit start text task let say menu base taskbrwher thick set end meta newparagraph fals posit start text command word ever bebrsaid vocabulari fix end meta newparagraph fals posit start text what call close vocabulari taskbrbut lot time languag model end meta newparagraph fals posit start text appli case dont know anybrword could use could word end meta newparagraph fals posit start text weve never seen train set sobrw call word oov end meta newparagraph fals posit start text vocabulari word one way dealingbrwith vocabulari word end meta newparagraph fals posit start text follow creat special token calledbrunk way train unk end meta newparagraph fals posit start text probabl creat fixedbrlexicon take train data end meta newparagraph fals posit start text first decid hold fewbrword rare word end meta newparagraph fals posit start text unimport word take thosebrword chang word unk end meta newparagraph fals posit start text train probabl unk likebra normal normal word end meta newparagraph fals posit start text corpu train corpu wordbrword word realli low end meta newparagraph fals posit start text probabl word word word word andbrwel take word well chang end meta newparagraph fals posit start text unk train bigram wordbrword word unk word word word end meta newparagraph fals posit start text unk word atbrdecod time see new word end meta newparagraph fals posit start text havent seen replac word withbrunk treat like get bigram end meta newparagraph fals posit start text probabl trigrambrprob unk word end meta newparagraph fals posit start text train set anoth import issu mbrgram web scale end meta newparagraph fals posit start text larg gram introduc googlebrm gram corpu earlier deal end meta newparagraph fals posit start text comput probabl largebrspac one answer prune end meta newparagraph fals posit start text store ngram largebrcount exampl high order end meta newparagraph fals posit start text n gram might want remov ofbrthos singleton thing end meta newparagraph fals posit start text count one ziff law theresbrgonna lot singleton count end meta newparagraph fals posit start text also use kind morebrsophist version dont end meta newparagraph fals posit start text remov thing count actuallybrus comput inaud perplex end meta newparagraph fals posit start text test set remov count arebrcontribut less probabl end meta newparagraph fals posit start text particular held set thatsbrprun number end meta newparagraph fals posit start text effici thing use efficientbrdata structur like tri use end meta newparagraph fals posit start text approxim languag model verybreffici guarante give end meta newparagraph fals posit start text exact probabl webrhav effici thing like dont end meta newparagraph fals posit start text store actual string storebrindex use huffman code end meta newparagraph fals posit start text often instead store probabilitiesbra big byte float might end meta newparagraph fals posit start text kind quantiz justbrstor small number bit end meta newparagraph fals posit start text probabl sound aboutbrsmooth web scale enneagram end meta newparagraph fals posit start text popular smooth method verybrlarg enneagram algorithm call end meta newparagraph fals posit start text stupid back stupid back calledbrstupid simpl end meta newparagraph fals posit start text work well larg scale andbrth fact shown work well end meta newparagraph fals posit start text complic algorithm youbrhav larg amount data mean end meta newparagraph fals posit start text intuit stupid back wannabrcomput stupid back probabl end meta newparagraph fals posit start text word given previou set word ibrus maximum likelihood estim end meta newparagraph fals posit start text count word divid bybrth count prefix count end meta newparagraph fals posit start text greater zero backbroff probabl end meta newparagraph fals posit start text previou lower order n gram prefixbrwith constant weight end meta newparagraph fals posit start text trigram would say occur usebrth count trigram doesnt end meta newparagraph fals posit start text take bigram probabl multiplybrit point four use end meta newparagraph fals posit start text get unigram ibrdont anyth use end meta newparagraph fals posit start text unigram use unigrambrprob call instead end meta newparagraph fals posit start text p stupid back doesntbrproduc probabl produc end meta newparagraph fals posit start text probabl would actual tobrus variou clever kind wait back end meta newparagraph fals posit start text algorithm discount thisbrprob leav mass left end meta newparagraph fals posit start text use bigram probabilitiesbrotherwis gonna end number end meta newparagraph fals posit start text greater one wontbrhav probabl stupid end meta newparagraph fals posit start text back produc someth like scoresbror rather probabl end meta newparagraph fals posit start text turn worksbrquit well summari smooth end meta newparagraph fals posit start text far add one smooth okay textbrcategor recommend end meta newparagraph fals posit start text languag model commonlybrus method well discuss advanc end meta newparagraph fals posit start text section week thebrinaud nye algorithm extend end meta newparagraph fals posit start text interpol inaud nye algorithmbrbut larg enneagram like end meta newparagraph fals posit start text situat your use webbrsimplist algorithm like stupid back end meta newparagraph fals posit start text actual work quit well aboutbradvanc languag model issu recent end meta newparagraph fals posit start text research focus thing likebrdiscrimin model idea end meta newparagraph fals posit start text pick enneagram weight instead ofbrpick fit train data end meta newparagraph fals posit start text whether maximum likelihood estimatebror smooth instead choos enneagram end meta newparagraph fals posit start text weight improv task wellbrpick whatev task machin end meta newparagraph fals posit start text translat speech recognit andbrchoos whatev enneagram weight make end meta newparagraph fals posit start text task like anoth thing webrcan instead use anagram end meta newparagraph fals posit start text use parser well see usebrof parser statist parser later end meta newparagraph fals posit start text cours use cachingbrmodel cach model assum end meta newparagraph fals posit start text word that use recent morebrlik appear end meta newparagraph fals posit start text probabl cach probabl abrword given histori mix end meta newparagraph fals posit start text probabl word somebrfunct histori like end meta newparagraph fals posit start text much often word occur thebrhistori weight two end meta newparagraph fals posit start text probabl togeth turn thatbrcach model dont work certain end meta newparagraph fals posit start text situat particularli theybrperform poorli speech recognit end meta newparagraph fals posit start text think might thatbra cach model perform poorli speech titl interpol versionno versionnumb video interpol videodescript videotitl interpol,[ 6  4 14 13 12]
138,Course2_W2-S1-L7_Good-Turing_Smoothing_15-35,ready talk advanced methods smoothing remember add one smoothing earlier add one smoothing add one numerator v denominator saw generalization k smoothing added k numerator kv denominator modify slightly create new version simply replace introduce new term new variable mkv new way writing add k smoothing going helpful way writing reason lets see next slide write write way adding every gram adding constant thats related one vocabulary size instead could add constant related uni gram probability word backing uni gram prior smoothing algorithm extension add k says instead using one v add every adding function one v every bigram count lets add something unigram probability really unigram prior kind interpolation variant interpolation adding count function unigram probability bigram count nonetheless although unigram prior smoothing works well still doesnt work well enough used language modeling instead intuition used many smoothing algorithms goodturing smoothing kneserney smoothing witten bell smoothing use count things weve seen estimate count things weve never seen goal smoothing algorithm replace unseen zeroes something else algorithms say look things youve seen things saw like things havent seen yet youre gonna seem test set see intuition works gonna introduce notation gonna notation gonna introduce big n sub c mean frequency frequency c meaning many things occurred frequency c big bin things occurred frequency c thats hard intuitive lets look intuitions lets take little sentence sam sam eat lets look unigram count occuring three times sam occuring twice eat occuring time n sub one many things occur one time well three three different word types occur one time n sub one three howbout many things occur two times well theres two n sub two two howbout things occur three times well one happens n sum three one alright intuition think frequencies frequencies lets apply get intuition goodturing smoothing imagine youre fishing scenario invented josh goodman youve caught ten carp three perch two whitefish one trout one salmon one eel dont know kind river stream ocean could nonetheless youve caught eighteen fish want estimate likely next species trout like words maybe word thats occurred ten times three two one want know likely occur well theres eighteen fish trouts occurred one time eighteen probability ought one eighteen lets ask likely next species new species catfish bass species havent seen something occurred zero times well goodturing intuition says lets use estimate things saw estimate new things weve never seen whats estimate things estimate things drawn n sub one many things occurred well whats n sub one n sub one three eighteen things saw three new things occurred one time lets use three eighteen estimate things weve never seen going use estimate things count things weve seen estimate things weve never seen going reserve probability mass unseen things well use three eighteen estimate unseen things could possibly see likely next species trout well already asked question said one eighteen cant true anymore must less one eighteen weve used probability mass original eighteen fish weve saved new fish weve never seen weve removed probability mass discount probabilities fish downward little bit gonna estimate discount factor much reduce counts heres equation goodturing heres answer question goodturing tells us probability things weve never seen p star things zero frequency exactly used previous slide n sub one count things occurred frequency one n thats saw three eighteen number well things didnt occur zero frequency use second part goodturing equation says new count c star goodturing count going n subc plus one divided n subc times c plus one lets work well well well give intuition second lets work work example first unseen fish lets say bass catfish havent seen training set maximum likelihood probability estimated probability zero didnt see training set zero words zero eighteen zero smoothed gonna use new good touring probability says n n n three saw three things previous slide eighteen things new probability going something weve seen like trout gonna reestimate trout well maximum likelihood estimate tells us count one maximum likelihood probability one eighteen new good touring formula says count trout c c one c two times n sub two n sub one thats going two x onethird n sub two one previous slide n sub one three two x onethird twothirds goodturing probability takes c star trout divides eighteen things weve seen two thirds slash eighteen instead count things saw one eighteen weve dropped two thirds twothirds eighteen weve discounted probability twothirds eighteenth weve used extra discounted probability mass account zero things weve never seen sound lets look nice intuition goodturing development herman ney colleagues imagine training set size c would training set words word another word heres another word heres another word lets going hold iteratively words training set lets first take one word first word blue word well write well think training set without word thats got c minus one words one held word blue word lets thing different word lets take lets say second word still c minus one include guy c minus one words left training one word held set well c times time well pull one word pulled words one one weve created held set thats size c word created training set missing word training set size c minus word imagine words corresponding training sets look picture developed dan klein think intuition weve turned held training sets side still length c ive written vertically lets think intuition ive got c training sets one size c one held set size one lets try answer question fraction held words unseen training well words n subzero words unseen training word thats unseen training occurred one time original training set removed took held data word occurred training n sub one occurred training take training set leaving c minus one words word occurs zero times training set new training set without word word held words n subzero n subzero words words n sub one original training set removed wanna know many words unseen training words occurred one time original training set n one c well correspondingly want know let clear fraction words seen k times training lets pick k perhaps two pick n sub two number things occur two times held set number things occurred three times original training removed one one copy words occur twice need think wanna know many words occurred k times training estimate really words occur k times original training set gonna wanna gonna wanna multiply number words occur words occurs k plus one times k plus one occurrences n sub k plus one bin n sub k plus one words well express fraction total words c remember total words c thats fraction held words seen k times training means future expect k times n sub k c words training count k since therere n sub k words training count k wanna fraction probability wanna distribute n sub k words n sub k words gonna occur probability k plus one times n sub k c n sub k distributing words means expected count would multiplied back c turn fraction back count expected count words occur training count k k sub star k plus one times ratio n sub k plus one n sub k one thing talked always compute count n sub k n sub k plus one words fact largest set k plus largest number lets say word fact word occured frequently corpus dont frequent word estimate large k goodturing estimator doesnt work well lots words may never occured lets say times even times going gaps well zeros maybe word word theres missing word theres missing words cant always using n word estimation simple replacement fact algorithm called simple goodturing counts get unreliable first know first counts replace estimator kind best fit power law dont actually use goodturing higher order numbers use lower bins lets look resulting goodturing numbers one example heres numbers church gale experiment used million words ap newswire heres remind heres goodturing equation count c star c times nc nc heres original count c heres replaced goodturing estimator little extra probability mass n sub one heres turned things occurred count two occur count things count three occur count counts discounted counts discounted lower number leave room things zero count last thing im gonna leave asking whats relationship counts original counts c counts c star notice general relationship,Course2,W2-S1-L7,W2,S1,L7,Good-Turing,2,1,7,readi talk advanc method smooth rememb add one smooth earlier add one smooth add one numer v denomin saw gener k smooth ad k numer kv denomin modifi slightli creat new version simpli replac introduc new term new variabl mkv new way write add k smooth go help way write reason let see next slide write write way ad everi gram ad constant that relat one vocabulari size instead could add constant relat uni gram probabl word back uni gram prior smooth algorithm extens add k say instead use one v add everi ad function one v everi bigram count let add someth unigram probabl realli unigram prior kind interpol variant interpol ad count function unigram probabl bigram count nonetheless although unigram prior smooth work well still doesnt work well enough use languag model instead intuit use mani smooth algorithm goodtur smooth kneserney smooth witten bell smooth use count thing weve seen estim count thing weve never seen goal smooth algorithm replac unseen zero someth els algorithm say look thing youv seen thing saw like thing havent seen yet your gonna seem test set see intuit work gonna introduc notat gonna notat gonna introduc big n sub c mean frequenc frequenc c mean mani thing occur frequenc c big bin thing occur frequenc c that hard intuit let look intuit let take littl sentenc sam sam eat let look unigram count occur three time sam occur twice eat occur time n sub one mani thing occur one time well three three differ word type occur one time n sub one three howbout mani thing occur two time well there two n sub two two howbout thing occur three time well one happen n sum three one alright intuit think frequenc frequenc let appli get intuit goodtur smooth imagin your fish scenario invent josh goodman youv caught ten carp three perch two whitefish one trout one salmon one eel dont know kind river stream ocean could nonetheless youv caught eighteen fish want estim like next speci trout like word mayb word that occur ten time three two one want know like occur well there eighteen fish trout occur one time eighteen probabl ought one eighteen let ask like next speci new speci catfish bass speci havent seen someth occur zero time well goodtur intuit say let use estim thing saw estim new thing weve never seen what estim thing estim thing drawn n sub one mani thing occur well what n sub one n sub one three eighteen thing saw three new thing occur one time let use three eighteen estim thing weve never seen go use estim thing count thing weve seen estim thing weve never seen go reserv probabl mass unseen thing well use three eighteen estim unseen thing could possibl see like next speci trout well alreadi ask question said one eighteen cant true anymor must less one eighteen weve use probabl mass origin eighteen fish weve save new fish weve never seen weve remov probabl mass discount probabl fish downward littl bit gonna estim discount factor much reduc count here equat goodtur here answer question goodtur tell us probabl thing weve never seen p star thing zero frequenc exactli use previou slide n sub one count thing occur frequenc one n that saw three eighteen number well thing didnt occur zero frequenc use second part goodtur equat say new count c star goodtur count go n subc plu one divid n subc time c plu one let work well well well give intuit second let work work exampl first unseen fish let say bass catfish havent seen train set maximum likelihood probabl estim probabl zero didnt see train set zero word zero eighteen zero smooth gonna use new good tour probabl say n n n three saw three thing previou slide eighteen thing new probabl go someth weve seen like trout gonna reestim trout well maximum likelihood estim tell us count one maximum likelihood probabl one eighteen new good tour formula say count trout c c one c two time n sub two n sub one that go two x onethird n sub two one previou slide n sub one three two x onethird twothird goodtur probabl take c star trout divid eighteen thing weve seen two third slash eighteen instead count thing saw one eighteen weve drop two third twothird eighteen weve discount probabl twothird eighteenth weve use extra discount probabl mass account zero thing weve never seen sound let look nice intuit goodtur develop herman ney colleagu imagin train set size c would train set word word anoth word here anoth word here anoth word let go hold iter word train set let first take one word first word blue word well write well think train set without word that got c minu one word one held word blue word let thing differ word let take let say second word still c minu one includ guy c minu one word left train one word held set well c time time well pull one word pull word one one weve creat held set that size c word creat train set miss word train set size c minu word imagin word correspond train set look pictur develop dan klein think intuit weve turn held train set side still length c ive written vertic let think intuit ive got c train set one size c one held set size one let tri answer question fraction held word unseen train well word n subzero word unseen train word that unseen train occur one time origin train set remov took held data word occur train n sub one occur train take train set leav c minu one word word occur zero time train set new train set without word word held word n subzero n subzero word word n sub one origin train set remov wanna know mani word unseen train word occur one time origin train set n one c well correspondingli want know let clear fraction word seen k time train let pick k perhap two pick n sub two number thing occur two time held set number thing occur three time origin train remov one one copi word occur twice need think wanna know mani word occur k time train estim realli word occur k time origin train set gonna wanna gonna wanna multipli number word occur word occur k plu one time k plu one occurr n sub k plu one bin n sub k plu one word well express fraction total word c rememb total word c that fraction held word seen k time train mean futur expect k time n sub k c word train count k sinc therer n sub k word train count k wanna fraction probabl wanna distribut n sub k word n sub k word gonna occur probabl k plu one time n sub k c n sub k distribut word mean expect count would multipli back c turn fraction back count expect count word occur train count k k sub star k plu one time ratio n sub k plu one n sub k one thing talk alway comput count n sub k n sub k plu one word fact largest set k plu largest number let say word fact word occur frequent corpu dont frequent word estim larg k goodtur estim doesnt work well lot word may never occur let say time even time go gap well zero mayb word word there miss word there miss word cant alway use n word estim simpl replac fact algorithm call simpl goodtur count get unreli first know first count replac estim kind best fit power law dont actual use goodtur higher order number use lower bin let look result goodtur number one exampl here number church gale experi use million word ap newswir here remind here goodtur equat count c star c time nc nc here origin count c here replac goodtur estim littl extra probabl mass n sub one here turn thing occur count two occur count thing count three occur count count discount count discount lower number leav room thing zero count last thing im gonna leav ask what relationship count origin count c count c star notic gener relationship,[ 4 14 13 12 11]
139,Course2_W2-S1-L8_Kneser-Ney_Smoothing_8-59,let’s talk kneserney smoothing one sophisticated forms smoothing also one beautiful elegant intuition remember goodturing talked ci’si discounted counts end goodturing discounted counts — count one discounted point four count two discounted — order save mass replace zero counts low number look actual values counts — nine eight — you’ll notice large number cases discounted count close relationship original count it’s really original count minus somewhat close practice goodturing often produce fixed small discount counts intuition fixed small discount applied directly call absolute discounting absolute discounting popular kind smoothing we’re showing absolute discounting interpolation intuition we’ll save time compute complicated goodturing numbers we’ll subtract maybe different discount value different corpora here’s equation absolute discounting we’re diagrams probability absolute discounted word given previous word discounted bigram interpolated interpolation weight unigram probability unigram probability pw bigram probability subtract fixed amount let’s say it’s count otherwise compute bigram probability normal way discounted bigram probability mixed weight shall talk later set weight unigram maybe might keep couple extra values counts one two counts one two saw previous slide weren’t quite subtracting model carefully separate counts problem absolute discounting unigram probability want talk changing unigram probability that’s fundamental intuition kneserney kneserney smoothing idea keep interpolation saw absolute discounting use better estimate probabilities lower unigrams intuition go back look classic shannon games remember shannon game we’re predicting word previous words see sentence “i can’t see without reading uuuu”uuuu what’s likely next word well “glasses” seems pretty likely well instead word “francisco” well seems unlikely situation yet “francisco” unigram common “glasses” reason “francisco” seems like bad thing “reading” one intuition might able get “francisco” always follows “san” often follows “san” “francisco” frequent it’s frequent context word “san” unigrams interpolation model we’re mixing unigram bigram specifically useful — they’re helpful — case haven’t seen bigram it’s unfortunate case haven’t seen bigram “reading francisco” we’re trusting “francisco”’s unigram weight shouldn’t trust instead using probability w likely word intuition going we’re backing something instead use continuation probability we’re going call p continuation word likely word appear novel continuation well measure novel continuation well word we’ll count number bigram types completes many different bigrams create appearing another word words bigram type novel continuation first time see new bigram words continuation probability going proportional cardinality set number words preceding words minus one occur word many words occur word bigram many preceding words cardinality set that’s number would like continuation probability proportional many times w appear novel continuation need turn probability divide total number word bigram types word bigrams occur zero times what’s cardinality set many different word bigram types we’re going divide two get probability continuation number word bigram types many w novel continuation turns there’s alternative metaphor kneserney equations see numerator total number word types precede w many word types w follow we’re going normalize number words could precede words sum words number word types precede word two number denominator denominator saw previous slide number possible bigram types number word type precede words summed words think second you’ll realize that’s true words kind kneserney model frequent word like “francisco” occurs one context like “san” low continuation probability put together intuition absolute discounting kneserney probability lower order ngram kneserney smoothing algorithm bigram absolute discounting — take bigram count subtract discount i’ve shown take max zero obviously discount happens higher probability don’t want negative probability we’re gonna interpolate continuation probability saw p continuation w sub lambda let’s talk compute lambda lambda gonna take probability mass normalized discounts took higherorder probabilities use weight much probability assign unigram we’re gonna combine lambda amount discount weight divided denominator it’s normalized discount we’re gonna multiply total number word types follow context wui minus oneu words many different word types discount many times apply normalized discount multiply together know much probably mass total assign continuation word bigram formulation kneserney slide we’re showing general recursive formulation ngrams general make slight change deal higherorder ngrams we’re showing kneserney probability word given prefix word like kneserney saw we’re interpolating higherorder ngram discounted lambda weight lowerorder probability need distinguish first toplevel time use count lower order counts we’re going use actual count highest order bigram we’re going use continuation value defined earlier lower order probabilities we’ll define new thing count kneserney dot mean actual count actual count let’s say we’re trigrams trigram recurse kneserney probability lower order things get bigrams unigrams we’ll using continuation count that’s single word context defined earlier kneserney smoothing excellent algorithm it’s commonly used speech recognition machine translation yet beautiful elegant intuition hope appreciate,Course2,W2-S1-L8,W2,S1,L8,Kneser-Ney,2,1,8,let’ talk kneserney smooth one sophist form smooth also one beauti eleg intuit rememb goodtur talk ci’si discount count end goodtur discount count — count one discount point four count two discount — order save mass replac zero count low number look actual valu count — nine eight — you’ll notic larg number case discount count close relationship origin count it’ realli origin count minu somewhat close practic goodtur often produc fix small discount count intuit fix small discount appli directli call absolut discount absolut discount popular kind smooth we’r show absolut discount interpol intuit we’ll save time comput complic goodtur number we’ll subtract mayb differ discount valu differ corpora here’ equat absolut discount we’r diagram probabl absolut discount word given previou word discount bigram interpol interpol weight unigram probabl unigram probabl pw bigram probabl subtract fix amount let’ say it’ count otherwis comput bigram probabl normal way discount bigram probabl mix weight shall talk later set weight unigram mayb might keep coupl extra valu count one two count one two saw previou slide weren’t quit subtract model care separ count problem absolut discount unigram probabl want talk chang unigram probabl that’ fundament intuit kneserney kneserney smooth idea keep interpol saw absolut discount use better estim probabl lower unigram intuit go back look classic shannon game rememb shannon game we’r predict word previou word see sentenc “i can’t see without read uuuu”uuuu what’ like next word well “glasses” seem pretti like well instead word “francisco” well seem unlik situat yet “francisco” unigram common “glasses” reason “francisco” seem like bad thing “reading” one intuit might abl get “francisco” alway follow “san” often follow “san” “francisco” frequent it’ frequent context word “san” unigram interpol model we’r mix unigram bigram specif use — they’r help — case haven’t seen bigram it’ unfortun case haven’t seen bigram “read francisco” we’r trust “francisco”’ unigram weight shouldn’t trust instead use probabl w like word intuit go we’r back someth instead use continu probabl we’r go call p continu word like word appear novel continu well measur novel continu well word we’ll count number bigram type complet mani differ bigram creat appear anoth word word bigram type novel continu first time see new bigram word continu probabl go proport cardin set number word preced word minu one occur word mani word occur word bigram mani preced word cardin set that’ number would like continu probabl proport mani time w appear novel continu need turn probabl divid total number word bigram type word bigram occur zero time what’ cardin set mani differ word bigram type we’r go divid two get probabl continu number word bigram type mani w novel continu turn there’ altern metaphor kneserney equat see numer total number word type preced w mani word type w follow we’r go normal number word could preced word sum word number word type preced word two number denomin denomin saw previou slide number possibl bigram type number word type preced word sum word think second you’ll realiz that’ true word kind kneserney model frequent word like “francisco” occur one context like “san” low continu probabl put togeth intuit absolut discount kneserney probabl lower order ngram kneserney smooth algorithm bigram absolut discount — take bigram count subtract discount i’v shown take max zero obvious discount happen higher probabl don’t want neg probabl we’r gonna interpol continu probabl saw p continu w sub lambda let’ talk comput lambda lambda gonna take probabl mass normal discount took higherord probabl use weight much probabl assign unigram we’r gonna combin lambda amount discount weight divid denomin it’ normal discount we’r gonna multipli total number word type follow context wui minu oneu word mani differ word type discount mani time appli normal discount multipli togeth know much probabl mass total assign continu word bigram formul kneserney slide we’r show gener recurs formul ngram gener make slight chang deal higherord ngram we’r show kneserney probabl word given prefix word like kneserney saw we’r interpol higherord ngram discount lambda weight lowerord probabl need distinguish first toplevel time use count lower order count we’r go use actual count highest order bigram we’r go use continu valu defin earlier lower order probabl we’ll defin new thing count kneserney dot mean actual count actual count let’ say we’r trigram trigram recurs kneserney probabl lower order thing get bigram unigram we’ll use continu count that’ singl word context defin earlier kneserney smooth excel algorithm it’ commonli use speech recognit machin translat yet beauti eleg intuit hope appreci,[ 4 14 13 12 11]
140,Course2_W2-S2-L1_The_Spelling_Correction_Task_5-39,description language code en dir ltr name english metadata note resourceuri apipartnersvideosvbacifslgnqwlanguagesensubtitles siteurl httpwwwamaraorgvideosvbacifslgnqwen subformat json subtitles end meta newparagraph true position start text today gonna talk spellingbrcorrection lots applications make use end meta newparagraph false position start text spelling correction example wordbrprocessing almost modern word end meta newparagraph false position start text processor take misspelled word likebrcomponent give end meta newparagraph false position start text suggestions like component e andbrautomatically replace modern end meta newparagraph false position start text search engines flag anbrerror language spelled without u end meta newparagraph false position start text give results ifbryou spelled word correctly end meta newparagraph false position start text modern phones additionally willbrautomatically find misspelled words end meta newparagraph false position start text typed layr replaced itbrautomatically suggests replacement end meta newparagraph false position start text late distinguish number ofbrseparate tasks spelling correction end meta newparagraph false position start text one detection error itselfbrand correction error end meta newparagraph false position start text youve found think aboutbrdifferent kinds correction might end meta newparagraph false position start text automatically correct error werebrpositive error know end meta newparagraph false position start text right answer error hte abrvery common misspelling end meta newparagraph false position start text many word processors automatically correctbrhte might suggest single end meta newparagraph false position start text correction theres one verybrlikely correction might suggest end meta newparagraph false position start text whole list corrections let userbrpick among distinguish two end meta newparagraph false position start text different classes spelling errors nonbrword errors errors end meta newparagraph false position start text user types word englishbrso graffe misspelling lets say end meta newparagraph false position start text giraffe word english bybrcontrast real word errors errors end meta newparagraph false position start text resulting soundbrmisspelling actually word english end meta newparagraph false position start text makes somewhat harder tobrdetect break real word end meta newparagraph false position start text errors ones produced reallybrtypographical processes meant end meta newparagraph false position start text type three typed inaudible letsbrsay cognitive errors user end meta newparagraph false position start text meant type word like inaudible andbrinstead typed homophone end meta newparagraph false position start text word uctooud instead ofbrinaudible cases whats end meta newparagraph false position start text produced real word english bybrmodeling differences end meta newparagraph false position start text kind errors might come withbrbetter ways fixing end meta newparagraph false position start text common spelling errors depends lotbron task web queries spelling end meta newparagraph false position start text errors extremely common sobrpractically one four words web end meta newparagraph false position start text query likely misspelled inbrweb processing tasks phones much end meta newparagraph false position start text harder get accurate number sobrtheres number studies end meta newparagraph false position start text studies done retyping youbrgive user passage type end meta newparagraph false position start text measure well type itbrand course thats quite end meta newparagraph false position start text users naturally writing messages orbrtyping nonetheless ask users end meta newparagraph false position start text retype dont let use thebrbackspace key make thirteen end meta newparagraph false position start text percent words thirteen percent ofbrthe words error indicating end meta newparagraph false position start text lot words correctbrthemselves backspace let end meta newparagraph false position start text correct trying tobrexperiment p style phone end meta newparagraph false position start text site organizer theyll correct aboutbrseven percent words end meta newparagraph false position start text theyll still leave two percent ofbrthe words uncorrected organizer end meta newparagraph false position start text similar numbers people doingbrretyping regular keyboard end meta newparagraph false position start text numbers two percent peoplebrtyping probably much higher number end meta newparagraph false position start text web queries probably much higherbrnumber people texting kind end meta newparagraph false position start text spelling spelling error inaudible thatbrwe see detect non word spelling end meta newparagraph false position start text errors traditional way usebra large dictionary word end meta newparagraph false position start text dictionary error largerbrthe dictionary turns better end meta newparagraph false position start text works correcting nonwordbrspelling errors generate set end meta newparagraph false position start text candidates thats real words arebrsimilar error pick end meta newparagraph false position start text whichever one best well talkbrabout noisychannel probability model end meta newparagraph false position start text also relatedbrto another method called shortest end meta newparagraph false position start text weighted inaudible distance myth webrfind words end meta newparagraph false position start text dictionary one generate abrset candidates going end meta newparagraph false position start text real words similar well talkbrabout similar means error end meta newparagraph false position start text well pick best one realbrword spelling errors algorithm end meta newparagraph false position start text quite similar word webrgenerate candidate set end meta newparagraph false position start text every word sentence notbrjust words end meta newparagraph false position start text dictionary real word spelling errorbrcorrection dont use dictionary end meta newparagraph false position start text course errors abrdictionary wouldnt help end meta newparagraph false position start text every word generate candidate setbrso might find candidate words end meta newparagraph false position start text similar pronunciations might findbrcandidate words similar spellings end meta newparagraph false position start text depending algorithm exactlybrand important gonna end meta newparagraph false position start text include word candidatebrset every word might end meta newparagraph false position start text misspelling real word itbrmight correct word fact end meta newparagraph false position start text words probably correct eachbrcandidate set possible error end meta newparagraph false position start text gonna include word andbrmost time fact gonna end meta newparagraph false position start text pick pick thebrwords might use noisy channel end meta newparagraph false position start text model might use classifier wellbrtalk well discuss end meta newparagraph false position start text different methods detecting thesebrerrors correcting next title spelling correction task versionno versionnumber video spelling correction task videodescription videotitle spelling correction task,Course2,W2-S2-L1,W2,S2,L1,The,2,2,1,descript languag code en dir ltr name english metadata note resourceuri apipartnersvideosvbacifslgnqwlanguagesensubtitl siteurl httpwwwamaraorgvideosvbacifslgnqwen subformat json subtitl end meta newparagraph true posit start text today gonna talk spellingbrcorrect lot applic make use end meta newparagraph fals posit start text spell correct exampl wordbrprocess almost modern word end meta newparagraph fals posit start text processor take misspel word likebrcompon give end meta newparagraph fals posit start text suggest like compon e andbrautomat replac modern end meta newparagraph fals posit start text search engin flag anbrerror languag spell without u end meta newparagraph fals posit start text give result ifbryou spell word correctli end meta newparagraph fals posit start text modern phone addit willbrautomat find misspel word end meta newparagraph fals posit start text type layr replac itbrautomat suggest replac end meta newparagraph fals posit start text late distinguish number ofbrsepar task spell correct end meta newparagraph fals posit start text one detect error itselfbrand correct error end meta newparagraph fals posit start text youv found think aboutbrdiffer kind correct might end meta newparagraph fals posit start text automat correct error werebrposit error know end meta newparagraph fals posit start text right answer error hte abrveri common misspel end meta newparagraph fals posit start text mani word processor automat correctbrht might suggest singl end meta newparagraph fals posit start text correct there one verybrlik correct might suggest end meta newparagraph fals posit start text whole list correct let userbrpick among distinguish two end meta newparagraph fals posit start text differ class spell error nonbrword error error end meta newparagraph fals posit start text user type word englishbrso graff misspel let say end meta newparagraph fals posit start text giraff word english bybrcontrast real word error error end meta newparagraph fals posit start text result soundbrmisspel actual word english end meta newparagraph fals posit start text make somewhat harder tobrdetect break real word end meta newparagraph fals posit start text error one produc reallybrtypograph process meant end meta newparagraph fals posit start text type three type inaud letsbrsay cognit error user end meta newparagraph fals posit start text meant type word like inaud andbrinstead type homophon end meta newparagraph fals posit start text word uctooud instead ofbrinaud case what end meta newparagraph fals posit start text produc real word english bybrmodel differ end meta newparagraph fals posit start text kind error might come withbrbett way fix end meta newparagraph fals posit start text common spell error depend lotbron task web queri spell end meta newparagraph fals posit start text error extrem common sobrpract one four word web end meta newparagraph fals posit start text queri like misspel inbrweb process task phone much end meta newparagraph fals posit start text harder get accur number sobrther number studi end meta newparagraph fals posit start text studi done retyp youbrgiv user passag type end meta newparagraph fals posit start text measur well type itbrand cours that quit end meta newparagraph fals posit start text user natur write messag orbrtyp nonetheless ask user end meta newparagraph fals posit start text retyp dont let use thebrbackspac key make thirteen end meta newparagraph fals posit start text percent word thirteen percent ofbrth word error indic end meta newparagraph fals posit start text lot word correctbrthemselv backspac let end meta newparagraph fals posit start text correct tri tobrexperi p style phone end meta newparagraph fals posit start text site organ theyll correct aboutbrseven percent word end meta newparagraph fals posit start text theyll still leav two percent ofbrth word uncorrect organ end meta newparagraph fals posit start text similar number peopl doingbrretyp regular keyboard end meta newparagraph fals posit start text number two percent peoplebrtyp probabl much higher number end meta newparagraph fals posit start text web queri probabl much higherbrnumb peopl text kind end meta newparagraph fals posit start text spell spell error inaud thatbrw see detect non word spell end meta newparagraph fals posit start text error tradit way usebra larg dictionari word end meta newparagraph fals posit start text dictionari error largerbrth dictionari turn better end meta newparagraph fals posit start text work correct nonwordbrspel error gener set end meta newparagraph fals posit start text candid that real word arebrsimilar error pick end meta newparagraph fals posit start text whichev one best well talkbrabout noisychannel probabl model end meta newparagraph fals posit start text also relatedbrto anoth method call shortest end meta newparagraph fals posit start text weight inaud distanc myth webrfind word end meta newparagraph fals posit start text dictionari one gener abrset candid go end meta newparagraph fals posit start text real word similar well talkbrabout similar mean error end meta newparagraph fals posit start text well pick best one realbrword spell error algorithm end meta newparagraph fals posit start text quit similar word webrgener candid set end meta newparagraph fals posit start text everi word sentenc notbrjust word end meta newparagraph fals posit start text dictionari real word spell errorbrcorrect dont use dictionari end meta newparagraph fals posit start text cours error abrdictionari wouldnt help end meta newparagraph fals posit start text everi word gener candid setbrso might find candid word end meta newparagraph fals posit start text similar pronunci might findbrcandid word similar spell end meta newparagraph fals posit start text depend algorithm exactlybrand import gonna end meta newparagraph fals posit start text includ word candidatebrset everi word might end meta newparagraph fals posit start text misspel real word itbrmight correct word fact end meta newparagraph fals posit start text word probabl correct eachbrcandid set possibl error end meta newparagraph fals posit start text gonna includ word andbrmost time fact gonna end meta newparagraph fals posit start text pick pick thebrword might use noisi channel end meta newparagraph fals posit start text model might use classifi wellbrtalk well discuss end meta newparagraph fals posit start text differ method detect thesebrerror correct next titl spell correct task versionno versionnumb video spell correct task videodescript videotitl spell correct task,[ 6  4 14 13 12]
141,Course2_W2-S2-L2_The_Noisy_Channel_Model_of_Spelling_19-30,description language code en dir ltr name english metadata note resourceuri apipartnersvideosgkptphqvzclanguagesensubtitles siteurl httpwwwamaraorgvideosgkptphqvzcen subformat json subtitles end meta newparagraph true position start text letus introduce noisy channel model spelling end meta newparagraph false position start text intuition noisy channelu end meta newparagraph false position start text comes throughout natural language processingu end meta newparagraph false position start text original signalu end meta newparagraph false position start text letus say itus wordu end meta newparagraph false position start text imagine goes channel end meta newparagraph false position start text idea originally invented speech end meta newparagraph false position start text talk tube end meta newparagraph false position start text go kind telecommunications line end meta newparagraph false position start text word distorted end meta newparagraph false position start text comes original word noisy word end meta newparagraph false position start text weuve represented weird font end meta newparagraph false position start text spelling case imagine end meta newparagraph false position start text ucoh somebody mistyped wordud end meta newparagraph false position start text channel typewriter person typing keyboard end meta newparagraph false position start text end youuve got misspelled version word end meta newparagraph false position start text goal noisy channel model end meta newparagraph false position start text take output noisy process end meta newparagraph false position start text modeling channel works end meta newparagraph false position start text build modelua probabilistic modeluof channel end meta newparagraph false position start text run possible original words channel end meta newparagraph false position start text see one looks like noisy word end meta newparagraph false position start text decoder take bunch hypotheses one end meta newparagraph false position start text run channel end meta newparagraph false position start text running hypothesis two channel end meta newparagraph false position start text run hypothesis three channel end meta newparagraph false position start text see word looks like noisy word end meta newparagraph false position start text pick end meta newparagraph false position start text original hypothesis word started end meta newparagraph false position start text letus look end meta newparagraph false position start text first weull introduce probability end meta newparagraph false position start text weull look examples end meta newparagraph false position start text noisy channel probabilistic model end meta newparagraph false position start text goal given observation x misspellingu end meta newparagraph false position start text word weuve seen end meta newparagraph false position start text surface thing weuve seen end meta newparagraph false position start text observation xu end meta newparagraph false position start text weud like find w correct word end meta newparagraph false position start text weure going model probabilistically end meta newparagraph false position start text saying weure looking best word end meta newparagraph false position start text word weud like replace misspelling end meta newparagraph false position start text word vocabulary maximizes probability end meta newparagraph false position start text probability end meta newparagraph false position start text probability word given misspelling end meta newparagraph false position start text word given weuve seen misspelling end meta newparagraph false position start text whatus likely word end meta newparagraph false position start text probable posterior probable word given misspelling end meta newparagraph false position start text weure going use bayes rule replace probability end meta newparagraph false position start text probability w given x end meta newparagraph false position start text weure going replace pxwpwpx end meta newparagraph false position start text also eliminate denominator end meta newparagraph false position start text whatever word maximizes equation end meta newparagraph false position start text also maximize equation end meta newparagraph false position start text weure asking given misspelling x end meta newparagraph false position start text whatus likely word end meta newparagraph false position start text since formula probability end meta newparagraph false position start text includes probability word misspelling x end meta newparagraph false position start text weure including probability end meta newparagraph false position start text every w weure considering end meta newparagraph false position start text w say w hypothesis one end meta newparagraph false position start text greater probability hypothesis two equation end meta newparagraph false position start text itull also greater probability equation end meta newparagraph false position start text w constant end meta newparagraph false position start text x misspelling weure trying decide end meta newparagraph false position start text w w better hypothesis end meta newparagraph false position start text means noisy channel model end meta newparagraph false position start text comes maximizing product two factors end meta newparagraph false position start text likelihood end meta newparagraph false position start text prior end meta newparagraph false position start text generally call term language model end meta newparagraph false position start text youuve seen language models end meta newparagraph false position start text thatus probability correct word w end meta newparagraph false position start text likelihood term end meta newparagraph false position start text often call channel model end meta newparagraph false position start text sometimes error model end meta newparagraph false position start text weuve got two factors end meta newparagraph false position start text language model channel model end meta newparagraph false position start text intuition language model tells us end meta newparagraph false position start text likely would word word end meta newparagraph false position start text perhaps context end meta newparagraph false position start text perhaps end meta newparagraph false position start text channel model says end meta newparagraph false position start text well word end meta newparagraph false position start text likely would generate exact error end meta newparagraph false position start text channel model sort modeling noisy channel end meta newparagraph false position start text turns correct word misspelling end meta newparagraph false position start text noisy channel model spelling proposed around end meta newparagraph false position start text independently two separate laboratories end meta newparagraph false position start text use speech recognition models like noisy channel end meta newparagraph false position start text came natural language processing right around end meta newparagraph false position start text mainly although exclusively work two labs end meta newparagraph false position start text ibm att bell labs end meta newparagraph false position start text examples weure going take rest example end meta newparagraph false position start text come two important early papers end meta newparagraph false position start text mays et al kernighan et al end meta newparagraph false position start text letus look example end meta newparagraph false position start text hereus misspelling word ucacressud end meta newparagraph false position start text think second could mean end meta newparagraph false position start text first weure going start generating candidates end meta newparagraph false position start text possible candidate words replace word end meta newparagraph false position start text think least couple obvious ways end meta newparagraph false position start text one weure going pick words similar spelling end meta newparagraph false position start text words similar spelling end meta newparagraph false position start text might naturally mistaken correct word end meta newparagraph false position start text weure going operationalize similar spelling end meta newparagraph false position start text small edit distance error end meta newparagraph false position start text could pick words similar pronunciation end meta newparagraph false position start text weure going pick word end meta newparagraph false position start text small edit distance pronunciation error end meta newparagraph false position start text weure going rest example end meta newparagraph false position start text ium going pick first approach end meta newparagraph false position start text weure going pick words similar spelling end meta newparagraph false position start text possible candidates end meta newparagraph false position start text operationalize similar spelling end meta newparagraph false position start text well weuve seen edit distance end meta newparagraph false position start text remember edit distance end meta newparagraph false position start text talked distance two strings end meta newparagraph false position start text minimal number edits turns one string another end meta newparagraph false position start text define edit insertion deletion substitution end meta newparagraph false position start text three end meta newparagraph false position start text spelling correction weure going want add end meta newparagraph false position start text fourth possible edit operation transposition end meta newparagraph false position start text practice spelling errors end meta newparagraph false position start text often transpose two letters end meta newparagraph false position start text version edit distance end meta newparagraph false position start text called dameraulevenshtein edit distance end meta newparagraph false position start text computed end meta newparagraph false position start text various dynamic programming approaches end meta newparagraph false position start text letus look candidates end meta newparagraph false position start text words within edit distance one end meta newparagraph false position start text misspelling ucacressud end meta newparagraph false position start text hereus error ucacressud end meta newparagraph false position start text different possible candidates end meta newparagraph false position start text hereus candidate ucactressud end meta newparagraph false position start text ucactressud turned ucacressud end meta newparagraph false position start text well uctud turns nothing uctud deleted end meta newparagraph false position start text deletion uctud end meta newparagraph false position start text deletion uctud turns ucactressud ucacressud end meta newparagraph false position start text proposed candidate word uccressud end meta newparagraph false position start text kind vegetable end meta newparagraph false position start text uccressud end meta newparagraph false position start text turn uccressud ucacressud add insert ucaud end meta newparagraph false position start text deletion insertion end meta newparagraph false position start text uccaressud end meta newparagraph false position start text turn uccaressud ucacressud turn uccaud ucacud end meta newparagraph false position start text transposition uccaud ucacud end meta newparagraph false position start text word coulduve ucaccessud end meta newparagraph false position start text substitution end meta newparagraph false position start text uccud turned ucrud end meta newparagraph false position start text another substitution end meta newparagraph false position start text word coulduve ucacrossud end meta newparagraph false position start text ucoud turned uceud end meta newparagraph false position start text ucsud coulduve inserted end meta newparagraph false position start text turn ucacresud ucacressud end meta newparagraph false position start text ucsud coulduve inserted either end meta newparagraph false position start text end meta newparagraph false position start text thereus two different ways end meta newparagraph false position start text source word could turned error form end meta newparagraph false position start text weull put two rows end meta newparagraph false position start text possible insertion locations positions end meta newparagraph false position start text iuve shown candidates within edit distance one end meta newparagraph false position start text turns percent spelling errors end meta newparagraph false position start text within edit distance one end meta newparagraph false position start text almost errors within edit distance two end meta newparagraph false position start text algorithms either consider end meta newparagraph false position start text edit distance one edit distance two possible candidates end meta newparagraph false position start text practice also want allow end meta newparagraph false position start text insertion substitution letters end meta newparagraph false position start text also spaces hyphens end meta newparagraph false position start text example user types ucthisideaud end meta newparagraph false position start text weud like realize insertion space end meta newparagraph false position start text original space fact deleted produce error form end meta newparagraph false position start text original dash word ucinlawud deleted end meta newparagraph false position start text produce error form ucinlawud end meta newparagraph false position start text weuve seen candidate generation end meta newparagraph false position start text weure ready talk rank candidates end meta newparagraph false position start text remember two factors end meta newparagraph false position start text language model channel model end meta newparagraph false position start text language model end meta newparagraph false position start text use language modeling algorithms weuve already learned end meta newparagraph false position start text use unigrams bigrams trigrams end meta newparagraph false position start text use kind backoff algorithm want use end meta newparagraph false position start text smoothing algorithm want use end meta newparagraph false position start text practice largescale webscale correction end meta newparagraph false position start text weure going use usual webscale things end meta newparagraph false position start text weure going use stupid backoff end meta newparagraph false position start text might want use smarter algorithms smaller kinds tasks end meta newparagraph false position start text letus look example language model end meta newparagraph false position start text picked simple unigram end meta newparagraph false position start text case weuve computed unigram end meta newparagraph false position start text corpus contemporary english end meta newparagraph false position start text one many possible corpora end meta newparagraph false position start text hereus counts end meta newparagraph false position start text hereus counts different possible candidates end meta newparagraph false position start text ucactressud uccressud uccaressud end meta newparagraph false position start text hereus frequency end meta newparagraph false position start text normalizing total number words get probability end meta newparagraph false position start text hereus total number words end meta newparagraph false position start text get normalizing count total count get probabilities end meta newparagraph false position start text hereus probabilities words assigned unigram language model end meta newparagraph false position start text computing channel model probability end meta newparagraph false position start text remember channel modelus also called end meta newparagraph false position start text error model edit probability end meta newparagraph false position start text weure going take simplifying assumption end meta newparagraph false position start text made kernighan church gale end meta newparagraph false position start text first proposed use noisy channel model end meta newparagraph false position start text letus first see end meta newparagraph false position start text letus assume misspelled word x set letters x xm end meta newparagraph false position start text correct word w set letters letus call w wn end meta newparagraph false position start text probability edit x given w end meta newparagraph false position start text going set deletions insertions substitutions transpositionsusome set edits end meta newparagraph false position start text way weure going model end meta newparagraph false position start text weure going create confusion matrix end meta newparagraph false position start text confusion matrix says given pair letters end meta newparagraph false position start text likely particular edit happen end meta newparagraph false position start text example pair letters xy end meta newparagraph false position start text want know often xy typed x end meta newparagraph false position start text meaning often deleted thereus x right end meta newparagraph false position start text weure going also keep count end meta newparagraph false position start text insertion probabilities often x typed xy end meta newparagraph false position start text often inserted x end meta newparagraph false position start text deleted x inserted x end meta newparagraph false position start text weull keep count substitutions end meta newparagraph false position start text often x typed end meta newparagraph false position start text meant type x typed end meta newparagraph false position start text thatus xuy substitution end meta newparagraph false position start text transposition often xy typed yx end meta newparagraph false position start text counts end meta newparagraph false position start text weull keep matrix counts every x every end meta newparagraph false position start text noticed weuve done implicitly end meta newparagraph false position start text weuve conditioned insertion deletion previous character end meta newparagraph false position start text whether deleted conditioned x end meta newparagraph false position start text could conditioneduchosen conditionu end meta newparagraph false position start text next character character five left thing end meta newparagraph false position start text generally condition previous character end meta newparagraph false position start text hereus example confusion matrix spelling errors end meta newparagraph false position start text font little small give basic idea end meta newparagraph false position start text hereus substitution matrix took kernighan et al end meta newparagraph false position start text hereus letter e end meta newparagraph false position start text itus likelyuin data timesuto substituted end meta newparagraph false position start text meant type e incorrectly typed end meta newparagraph false position start text might typed might typed end meta newparagraph false position start text vowels likely mistaken end meta newparagraph false position start text similarly letter often gets mistyped n end meta newparagraph false position start text high probability n substituted end meta newparagraph false position start text theyure next keyboard end meta newparagraph false position start text sound alike end meta newparagraph false position start text lots reasons substituted end meta newparagraph false position start text hereus set confusion matrices end meta newparagraph false position start text compute four end meta newparagraph false position start text one substitution end meta newparagraph false position start text one insertion end meta newparagraph false position start text one deletion end meta newparagraph false position start text one transposition end meta newparagraph false position start text iuve shown table comes kernighan et al end meta newparagraph false position start text could also generate table end meta newparagraph false position start text example peter norvig post website lovely list errors end meta newparagraph false position start text errors taken wikipedia end meta newparagraph false position start text places talks website end meta newparagraph false position start text set errors like end meta newparagraph false position start text misspellings ucadaptableud ucadabtableud end meta newparagraph false position start text ucimmatureud one ucmud end meta newparagraph false position start text various kinds likely misspellings end meta newparagraph false position start text list errors end meta newparagraph false position start text get list counts every possible single error end meta newparagraph false position start text single edit error often happens end meta newparagraph false position start text build little confusion matrix end meta newparagraph false position start text confusion matrix generate probabilities end meta newparagraph false position start text every time particular previous letter happens end meta newparagraph false position start text look confusion matrix end meta newparagraph false position start text say often xi inserted end meta newparagraph false position start text particular letter w sub minus one end meta newparagraph false position start text divide number times w minus one occurred end meta newparagraph false position start text thatus going probability end meta newparagraph false position start text particular insertion happening word end meta newparagraph false position start text generate probability surface form end meta newparagraph false position start text possible single edit errorudu end meta newparagraph false position start text weure assuming single edit end meta newparagraph false position start text one one happensu end meta newparagraph false position start text generate candidate end meta newparagraph false position start text whichever one end meta newparagraph false position start text compute probability end meta newparagraph false position start text normalizing count deletion insertion substitution transposition end meta newparagraph false position start text appropriate count end meta newparagraph false position start text generate probability end meta newparagraph false position start text channel model end meta newparagraph false position start text example word like ucactressud end meta newparagraph false position start text generated ucacressud typed ucctud end meta newparagraph false position start text typed uccud word ucctud error uccud end meta newparagraph false position start text whatus probability deleting uctud following uccud end meta newparagraph false position start text weud normalized probabilities confusion matrix end meta newparagraph false position start text hereus likelihood word ucactressud end meta newparagraph false position start text realized misspelling ucacressud end meta newparagraph false position start text itus end meta newparagraph false position start text language model hereus error model channel model end meta newparagraph false position start text add language model iull write lm end meta newparagraph false position start text channel model end meta newparagraph false position start text likely ucctud errorfully turned uccud end meta newparagraph false position start text uctud deleted end meta newparagraph false position start text likely word ucactressud anyway end meta newparagraph false position start text multiply together end meta newparagraph false position start text weull end meta newparagraph false position start text small numbers end meta newparagraph false position start text weull multiply everything ten ninth make readable end meta newparagraph false position start text would times ten minus ninth end meta newparagraph false position start text weud multiplied everything ten ninth end meta newparagraph false position start text see likely word ucacrossud end meta newparagraph false position start text particular particular channel model end meta newparagraph false position start text particular language model likely word ucacrossud end meta newparagraph false position start text ucactressud also quite likely end meta newparagraph false position start text ucacresud seems reasonably likelihood end meta newparagraph false position start text word uccressud end meta newparagraph false position start text rare word end meta newparagraph false position start text see low probability end meta newparagraph false position start text unusual error inserting ucaud beginning end meta newparagraph false position start text makes low probability correction end meta newparagraph false position start text noisy channel model likes word ucacrossud possible replacement end meta newparagraph false position start text unfortunately see original sentence end meta newparagraph false position start text taken kernighan et alus paper end meta newparagraph false position start text original sentence ucacrossud wrong word end meta newparagraph false position start text original sentence end meta newparagraph false position start text uca stellar versatile acress whose combination sass glamouruud end meta newparagraph false position start text clear word ucactressud end meta newparagraph false position start text ucacrossud wrong word end meta newparagraph false position start text using unigram model noisy channel makes mistake end meta newparagraph false position start text letus look bigram model end meta newparagraph false position start text well bigram model end meta newparagraph false position start text computed simple bigram model end meta newparagraph false position start text using ucaddone smoothingud end meta newparagraph false position start text corpus contemporary american english end meta newparagraph false position start text probability ucactressud given ucversatileud end meta newparagraph false position start text look three words ignore rest end meta newparagraph false position start text ucactressud given ucversatileud end meta newparagraph false position start text probability end meta newparagraph false position start text ucwhoseud given ucactressud weull compute end meta newparagraph false position start text letus thing another candidate end meta newparagraph false position start text original candidate preferred unigram model end meta newparagraph false position start text word ucacrossud end meta newparagraph false position start text weull put ucacrossud instead hypothesis end meta newparagraph false position start text weull compute probability ucacrossud giving ucversatileud end meta newparagraph false position start text times probability ucwhoseud giving ucacrossud end meta newparagraph false position start text hereus probabilities end meta newparagraph false position start text see probability ucwhoseud given ucactressud end meta newparagraph false position start text much higher probability ucwhoseud given ucacrossud end meta newparagraph false position start text ucactress whoseud likely sequence end meta newparagraph false position start text sure enough multiply things end meta newparagraph false position start text probability ucversatile actress whoseud much higher probability end meta newparagraph false position start text probability sequence ucversatile across whoseud end meta newparagraph false position start text much higher probability end meta newparagraph false position start text noisy channel model bigram language model end meta newparagraph false position start text correctly picks correction ucactressud end meta newparagraph false position start text going evaluate noisy channel kinds models end meta newparagraph false position start text lots good spelling error test sets end meta newparagraph false position start text wikipedia list common english misspellings end meta newparagraph false position start text thereus filtered version aspell end meta newparagraph false position start text thereus spelling error corpus birkbeck end meta newparagraph false position start text letus look wikipedia list end meta newparagraph false position start text thereus wikipediaus list common english misspellings end meta newparagraph false position start text iuve shown slide end meta newparagraph false position start text various possible lists go look end meta newparagraph false position start text lists misspellings end meta newparagraph false position start text would generate training set train channel model end meta newparagraph false position start text development set test model end meta newparagraph false position start text final test set see well model works end null meta newparagraph false position start text thatus noisy channel model spelling applied nonreal words title noisy channel model spelling versionno versionnumber video noisy channel model spelling videodescription videotitle noisy channel model spelling,Course2,W2-S2-L2,W2,S2,L2,The,2,2,2,descript languag code en dir ltr name english metadata note resourceuri apipartnersvideosgkptphqvzclanguagesensubtitl siteurl httpwwwamaraorgvideosgkptphqvzcen subformat json subtitl end meta newparagraph true posit start text letu introduc noisi channel model spell end meta newparagraph fals posit start text intuit noisi channelu end meta newparagraph fals posit start text come throughout natur languag processingu end meta newparagraph fals posit start text origin signalu end meta newparagraph fals posit start text letu say itu wordu end meta newparagraph fals posit start text imagin goe channel end meta newparagraph fals posit start text idea origin invent speech end meta newparagraph fals posit start text talk tube end meta newparagraph fals posit start text go kind telecommun line end meta newparagraph fals posit start text word distort end meta newparagraph fals posit start text come origin word noisi word end meta newparagraph fals posit start text weuv repres weird font end meta newparagraph fals posit start text spell case imagin end meta newparagraph fals posit start text ucoh somebodi mistyp wordud end meta newparagraph fals posit start text channel typewrit person type keyboard end meta newparagraph fals posit start text end youuv got misspel version word end meta newparagraph fals posit start text goal noisi channel model end meta newparagraph fals posit start text take output noisi process end meta newparagraph fals posit start text model channel work end meta newparagraph fals posit start text build modelua probabilist modeluof channel end meta newparagraph fals posit start text run possibl origin word channel end meta newparagraph fals posit start text see one look like noisi word end meta newparagraph fals posit start text decod take bunch hypothes one end meta newparagraph fals posit start text run channel end meta newparagraph fals posit start text run hypothesi two channel end meta newparagraph fals posit start text run hypothesi three channel end meta newparagraph fals posit start text see word look like noisi word end meta newparagraph fals posit start text pick end meta newparagraph fals posit start text origin hypothesi word start end meta newparagraph fals posit start text letu look end meta newparagraph fals posit start text first weull introduc probabl end meta newparagraph fals posit start text weull look exampl end meta newparagraph fals posit start text noisi channel probabilist model end meta newparagraph fals posit start text goal given observ x misspellingu end meta newparagraph fals posit start text word weuv seen end meta newparagraph fals posit start text surfac thing weuv seen end meta newparagraph fals posit start text observ xu end meta newparagraph fals posit start text weud like find w correct word end meta newparagraph fals posit start text weur go model probabilist end meta newparagraph fals posit start text say weur look best word end meta newparagraph fals posit start text word weud like replac misspel end meta newparagraph fals posit start text word vocabulari maxim probabl end meta newparagraph fals posit start text probabl end meta newparagraph fals posit start text probabl word given misspel end meta newparagraph fals posit start text word given weuv seen misspel end meta newparagraph fals posit start text whatu like word end meta newparagraph fals posit start text probabl posterior probabl word given misspel end meta newparagraph fals posit start text weur go use bay rule replac probabl end meta newparagraph fals posit start text probabl w given x end meta newparagraph fals posit start text weur go replac pxwpwpx end meta newparagraph fals posit start text also elimin denomin end meta newparagraph fals posit start text whatev word maxim equat end meta newparagraph fals posit start text also maxim equat end meta newparagraph fals posit start text weur ask given misspel x end meta newparagraph fals posit start text whatu like word end meta newparagraph fals posit start text sinc formula probabl end meta newparagraph fals posit start text includ probabl word misspel x end meta newparagraph fals posit start text weur includ probabl end meta newparagraph fals posit start text everi w weur consid end meta newparagraph fals posit start text w say w hypothesi one end meta newparagraph fals posit start text greater probabl hypothesi two equat end meta newparagraph fals posit start text itul also greater probabl equat end meta newparagraph fals posit start text w constant end meta newparagraph fals posit start text x misspel weur tri decid end meta newparagraph fals posit start text w w better hypothesi end meta newparagraph fals posit start text mean noisi channel model end meta newparagraph fals posit start text come maxim product two factor end meta newparagraph fals posit start text likelihood end meta newparagraph fals posit start text prior end meta newparagraph fals posit start text gener call term languag model end meta newparagraph fals posit start text youuv seen languag model end meta newparagraph fals posit start text thatu probabl correct word w end meta newparagraph fals posit start text likelihood term end meta newparagraph fals posit start text often call channel model end meta newparagraph fals posit start text sometim error model end meta newparagraph fals posit start text weuv got two factor end meta newparagraph fals posit start text languag model channel model end meta newparagraph fals posit start text intuit languag model tell us end meta newparagraph fals posit start text like would word word end meta newparagraph fals posit start text perhap context end meta newparagraph fals posit start text perhap end meta newparagraph fals posit start text channel model say end meta newparagraph fals posit start text well word end meta newparagraph fals posit start text like would gener exact error end meta newparagraph fals posit start text channel model sort model noisi channel end meta newparagraph fals posit start text turn correct word misspel end meta newparagraph fals posit start text noisi channel model spell propos around end meta newparagraph fals posit start text independ two separ laboratori end meta newparagraph fals posit start text use speech recognit model like noisi channel end meta newparagraph fals posit start text came natur languag process right around end meta newparagraph fals posit start text mainli although exclus work two lab end meta newparagraph fals posit start text ibm att bell lab end meta newparagraph fals posit start text exampl weur go take rest exampl end meta newparagraph fals posit start text come two import earli paper end meta newparagraph fals posit start text may et al kernighan et al end meta newparagraph fals posit start text letu look exampl end meta newparagraph fals posit start text hereu misspel word ucacressud end meta newparagraph fals posit start text think second could mean end meta newparagraph fals posit start text first weur go start gener candid end meta newparagraph fals posit start text possibl candid word replac word end meta newparagraph fals posit start text think least coupl obviou way end meta newparagraph fals posit start text one weur go pick word similar spell end meta newparagraph fals posit start text word similar spell end meta newparagraph fals posit start text might natur mistaken correct word end meta newparagraph fals posit start text weur go operation similar spell end meta newparagraph fals posit start text small edit distanc error end meta newparagraph fals posit start text could pick word similar pronunci end meta newparagraph fals posit start text weur go pick word end meta newparagraph fals posit start text small edit distanc pronunci error end meta newparagraph fals posit start text weur go rest exampl end meta newparagraph fals posit start text ium go pick first approach end meta newparagraph fals posit start text weur go pick word similar spell end meta newparagraph fals posit start text possibl candid end meta newparagraph fals posit start text operation similar spell end meta newparagraph fals posit start text well weuv seen edit distanc end meta newparagraph fals posit start text rememb edit distanc end meta newparagraph fals posit start text talk distanc two string end meta newparagraph fals posit start text minim number edit turn one string anoth end meta newparagraph fals posit start text defin edit insert delet substitut end meta newparagraph fals posit start text three end meta newparagraph fals posit start text spell correct weur go want add end meta newparagraph fals posit start text fourth possibl edit oper transposit end meta newparagraph fals posit start text practic spell error end meta newparagraph fals posit start text often transpos two letter end meta newparagraph fals posit start text version edit distanc end meta newparagraph fals posit start text call dameraulevenshtein edit distanc end meta newparagraph fals posit start text comput end meta newparagraph fals posit start text variou dynam program approach end meta newparagraph fals posit start text letu look candid end meta newparagraph fals posit start text word within edit distanc one end meta newparagraph fals posit start text misspel ucacressud end meta newparagraph fals posit start text hereu error ucacressud end meta newparagraph fals posit start text differ possibl candid end meta newparagraph fals posit start text hereu candid ucactressud end meta newparagraph fals posit start text ucactressud turn ucacressud end meta newparagraph fals posit start text well uctud turn noth uctud delet end meta newparagraph fals posit start text delet uctud end meta newparagraph fals posit start text delet uctud turn ucactressud ucacressud end meta newparagraph fals posit start text propos candid word uccressud end meta newparagraph fals posit start text kind veget end meta newparagraph fals posit start text uccressud end meta newparagraph fals posit start text turn uccressud ucacressud add insert ucaud end meta newparagraph fals posit start text delet insert end meta newparagraph fals posit start text uccaressud end meta newparagraph fals posit start text turn uccaressud ucacressud turn uccaud ucacud end meta newparagraph fals posit start text transposit uccaud ucacud end meta newparagraph fals posit start text word coulduv ucaccessud end meta newparagraph fals posit start text substitut end meta newparagraph fals posit start text uccud turn ucrud end meta newparagraph fals posit start text anoth substitut end meta newparagraph fals posit start text word coulduv ucacrossud end meta newparagraph fals posit start text ucoud turn uceud end meta newparagraph fals posit start text ucsud coulduv insert end meta newparagraph fals posit start text turn ucacresud ucacressud end meta newparagraph fals posit start text ucsud coulduv insert either end meta newparagraph fals posit start text end meta newparagraph fals posit start text thereu two differ way end meta newparagraph fals posit start text sourc word could turn error form end meta newparagraph fals posit start text weull put two row end meta newparagraph fals posit start text possibl insert locat posit end meta newparagraph fals posit start text iuv shown candid within edit distanc one end meta newparagraph fals posit start text turn percent spell error end meta newparagraph fals posit start text within edit distanc one end meta newparagraph fals posit start text almost error within edit distanc two end meta newparagraph fals posit start text algorithm either consid end meta newparagraph fals posit start text edit distanc one edit distanc two possibl candid end meta newparagraph fals posit start text practic also want allow end meta newparagraph fals posit start text insert substitut letter end meta newparagraph fals posit start text also space hyphen end meta newparagraph fals posit start text exampl user type ucthisideaud end meta newparagraph fals posit start text weud like realiz insert space end meta newparagraph fals posit start text origin space fact delet produc error form end meta newparagraph fals posit start text origin dash word ucinlawud delet end meta newparagraph fals posit start text produc error form ucinlawud end meta newparagraph fals posit start text weuv seen candid gener end meta newparagraph fals posit start text weur readi talk rank candid end meta newparagraph fals posit start text rememb two factor end meta newparagraph fals posit start text languag model channel model end meta newparagraph fals posit start text languag model end meta newparagraph fals posit start text use languag model algorithm weuv alreadi learn end meta newparagraph fals posit start text use unigram bigram trigram end meta newparagraph fals posit start text use kind backoff algorithm want use end meta newparagraph fals posit start text smooth algorithm want use end meta newparagraph fals posit start text practic largescal webscal correct end meta newparagraph fals posit start text weur go use usual webscal thing end meta newparagraph fals posit start text weur go use stupid backoff end meta newparagraph fals posit start text might want use smarter algorithm smaller kind task end meta newparagraph fals posit start text letu look exampl languag model end meta newparagraph fals posit start text pick simpl unigram end meta newparagraph fals posit start text case weuv comput unigram end meta newparagraph fals posit start text corpu contemporari english end meta newparagraph fals posit start text one mani possibl corpora end meta newparagraph fals posit start text hereu count end meta newparagraph fals posit start text hereu count differ possibl candid end meta newparagraph fals posit start text ucactressud uccressud uccaressud end meta newparagraph fals posit start text hereu frequenc end meta newparagraph fals posit start text normal total number word get probabl end meta newparagraph fals posit start text hereu total number word end meta newparagraph fals posit start text get normal count total count get probabl end meta newparagraph fals posit start text hereu probabl word assign unigram languag model end meta newparagraph fals posit start text comput channel model probabl end meta newparagraph fals posit start text rememb channel modelu also call end meta newparagraph fals posit start text error model edit probabl end meta newparagraph fals posit start text weur go take simplifi assumpt end meta newparagraph fals posit start text made kernighan church gale end meta newparagraph fals posit start text first propos use noisi channel model end meta newparagraph fals posit start text letu first see end meta newparagraph fals posit start text letu assum misspel word x set letter x xm end meta newparagraph fals posit start text correct word w set letter letu call w wn end meta newparagraph fals posit start text probabl edit x given w end meta newparagraph fals posit start text go set delet insert substitut transpositionsusom set edit end meta newparagraph fals posit start text way weur go model end meta newparagraph fals posit start text weur go creat confus matrix end meta newparagraph fals posit start text confus matrix say given pair letter end meta newparagraph fals posit start text like particular edit happen end meta newparagraph fals posit start text exampl pair letter xy end meta newparagraph fals posit start text want know often xy type x end meta newparagraph fals posit start text mean often delet thereu x right end meta newparagraph fals posit start text weur go also keep count end meta newparagraph fals posit start text insert probabl often x type xy end meta newparagraph fals posit start text often insert x end meta newparagraph fals posit start text delet x insert x end meta newparagraph fals posit start text weull keep count substitut end meta newparagraph fals posit start text often x type end meta newparagraph fals posit start text meant type x type end meta newparagraph fals posit start text thatu xuy substitut end meta newparagraph fals posit start text transposit often xy type yx end meta newparagraph fals posit start text count end meta newparagraph fals posit start text weull keep matrix count everi x everi end meta newparagraph fals posit start text notic weuv done implicitli end meta newparagraph fals posit start text weuv condit insert delet previou charact end meta newparagraph fals posit start text whether delet condit x end meta newparagraph fals posit start text could conditioneduchosen conditionu end meta newparagraph fals posit start text next charact charact five left thing end meta newparagraph fals posit start text gener condit previou charact end meta newparagraph fals posit start text hereu exampl confus matrix spell error end meta newparagraph fals posit start text font littl small give basic idea end meta newparagraph fals posit start text hereu substitut matrix took kernighan et al end meta newparagraph fals posit start text hereu letter e end meta newparagraph fals posit start text itu likelyuin data timesuto substitut end meta newparagraph fals posit start text meant type e incorrectli type end meta newparagraph fals posit start text might type might type end meta newparagraph fals posit start text vowel like mistaken end meta newparagraph fals posit start text similarli letter often get mistyp n end meta newparagraph fals posit start text high probabl n substitut end meta newparagraph fals posit start text theyur next keyboard end meta newparagraph fals posit start text sound alik end meta newparagraph fals posit start text lot reason substitut end meta newparagraph fals posit start text hereu set confus matric end meta newparagraph fals posit start text comput four end meta newparagraph fals posit start text one substitut end meta newparagraph fals posit start text one insert end meta newparagraph fals posit start text one delet end meta newparagraph fals posit start text one transposit end meta newparagraph fals posit start text iuv shown tabl come kernighan et al end meta newparagraph fals posit start text could also gener tabl end meta newparagraph fals posit start text exampl peter norvig post websit love list error end meta newparagraph fals posit start text error taken wikipedia end meta newparagraph fals posit start text place talk websit end meta newparagraph fals posit start text set error like end meta newparagraph fals posit start text misspel ucadaptableud ucadabtableud end meta newparagraph fals posit start text ucimmatureud one ucmud end meta newparagraph fals posit start text variou kind like misspel end meta newparagraph fals posit start text list error end meta newparagraph fals posit start text get list count everi possibl singl error end meta newparagraph fals posit start text singl edit error often happen end meta newparagraph fals posit start text build littl confus matrix end meta newparagraph fals posit start text confus matrix gener probabl end meta newparagraph fals posit start text everi time particular previou letter happen end meta newparagraph fals posit start text look confus matrix end meta newparagraph fals posit start text say often xi insert end meta newparagraph fals posit start text particular letter w sub minu one end meta newparagraph fals posit start text divid number time w minu one occur end meta newparagraph fals posit start text thatu go probabl end meta newparagraph fals posit start text particular insert happen word end meta newparagraph fals posit start text gener probabl surfac form end meta newparagraph fals posit start text possibl singl edit errorudu end meta newparagraph fals posit start text weur assum singl edit end meta newparagraph fals posit start text one one happensu end meta newparagraph fals posit start text gener candid end meta newparagraph fals posit start text whichev one end meta newparagraph fals posit start text comput probabl end meta newparagraph fals posit start text normal count delet insert substitut transposit end meta newparagraph fals posit start text appropri count end meta newparagraph fals posit start text gener probabl end meta newparagraph fals posit start text channel model end meta newparagraph fals posit start text exampl word like ucactressud end meta newparagraph fals posit start text gener ucacressud type ucctud end meta newparagraph fals posit start text type uccud word ucctud error uccud end meta newparagraph fals posit start text whatu probabl delet uctud follow uccud end meta newparagraph fals posit start text weud normal probabl confus matrix end meta newparagraph fals posit start text hereu likelihood word ucactressud end meta newparagraph fals posit start text realiz misspel ucacressud end meta newparagraph fals posit start text itu end meta newparagraph fals posit start text languag model hereu error model channel model end meta newparagraph fals posit start text add languag model iull write lm end meta newparagraph fals posit start text channel model end meta newparagraph fals posit start text like ucctud error turn uccud end meta newparagraph fals posit start text uctud delet end meta newparagraph fals posit start text like word ucactressud anyway end meta newparagraph fals posit start text multipli togeth end meta newparagraph fals posit start text weull end meta newparagraph fals posit start text small number end meta newparagraph fals posit start text weull multipli everyth ten ninth make readabl end meta newparagraph fals posit start text would time ten minu ninth end meta newparagraph fals posit start text weud multipli everyth ten ninth end meta newparagraph fals posit start text see like word ucacrossud end meta newparagraph fals posit start text particular particular channel model end meta newparagraph fals posit start text particular languag model like word ucacrossud end meta newparagraph fals posit start text ucactressud also quit like end meta newparagraph fals posit start text ucacresud seem reason likelihood end meta newparagraph fals posit start text word uccressud end meta newparagraph fals posit start text rare word end meta newparagraph fals posit start text see low probabl end meta newparagraph fals posit start text unusu error insert ucaud begin end meta newparagraph fals posit start text make low probabl correct end meta newparagraph fals posit start text noisi channel model like word ucacrossud possibl replac end meta newparagraph fals posit start text unfortun see origin sentenc end meta newparagraph fals posit start text taken kernighan et alu paper end meta newparagraph fals posit start text origin sentenc ucacrossud wrong word end meta newparagraph fals posit start text origin sentenc end meta newparagraph fals posit start text uca stellar versatil acress whose combin sass glamouruud end meta newparagraph fals posit start text clear word ucactressud end meta newparagraph fals posit start text ucacrossud wrong word end meta newparagraph fals posit start text use unigram model noisi channel make mistak end meta newparagraph fals posit start text letu look bigram model end meta newparagraph fals posit start text well bigram model end meta newparagraph fals posit start text comput simpl bigram model end meta newparagraph fals posit start text use ucaddon smoothingud end meta newparagraph fals posit start text corpu contemporari american english end meta newparagraph fals posit start text probabl ucactressud given ucversatileud end meta newparagraph fals posit start text look three word ignor rest end meta newparagraph fals posit start text ucactressud given ucversatileud end meta newparagraph fals posit start text probabl end meta newparagraph fals posit start text ucwhoseud given ucactressud weull comput end meta newparagraph fals posit start text letu thing anoth candid end meta newparagraph fals posit start text origin candid prefer unigram model end meta newparagraph fals posit start text word ucacrossud end meta newparagraph fals posit start text weull put ucacrossud instead hypothesi end meta newparagraph fals posit start text weull comput probabl ucacrossud give ucversatileud end meta newparagraph fals posit start text time probabl ucwhoseud give ucacrossud end meta newparagraph fals posit start text hereu probabl end meta newparagraph fals posit start text see probabl ucwhoseud given ucactressud end meta newparagraph fals posit start text much higher probabl ucwhoseud given ucacrossud end meta newparagraph fals posit start text ucactress whoseud like sequenc end meta newparagraph fals posit start text sure enough multipli thing end meta newparagraph fals posit start text probabl ucversatil actress whoseud much higher probabl end meta newparagraph fals posit start text probabl sequenc ucversatil across whoseud end meta newparagraph fals posit start text much higher probabl end meta newparagraph fals posit start text noisi channel model bigram languag model end meta newparagraph fals posit start text correctli pick correct ucactressud end meta newparagraph fals posit start text go evalu noisi channel kind model end meta newparagraph fals posit start text lot good spell error test set end meta newparagraph fals posit start text wikipedia list common english misspel end meta newparagraph fals posit start text thereu filter version aspel end meta newparagraph fals posit start text thereu spell error corpu birkbeck end meta newparagraph fals posit start text letu look wikipedia list end meta newparagraph fals posit start text thereu wikipediau list common english misspel end meta newparagraph fals posit start text iuv shown slide end meta newparagraph fals posit start text variou possibl list go look end meta newparagraph fals posit start text list misspel end meta newparagraph fals posit start text would gener train set train channel model end meta newparagraph fals posit start text develop set test model end meta newparagraph fals posit start text final test set see well model work end null meta newparagraph fals posit start text thatu noisi channel model spell appli nonreal word titl noisi channel model spell versionno versionnumb video noisi channel model spell videodescript videotitl noisi channel model spell,[ 6  4 14 13 12]
142,Course2_W2-S2-L3_Real-Word_Spelling_Correction_9-19,description language code en dir ltr name english metadata note resourceuri apipartnersvideosrhmmjdvjfflanguagesensubtitles siteurl httpwwwamaraorgvideosrhmmjdvjffen subformat json subtitles end meta newparagraph true position start text weve seen correct nonwords ofbrenglish happens error end meta newparagraph false position start text produces real word turns bebra common problem maybe end meta newparagraph false position start text quarter half spelling errorsbrdepending application turn end meta newparagraph false position start text real words examples frombrfrom classic paper karen kukich end meta newparagraph false position start text word minutes misspelled minuetsbrperfectly resembling english word end meta newparagraph false position start text word misspelled word abrvery common english word leave end meta newparagraph false position start text lave arebrnot english words end meta newparagraph false position start text quite common andbrfrequently used english words much end meta newparagraph false position start text tougher problem solving real wordbrspelling error task end meta newparagraph false position start text gonna real word spelling errorbris similar end meta newparagraph false position start text non real words gonna generate abrcandidate set includes word end meta newparagraph false position start text single letter inaudiblebrthat produce english words may end meta newparagraph false position start text well also version producebrincludes word inaudible sound end meta newparagraph false position start text like giving inaudible setbrfor word well choose best end meta newparagraph false position start text candidates either using noisy channelbrmodel imagine well talk later end meta newparagraph false position start text complex models usebrclassifiers lets look end meta newparagraph false position start text detail given sentence word onebrthrough word n going generate end meta newparagraph false position start text word set candidates forbrword one candidates word one end meta newparagraph false position start text bunch varies thebrsingle edit distance neighbors end meta newparagraph false position start text word word one prime word one doublebrprime word one triple prime word two end meta newparagraph false position start text word two prime word two double primebrworld two triple prime end meta newparagraph false position start text words whole lot ofbrcandidates words end meta newparagraph false position start text gonna choose sequence capitalbrw sequence candidates end meta newparagraph false position start text maximize maximal probability inbrother words might pick word one end meta newparagraph false position start text candidate set word two primebrprime candidate set word end meta newparagraph false position start text three prime prime prime thisbrcandidate set word end meta newparagraph false position start text gonna pick candidate whichbrmight word end meta newparagraph false position start text correction word gonnabrpick sequence likely end meta newparagraph false position start text lets look example havebrthe imagine three words mini end meta newparagraph false position start text mini sentence two thew thew thewbrso word word two word end meta newparagraph false position start text word inaudible generatebrpotential corrections end meta newparagraph false position start text word english inaudible ofbrone ive shown two end meta newparagraph false position start text could word wordbrif original word two error end meta newparagraph false position start text insertion w could havebrbeen word tao error end meta newparagraph false position start text substitution w could havebrbeen word substitution end meta newparagraph false position start text w could correctly thebrword two could correct similarly end meta newparagraph false position start text could correct word couldbrhave deletion end meta newparagraph false position start text f three candidates onbrand including word end meta newparagraph false position start text word real word englishbrcould word end meta newparagraph false position start text r got deleted word thaw thebrword common word ew end meta newparagraph false position start text likely error turns wbris right next e keyboard end meta newparagraph false position start text candidatebrsets wanna ask end meta newparagraph false position start text possible sets sentences producedbrby paths graph heres end meta newparagraph false position start text one two heres another onebrtwo thaw heres another one two end meta newparagraph false position start text thosebrpossible sentences whats likely end meta newparagraph false position start text one according noisy channel webrpick mo excuse probably end meta newparagraph false position start text one according noisy channel andbrhopefully noisy channel good end meta newparagraph false position start text noisy channel model predict willbrpick correct answer end meta newparagraph false position start text likely sequence practicebrfor spelling correction often make end meta newparagraph false position start text simplification seeing onebrerror rather letting every word end meta newparagraph false position start text possible error otherbrwords set sequences consider end meta newparagraph false position start text sequences one thebrwords error rest end meta newparagraph false position start text words correct typed wordbrone word three word four end meta newparagraph false position start text correct typed word two thatbrwas misspelled replace word end meta newparagraph false position start text two doubleprime lets say thisbrsequence word three end meta newparagraph false position start text misspelled thew misspelled thebrwas misspelled thew heres end meta newparagraph false position start text mis error threebrwords correct end meta newparagraph false position start text smaller set possible candidatebrsequences instead end meta newparagraph false position start text consider n squared possible sequencesbrwere considering constant times n end meta newparagraph false position start text possible sequences set webrchoose sequence maximizes end meta newparagraph false position start text maximum probabilities webrpicked likely end meta newparagraph false position start text probable conditionally probablebrset sequence sequence candidates end meta newparagraph false position start text sound get thesebrprobabilities language end meta newparagraph false position start text model saw ourbrunigram bigram use end meta newparagraph false position start text whatever smoothing method wed like thebrchannel models end meta newparagraph false position start text non word spelling errorbrcorrection difference end meta newparagraph false position start text need probability errorbrbecause course assuming end meta newparagraph false position start text one words error webrhave probability end meta newparagraph false position start text words anbrerror need able decide end meta newparagraph false position start text error word isbrin fact correct meaning end meta newparagraph false position start text probability word given thebrword high unlikely end meta newparagraph false position start text error compute probabilitybrof error whats channel end meta newparagraph false position start text probability correctly typed wordbrand obviously depends end meta newparagraph false position start text application might make thebrassumption particular end meta newparagraph false position start text application know one word one ten isbrtyped wrong means end meta newparagraph false position start text probability correctly typed wordbris might instead end meta newparagraph false position start text assumption one word wrongbrand probability end meta newparagraph false position start text word typed correctly sobrtheres channel model probability end meta newparagraph false position start text word changing lets assume thebrchannel model task end meta newparagraph false position start text probability one twenty errorbrmeaning percent time word end meta newparagraph false position start text correct typed heres examplebrfrom peter norvig end meta newparagraph false position start text spelling error thew want knowbrwhether word end meta newparagraph false position start text word correct correct asbrtyped thaw three end meta newparagraph false position start text one generatebrour channel model channel end meta newparagraph false position start text models exactly computed waybras probability end meta newparagraph false position start text substitution subset beingbrsubstituted e r end meta newparagraph false position start text deleted h canbrcompute channel models well end meta newparagraph false position start text heres channel model probabilitiesbrand language model end meta newparagraph false position start text probability arebrexamples pierre norbert computed end meta newparagraph false position start text google anagram counts wevebrassumed channel model word end meta newparagraph false position start text changing error x beingbrgenerated correct correctly generated end meta newparagraph false position start text word x multiply thesebrtogether multiply together channel end meta newparagraph false position start text model language model againbrshowing multiplied ten end meta newparagraph false position start text ninth make easy read seebrthat word correctly chosen end meta newparagraph false position start text high probability misspellingbrof word context end meta newparagraph false position start text using unigram languagebrmodel using bigram end meta newparagraph false position start text trigram even likely probably bebrable distinguish word end meta newparagraph false position start text really word wasbrthe word thats real word end meta newparagraph false position start text spelling correction simple take thebrthe standard algorithm noisy channel end meta newparagraph false position start text algorithm non real words add abrprobability edit happening end meta newparagraph false position start text allow every word imagine every wordbrcould error look end meta newparagraph false position start text likely sequence simplifyingbrusually inaudible one error per title realword spelling correction versionno versionnumber video realword spelling correction videodescription videotitle realword spelling correction,Course2,W2-S2-L3,W2,S2,L3,Real-Word,2,2,3,descript languag code en dir ltr name english metadata note resourceuri apipartnersvideosrhmmjdvjfflanguagesensubtitl siteurl httpwwwamaraorgvideosrhmmjdvjffen subformat json subtitl end meta newparagraph true posit start text weve seen correct nonword ofbrenglish happen error end meta newparagraph fals posit start text produc real word turn bebra common problem mayb end meta newparagraph fals posit start text quarter half spell errorsbrdepend applic turn end meta newparagraph fals posit start text real word exampl frombrfrom classic paper karen kukich end meta newparagraph fals posit start text word minut misspel minuetsbrperfectli resembl english word end meta newparagraph fals posit start text word misspel word abrveri common english word leav end meta newparagraph fals posit start text lave arebrnot english word end meta newparagraph fals posit start text quit common andbrfrequ use english word much end meta newparagraph fals posit start text tougher problem solv real wordbrspel error task end meta newparagraph fals posit start text gonna real word spell errorbri similar end meta newparagraph fals posit start text non real word gonna gener abrcandid set includ word end meta newparagraph fals posit start text singl letter inaudiblebrthat produc english word may end meta newparagraph fals posit start text well also version producebrinclud word inaud sound end meta newparagraph fals posit start text like give inaud setbrfor word well choos best end meta newparagraph fals posit start text candid either use noisi channelbrmodel imagin well talk later end meta newparagraph fals posit start text complex model usebrclassifi let look end meta newparagraph fals posit start text detail given sentenc word onebrthrough word n go gener end meta newparagraph fals posit start text word set candid forbrword one candid word one end meta newparagraph fals posit start text bunch vari thebrsingl edit distanc neighbor end meta newparagraph fals posit start text word word one prime word one doublebrprim word one tripl prime word two end meta newparagraph fals posit start text word two prime word two doubl primebrworld two tripl prime end meta newparagraph fals posit start text word whole lot ofbrcandid word end meta newparagraph fals posit start text gonna choos sequenc capitalbrw sequenc candid end meta newparagraph fals posit start text maxim maxim probabl inbroth word might pick word one end meta newparagraph fals posit start text candid set word two primebrprim candid set word end meta newparagraph fals posit start text three prime prime prime thisbrcandid set word end meta newparagraph fals posit start text gonna pick candid whichbrmight word end meta newparagraph fals posit start text correct word gonnabrpick sequenc like end meta newparagraph fals posit start text let look exampl havebrth imagin three word mini end meta newparagraph fals posit start text mini sentenc two thew thew thewbrso word word two word end meta newparagraph fals posit start text word inaud generatebrpotenti correct end meta newparagraph fals posit start text word english inaud ofbron ive shown two end meta newparagraph fals posit start text could word wordbrif origin word two error end meta newparagraph fals posit start text insert w could havebrbeen word tao error end meta newparagraph fals posit start text substitut w could havebrbeen word substitut end meta newparagraph fals posit start text w could correctli thebrword two could correct similarli end meta newparagraph fals posit start text could correct word couldbrhav delet end meta newparagraph fals posit start text f three candid onbrand includ word end meta newparagraph fals posit start text word real word englishbrcould word end meta newparagraph fals posit start text r got delet word thaw thebrword common word ew end meta newparagraph fals posit start text like error turn wbri right next e keyboard end meta newparagraph fals posit start text candidatebrset wanna ask end meta newparagraph fals posit start text possibl set sentenc producedbrbi path graph here end meta newparagraph fals posit start text one two here anoth onebrtwo thaw here anoth one two end meta newparagraph fals posit start text thosebrposs sentenc what like end meta newparagraph fals posit start text one accord noisi channel webrpick mo excus probabl end meta newparagraph fals posit start text one accord noisi channel andbrhop noisi channel good end meta newparagraph fals posit start text noisi channel model predict willbrpick correct answer end meta newparagraph fals posit start text like sequenc practicebrfor spell correct often make end meta newparagraph fals posit start text simplif see onebrerror rather let everi word end meta newparagraph fals posit start text possibl error otherbrword set sequenc consid end meta newparagraph fals posit start text sequenc one thebrword error rest end meta newparagraph fals posit start text word correct type wordbron word three word four end meta newparagraph fals posit start text correct type word two thatbrwa misspel replac word end meta newparagraph fals posit start text two doubleprim let say thisbrsequ word three end meta newparagraph fals posit start text misspel thew misspel thebrwa misspel thew here end meta newparagraph fals posit start text mi error threebrword correct end meta newparagraph fals posit start text smaller set possibl candidatebrsequ instead end meta newparagraph fals posit start text consid n squar possibl sequencesbrwer consid constant time n end meta newparagraph fals posit start text possibl sequenc set webrchoos sequenc maxim end meta newparagraph fals posit start text maximum probabl webrpick like end meta newparagraph fals posit start text probabl condit probablebrset sequenc sequenc candid end meta newparagraph fals posit start text sound get thesebrprob languag end meta newparagraph fals posit start text model saw ourbrunigram bigram use end meta newparagraph fals posit start text whatev smooth method wed like thebrchannel model end meta newparagraph fals posit start text non word spell errorbrcorrect differ end meta newparagraph fals posit start text need probabl errorbrbecaus cours assum end meta newparagraph fals posit start text one word error webrhav probabl end meta newparagraph fals posit start text word anbrerror need abl decid end meta newparagraph fals posit start text error word isbrin fact correct mean end meta newparagraph fals posit start text probabl word given thebrword high unlik end meta newparagraph fals posit start text error comput probabilitybrof error what channel end meta newparagraph fals posit start text probabl correctli type wordbrand obvious depend end meta newparagraph fals posit start text applic might make thebrassumpt particular end meta newparagraph fals posit start text applic know one word one ten isbrtyp wrong mean end meta newparagraph fals posit start text probabl correctli type wordbri might instead end meta newparagraph fals posit start text assumpt one word wrongbrand probabl end meta newparagraph fals posit start text word type correctli sobrther channel model probabl end meta newparagraph fals posit start text word chang let assum thebrchannel model task end meta newparagraph fals posit start text probabl one twenti errorbrmean percent time word end meta newparagraph fals posit start text correct type here examplebrfrom peter norvig end meta newparagraph fals posit start text spell error thew want knowbrwheth word end meta newparagraph fals posit start text word correct correct asbrtyp thaw three end meta newparagraph fals posit start text one generatebrour channel model channel end meta newparagraph fals posit start text model exactli comput waybra probabl end meta newparagraph fals posit start text substitut subset beingbrsubstitut e r end meta newparagraph fals posit start text delet h canbrcomput channel model well end meta newparagraph fals posit start text here channel model probabilitiesbrand languag model end meta newparagraph fals posit start text probabl arebrexampl pierr norbert comput end meta newparagraph fals posit start text googl anagram count wevebrassum channel model word end meta newparagraph fals posit start text chang error x beingbrgener correct correctli gener end meta newparagraph fals posit start text word x multipli thesebrtogeth multipli togeth channel end meta newparagraph fals posit start text model languag model againbrshow multipli ten end meta newparagraph fals posit start text ninth make easi read seebrthat word correctli chosen end meta newparagraph fals posit start text high probabl misspellingbrof word context end meta newparagraph fals posit start text use unigram languagebrmodel use bigram end meta newparagraph fals posit start text trigram even like probabl bebrabl distinguish word end meta newparagraph fals posit start text realli word wasbrth word that real word end meta newparagraph fals posit start text spell correct simpl take thebrth standard algorithm noisi channel end meta newparagraph fals posit start text algorithm non real word add abrprob edit happen end meta newparagraph fals posit start text allow everi word imagin everi wordbrcould error look end meta newparagraph fals posit start text like sequenc simplifyingbrusu inaud one error per titl realword spell correct versionno versionnumb video realword spell correct videodescript videotitl realword spell correct,[ 6  4 14 13 12]
143,Course2_W2-S2-L4_State_of_the_Art_Systems_7-10,description language code en dir ltr name english metadata note resourceuri apipartnersvideoshlbldinibrdlanguagesensubtitles siteurl httpwwwamaraorgvideoshlbldinibrden subformat json subtitles end meta newparagraph true position start text issues come upbrin spelling correction wanna end meta newparagraph false position start text include kind stateoftheartbrsystem one hci issues human computer end meta newparagraph false position start text interaction issues verybrconfident correction example end meta newparagraph false position start text might wanna autocorrect thatbrhappens often talked end meta newparagraph false position start text earlier example hte abrvery common misspelling end meta newparagraph false position start text slightly less confident might wannabrgive single best correction end meta newparagraph false position start text user say yes ifbrwere even less confident might wanna end meta newparagraph false position start text give user whole list let thembrpick list end meta newparagraph false position start text unconfident pretty surebrwe saw error dont know end meta newparagraph false position start text fix might flag thebruser typed error various things end meta newparagraph false position start text depending application andbrdepending probability end meta newparagraph false position start text confidence value might generatebrin practice almost noisy channel end meta newparagraph false position start text models even though define thatbrmodel multiplying prior end meta newparagraph false position start text likelihood error model practicebrthese two probabilities computed end meta newparagraph false position start text making lot independencebrassumptions many errors end meta newparagraph false position start text fact spellingbris independent neighboring words end meta newparagraph false position start text really true resultbrof incorrect independence end meta newparagraph false position start text assumptions means twobrprobabilities often commensurate end meta newparagraph false position start text fact instead justbrmultiplying two weight end meta newparagraph false position start text way since multiplyingbrprobabilities weight raising end meta newparagraph false position start text one power cantbrobviously multiply one end meta newparagraph false position start text something weight bybrraising one power lambda end meta newparagraph false position start text learn lambda developmentbrtests pick whatever lambda end meta newparagraph false position start text raise language multiplebrprobability product end meta newparagraph false position start text likely pick errorsbrthat really errors use end meta newparagraph false position start text weighting noisy channelbrmodel almost application end meta newparagraph false position start text see noisy channel model somethingbrelse thats used state art end meta newparagraph false position start text systems use spellingbrbut pronunciation word help end meta newparagraph false position start text us find errors metaphone systembrwhich used gnu aspell instead end meta newparagraph false position start text asking candidates abrinaudible similar spelling ask end meta newparagraph false position start text candidates similarbrpronunciation thats done first end meta newparagraph false position start text converting misspelling abrpronunciation metaphone end meta newparagraph false position start text simplified pronunciation system abrset rules convert word end meta newparagraph false position start text something approximating pronunciationbrand heres rules get used drop end meta newparagraph false position start text duplicate adjacent letters except cbrif word begins kn gn drop end meta newparagraph false position start text first letter drop b anbrm end word end meta newparagraph false position start text dropping various silentbrletters various rules like end meta newparagraph false position start text convert misspelling kind abrrepresentation pronunciation end meta newparagraph false position start text single vowel beginning abrset consonants find words end meta newparagraph false position start text whose pronunciation nearby thebrmisspellings pronunciation weve end meta newparagraph false position start text converted words intobrthe metaphone pronunciation find similar end meta newparagraph false position start text words score words bybrsome combination two eta distances end meta newparagraph false position start text likely candidate bebrorthographically changed end meta newparagraph false position start text misspelling well use kind ofbrchannel model like thing end meta newparagraph false position start text thing pronunciation likelybris misspelling pronounced like end meta newparagraph false position start text candidate metaphone systembrdoesnt use language model use end meta newparagraph false position start text pronunciationbased kind channel modelbrand imagine also combining end meta newparagraph false position start text pronunciationbased model noisybrchannel model modern models end meta newparagraph false position start text channel last decade allow abrnumber kind improvements like end meta newparagraph false position start text incorporating pronunciation componentbrinto channel model one end meta newparagraph false position start text might also want allow richer edits sobrnot single letter edits kinda end meta newparagraph false position start text edits like p ph incorrectly typedbras f common error end meta newparagraph false position start text es mistakenly typed asbrbut sequence ent likely end meta newparagraph false position start text mistyped ant couple ofbrdifferent improvements state end meta newparagraph false position start text art system might channelbrmodel fact could consider end meta newparagraph false position start text large number factors couldbrinfluence probability misspelling end meta newparagraph false position start text given word channel model wevebrtalked source letter end meta newparagraph false position start text target letter weve talked youbrknow maybe one surrounding letter end meta newparagraph false position start text could look surrounding letters orbrwe could look position word end meta newparagraph false position start text maybe errors happen middle ofbrthe word errors happen end end meta newparagraph false position start text might explicitly model keyboardbrand talk nearby keys end meta newparagraph false position start text keyboard homology likely tobrmistype word left hand end meta newparagraph false position start text third finger using right handbrthird finger key end meta newparagraph false position start text finger alternate hand isbrhomologous might use end meta newparagraph false position start text pronunciations might use kind ofbrlikely morpheme transformations talked end meta newparagraph false position start text last slide lots possiblebrfactors could influence proba end meta newparagraph false position start text channel model heres picture ofbrone keyboard might wanna end meta newparagraph false position start text say r w likely misbrmistypings e end meta newparagraph false position start text kind phone keyboard sobrcombining different factors end meta newparagraph false position start text often done classifierbased modelbrso classifierbased model end meta newparagraph false position start text alternative way realwordbrspelling correction instead end meta newparagraph false position start text two models channel model abrlanguage model might take two end meta newparagraph false position start text number models combinebrthem big classifier well talk end meta newparagraph false position start text classifiers next lecture forbrexample specific pair like end meta newparagraph false position start text whether weather commonly confusedbrreal word confusions might look end meta newparagraph false position start text features like well word cloudybryou know window plus minus ten end meta newparagraph false position start text words followed word two inbrsome verb end meta newparagraph false position start text word cloudy nearby im probablybrthe word weather im followed two end meta newparagraph false position start text verb im probably word whether sobrwhether go whether say whether end meta newparagraph false position start text probably whether similarly ifbrim followed im probably end meta newparagraph false position start text weather featuresbrplus language model plus channel end meta newparagraph false position start text model could combined onebrclassifier could make decision end meta newparagraph false position start text mean might build separate classifierbrfor possible likely pair words end meta newparagraph false position start text summary real words done correctionbrcan done noisy channel end meta newparagraph false position start text algorithm thats used non wordsbrspellings correction also use end meta newparagraph false position start text classifier based approach combine abrlot features build classifiers end meta newparagraph false position start text frequent kinds errors like tobrmodel explicitly title state art systems versionno versionnumber video state art systems videodescription videotitle state art systems,Course2,W2-S2-L4,W2,S2,L4,State,2,2,4,descript languag code en dir ltr name english metadata note resourceuri apipartnersvideoshlbldinibrdlanguagesensubtitl siteurl httpwwwamaraorgvideoshlbldinibrden subformat json subtitl end meta newparagraph true posit start text issu come upbrin spell correct wanna end meta newparagraph fals posit start text includ kind stateoftheartbrsystem one hci issu human comput end meta newparagraph fals posit start text interact issu verybrconfid correct exampl end meta newparagraph fals posit start text might wanna autocorrect thatbrhappen often talk end meta newparagraph fals posit start text earlier exampl hte abrveri common misspel end meta newparagraph fals posit start text slightli less confid might wannabrg singl best correct end meta newparagraph fals posit start text user say ye ifbrwer even less confid might wanna end meta newparagraph fals posit start text give user whole list let thembrpick list end meta newparagraph fals posit start text unconfid pretti surebrw saw error dont know end meta newparagraph fals posit start text fix might flag thebrus type error variou thing end meta newparagraph fals posit start text depend applic andbrdepend probabl end meta newparagraph fals posit start text confid valu might generatebrin practic almost noisi channel end meta newparagraph fals posit start text model even though defin thatbrmodel multipli prior end meta newparagraph fals posit start text likelihood error model practicebrthes two probabl comput end meta newparagraph fals posit start text make lot independencebrassumpt mani error end meta newparagraph fals posit start text fact spellingbri independ neighbor word end meta newparagraph fals posit start text realli true resultbrof incorrect independ end meta newparagraph fals posit start text assumpt mean twobrprob often commensur end meta newparagraph fals posit start text fact instead justbrmultipli two weight end meta newparagraph fals posit start text way sinc multiplyingbrprob weight rais end meta newparagraph fals posit start text one power cantbrobvi multipli one end meta newparagraph fals posit start text someth weight bybrrais one power lambda end meta newparagraph fals posit start text learn lambda developmentbrtest pick whatev lambda end meta newparagraph fals posit start text rais languag multiplebrprob product end meta newparagraph fals posit start text like pick errorsbrthat realli error use end meta newparagraph fals posit start text weight noisi channelbrmodel almost applic end meta newparagraph fals posit start text see noisi channel model somethingbrels that use state art end meta newparagraph fals posit start text system use spellingbrbut pronunci word help end meta newparagraph fals posit start text us find error metaphon systembrwhich use gnu aspel instead end meta newparagraph fals posit start text ask candid abrinaud similar spell ask end meta newparagraph fals posit start text candid similarbrpronunci that done first end meta newparagraph fals posit start text convert misspel abrpronunci metaphon end meta newparagraph fals posit start text simplifi pronunci system abrset rule convert word end meta newparagraph fals posit start text someth approxim pronunciationbrand here rule get use drop end meta newparagraph fals posit start text duplic adjac letter except cbrif word begin kn gn drop end meta newparagraph fals posit start text first letter drop b anbrm end word end meta newparagraph fals posit start text drop variou silentbrlett variou rule like end meta newparagraph fals posit start text convert misspel kind abrrepresent pronunci end meta newparagraph fals posit start text singl vowel begin abrset conson find word end meta newparagraph fals posit start text whose pronunci nearbi thebrmisspel pronunci weve end meta newparagraph fals posit start text convert word intobrth metaphon pronunci find similar end meta newparagraph fals posit start text word score word bybrsom combin two eta distanc end meta newparagraph fals posit start text like candid bebrorthograph chang end meta newparagraph fals posit start text misspel well use kind ofbrchannel model like thing end meta newparagraph fals posit start text thing pronunci likelybri misspel pronounc like end meta newparagraph fals posit start text candid metaphon systembrdoesnt use languag model use end meta newparagraph fals posit start text pronunciationbas kind channel modelbrand imagin also combin end meta newparagraph fals posit start text pronunciationbas model noisybrchannel model modern model end meta newparagraph fals posit start text channel last decad allow abrnumb kind improv like end meta newparagraph fals posit start text incorpor pronunci componentbrinto channel model one end meta newparagraph fals posit start text might also want allow richer edit sobrnot singl letter edit kinda end meta newparagraph fals posit start text edit like p ph incorrectli typedbra f common error end meta newparagraph fals posit start text es mistakenli type asbrbut sequenc ent like end meta newparagraph fals posit start text mistyp ant coupl ofbrdiffer improv state end meta newparagraph fals posit start text art system might channelbrmodel fact could consid end meta newparagraph fals posit start text larg number factor couldbrinflu probabl misspel end meta newparagraph fals posit start text given word channel model wevebrtalk sourc letter end meta newparagraph fals posit start text target letter weve talk youbrknow mayb one surround letter end meta newparagraph fals posit start text could look surround letter orbrw could look posit word end meta newparagraph fals posit start text mayb error happen middl ofbrth word error happen end end meta newparagraph fals posit start text might explicitli model keyboardbrand talk nearbi key end meta newparagraph fals posit start text keyboard homolog like tobrmistyp word left hand end meta newparagraph fals posit start text third finger use right handbrthird finger key end meta newparagraph fals posit start text finger altern hand isbrhomolog might use end meta newparagraph fals posit start text pronunci might use kind ofbrlik morphem transform talk end meta newparagraph fals posit start text last slide lot possiblebrfactor could influenc proba end meta newparagraph fals posit start text channel model here pictur ofbron keyboard might wanna end meta newparagraph fals posit start text say r w like misbrmistyp e end meta newparagraph fals posit start text kind phone keyboard sobrcombin differ factor end meta newparagraph fals posit start text often done classifierbas modelbrso classifierbas model end meta newparagraph fals posit start text altern way realwordbrspel correct instead end meta newparagraph fals posit start text two model channel model abrlanguag model might take two end meta newparagraph fals posit start text number model combinebrthem big classifi well talk end meta newparagraph fals posit start text classifi next lectur forbrexampl specif pair like end meta newparagraph fals posit start text whether weather commonli confusedbrr word confus might look end meta newparagraph fals posit start text featur like well word cloudybry know window plu minu ten end meta newparagraph fals posit start text word follow word two inbrsom verb end meta newparagraph fals posit start text word cloudi nearbi im probablybrth word weather im follow two end meta newparagraph fals posit start text verb im probabl word whether sobrwheth go whether say whether end meta newparagraph fals posit start text probabl whether similarli ifbrim follow im probabl end meta newparagraph fals posit start text weather featuresbrplu languag model plu channel end meta newparagraph fals posit start text model could combin onebrclassifi could make decis end meta newparagraph fals posit start text mean might build separ classifierbrfor possibl like pair word end meta newparagraph fals posit start text summari real word done correctionbrcan done noisi channel end meta newparagraph fals posit start text algorithm that use non wordsbrspel correct also use end meta newparagraph fals posit start text classifi base approach combin abrlot featur build classifi end meta newparagraph fals posit start text frequent kind error like tobrmodel explicitli titl state art system versionno versionnumb video state art system videodescript videotitl state art system,[ 6  4 14 13 12]
144,Course2_W3-S1-L1_What_is_Text_Classification_8-12,lecture well introduce topic text classification naive bayes algorithm one important ways text classification lets begin looking examples text classification applications ive shown email actually received day know email spam take look mail think features might automatically extract email tells spam might notice word greats mis misspelling great typo maybe might notice important notice maybe exclamation point pretty rare universities put exclamation points subject headers might notice theres dan addressed particular undisclosed recipients theres particular address urls little funny thats stanford url maybe word exciting features combined classified give us evidence got piece spam another important text classification class authorship attribution know author wrote piece text one famous examples authorship attribution famous anonymous essays called federalist papers written beginning history country part convince state new york ratify early constitution three authors wrote various numbers letters twelve letters wasnt clear author wrote mosteller wallace show bayesian methods able distinguish letters written madison letters written hamilton bayesian methods used gave rise naive base method going talking today another text classification task gender identification determining author male female recent research gender identification shown look number pronouns features number determiners number noun phrases subtlety indicative difference male female writers female writers tend use pronouns male writers tend use facts determiners noun phrases see lot pronouns lot determiners factual sentences copula verbs might determine fact male female would correct author margaret drabble author anthony grey another text classification task sentiment analysis one classic sentiment analysis tasks movie review identification given review whether movie product tell whether review positive negative although im gonna show example movies apply product review product service might find web actually important commercial application suppose saw review said unbelievably disappointing well thats clearly negative review howbout full zany characters richly applied satire positive howbout greatest screwball comedy ever filmed weve got words like greatest greatest ever thats positive howbout pathetic worst part boxing scenes weve got evidence like pathetic worst tell us fact negative review text classification often also apply text classification scientific articles example deciding topic particular article data base like inaudible line might example might decide know automatically indexing article various subjects antagonist blood supply drug therapy epidemiology apply particular article thats written thats data base summary text classification task assigning kind topic category piece text could subject categories kind online data base could detecting spam could choosing author set authors choosing gender maybe age want find young writers old writers telling language inaudible text written one language versus another language important application sentimentals examples text classification lets define task text classification input document fixed set classes set c j classes c c til cj goal given document set classes predict class c set classes job take document assign class document simplest possible text classification method use hand return rules example spam detection might list bad email addresses blacklists people probably spammers might look phrases like millions dollars selected good indications spam rules carefully refined expert get high accuracy hand written rules general building maintaining rules expensive although hand coded rules often used part system text classification generally combine important method machine learning method supervised machine learning supervised machine learning document fixed set classes need one thing need training set la documents hand labeled class document one know class one document two class maybe document label class document given document set classes fixed training set hand labeled documents goal machine learning produce classifier well using gamma refer classifier gammas function given new document give us class given set training labels documents classes well learn classifier maps document class theres lots kinds machine learning classifiers going talk today naive bayes well see well look later course well talk logistic regression well touch kinds classifiers like support vector machines called svms inaudible neighbors lots classifiers matter classifier use task text classification take document text kinds features extract features represent document build classifier tell us class document belongs,Course2,W3-S1-L1,W3,S1,L1,What,3,1,1,lectur well introduc topic text classif naiv bay algorithm one import way text classif let begin look exampl text classif applic ive shown email actual receiv day know email spam take look mail think featur might automat extract email tell spam might notic word great mi misspel great typo mayb might notic import notic mayb exclam point pretti rare univers put exclam point subject header might notic there dan address particular undisclos recipi there particular address url littl funni that stanford url mayb word excit featur combin classifi give us evid got piec spam anoth import text classif class authorship attribut know author wrote piec text one famou exampl authorship attribut famou anonym essay call federalist paper written begin histori countri part convinc state new york ratifi earli constitut three author wrote variou number letter twelv letter wasnt clear author wrote mostel wallac show bayesian method abl distinguish letter written madison letter written hamilton bayesian method use gave rise naiv base method go talk today anoth text classif task gender identif determin author male femal recent research gender identif shown look number pronoun featur number determin number noun phrase subtleti indic differ male femal writer femal writer tend use pronoun male writer tend use fact determin noun phrase see lot pronoun lot determin factual sentenc copula verb might determin fact male femal would correct author margaret drabbl author anthoni grey anoth text classif task sentiment analysi one classic sentiment analysi task movi review identif given review whether movi product tell whether review posit neg although im gonna show exampl movi appli product review product servic might find web actual import commerci applic suppos saw review said unbeliev disappoint well that clearli neg review howbout full zani charact richli appli satir posit howbout greatest screwbal comedi ever film weve got word like greatest greatest ever that posit howbout pathet worst part box scene weve got evid like pathet worst tell us fact neg review text classif often also appli text classif scientif articl exampl decid topic particular articl data base like inaud line might exampl might decid know automat index articl variou subject antagonist blood suppli drug therapi epidemiolog appli particular articl that written that data base summari text classif task assign kind topic categori piec text could subject categori kind onlin data base could detect spam could choos author set author choos gender mayb age want find young writer old writer tell languag inaud text written one languag versu anoth languag import applic sentiment exampl text classif let defin task text classif input document fix set class set c j class c c til cj goal given document set class predict class c set class job take document assign class document simplest possibl text classif method use hand return rule exampl spam detect might list bad email address blacklist peopl probabl spammer might look phrase like million dollar select good indic spam rule care refin expert get high accuraci hand written rule gener build maintain rule expens although hand code rule often use part system text classif gener combin import method machin learn method supervis machin learn supervis machin learn document fix set class need one thing need train set la document hand label class document one know class one document two class mayb document label class document given document set class fix train set hand label document goal machin learn produc classifi well use gamma refer classifi gamma function given new document give us class given set train label document class well learn classifi map document class there lot kind machin learn classifi go talk today naiv bay well see well look later cours well talk logist regress well touch kind classifi like support vector machin call svm inaud neighbor lot classifi matter classifi use task text classif take document text kind featur extract featur repres document build classifi tell us class document belong,[ 2  4  5  0 14]
145,Course2_W3-S1-L2_Naive_Bayes_3-19,naive bayes algorithm one important algorithms text classification intuition naive bayes algorithm really quite simple based bayes rule well see second relies simple representation document called bag words representation lets see intuition bag words representation imagine document says love movie sweet satirical humor job build function gamma takes document returns class class could positive class could negative case sentiment analysis positive negative order solve task one thing might look individual words document like love satirical great might look words kinds text classification gonna look word gonna look every single word cases well look subset look subset might imagine document looks something like looks like word love word satirical word great words disappeared whether use subset words words document bag words representation loses information order words document represent document set words occurred accounts example previous document might represent document vector words great love recommend laugh happy one account great occurred twice love occurred twice recommend occurred keep words document well often keep words document idea words particularly indicative cues idea bag words representation gonna represent document list words counts throw away everything else document order words occurred font anything else function function gamma classifier take representation assign us class positive negative applies ive shown two class problem sentiment analysis positive negative sentiment applies sorts document classification tasks might document need classify different computer science topic im building online library computer science papers im giving advice computer science topics text document words like parser language label translation wanna know aspect computer science go file paper automatically good text classifier automatically figure thats thats natural language processing paper thats intuition naive base classifier,Course2,W3-S1-L2,W3,S1,L2,Naive,3,1,2,naiv bay algorithm one import algorithm text classif intuit naiv bay algorithm realli quit simpl base bay rule well see second reli simpl represent document call bag word represent let see intuit bag word represent imagin document say love movi sweet satir humor job build function gamma take document return class class could posit class could neg case sentiment analysi posit neg order solv task one thing might look individu word document like love satir great might look word kind text classif gonna look word gonna look everi singl word case well look subset look subset might imagin document look someth like look like word love word satir word great word disappear whether use subset word word document bag word represent lose inform order word document repres document set word occur account exampl previou document might repres document vector word great love recommend laugh happi one account great occur twice love occur twice recommend occur keep word document well often keep word document idea word particularli indic cue idea bag word represent gonna repres document list word count throw away everyth els document order word occur font anyth els function function gamma classifi take represent assign us class posit neg appli ive shown two class problem sentiment analysi posit neg sentiment appli sort document classif task might document need classifi differ comput scienc topic im build onlin librari comput scienc paper im give advic comput scienc topic text document word like parser languag label translat wanna know aspect comput scienc go file paper automat good text classifi automat figur that that natur languag process paper that intuit naiv base classifi,[ 2  4 14 13 12]
146,Course2_W3-S1-L3_Formalizing_the_Naive_Bayes_Classifier_9-28,lets formalize naive based classifier classification document class c goal compute probability class proba conditional probability given document gonna see gonna use probability pick best class compute probability class given document bayes rule equal probability document given class times probability class probability document lets see use classifier best class maximum inaudible class class looking assign document classes one maximizes probability class given document looking class whose probability given document greatest bayes rule thats whichever class maximizes probability c given also maximizes equation probability given c probability class probability document traditional inaudible classification whichever document whichever excuse whichever class maximizes equation also maximizes equation weve done dropped denominator crossed denominator okay cross denominator probability likely document give document say ten classes document belong classes im computing probability document given class probability class probability document document probably document identical ten classes class one time compute probability document means im comparing ten things divided probability document probability document constant eliminate likely class camp class maximizes product two probabilities probability document given class well call likelihood sound probability class well call prior prior probability class likely class one maximizes product two probabilities probability class turn relatively simple compute mean probability document given class mean say particular movie review likely isnt given class positive seems like complicated confusing things compute one way operationalize say lets represent document whole set features x one x n say probability document given class im gonna im gonna say means probability vector features given class p given c going represent probability joint probability x x xn given class words representing document set features x xn still doesnt tell compute probability itll start lets talk two pieces compute probability class well really thats asking often class occur positive reviews much common negative reviews madison much frequent author computing probability class done counting relative frequencies corpus data set probability class relatively easy compute likelihood document features document given class well theres lot parameters probability theres theres n different features certain length thats lot parameters computed compute one class thats thats far many parameters could possibly computer estimate number huge number trainee examples easy dont enormous amount trainee examples gonna make simplifying assumptions ibased classifier make computation possible first simplifying assumption going make called bag words assumption going assume position document doesnt matter gave intuition slides ago position doc word document whether first word seventh word one hundred fiftieth word isnt going matter care word feature occurs second thing gonna second assumption gonna make gonna assume different features x x x probabilities independent given class whether one feature occurs given class whether another feature occurs given class independently gonna true course assumptions incorrect simplifying assumptions theyre theyre absolutely wrong theyre theyre theyre terribly completely true nonetheless making simplifying incorrect simplifying assumptions make problems much simpler practice able solve problem high degree accuracy despite simplifications result two simplifying assumptions gonna represent probability joint probability whole set features x x condition class product whole bunch independent probabilities probability x given class probability x given class probability x given class probability xn given class going multiply together going care x wh position occurred care particular order feature going care dependencies x x words order compute simplifying ebased assumption compute likely class multiplying likelihood probability whole whole joint string features times prior probability class gonna simplify say best class naive bayes assumption class maximizes prior probability class thats simply gonna multiply every feature set features probability feature given class much simpler equation sound looking specifically text first lets look gonna assume gonna look positions word positions text document text document words position word number one position number two position number three gonna take look classes class gonna say whats probability class class gonna walk every position text position gonna look word position ask whats probability given class im looking well class one well compute p class one times product p word given class one well compute well class two class two well compute p class two product overall positions p word given class two going pick whichever two highest higher going pick class two assign document higher well assign class one document course ive shown two classes general true number classes thats formalization ie based classifier,Course2,W3-S1-L3,W3,S1,L3,Formalizing,3,1,3,let formal naiv base classifi classif document class c goal comput probabl class proba condit probabl given document gonna see gonna use probabl pick best class comput probabl class given document bay rule equal probabl document given class time probabl class probabl document let see use classifi best class maximum inaud class class look assign document class one maxim probabl class given document look class whose probabl given document greatest bay rule that whichev class maxim probabl c given also maxim equat probabl given c probabl class probabl document tradit inaud classif whichev document whichev excus whichev class maxim equat also maxim equat weve done drop denomin cross denomin okay cross denomin probabl like document give document say ten class document belong class im comput probabl document given class probabl class probabl document document probabl document ident ten class class one time comput probabl document mean im compar ten thing divid probabl document probabl document constant elimin like class camp class maxim product two probabl probabl document given class well call likelihood sound probabl class well call prior prior probabl class like class one maxim product two probabl probabl class turn rel simpl comput mean probabl document given class mean say particular movi review like isnt given class posit seem like complic confus thing comput one way operation say let repres document whole set featur x one x n say probabl document given class im gonna im gonna say mean probabl vector featur given class p given c go repres probabl joint probabl x x xn given class word repres document set featur x xn still doesnt tell comput probabl itll start let talk two piec comput probabl class well realli that ask often class occur posit review much common neg review madison much frequent author comput probabl class done count rel frequenc corpu data set probabl class rel easi comput likelihood document featur document given class well there lot paramet probabl there there n differ featur certain length that lot paramet comput comput one class that that far mani paramet could possibl comput estim number huge number traine exampl easi dont enorm amount traine exampl gonna make simplifi assumpt ibas classifi make comput possibl first simplifi assumpt go make call bag word assumpt go assum posit document doesnt matter gave intuit slide ago posit doc word document whether first word seventh word one hundr fiftieth word isnt go matter care word featur occur second thing gonna second assumpt gonna make gonna assum differ featur x x x probabl independ given class whether one featur occur given class whether anoth featur occur given class independ gonna true cours assumpt incorrect simplifi assumpt theyr theyr absolut wrong theyr theyr theyr terribl complet true nonetheless make simplifi incorrect simplifi assumpt make problem much simpler practic abl solv problem high degre accuraci despit simplif result two simplifi assumpt gonna repres probabl joint probabl whole set featur x x condit class product whole bunch independ probabl probabl x given class probabl x given class probabl x given class probabl xn given class go multipli togeth go care x wh posit occur care particular order featur go care depend x x word order comput simplifi ebas assumpt comput like class multipli likelihood probabl whole whole joint string featur time prior probabl class gonna simplifi say best class naiv bay assumpt class maxim prior probabl class that simpli gonna multipli everi featur set featur probabl featur given class much simpler equat sound look specif text first let look gonna assum gonna look posit word posit text document text document word posit word number one posit number two posit number three gonna take look class class gonna say what probabl class class gonna walk everi posit text posit gonna look word posit ask what probabl given class im look well class one well comput p class one time product p word given class one well comput well class two class two well comput p class two product overal posit p word given class two go pick whichev two highest higher go pick class two assign document higher well assign class one document cours ive shown two class gener true number class that formal ie base classifi,[ 2  4  5 14 13]
147,Course2_W3-S1-L4_Naive_Bayes-_Learning_5-22,learn perimeters naive bayes simplest way multinomial naive bayes model use maximum likelihood estimates simplest use frequencies data trying compute prior probably particular document class j count documents documents many documents class j thats prior random documents class j likelihood probability word wi given class j count number times wordui occurs documents class ju normalize total number words class j sum words normalise total number words vocabulary words documents class j many particular words looking going compute fraction times word wi appears among words document topic cj going creating kind mega document topic j concatenating documents topic together use frequency w document sentiment analysis might documents positive mega document positive documents gonna concatenate altogether big mega document might docneg lying dont fact use maximum likelihood naive bayes reason following imaging looking word fantastic introduce word fantastic might occurred test set happens appear training set topic positive probability fantastic class positive training set maximum likelihood count fantastic occur positive normalise sum counts words positive fantastic never occurs count zero maximum likelihood estimate likelihood fantastic given positive zero bad zero probabilities never conditioned away looking likely class thats argmax classes prior times likelihood one likelihood terms zero whole thing zero never gonna pick class solution simple add one smoothing computation without smoothing add one smoothing simply add one counts add one count every time count numerator every time count occurs denominator add one rewrite form seen take total number word class c add vocabulary size add one every vocabulary denominator classic laplace add smoothing lets walk calculation parameters first training corpus gonna extract vocabulary list words next gonna calculate priors class j class gonna get listset documents class cj number documents set divided total number give us prior probability particular class likelihood gonna want compute likelihood every wk given every topic cj first gonna create mega document concatenating documents called text j word wk vocabulary gonna count times wk occurs mega document text j nk probabilitythe likelihood word wk class cj add one smooth shown add alpha smooth version naive bayes algorithm added alpha nk denominator n total number tokens class j unknown words simple way deal unknown words add one extra word vocabulary might call wuu unknown word u likelihood equation likelihood equation written new word wuu increased vocabulary size oneu count wuu particular class cu well word unknown never occurred training set count zero likelihood term unknown word gonna simply nthats number tokens class c plus v unknown word gonna modeled simplistic model probability unknown word learning parameters naive bayes simple computation prior slightly complex computation likelihood use add smoothing deal unknown words,Course2,W3-S1-L4,W3,S1,L4,Naive,3,1,4,learn perimet naiv bay simplest way multinomi naiv bay model use maximum likelihood estim simplest use frequenc data tri comput prior probabl particular document class j count document document mani document class j that prior random document class j likelihood probabl word wi given class j count number time wordui occur document class ju normal total number word class j sum word normalis total number word vocabulari word document class j mani particular word look go comput fraction time word wi appear among word document topic cj go creat kind mega document topic j concaten document topic togeth use frequenc w document sentiment analysi might document posit mega document posit document gonna concaten altogeth big mega document might docneg lie dont fact use maximum likelihood naiv bay reason follow imag look word fantast introduc word fantast might occur test set happen appear train set topic posit probabl fantast class posit train set maximum likelihood count fantast occur posit normalis sum count word posit fantast never occur count zero maximum likelihood estim likelihood fantast given posit zero bad zero probabl never condit away look like class that argmax class prior time likelihood one likelihood term zero whole thing zero never gonna pick class solut simpl add one smooth comput without smooth add one smooth simpli add one count add one count everi time count numer everi time count occur denomin add one rewrit form seen take total number word class c add vocabulari size add one everi vocabulari denomin classic laplac add smooth let walk calcul paramet first train corpu gonna extract vocabulari list word next gonna calcul prior class j class gonna get listset document class cj number document set divid total number give us prior probabl particular class likelihood gonna want comput likelihood everi wk given everi topic cj first gonna creat mega document concaten document call text j word wk vocabulari gonna count time wk occur mega document text j nk probabilityth likelihood word wk class cj add one smooth shown add alpha smooth version naiv bay algorithm ad alpha nk denomin n total number token class j unknown word simpl way deal unknown word add one extra word vocabulari might call wuu unknown word u likelihood equat likelihood equat written new word wuu increas vocabulari size oneu count wuu particular class cu well word unknown never occur train set count zero likelihood term unknown word gonna simpli nthat number token class c plu v unknown word gonna model simplist model probabl unknown word learn paramet naiv bay simpl comput prior slightli complex comput likelihood use add smooth deal unknown word,[ 2  4 14 13 12]
148,Course2_W3-S1-L5_Naive_Bayes-_Relationship_to_Language_Modeling_4-35,turns naive bayes close relationship language modeling lets see well start looking generative model multinomial naive bayes imagine class lets say china imagine randomly generating document china might start saying well first word except one shanghai second word third word shung jung forth word issue fifth word bounce weve generated little document random little document china generative model shows word independently generated word class generated certain probability little set probabilities keeping word lets think general naive bayes classifiers use sorts features urls email addresses well talk spam detection previous slide use word features use words text turns generative model naive bayes gives important similarity language model naive bayes turns kind language model particular naive bayes class naive bayes classifier class unigram language model mean way think word naive bayes classifier likelihood term assigns word probability word given class sentence naive bayes classifier since multiplying together sentence even even whole document since multiplying together probabilities words compute probability sentence given class multiplying together words likelihoods words class lets see works imagine class positive likelihoods likelihood given positive thats p given positive p love given positive p given positive p given positive p love given positive okay heres naive bayes classifier well think exactly language model sequence words generate words love fun film naive bayes assigning sequence words set probabilities one word class multiplied together get probability sentence naive bayes class unigram language model conditioned class ask question class assigns higher probability document like running two separate language models ive shown two separate language models positive class negative class one separate probabilities heres probability given negative heres probability given positive guess people likely use word dont like something take particular sentence love fun film say probabilities sequence proba whats probability sequence according first model whats probability according second model one assigns probability word thats naive bayes likelihoods multiply together show multiply together sort see inspection positive probabilities multiplied together gonna become mimics one one gonna much higher negative probabilities think naive bayes class separate class condition language model gonna run language model compute likelihood test sentence well pick whichever language model higher probability class likely class thats close relationship naive based language modeling,Course2,W3-S1-L5,W3,S1,L5,Naive,3,1,5,turn naiv bay close relationship languag model let see well start look gener model multinomi naiv bay imagin class let say china imagin randomli gener document china might start say well first word except one shanghai second word third word shung jung forth word issu fifth word bounc weve gener littl document random littl document china gener model show word independ gener word class gener certain probabl littl set probabl keep word let think gener naiv bay classifi use sort featur url email address well talk spam detect previou slide use word featur use word text turn gener model naiv bay give import similar languag model naiv bay turn kind languag model particular naiv bay class naiv bay classifi class unigram languag model mean way think word naiv bay classifi likelihood term assign word probabl word given class sentenc naiv bay classifi sinc multipli togeth sentenc even even whole document sinc multipli togeth probabl word comput probabl sentenc given class multipli togeth word likelihood word class let see work imagin class posit likelihood likelihood given posit that p given posit p love given posit p given posit p given posit p love given posit okay here naiv bay classifi well think exactli languag model sequenc word gener word love fun film naiv bay assign sequenc word set probabl one word class multipli togeth get probabl sentenc naiv bay class unigram languag model condit class ask question class assign higher probabl document like run two separ languag model ive shown two separ languag model posit class neg class one separ probabl here probabl given neg here probabl given posit guess peopl like use word dont like someth take particular sentenc love fun film say probabl sequenc proba what probabl sequenc accord first model what probabl accord second model one assign probabl word that naiv bay likelihood multipli togeth show multipli togeth sort see inspect posit probabl multipli togeth gonna becom mimic one one gonna much higher neg probabl think naiv bay class separ class condit languag model gonna run languag model comput likelihood test sentenc well pick whichev languag model higher probabl class like class that close relationship naiv base languag model,[ 4  2 14 13 12]
149,Course2_W3-S1-L6_Multinomial_Naive_Bayes-_A_Worked_Example_8-58,lets walk detailed worked example multi numeral naive based example put equations naive based probability class number documents class total number documents likely hood word given class well simple add moving count word going class divided count words class add one smoothing lets assume four training documents one two three four one test document documents simplified obviously documents document three words chinese beijing chinese document three words chinese chinese shanghai lets say two classes chinese japanese job asian news topic classification three training documents class china one class japan question whats class test document right lets computation first thing going compute primers need compute p c code class china p j class japan p c many times class chinese occur training set three many total classes trade documents training set four three four training documents thats three four china probabil prior probability document topic china three four japan well theres one document training set japan four documents probability japan onequarter weve seen compute prior probabilities lets move likelihoods lets compute probabilities whats probability word chinese given class c whats probability word chinese given class j alright probability chinese many times word chinese occur training set documents class china okay word chinese many times occur three documents one two three four five five add one inaudible adding one many total words training set one two three four five six seven eight chinese class gonna add vocabulary size v add one smoothing add vocabulary size v vocabulary six one two three four five six thats five plus oneeighth plus six threesevenths well thing word tokyo tokyo doesnt occur three classes count zero add moving add one thats denominator number total words vocabulary size chinese class would one fourteen exact thing true word japan doesnt occur training set lets turn class j japan word chinese occur poss one add one smoothing divide count words class theres three words meta document one document vocabulary size weve got two nines compute next two numbers way priors conditional probabilities ready decide class likely test document test document well call document five need compute prior likelihoods prior three quarters prior chinese document five three quarters thats prior chinese times word chinese occurs three times one two three one gets probability probability chinese given three three sevenths three sevenths threesevenths word tokyo occurs thats got probability onefourteenth word japan onefourth multiply together get approximately notice say probability proportional product computing argmax two classes computing class maximizes product two probabilities actual probability denominator would divided p document skipping part actually computing probability computing numerator probability since document five p document five chinese japanese classes gonna compute proportional equal sign good probability document class japanese given document five prior document japanese multiply probability chinese given class japanese thats twoninths times twoninths times twoninths probability tokyo given japanese thats another twoninths japan given japanese another twoninths get slightly lower probability sound example model would choose class document would choose chinese bigger weve talked naive bays using word feature using words lots examples naive bays gonna use richer features words gonna use specific kind words things spam assassins naive bays classified spam detection looks things like phrase generic viagra mentioned phrase online pharmacy mentioned little regular expression millions dollars mentioned phrase impress nearby word girl firm lot numbers subject caps sorts different features use combine look spamassassin website see common set features summary naive bayes actually naive fast algorithm low storage requirements pretty robust irrelevant features tend cancel works well domains lots equally important features turns problem classifiers particular decisions trees advantages numeric domains dont work well lots features equally important independence assumptions hold assumed independence correct turns naive bayes fact optimal classifier problem course thats rare independence assumptions really true happen true close true turns naive bayes fact optimal general naive bayes good dependable baseline text classification although see classifiers give much better,Course2,W3-S1-L6,W3,S1,L6,Multinomial,3,1,6,let walk detail work exampl multi numer naiv base exampl put equat naiv base probabl class number document class total number document like hood word given class well simpl add move count word go class divid count word class add one smooth let assum four train document one two three four one test document document simplifi obvious document document three word chines beij chines document three word chines chines shanghai let say two class chines japanes job asian news topic classif three train document class china one class japan question what class test document right let comput first thing go comput primer need comput p c code class china p j class japan p c mani time class chines occur train set three mani total class trade document train set four three four train document that three four china probabil prior probabl document topic china three four japan well there one document train set japan four document probabl japan onequart weve seen comput prior probabl let move likelihood let comput probabl what probabl word chines given class c what probabl word chines given class j alright probabl chines mani time word chines occur train set document class china okay word chines mani time occur three document one two three four five five add one inaud ad one mani total word train set one two three four five six seven eight chines class gonna add vocabulari size v add one smooth add vocabulari size v vocabulari six one two three four five six that five plu oneeighth plu six threeseventh well thing word tokyo tokyo doesnt occur three class count zero add move add one that denomin number total word vocabulari size chines class would one fourteen exact thing true word japan doesnt occur train set let turn class j japan word chines occur poss one add one smooth divid count word class there three word meta document one document vocabulari size weve got two nine comput next two number way prior condit probabl readi decid class like test document test document well call document five need comput prior likelihood prior three quarter prior chines document five three quarter that prior chines time word chines occur three time one two three one get probabl probabl chines given three three seventh three seventh threeseventh word tokyo occur that got probabl onefourteenth word japan onefourth multipli togeth get approxim notic say probabl proport product comput argmax two class comput class maxim product two probabl actual probabl denomin would divid p document skip part actual comput probabl comput numer probabl sinc document five p document five chines japanes class gonna comput proport equal sign good probabl document class japanes given document five prior document japanes multipli probabl chines given class japanes that twoninth time twoninth time twoninth probabl tokyo given japanes that anoth twoninth japan given japanes anoth twoninth get slightli lower probabl sound exampl model would choos class document would choos chines bigger weve talk naiv bay use word featur use word lot exampl naiv bay gonna use richer featur word gonna use specif kind word thing spam assassin naiv bay classifi spam detect look thing like phrase gener viagra mention phrase onlin pharmaci mention littl regular express million dollar mention phrase impress nearbi word girl firm lot number subject cap sort differ featur use combin look spamassassin websit see common set featur summari naiv bay actual naiv fast algorithm low storag requir pretti robust irrelev featur tend cancel work well domain lot equal import featur turn problem classifi particular decis tree advantag numer domain dont work well lot featur equal import independ assumpt hold assum independ correct turn naiv bay fact optim classifi problem cours that rare independ assumpt realli true happen true close true turn naiv bay fact optim gener naiv bay good depend baselin text classif although see classifi give much better,[ 2  4  5 14 13]
150,Course2_W3-S1-L7_Precision_Recall_and_the_F_measure_16-16,okay let talk evaluate text categorization im gonna come back concepts precision recall introduced informally define formally show get put together combine measure f measure gets applied text classification isnt applied text classification youll see concepts coming back ways evaluation tasks natural language processing starting point understanding measures following two two contingency table particular piece data evaluating essentially four states one axis choosing whether piece data correctly belongs class whether doesnt correctly belong class example wanting decide whether piece email spam either spam correct class correct class axis describing truth weve done built system tries detect truth secondly going look status system system could saying piece data spam could saying spam go take four things look four possibilities occur looking spam one possibility truth spam said spam thats referred true positive another possibility system thought wasnt spam even though actually thats false negative treated negative falsely side dial possible piece email wasnt spam case two possibilities system mistakenly thought spam lets say false positive classifying something positively wrongly possibility wasnt piece spam system said wasnt piece spam dealing true naked inaudible sound okay kind classification two classes perhaps equally common email spam non spam seems reasonable thing look accuracy accuracy ones count youre getting answer correct ones wanna work accuracy accuracy equals true positives plus true negatives four classes true positives plus false positives plus false negatives plus true negatives many applications accuracy useful measure systems theres particular scenario useful measure scenario youre dealing things uncommon example suppose wanting detect mentions shoe brands web pages correct class something shoe brand class anything else well youre looking random web pages tweets something looking mentions shoe brands probably dont know percent words encounter gonna name shoe brands mean likely mean perhaps total lets say ten mentions shoe brand word sample thousand nine hundred ninety words shoe brands well build classifier presumably even mediocre classifier going say words shoe brand go straight limit case one possibility classifier could say word shoe brand doesnt select words shoe brands says words shoe brands means number true negatives thousand nine hundred ninety false negatives shoe brands number ten actually work accuracy system youll see accuracy system sound percent accurate system system done precisely nothing easy system write write one line code says return false token youre done yet accuracy amazing whats clearly missing deal playing wanted wanted detect shoe brands whats important us ten tokens instances shoe brands want come evaluation metric much focused detecting words names shoe brands thats matrix precision recall tells things selecting correct things precision saying things selecting row percentage correct things column precision number true positives things selected true positives plus false positives recall opposite measure recall well say things correct percentage find recall numerator true positives time denominator true positives plus selected false negatives note looking measures solves problem last time shoe brands fact token true negatives effect measures would happen previous case ten tokens zero tokens since classifier said every word wasnt shoe brand wed find wed say recall system zero finding none wanted find basically say system zero recall isnt interesting suggests weve actually better returning stuff wed like label things shoe brands could revise system revise system maybe well well return things name shoes perhaps find eight shoe brands itll make mistakes claim things shoe brands arent therell tokens therell two tokens remainder tokens point say well recall system pretty good finding eight ten instances shoe brands recall problem recall came cost also claiming lots things shoe brands arent precision system eight eight plus equals eight equals twenty percent one five things returns correct lot practical applications kind precision going low depends whether youre really interested finding references shoe brands prepared human go check individually want something automatic twenty percent low precision see secret tradeoff balances recall precision cuz almost inevitably youre going increase recall find instances something youre going make mistakes precision going go try boost recall precision going starting drop people trading precision recall thats good thing two measures discuss tradeoff say important someone precision whats returned versus important find stuff high recall thats trade played differently different applications various applications things like legal applications want find appropriate evidence discovery procedures really really want system thats high recall finds much relevant stuff possible contexts youre going show sampling stuff user might interested stuff thats shown user stuff correct looks good precision high doesnt really matter youre showing user onetenth things satisfy query sometimes explicit tradeoff really useful really good concept remember youre building inaudible systems cuz practice whenever youre building one systems youre choosing tradeoff point much youre emphasizing precision recall different tradeoff points appropriate different applications sometimes thats slightly annoying thing two measures people wanting compare systems say one better need way compare find measures thats thought well sound standard way thats proposed combine measures something thats called f measure disciplines like information retrieval name density recognition f measure f measure neither less weighted harmonic mean point kind revise little bit math means doubtless remember arithmetic mean average two things perhaps also remember geometric mean addition theres harmonic mean harmonic mean take reciprocal two quantities add take reciprocal work detail harmonic mean ends conservative average put two numbers harmonic mean fairly close minimum two numbers completely minimum nearer minimum either arithmetic geometric means particular f measure allowed put weights two terms alpha factor weighting much pay attention precision much pay attention recall application know precision much important actually express utility tradeoff putting particular value alpha expresses formulation clearly shows f measure weighted harmonic mean actually thats formulation normally see books formulation normally see books actually one guess little bit tidier write two formulations related work one exercises work relationship alpha beta effectively formula also expresses weighted harmonic mean also control parameter beta much emphasis put precision versus recall inaudible control parameter useful absence evidence mostly commonly used thing balanced f measure balanced f measure gets referred f measure thats meaning youre setting theta equal one gives equal balance precision recall corresponds turn alpha half balance def measure formula simplifies formula really thing youll see common dont remember rest stuff remember little formula f measure two times precision times recall divided sum precision recall give good sense measures precision record combined f measure theyre useful use evaluate text classification,Course2,W3-S1-L7,W3,S1,L7,Precision,3,1,7,okay let talk evalu text categor im gonna come back concept precis recal introduc inform defin formal show get put togeth combin measur f measur get appli text classif isnt appli text classif youll see concept come back way evalu task natur languag process start point understand measur follow two two conting tabl particular piec data evalu essenti four state one axi choos whether piec data correctli belong class whether doesnt correctli belong class exampl want decid whether piec email spam either spam correct class correct class axi describ truth weve done built system tri detect truth secondli go look statu system system could say piec data spam could say spam go take four thing look four possibl occur look spam one possibl truth spam said spam that refer true posit anoth possibl system thought wasnt spam even though actual that fals neg treat neg fals side dial possibl piec email wasnt spam case two possibl system mistakenli thought spam let say fals posit classifi someth posit wrongli possibl wasnt piec spam system said wasnt piec spam deal true nake inaud sound okay kind classif two class perhap equal common email spam non spam seem reason thing look accuraci accuraci one count your get answer correct one wanna work accuraci accuraci equal true posit plu true neg four class true posit plu fals posit plu fals neg plu true neg mani applic accuraci use measur system there particular scenario use measur scenario your deal thing uncommon exampl suppos want detect mention shoe brand web page correct class someth shoe brand class anyth els well your look random web page tweet someth look mention shoe brand probabl dont know percent word encount gonna name shoe brand mean like mean perhap total let say ten mention shoe brand word sampl thousand nine hundr nineti word shoe brand well build classifi presum even mediocr classifi go say word shoe brand go straight limit case one possibl classifi could say word shoe brand doesnt select word shoe brand say word shoe brand mean number true neg thousand nine hundr nineti fals neg shoe brand number ten actual work accuraci system youll see accuraci system sound percent accur system system done precis noth easi system write write one line code say return fals token your done yet accuraci amaz what clearli miss deal play want want detect shoe brand what import us ten token instanc shoe brand want come evalu metric much focus detect word name shoe brand that matrix precis recal tell thing select correct thing precis say thing select row percentag correct thing column precis number true posit thing select true posit plu fals posit recal opposit measur recal well say thing correct percentag find recal numer true posit time denomin true posit plu select fals neg note look measur solv problem last time shoe brand fact token true neg effect measur would happen previou case ten token zero token sinc classifi said everi word wasnt shoe brand wed find wed say recal system zero find none want find basic say system zero recal isnt interest suggest weve actual better return stuff wed like label thing shoe brand could revis system revis system mayb well well return thing name shoe perhap find eight shoe brand itll make mistak claim thing shoe brand arent therel token therel two token remaind token point say well recal system pretti good find eight ten instanc shoe brand recal problem recal came cost also claim lot thing shoe brand arent precis system eight eight plu equal eight equal twenti percent one five thing return correct lot practic applic kind precis go low depend whether your realli interest find refer shoe brand prepar human go check individu want someth automat twenti percent low precis see secret tradeoff balanc recal precis cuz almost inevit your go increas recal find instanc someth your go make mistak precis go go tri boost recal precis go start drop peopl trade precis recal that good thing two measur discuss tradeoff say import someon precis what return versu import find stuff high recal that trade play differ differ applic variou applic thing like legal applic want find appropri evid discoveri procedur realli realli want system that high recal find much relev stuff possibl context your go show sampl stuff user might interest stuff that shown user stuff correct look good precis high doesnt realli matter your show user onetenth thing satisfi queri sometim explicit tradeoff realli use realli good concept rememb your build inaud system cuz practic whenev your build one system your choos tradeoff point much your emphas precis recal differ tradeoff point appropri differ applic sometim that slightli annoy thing two measur peopl want compar system say one better need way compar find measur that thought well sound standard way that propos combin measur someth that call f measur disciplin like inform retriev name densiti recognit f measur f measur neither less weight harmon mean point kind revis littl bit math mean doubtless rememb arithmet mean averag two thing perhap also rememb geometr mean addit there harmon mean harmon mean take reciproc two quantiti add take reciproc work detail harmon mean end conserv averag put two number harmon mean fairli close minimum two number complet minimum nearer minimum either arithmet geometr mean particular f measur allow put weight two term alpha factor weight much pay attent precis much pay attent recal applic know precis much import actual express util tradeoff put particular valu alpha express formul clearli show f measur weight harmon mean actual that formul normal see book formul normal see book actual one guess littl bit tidier write two formul relat work one exercis work relationship alpha beta effect formula also express weight harmon mean also control paramet beta much emphasi put precis versu recal inaud control paramet use absenc evid mostli commonli use thing balanc f measur balanc f measur get refer f measur that mean your set theta equal one give equal balanc precis recal correspond turn alpha half balanc def measur formula simplifi formula realli thing youll see common dont rememb rest stuff rememb littl formula f measur two time precis time recal divid sum precis recal give good sens measur precis record combin f measur theyr use use evalu text classif,[ 4  6  2 14 13]
151,Course2_W3-S1-L8_Text_Classification-_Evaluation_7-17,weve introduced precision recall lets turn remaining issues evaluation text classification commonly used data set text classification reuters data set got documents standard training test splits set categories class multivalued classification article one category means going learning separate classifiers one making binary distinction average document one class heres common categories numbers training test documents theres training documents grain test documents grain classes like wheat corn interest heres sample reuters document see livestock hogs two topics heres text task given text classify document livestock hogs confusion matrix important multiclass classification confusion matrix tells us pair classes c c many documents c incorrectly assigned c heres little example documents poultry wheat coffee heres true classes numbers documents true classes heres classifier assigned c sub documents wheat classifier thought poultry classifier loves chickens cell classifier tells us many documents class classified class means diagonals confusion matrix give us correct classifications documents said uk fact uk documents said wheat actually wheat use confusion matrix compute measures weve talked precision recall lets start recall recall fraction documents class classified correctly many class documents find true positives c sub ii divided sum entire row lets go back look table heres entire row documents actually wheat lets say true positives zero bad classifier wheat divide zero sum numbers gonna give us precision recall excuse precision gonna ask documents returned thats entire column column many documents correct documents said wheat many truly wheat documents wheat divided documents said sum documents said wheat accuracy fraction documents classified correctly sum diagonal entries divided sum entries confusion matrix since one class gonna need way combine values precision recall values get class one measure cause often useful single measure theres two standard ways macroaveraging compute performance precision recall f score class average get average value classes gonna compute precisions going average get macroaveraged precision microaveraging instead collect decisions classes one single contingency table get evaluate precision lets look example two classes class one class two heres things true yesses true noes class one heres things really really class two really heres classifier returns macroaveraged precision gonna compute precision separately two classes class one ten ten plus ten thats class two plus ten macroaveraged precision average point five point nine get point seven microaveraging hand gonna take two contingency tables add together get single microaveraged contingency table gonna compute precision directly well get plus twenty see microaveraged score dominated score common class class two much common class one numbers much bigger microaveraging class dominate summed numbers summed contingency table macroaveraging class gonna participate equally text classification evaluation need precision recall many machine learning based algorithms natural language processing well need training set test set measuring performance something called development test set dev set training set well compute parameters well dev set test performance developing system whether looking precision recall f whether looking accuracy well look scores development test find bugs algorithm develop new features done developing algorithm test clean unseen test set reason important clean unseen test set otherwise report numbers development test set weve using along gonna end overfitting gonna report much higher accuracies probably reasonable weve tuned algorithm development test set clean unseen test set gives us conservative estimate performance get sampling errors due small datasets maybe test set small training set unrepresentative common talked earlier crossvalidation going tell multiple splits data crossvalidate example lets say set aside portion data dev set well take rest well train training set look performance dev set well take different split train part training set report dev set take part training set get performance dev set going pool results split compute total pooled dev set performance lets us avoid small test sets unrepresentative test sets lot data gets used training test different splits still end need clean unseen test set dont overfit dev sets weve given number ways evaluate text classification weve introduced precision recall f score talked multiclass problem two classes well see use ideas also microaveraging macroaveraging throughout natural language processing,Course2,W3-S1-L8,W3,S1,L8,Text,3,1,8,weve introduc precis recal let turn remain issu evalu text classif commonli use data set text classif reuter data set got document standard train test split set categori class multivalu classif articl one categori mean go learn separ classifi one make binari distinct averag document one class here common categori number train test document there train document grain test document grain class like wheat corn interest here sampl reuter document see livestock hog two topic here text task given text classifi document livestock hog confus matrix import multiclass classif confus matrix tell us pair class c c mani document c incorrectli assign c here littl exampl document poultri wheat coffe here true class number document true class here classifi assign c sub document wheat classifi thought poultri classifi love chicken cell classifi tell us mani document class classifi class mean diagon confus matrix give us correct classif document said uk fact uk document said wheat actual wheat use confus matrix comput measur weve talk precis recal let start recal recal fraction document class classifi correctli mani class document find true posit c sub ii divid sum entir row let go back look tabl here entir row document actual wheat let say true posit zero bad classifi wheat divid zero sum number gonna give us precis recal excus precis gonna ask document return that entir column column mani document correct document said wheat mani truli wheat document wheat divid document said sum document said wheat accuraci fraction document classifi correctli sum diagon entri divid sum entri confus matrix sinc one class gonna need way combin valu precis recal valu get class one measur caus often use singl measur there two standard way macroaverag comput perform precis recal f score class averag get averag valu class gonna comput precis go averag get macroaverag precis microaverag instead collect decis class one singl conting tabl get evalu precis let look exampl two class class one class two here thing true yess true noe class one here thing realli realli class two realli here classifi return macroaverag precis gonna comput precis separ two class class one ten ten plu ten that class two plu ten macroaverag precis averag point five point nine get point seven microaverag hand gonna take two conting tabl add togeth get singl microaverag conting tabl gonna comput precis directli well get plu twenti see microaverag score domin score common class class two much common class one number much bigger microaverag class domin sum number sum conting tabl macroaverag class gonna particip equal text classif evalu need precis recal mani machin learn base algorithm natur languag process well need train set test set measur perform someth call develop test set dev set train set well comput paramet well dev set test perform develop system whether look precis recal f whether look accuraci well look score develop test find bug algorithm develop new featur done develop algorithm test clean unseen test set reason import clean unseen test set otherwis report number develop test set weve use along gonna end overfit gonna report much higher accuraci probabl reason weve tune algorithm develop test set clean unseen test set give us conserv estim perform get sampl error due small dataset mayb test set small train set unrepres common talk earlier crossvalid go tell multipl split data crossvalid exampl let say set asid portion data dev set well take rest well train train set look perform dev set well take differ split train part train set report dev set take part train set get perform dev set go pool result split comput total pool dev set perform let us avoid small test set unrepres test set lot data get use train test differ split still end need clean unseen test set dont overfit dev set weve given number way evalu text classif weve introduc precis recal f score talk multiclass problem two class well see use idea also microaverag macroaverag throughout natur languag process,[ 2  4 14 13 12]
152,Course2_W3-S1-L9_Practical_Issues_in_Text_Classification_5-56,finally lets talk practical issues text classification seen math naive bayes turn real world questions practice build classifiers real world classifier build depends lot kind data lets suppose training data well case right thing use manly written rules rule deciding document grain lets say might say word wheat word grain doesnt word whole word bread recipe say grain document manually written rules difficult need careful crafting tuned developed data time consuming take days write rules class training data may right approach little data well little data naive bayes algorithm naive bayes whats called high bias algorithm machine learning high bias algorithm one doesnt tend overfit training data badly sort trades variance generalization new new test set doesnt overfit much small amount data thats advantage naive bayes also important get data often find clever ways get humans label data mean thats important thing dont enough data get data theres also various ways going talk much class semisupervised training find way use small amount data help train larger amount data thats called bootstrapping another thing might little data reasonable amount data might try clever classifiers well talk later quarter classifiers like regularized logistic regression support vector machines fact could even use decision trees decision trees advantages disadvantages big advantage decision trees theyre user interpretable thats helpful people like able modify rule hack classifier easy modify decision much easier modify decision tree rule change threshold hand much harder svm logistic aggression huge amount data well achieve high accuracy although cost many classifiers take long time svms especially hm knearest neighbors slow train classifier logistic rationing somewhat better really huge amount data may efficient train naive bayes quite fast actually huge amount data may turn classifier may matter heres result brill banko spelling correction comparing performance three four different machines running algorithms memory based learner wino perception naive bayes spelling correction task million words ten million words million log scale measuring accurate classifiers see difference classifiers much smaller difference get adding data hm fact things depending much data classifiers cross performance curve enough data may matter classifier real world system general combine kind automatic classification whether rules supervised machine learning manual review uncertain difficult new cases important details computation naive bayes one underflow prevention turns multiplying lots probabilities result floating point underflow talked language modeling since b definition logarithm log xy log x plus log general keep store probabilities form logs add instead multiply still formula heres naive bayes formula expressed terms log probabilities instead probabilities still inaudible max instead multiplying probability product likelihoods adding log probability sum log likelihoods model maximizing choosing class maximizes sum weights simple model finally going want treat performance domain specific features particular task domain specific weights important performance real systems example sometimes going want collapse terms lets say dealing part numbers inventory task might want class part numbers together part number class part number kind word chemical formula might want one named entity called chemical formula kind collapsing stemming generally doesnt help know whether need collapse terms also important upweight upweighting counting word occurs twice often upweight title words might upweight first sentence paragraph sentences words occur title might upweight words sentence small ways help treating performance weve seen number practical things building real world test classification system,Course2,W3-S1-L9,W3,S1,L9,Practical,3,1,9,final let talk practic issu text classif seen math naiv bay turn real world question practic build classifi real world classifi build depend lot kind data let suppos train data well case right thing use manli written rule rule decid document grain let say might say word wheat word grain doesnt word whole word bread recip say grain document manual written rule difficult need care craft tune develop data time consum take day write rule class train data may right approach littl data well littl data naiv bay algorithm naiv bay what call high bia algorithm machin learn high bia algorithm one doesnt tend overfit train data badli sort trade varianc gener new new test set doesnt overfit much small amount data that advantag naiv bay also import get data often find clever way get human label data mean that import thing dont enough data get data there also variou way go talk much class semisupervis train find way use small amount data help train larger amount data that call bootstrap anoth thing might littl data reason amount data might tri clever classifi well talk later quarter classifi like regular logist regress support vector machin fact could even use decis tree decis tree advantag disadvantag big advantag decis tree theyr user interpret that help peopl like abl modifi rule hack classifi easi modifi decis much easier modifi decis tree rule chang threshold hand much harder svm logist aggress huge amount data well achiev high accuraci although cost mani classifi take long time svm especi hm knearest neighbor slow train classifi logist ration somewhat better realli huge amount data may effici train naiv bay quit fast actual huge amount data may turn classifi may matter here result brill banko spell correct compar perform three four differ machin run algorithm memori base learner wino percept naiv bay spell correct task million word ten million word million log scale measur accur classifi see differ classifi much smaller differ get ad data hm fact thing depend much data classifi cross perform curv enough data may matter classifi real world system gener combin kind automat classif whether rule supervis machin learn manual review uncertain difficult new case import detail comput naiv bay one underflow prevent turn multipli lot probabl result float point underflow talk languag model sinc b definit logarithm log xy log x plu log gener keep store probabl form log add instead multipli still formula here naiv bay formula express term log probabl instead probabl still inaud max instead multipli probabl product likelihood ad log probabl sum log likelihood model maxim choos class maxim sum weight simpl model final go want treat perform domain specif featur particular task domain specif weight import perform real system exampl sometim go want collaps term let say deal part number inventori task might want class part number togeth part number class part number kind word chemic formula might want one name entiti call chemic formula kind collaps stem gener doesnt help know whether need collaps term also import upweight upweight count word occur twice often upweight titl word might upweight first sentenc paragraph sentenc word occur titl might upweight word sentenc small way help treat perform weve seen number practic thing build real world test classif system,[ 4  2  0 14 13]
158,Course2_W4-S1-L1_Generative_vs,section course going start looking discriminative models gonna look contrast generative models weve looked far particular go detailed examination maximum entropy models far weve looking generative models particular weve looked language models naïve bayes models addition kind generative models theres much use conditional discrimative models natural language processing also related fields like speech information retrieval machine learning generally thats couple good reasons models tend high accuracy performance also theyre linguistically interesting make easy include lots linguistically important features hence easily allow building language independent retargetable nlp systems let contrast joint versus discriminate models cases going assume data c paired observations data hidden class c defining characteristic joint models place probabilities observed data hidden stuff probability c commonly way thats done theres generative model generative models generate observed data hidden stuff kind joint generative models comprise classic statistical nlp models ngram models naïve bayes classifiers weve seen already also hidden markov models probabilistic contextfree grammars ibm machine translation alignment models contrast discriminative conditional models directly target classification decision want make formally put probability distributions conditional probability class given data discriminative models include logistic regression general whole area conditional log linear models including particular maximum entropy models generalization sequences conditional random fields many machine learning methods svms perceptrons also discriminative classifiers theyre directly probabilistic models one way showing difference two model classes means graphical model diagrams used probabilistic model general diagrams draw circles random variables lines direct dependencies variables normally observed others hidden node like little classifier based incoming arcs draw kind models find naïve bayes model picture looks like classification time observing various words document given data based want predict class terms probabilistic model probability factors prior probability class probability words document given class generative direction words generated class rather actually predicting havent observed discriminative model logistic regression situation opposite weve observed words document want predict class time directly putting probability class given data weve observed generative models look joint probability data class try attempt maximize joint likelihood weve already seen categorical models trivial optimize joint likelihood take relative frequencies different events count often different things occur divide normalizing denominator done thats practical estimation contrast conditional model want work probabilities c given conditional model well attempt directly maximize conditional likelihood probability classes observed given data well see turns much harder perhaps useful directly related classification success error slide gives initial motivation interested conditional models showing high performance slide show results experiments done dan klein task word sense disambiguation essentially thats task text classification weve looked earlier youre wanting choose one number senses words instance star whether rock star astronomical aspects object based evidence words around see document experiments careful set two models exactly respects exactly features exactly methods smoothing data differed whether conditional estimation joint estimation see results experiments see well skip first straight performs test data see good news story conditional likelihood conditional likelihood giving us performance thats percent better got joint likelihood naive bayes model joint likelihood model naive bayes model thats nice gain something appealing building practical nlp systems incidentally also notice something interesting looking performance training set look performance training set joint model gets goes percent conditional likelihood model could think thats good news also actually cause worry find conditional models easily memorize much training set theyre prone overfitting memorizing much happened receiving training data may reappear test data well go look control kind overfitting later discussion okay thats motivating introduction discriminative models discriminative estimation next section well start looking define features used models go details estimated,Course2,W4-S1-L1,W4,S1,L1,Generative,4,1,1,section cours go start look discrimin model gonna look contrast gener model weve look far particular go detail examin maximum entropi model far weve look gener model particular weve look languag model naïv bay model addit kind gener model there much use condit discrim model natur languag process also relat field like speech inform retriev machin learn gener that coupl good reason model tend high accuraci perform also theyr linguist interest make easi includ lot linguist import featur henc easili allow build languag independ retarget nlp system let contrast joint versu discrimin model case go assum data c pair observ data hidden class c defin characterist joint model place probabl observ data hidden stuff probabl c commonli way that done there gener model gener model gener observ data hidden stuff kind joint gener model compris classic statist nlp model ngram model naïv bay classifi weve seen alreadi also hidden markov model probabilist contextfre grammar ibm machin translat align model contrast discrimin condit model directli target classif decis want make formal put probabl distribut condit probabl class given data discrimin model includ logist regress gener whole area condit log linear model includ particular maximum entropi model gener sequenc condit random field mani machin learn method svm perceptron also discrimin classifi theyr directli probabilist model one way show differ two model class mean graphic model diagram use probabilist model gener diagram draw circl random variabl line direct depend variabl normal observ other hidden node like littl classifi base incom arc draw kind model find naïv bay model pictur look like classif time observ variou word document given data base want predict class term probabilist model probabl factor prior probabl class probabl word document given class gener direct word gener class rather actual predict havent observ discrimin model logist regress situat opposit weve observ word document want predict class time directli put probabl class given data weve observ gener model look joint probabl data class tri attempt maxim joint likelihood weve alreadi seen categor model trivial optim joint likelihood take rel frequenc differ event count often differ thing occur divid normal denomin done that practic estim contrast condit model want work probabl c given condit model well attempt directli maxim condit likelihood probabl class observ given data well see turn much harder perhap use directli relat classif success error slide give initi motiv interest condit model show high perform slide show result experi done dan klein task word sens disambigu essenti that task text classif weve look earlier your want choos one number sens word instanc star whether rock star astronom aspect object base evid word around see document experi care set two model exactli respect exactli featur exactli method smooth data differ whether condit estim joint estim see result experi see well skip first straight perform test data see good news stori condit likelihood condit likelihood give us perform that percent better got joint likelihood naiv bay model joint likelihood model naiv bay model that nice gain someth appeal build practic nlp system incident also notic someth interest look perform train set look perform train set joint model get goe percent condit likelihood model could think that good news also actual caus worri find condit model easili memor much train set theyr prone overfit memor much happen receiv train data may reappear test data well go look control kind overfit later discuss okay that motiv introduct discrimin model discrimin estim next section well start look defin featur use model go detail estim,[4 5 1 8 2]
159,Course2_W4-S1-L2_Making_features_from_text_for_discriminative_NLP_models_18-11,section im going look get features text use side discriminate models various kinds classification tasks slides max inaudible work feature mean elementary piece evidence links observe data piece category c want present feature function bounded real value write feature f mapping space classes pieces data onto real number couple examples kind features lets look features respect particular pieces data okay first feature purple feature saying class locations examples pieces data im assuming im looking last word sequence class shown orange feature inaudible examples class locations two pieces data left previous word word capitalized three criteria true two pieces data arent true two pieces data right really multiple reasons class wrong proceeding word also wrong okay go look second feature saying classs location rest feature put anything seems like useful might think useful feature know whether word something like accent latin characters plain z characters know weve sort noticed names accented letters feature true piece data true pieces data okay lets look feature three feature three set class drug word ends c thats feature thats true piece data others general thats whats happening imagining training well data already gives us right answers theyll words class example also assuming particular position looking basic word w want refer something like word minus one ask features nearby words feature ask question class drug question words normally wont sort ask complete sequence words ask might ask features like individual word something like example gave looking whether word ends letter c point next thing model assign feature weight weight real number might something like isnt value feature weight feature positive weight votes configuration likely correct kind thing happens real text thats something like feature one well preceding word word capitalized well thats indeed good indicator word location feature matches two pieces data wed expect positive light sound choice negative weight negative weight votes configuration likely incorrect could another feature example kind like feature one might feature fourteen said class equals inaudible rest condition feature would match different classification arcadia saying arcadia drug feature fourteen would match configuration wed like say thats unlikely correct could express giving negative weight like later working features crucially make use two expectations expectation actual predicted count feature firing two expectations one supervised data actually look often particular feature satisfied pieces data ill refer imperial count imperial expectation feature thats guy simply look every piece data training ask feature true piece data count number times true sounds per expectation expectation model expectation feature well look minute going probability distribution pairs class data set gonna using distribution gonna consider classes pieces data say well given expected value f based probabilities different configurations value f configurations particular case natural language processing applications find usually feature particular form feature consists firstly indicator function yesno boolean matching function properties input secondly specifies particular class arbitrary feature feature class state pair returning real number practice features define particular form matching predicate data thats something like ns letter c theres capitalized word thats conjoined together saying like particular classes matched return value feature general real number features logical bullion predicates return value either zero one every feature going present class exactly form thats true percent features used natural language processing work equally say phi feature data class cj conjunction phi c equals cj feature class data pair thats fi feature pair lot time well find well think terms phi features remember actual math programming write going working terms fi features index particular matching data predicate particular class sound nevertheless good way think things fit feature identifies subset data suggests label thats saw examples feature based models decision data point based entirely features active point lets look couple examples inaudible applications kind features get defined one simplest cases text categorization article data like stocks hit yearly low assigned topical class business typically kind text categorization applications features words occur articles word typed occurs article feature five feature complete f feature word stocks occurred class business word stocks occurred class sports word sense disambiguation task determining sense words context example whether usage word bank referring bank river financial institution deci making classifier decides senses words practice comes kind like text categorization example regard context word little mini document around word observed data sequence words heres word wanna inaudible inaudible particular class training data instance money wand could use kind features bag words set features words around context also use particular features look particular things context could feature says word restructure word afterwards dead kinds features wanted like feature length sentence twelve really think make features way want good predicting class commonly practice youll find word since disambiguation systems bag words features like features ask particular words adjacent left right shown actually types useful heres one example thats slightly different working part speech word context whether noun verb heres fall verb fall noun particular case actual class noun wanting part speech taking likely want know particular things narrow content things particular positions wed want know word tagging current word fall likely would indicate whether verb noun previous word example points direction want sequence classifying decisions actually wanna classify word thats topic well return later show build sequence models easiest way think going give word part speech turn deciding words part speech assume weve already given parts speech words precede also use feature deciding words part speech part speech previous word feature yeah saying previous tag adjective look couple examples detail lets go couple bits work feature based classifiers young oles one well known piece work text categorization features used precisely bag words features features precisely presence word document document class actually dont use ever word occurs document document common refinement process feature selection pick particular subset words deemed reliable indicators might dropping rare words also dropping extremely common words sort viewed semantically empty test set use well known text categorization data set reuters news wire data set indicative results find build naive base model gets percent f across different classes compare several discriminative classifiers theyve got linear aggression model despite fact linear regression models shouldnt right kind categorical text prediprediction tasks various reasons numbers going bounds probabilities actually see linear regression works already much better naive bayes model fact find works almost well logistic regression model logistic regression model fraction better actually much also train support vector machine model another common popular discriminative classifier use widely text base models like text categorization performance almost distinguishably different linguistic regression model thats something interesting comes lot recent work see big difference naive bayes model discriminate models discriminate models much better turn differences discriminate models small lot time yes want use appropriate model doesnt actually matter much model choose whats going matter much quality features define another thing paper emphasizes well come back importance regularization smoothing successful use discriminative models something wasnt used much early inaudible work discriminative models turns important many places use maxm classifier whenever data points want assign one number classes lets look thru examples fairly straightforward one want decide whether period end sentence abbreviation might something like ie us sound period wed like say period isnt end sentence kind think ways could tell seeing abbreviation beforehand well wanting make features look stuff left period perhaps also things right period inaudible word isnt capitalized weve already talked bit sentiment analysis whether someone giving positive negative review make discriminative classifier putting features words perhaps kinds features like word pair features different parts speech things like prepositional phrase attachment task working prepositional phrase like beard modifying particular noun verb thats getting us realm syntactic decisions general one way general make parsing decisions structure sentence build classifiers particular decisions involved making classifier okay hope thats given good sense features theyre used modern discriminative nop systems idea kind applications applied,Course2,W4-S1-L2,W4,S1,L2,Making,4,1,2,section im go look get featur text use side discrimin model variou kind classif task slide max inaud work featur mean elementari piec evid link observ data piec categori c want present featur function bound real valu write featur f map space class piec data onto real number coupl exampl kind featur let look featur respect particular piec data okay first featur purpl featur say class locat exampl piec data im assum im look last word sequenc class shown orang featur inaud exampl class locat two piec data left previou word word capit three criteria true two piec data arent true two piec data right realli multipl reason class wrong proceed word also wrong okay go look second featur say classs locat rest featur put anyth seem like use might think use featur know whether word someth like accent latin charact plain z charact know weve sort notic name accent letter featur true piec data true piec data okay let look featur three featur three set class drug word end c that featur that true piec data other gener that what happen imagin train well data alreadi give us right answer theyll word class exampl also assum particular posit look basic word w want refer someth like word minu one ask featur nearbi word featur ask question class drug question word normal wont sort ask complet sequenc word ask might ask featur like individu word someth like exampl gave look whether word end letter c point next thing model assign featur weight weight real number might someth like isnt valu featur weight featur posit weight vote configur like correct kind thing happen real text that someth like featur one well preced word word capit well that inde good indic word locat featur match two piec data wed expect posit light sound choic neg weight neg weight vote configur like incorrect could anoth featur exampl kind like featur one might featur fourteen said class equal inaud rest condit featur would match differ classif arcadia say arcadia drug featur fourteen would match configur wed like say that unlik correct could express give neg weight like later work featur crucial make use two expect expect actual predict count featur fire two expect one supervis data actual look often particular featur satisfi piec data ill refer imperi count imperi expect featur that guy simpli look everi piec data train ask featur true piec data count number time true sound per expect expect model expect featur well look minut go probabl distribut pair class data set gonna use distribut gonna consid class piec data say well given expect valu f base probabl differ configur valu f configur particular case natur languag process applic find usual featur particular form featur consist firstli indic function yesno boolean match function properti input secondli specifi particular class arbitrari featur featur class state pair return real number practic featur defin particular form match predic data that someth like ns letter c there capit word that conjoin togeth say like particular class match return valu featur gener real number featur logic bullion predic return valu either zero one everi featur go present class exactli form that true percent featur use natur languag process work equal say phi featur data class cj conjunct phi c equal cj featur class data pair that fi featur pair lot time well find well think term phi featur rememb actual math program write go work term fi featur index particular match data predic particular class sound nevertheless good way think thing fit featur identifi subset data suggest label that saw exampl featur base model decis data point base entir featur activ point let look coupl exampl inaud applic kind featur get defin one simplest case text categor articl data like stock hit yearli low assign topic class busi typic kind text categor applic featur word occur articl word type occur articl featur five featur complet f featur word stock occur class busi word stock occur class sport word sens disambigu task determin sens word context exampl whether usag word bank refer bank river financi institut deci make classifi decid sens word practic come kind like text categor exampl regard context word littl mini document around word observ data sequenc word here word wanna inaud inaud particular class train data instanc money wand could use kind featur bag word set featur word around context also use particular featur look particular thing context could featur say word restructur word afterward dead kind featur want like featur length sentenc twelv realli think make featur way want good predict class commonli practic youll find word sinc disambigu system bag word featur like featur ask particular word adjac left right shown actual type use here one exampl that slightli differ work part speech word context whether noun verb here fall verb fall noun particular case actual class noun want part speech take like want know particular thing narrow content thing particular posit wed want know word tag current word fall like would indic whether verb noun previou word exampl point direct want sequenc classifi decis actual wanna classifi word that topic well return later show build sequenc model easiest way think go give word part speech turn decid word part speech assum weve alreadi given part speech word preced also use featur decid word part speech part speech previou word featur yeah say previou tag adject look coupl exampl detail let go coupl bit work featur base classifi young ole one well known piec work text categor featur use precis bag word featur featur precis presenc word document document class actual dont use ever word occur document document common refin process featur select pick particular subset word deem reliabl indic might drop rare word also drop extrem common word sort view semant empti test set use well known text categor data set reuter news wire data set indic result find build naiv base model get percent f across differ class compar sever discrimin classifi theyv got linear aggress model despit fact linear regress model shouldnt right kind categor text predipredict task variou reason number go bound probabl actual see linear regress work alreadi much better naiv bay model fact find work almost well logist regress model logist regress model fraction better actual much also train support vector machin model anoth common popular discrimin classifi use wide text base model like text categor perform almost distinguish differ linguist regress model that someth interest come lot recent work see big differ naiv bay model discrimin model discrimin model much better turn differ discrimin model small lot time ye want use appropri model doesnt actual matter much model choos what go matter much qualiti featur defin anoth thing paper emphas well come back import regular smooth success use discrimin model someth wasnt use much earli inaud work discrimin model turn import mani place use maxm classifi whenev data point want assign one number class let look thru exampl fairli straightforward one want decid whether period end sentenc abbrevi might someth like ie us sound period wed like say period isnt end sentenc kind think way could tell see abbrevi beforehand well want make featur look stuff left period perhap also thing right period inaud word isnt capit weve alreadi talk bit sentiment analysi whether someon give posit neg review make discrimin classifi put featur word perhap kind featur like word pair featur differ part speech thing like preposit phrase attach task work preposit phrase like beard modifi particular noun verb that get us realm syntact decis gener one way gener make pars decis structur sentenc build classifi particular decis involv make classifi okay hope that given good sens featur theyr use modern discrimin nop system idea kind applic appli,[ 5  4 14 13 12]
160,Course2_W4-S1-L3_Feature-Based_Linear_Classifiers_13-34,okay lets look features used inside classifier particular want introduce notion linear classifiers feature based maxent classifiers linear classifiers means end day theyre going going set features gonna calculate linear function features end score particular class way feature fi assign weight lambda gonna consider observed datum every particular class give gonna work features match datum therefore weight votes class terms land dry weights lets look concrete example example gonna wanting determine class word choices person location drug gonna consider classes gets votes terms three features defined vote class going sums weights assigned features remember three features first feature looked whether preceding word whether word capitalized whether class location gonna match going match thats feature would tend pick locations well assume positive weight second feature also feature matched locations looked accented latin character match datum two datums general im assuming feature least american english negative weight youre likely see accents things like person names might weight okay third feature defined class drug word ends c three datums one match one true case general think thats likely true quite drug names like zantac intimacy many regular names maybe well give weak positive vote say feature going going choosing class highest total votes three class choices class choice person actually matched features total vote zero kind neither positive negative preference class choice location aggregate vote class choice drug aggregate vote going choose class maximizes voting quantity going location sound many ways chose weights features classifier example inaudible algorithms look currently misclassified example nudge weights direction correct classification one example another popular form discriminative classifier support vector machines gonna cover classes theyre trying create lot distance examples different classes adjusting feature weights achieve direction theyre called max margin classifiers classifiers gonna look max classifiers family referred exponential log linear classifiers general family lots lots lots names theres things class also referred logistic classifiers gibs models closely related max models idea class gonna gonna make probabilistic model linear combination already saw sum dot product dot product lambda f dot product sum lambda ifi c well going imagine kind simplest possible way heres sum lambda ifi function c problem linear combination might come either positive negative thats bad youre probability gonna want gonna make always positive way gonna taking exponential gonna take e power something always make vote positive sound form im showing e thats know etc youre raising feature dot product associate inaudible e lambda dot f vectors features okay weve got something thats positive quantity necessarily probability going make probability simplest way possible going normalize kind score one class work score every class divide sound sound okay gonna say quantity probability assign class probability particular class datum given certain set parameters vector parameters work concrete example previous example probability choosing location given quebec well going work numerator weve got features match working e going divide total votes classes one class features matching second class single vote heres one location e work calculator comes equal kinda inaudible also simplify exponents multiplying also think inaudible one point eight times e minus zero point six thats sense features give extra multiplicative term changes probability add feature youre adding extra multiplicative term changes probability work examples different classes ill skip show math get different probabilities note giving probability three class choices together add one thats precisely normalization weve defined weights lambda perimeters probability distribution combining like function function also referred soft max function reason youre using exponent function make things positive things get bigger takes something like eight three thats approximately something effect features strongest votes dominate strongly form sort like soft maximum inaudible give value much strongly determined biggest votes inaudible exponential models wanting say given model form gonna try choose parameters lambda maximize conditional likelihood data according model weve defined probabilities data classes going maximize choice classes according probability distribution feature models construct classification function also probability distribution classifications many good ways discriminating classes svms boosting perceptions many machine learning algorithms give way distinguish classes ones arent naturally inherently thought distributions classes people thought ways extend direction may seen logistic regression multiclass logistic regression statistics machine loading model maybe divides people havent seen dont worry itcause presentation derivations show completely self contained hand seen models something might want think presented little bit different effectively different two ways one choice parameterization parameterization slightly different parameterization thats normally shown multi class logistic regression models statistics kind pluses minuses presentation minus slightly statistically inelegant models overparameterized plus style parameterization actually inaudible take nop models property theyre always large number features inactive particular data secondly thing observe way things formulated terms feature functions feature functions observed data class shows put text model simply actually also general formulation normally see multiclass logistic regression models actually wont use general formulation material presented actually shown useful go onto complex natural language processing structure prediction tasks sound heres quiz question guys work still situation wanting choose class inaudible final word inaudible choice classes classes personal location drug exactly three features weve using throughout examples wed like concretely work probabilities assigned three classes time okay hope understand math accent model sense one family linear,Course2,W4-S1-L3,W4,S1,L3,Feature-Based,4,1,3,okay let look featur use insid classifi particular want introduc notion linear classifi featur base maxent classifi linear classifi mean end day theyr go go set featur gonna calcul linear function featur end score particular class way featur fi assign weight lambda gonna consid observ datum everi particular class give gonna work featur match datum therefor weight vote class term land dri weight let look concret exampl exampl gonna want determin class word choic person locat drug gonna consid class get vote term three featur defin vote class go sum weight assign featur rememb three featur first featur look whether preced word whether word capit whether class locat gonna match go match that featur would tend pick locat well assum posit weight second featur also featur match locat look accent latin charact match datum two datum gener im assum featur least american english neg weight your like see accent thing like person name might weight okay third featur defin class drug word end c three datum one match one true case gener think that like true quit drug name like zantac intimaci mani regular name mayb well give weak posit vote say featur go go choos class highest total vote three class choic class choic person actual match featur total vote zero kind neither posit neg prefer class choic locat aggreg vote class choic drug aggreg vote go choos class maxim vote quantiti go locat sound mani way chose weight featur classifi exampl inaud algorithm look current misclassifi exampl nudg weight direct correct classif one exampl anoth popular form discrimin classifi support vector machin gonna cover class theyr tri creat lot distanc exampl differ class adjust featur weight achiev direct theyr call max margin classifi classifi gonna look max classifi famili refer exponenti log linear classifi gener famili lot lot lot name there thing class also refer logist classifi gib model close relat max model idea class gonna gonna make probabilist model linear combin alreadi saw sum dot product dot product lambda f dot product sum lambda ifi c well go imagin kind simplest possibl way here sum lambda ifi function c problem linear combin might come either posit neg that bad your probabl gonna want gonna make alway posit way gonna take exponenti gonna take e power someth alway make vote posit sound form im show e that know etc your rais featur dot product associ inaud e lambda dot f vector featur okay weve got someth that posit quantiti necessarili probabl go make probabl simplest way possibl go normal kind score one class work score everi class divid sound sound okay gonna say quantiti probabl assign class probabl particular class datum given certain set paramet vector paramet work concret exampl previou exampl probabl choos locat given quebec well go work numer weve got featur match work e go divid total vote class one class featur match second class singl vote here one locat e work calcul come equal kinda inaud also simplifi expon multipli also think inaud one point eight time e minu zero point six that sens featur give extra multipl term chang probabl add featur your ad extra multipl term chang probabl work exampl differ class ill skip show math get differ probabl note give probabl three class choic togeth add one that precis normal weve defin weight lambda perimet probabl distribut combin like function function also refer soft max function reason your use expon function make thing posit thing get bigger take someth like eight three that approxim someth effect featur strongest vote domin strongli form sort like soft maximum inaud give valu much strongli determin biggest vote inaud exponenti model want say given model form gonna tri choos paramet lambda maxim condit likelihood data accord model weve defin probabl data class go maxim choic class accord probabl distribut featur model construct classif function also probabl distribut classif mani good way discrimin class svm boost percept mani machin learn algorithm give way distinguish class one arent natur inher thought distribut class peopl thought way extend direct may seen logist regress multiclass logist regress statist machin load model mayb divid peopl havent seen dont worri itcaus present deriv show complet self contain hand seen model someth might want think present littl bit differ effect differ two way one choic parameter parameter slightli differ parameter that normal shown multi class logist regress model statist kind pluse minus present minu slightli statist ineleg model overparameter plu style parameter actual inaud take nop model properti theyr alway larg number featur inact particular data secondli thing observ way thing formul term featur function featur function observ data class show put text model simpli actual also gener formul normal see multiclass logist regress model actual wont use gener formul materi present actual shown use go onto complex natur languag process structur predict task sound here quiz question guy work still situat want choos class inaud final word inaud choic class class person locat drug exactli three featur weve use throughout exampl wed like concret work probabl assign three class time okay hope understand math accent model sens one famili linear,[ 5  4 14 13 12]
161,Course2_W4-S1-L4_Building_a_Maxent_Model-_The_Nuts_and_Bolts_8-04,let go quite concretely go building maxent model kind stuff assignment week first step define features features boolean functions binary indicator functions data points thinking things well pick sets data points say something distinctive classification task something like text classification sentiment definitely going want include individual words features sometimes arent best good features example infinite space numbers like going hard usefully use numbers words likely numbers turn test time werent training data get lot value defining general features might simply word contains digit might something specific like floating point number also many cases useful kinds features many languages including english end word gives information class words like helping making drying tell words participial forms verbs looking ing end course isnt always true youll get word like sting nevertheless good indicator feature like something expecting get positive weight model youre wanting model classifies words something like part speech class commonly done practice phi feature picks data context represent string string could word um computer whatever since also going features like word contains number commonly wanna kind informal name space might word equals computer one might num equals decimal feature might end equals ing weve got informal name space features reason want want represent feature unique string use something like hash map look weight feature course dont encode features strings could encode general structured object practice lot time strings used cause theyre fairly flexible informal way encode features okay remember part gives us phi feature phi feature gonna construct f features f features going phi feature particular class particular phi feature going build set f features one choice class cj particular f features like f f f going given weights like whatever concentrate defining phi features terms useful features problem domain remember presentation follows code code working terms indices individual f features phi feature turned set f features one class go building maxent model well normally start define features youre basically sure going useful might things like words document word word want classify something like typically go iterative development process initially build one model features test development data final test data see gets performance level like might wed like bit better gonna come refined second model repeat well commonly number times weve tried come best model think question well next model general look features see ones useful good features bad features often easiest way make progress practice try define features mark bad situations look classified development data youll see piece data classified something drug shouldnt drug maybe look actual observed data see something maybe theres smiley face something like say well isnt likely piece text smiley face near going talking drug define feature combination kind style features target errors find way explain model thats bad configuration often one effective ways add features particular set features weights gonna want able calculate data conditional likelihood probability class given data item already know thats preceding slides worked examples secondly also gonna want work derivative derivative condition likelihood respect feature weight thats gonna want partial derivative overall probability according particular feature like lambda gonna work math next section crucially gonna end using two feature expectations defined earlier using two things well able work optimum feature weights okay hope thats given good sense features discriminative maxent models least idea go defining build classifier practical context though question kinds features good something well come back later looking particular problems work math properties maximum entropy models,Course2,W4-S1-L4,W4,S1,L4,Building,4,1,4,let go quit concret go build maxent model kind stuff assign week first step defin featur featur boolean function binari indic function data point think thing well pick set data point say someth distinct classif task someth like text classif sentiment definit go want includ individu word featur sometim arent best good featur exampl infinit space number like go hard use use number word like number turn test time werent train data get lot valu defin gener featur might simpli word contain digit might someth specif like float point number also mani case use kind featur mani languag includ english end word give inform class word like help make dri tell word participi form verb look ing end cours isnt alway true youll get word like sting nevertheless good indic featur like someth expect get posit weight model your want model classifi word someth like part speech class commonli done practic phi featur pick data context repres string string could word um comput whatev sinc also go featur like word contain number commonli wanna kind inform name space might word equal comput one might num equal decim featur might end equal ing weve got inform name space featur reason want want repres featur uniqu string use someth like hash map look weight featur cours dont encod featur string could encod gener structur object practic lot time string use caus theyr fairli flexibl inform way encod featur okay rememb part give us phi featur phi featur gonna construct f featur f featur go phi featur particular class particular phi featur go build set f featur one choic class cj particular f featur like f f f go given weight like whatev concentr defin phi featur term use featur problem domain rememb present follow code code work term indic individu f featur phi featur turn set f featur one class go build maxent model well normal start defin featur your basic sure go use might thing like word document word word want classifi someth like typic go iter develop process initi build one model featur test develop data final test data see get perform level like might wed like bit better gonna come refin second model repeat well commonli number time weve tri come best model think question well next model gener look featur see one use good featur bad featur often easiest way make progress practic tri defin featur mark bad situat look classifi develop data youll see piec data classifi someth drug shouldnt drug mayb look actual observ data see someth mayb there smiley face someth like say well isnt like piec text smiley face near go talk drug defin featur combin kind style featur target error find way explain model that bad configur often one effect way add featur particular set featur weight gonna want abl calcul data condit likelihood probabl class given data item alreadi know that preced slide work exampl secondli also gonna want work deriv deriv condit likelihood respect featur weight that gonna want partial deriv overal probabl accord particular featur like lambda gonna work math next section crucial gonna end use two featur expect defin earlier use two thing well abl work optimum featur weight okay hope that given good sens featur discrimin maxent model least idea go defin build classifi practic context though question kind featur good someth well come back later look particular problem work math properti maximum entropi model,[ 5  4 14 13 12]
162,Course2_W4-S1-L5_Generative_vs,hello section want point problem generative models naive bayes models overcount evidence correlated least hinted maxent models solve problem im gonna use example example heres training data training data eight documents four documents europe four documents class asia wanting two class classification problem documents europe asia keep example small im building text classifier teeny vocabulary always truncate vocabulary used naive bayes text classifier people often make models smaller compact example ive done extreme extent three words left vocabulary documents looking instances words first document theres two instances monaco instances two words vocabulary hong kong okay look overall statistics starting find weve got four documents class eight documents total prior probabilities asia europe half total europe eight words six monaco probability word monaco given europe category three quarters okay looking asia category eight words training data two monaco probability word monaco asia category one quarter lets suppose weve built naive bayes model going classify document heres picture naive bayes model particular document word vocabulary appears monaco appears model predict joint probability asia monaco weve got prior probability asia thats half times probability monaco given asia quarter joint probability europe monaco prior probability half times three quarters work posterior probabilities going take terms divided sum two normalize oneeighths threeeighths going give us europe going take threeeighths threeeighths plus oneeighth simplifies threequarters isnt surprising gives us exactly wed expect saw training data monaco appeared six times twice two classes equally likely youve got document word monica appearing well shall expect say theres three quarters chance document europe thats naive bayes model working correctly lets look another example example exactly training data prior probabilities class half time going working documents hong kong probability word hong given asia thats three eight probability kong asia three eight thats threeeighths europe theres one instance hong kong eight words total weve got probabilities oneeighth okay lets move test time heres naive bayes model document testing words hong kong appearing nothing else vocabulary okay work kinds probabilities get onehalf times threeeighths times threeeighths get half times oneeighth times oneeighth denominators always look numerators working proportional nine proportional one work posterior probabilities asia get plus one equals ninetenths europe get one nine plus one equals onetenth look whats happened classifiers giving percent probability asia document rather three quarters intuitively doesnt make sense answer exactly document monaco cause look hong kong appeared europe document occurred three times asia documents probability three quarters happen well didnt happen although im suggesting hong kong one word name one place model treated two completely independent words nothing counts evidence one separately thats precisely youre getting threeeighths times threeeighths oneeighth times oneeighth ends odds ratio nine one favor asia whereas previously document monaco ratio three one start multiplying odds factor three repeated instance word whats going really one piece evidence appearance word place name hong kong two tokens treating two separate pieces evidence double count evidence falsely confident document asia create problems classification turns lets look one example example everything naive bayes factors exactly training data building model time test time weve got exactly naive bayes model document interest hong kong monaco one time mean get work naive bayes model predictions joint probability asia words half times hong given asia threeeighths times threeeighths kong times one quarter monaco probability joint probability half times oneeighth times oneeighth times three quarters denominators ignored posterior probability asia going three times three nine nine plus three thats three quarters posterior probability europe going plus equals one quarter look weve gotten theres percent chance document asia document intuitively doesnt make sense informally training data two documents look like one europe one asia precisely statistics word place monaco appears threequarters time europe documents onequarter time place hong kong occurs threequarters time onequarter time europe factors completely cancel say document equally likely document europe asia dont get effect two tokens wrongly treated independent sources evidence therefore think still three one odds ratio favor document asia okay weve seen naive bayes models multi count evidence treats evidence independent even partly totally correlated pieces evidence time see feature multiplied gonna show upcoming part maximum entropy models pretty much solve problem completely solve two pieces evidence completely correlated shall see done weighting features complex algorithm ends meaning model expectations features match observed empirical expectations obviously names places like hong kong saudi arabia provide two tokens correlated might still thinking doesnt come much truth youre building feature rich classifiers defining features discussed happens ton let quickly give one example suppose medical documents word might document collection xanax well word feature discussed commonly youll also want substring features prefixes suffixes means might feature first four letters word xana capital x first three letters word xan first two letters first letter problem quite likely training data word ever see starts capital xan xanax rare beginning words therefore feature fire word xanax feature fire word xanax three features occurrence completely correlated build naive bayes model three features well triple count one piece whats really one piece evidence whereas maximum entropy model wont okay hope thats given sense problem motivated stick math comes ahead explaining maximum entropy models work,Course2,W4-S1-L5,W4,S1,L5,Generative,4,1,5,hello section want point problem gener model naiv bay model overcount evid correl least hint maxent model solv problem im gonna use exampl exampl here train data train data eight document four document europ four document class asia want two class classif problem document europ asia keep exampl small im build text classifi teeni vocabulari alway truncat vocabulari use naiv bay text classifi peopl often make model smaller compact exampl ive done extrem extent three word left vocabulari document look instanc word first document there two instanc monaco instanc two word vocabulari hong kong okay look overal statist start find weve got four document class eight document total prior probabl asia europ half total europ eight word six monaco probabl word monaco given europ categori three quarter okay look asia categori eight word train data two monaco probabl word monaco asia categori one quarter let suppos weve built naiv bay model go classifi document here pictur naiv bay model particular document word vocabulari appear monaco appear model predict joint probabl asia monaco weve got prior probabl asia that half time probabl monaco given asia quarter joint probabl europ monaco prior probabl half time three quarter work posterior probabl go take term divid sum two normal oneeighth threeeighth go give us europ go take threeeighth threeeighth plu oneeighth simplifi threequart isnt surpris give us exactli wed expect saw train data monaco appear six time twice two class equal like youv got document word monica appear well shall expect say there three quarter chanc document europ that naiv bay model work correctli let look anoth exampl exampl exactli train data prior probabl class half time go work document hong kong probabl word hong given asia that three eight probabl kong asia three eight that threeeighth europ there one instanc hong kong eight word total weve got probabl oneeighth okay let move test time here naiv bay model document test word hong kong appear noth els vocabulari okay work kind probabl get onehalf time threeeighth time threeeighth get half time oneeighth time oneeighth denomin alway look numer work proport nine proport one work posterior probabl asia get plu one equal ninetenth europ get one nine plu one equal onetenth look what happen classifi give percent probabl asia document rather three quarter intuit doesnt make sens answer exactli document monaco caus look hong kong appear europ document occur three time asia document probabl three quarter happen well didnt happen although im suggest hong kong one word name one place model treat two complet independ word noth count evid one separ that precis your get threeeighth time threeeighth oneeighth time oneeighth end odd ratio nine one favor asia wherea previous document monaco ratio three one start multipli odd factor three repeat instanc word what go realli one piec evid appear word place name hong kong two token treat two separ piec evid doubl count evid fals confid document asia creat problem classif turn let look one exampl exampl everyth naiv bay factor exactli train data build model time test time weve got exactli naiv bay model document interest hong kong monaco one time mean get work naiv bay model predict joint probabl asia word half time hong given asia threeeighth time threeeighth kong time one quarter monaco probabl joint probabl half time oneeighth time oneeighth time three quarter denomin ignor posterior probabl asia go three time three nine nine plu three that three quarter posterior probabl europ go plu equal one quarter look weve gotten there percent chanc document asia document intuit doesnt make sens inform train data two document look like one europ one asia precis statist word place monaco appear threequart time europ document onequart time place hong kong occur threequart time onequart time europ factor complet cancel say document equal like document europ asia dont get effect two token wrongli treat independ sourc evid therefor think still three one odd ratio favor document asia okay weve seen naiv bay model multi count evid treat evid independ even partli total correl piec evid time see featur multipli gonna show upcom part maximum entropi model pretti much solv problem complet solv two piec evid complet correl shall see done weight featur complex algorithm end mean model expect featur match observ empir expect obvious name place like hong kong saudi arabia provid two token correl might still think doesnt come much truth your build featur rich classifi defin featur discuss happen ton let quickli give one exampl suppos medic document word might document collect xanax well word featur discuss commonli youll also want substr featur prefix suffix mean might featur first four letter word xana capit x first three letter word xan first two letter first letter problem quit like train data word ever see start capit xan xanax rare begin word therefor featur fire word xanax featur fire word xanax three featur occurr complet correl build naiv bay model three featur well tripl count one piec what realli one piec evid wherea maximum entropi model wont okay hope that given sens problem motiv stick math come ahead explain maximum entropi model work,[ 4  2  5 14 13]
163,Course2_W4-S1-L6_Maximizing_the_Likelihood_10-29,last segment saw define maximum entropy exponential model parameters weights various features segment going look set parameters maximize likelihood observed data log conditional likelihood maxent model function observed data actual datums classes c assume independent identically distributed parameters lambda model form see form model saw previous section see principle straightforward work log likelihood data gonna easy practice number classes reasonably modest summing classes well actually come back issue later segment take log likelihood separate two components used numerator used denominator say log likelihood entire model difference log likelihood numerator log likelihood denominator derivative numerator really easy work start working partial derivative numerator respect parameter model numerator well first see log exp cancels theyre inverses gives us form simply move partial derivative inside sum twice move ask partial derivatives respect lambda fi terms well partial derivative respect lambda going zero except term involves lambda partial derivative term simply f fi cd summing give get numerator thing notice numerator actually simple intuitive form calculating precisely empirical count feature fi derivative numerator empirical count working derivative denominator fraction complex actually need remember little bit calculus one hard taken denominator term taking derivative respect parameter weight partial derivative first move partial derivative inside sum need take derivative log something use chain rule chain rule take derivative outside function times original function times derivative inside function derivative log one x function get one x original function take derivative inside function right working right hand side move derivative inside sum getting derivative exp function point invoke chain rule second time derivative exp exp times inside function take derivative inside function okay point go next line regrouping two terms gives part right hand side still working derivative function well time function form saw numerator since weve taken derivative sum term going nonzero one involves lambda term derivative fi c prime end term looks exactly like model weve got exp exp particular class sum exp different classes model probability class c prime function feature value value c prime getting model expectation ie predicted count fi given parameter weights lambda putting two parts together derivative log likelihood respect particular parameter lambda simply difference actual count feature minus predicted count feature according current parameter weights well maximize function maximize function standard case point derivative zero want difference zero words saying optimum parameters model found models predicted expectation equals empirical expectation optimum always unique actual parameter values give may unique value maximum function unique convex function providing estimate models real data always exists something come back later models also called maximum entropy models find actually finding model maximum entropy satisfying constraints expectations okay know working partial derivatives conditional log likelihood function recap want choose values parameters lambda one lambda two et cetera maximize conditional log likelihood training data find way make use partial derivatives particular take vector partial derivatives gives us gradient function lets see picture imagining two parameters lambda one lambda two depending settings parameters get different values conditional log likelihood map get kind likelihood surface idea want going start lambda one lambda two set value going calculate derivatives partial derivatives point partial derivatives give us gradient direction steepest ascent likelihood surface gonna walk direction little calculate gradient walk direction gradient bit calculate gradient walk gonna head come maximum value conditional log likelihood function maxent models log likelihood surface always convex maximum looks fairly easy maximize example real problems deal might hundreds thousands millions parameters considerably difficult solve problem find good numerical optimization package get find good parameter values particular let note commonly packages including one use programming assignment actually minimizes instead minimize negative conditional log likelihood equivalent many numerical optimization techniques simplest gradient descent says walk always direction gradient var variant stochastic version stochastic gradient descent one actually quite effective still often used big problems sound early work maxent models statistics nlp people commonly use iterative proportional fitting methods like generalized iterative scaling improved iterative scaling arent used much anymore though standard numerical optimization methods like conjugate gradient quite effective method choice thats usually used days quasi newton methods particular one well known algorithm lbfgs one use assignment code thats good method use general okay hope means youve got good sense calculate partial derivatives log likelihood function understand used find optimal parameters model,Course2,W4-S1-L6,W4,S1,L6,Maximizing,4,1,6,last segment saw defin maximum entropi exponenti model paramet weight variou featur segment go look set paramet maxim likelihood observ data log condit likelihood maxent model function observ data actual datum class c assum independ ident distribut paramet lambda model form see form model saw previou section see principl straightforward work log likelihood data gonna easi practic number class reason modest sum class well actual come back issu later segment take log likelihood separ two compon use numer use denomin say log likelihood entir model differ log likelihood numer log likelihood denomin deriv numer realli easi work start work partial deriv numer respect paramet model numer well first see log exp cancel theyr invers give us form simpli move partial deriv insid sum twice move ask partial deriv respect lambda fi term well partial deriv respect lambda go zero except term involv lambda partial deriv term simpli f fi cd sum give get numer thing notic numer actual simpl intuit form calcul precis empir count featur fi deriv numer empir count work deriv denomin fraction complex actual need rememb littl bit calculu one hard taken denomin term take deriv respect paramet weight partial deriv first move partial deriv insid sum need take deriv log someth use chain rule chain rule take deriv outsid function time origin function time deriv insid function deriv log one x function get one x origin function take deriv insid function right work right hand side move deriv insid sum get deriv exp function point invok chain rule second time deriv exp exp time insid function take deriv insid function okay point go next line regroup two term give part right hand side still work deriv function well time function form saw numer sinc weve taken deriv sum term go nonzero one involv lambda term deriv fi c prime end term look exactli like model weve got exp exp particular class sum exp differ class model probabl class c prime function featur valu valu c prime get model expect ie predict count fi given paramet weight lambda put two part togeth deriv log likelihood respect particular paramet lambda simpli differ actual count featur minu predict count featur accord current paramet weight well maxim function maxim function standard case point deriv zero want differ zero word say optimum paramet model found model predict expect equal empir expect optimum alway uniqu actual paramet valu give may uniqu valu maximum function uniqu convex function provid estim model real data alway exist someth come back later model also call maximum entropi model find actual find model maximum entropi satisfi constraint expect okay know work partial deriv condit log likelihood function recap want choos valu paramet lambda one lambda two et cetera maxim condit log likelihood train data find way make use partial deriv particular take vector partial deriv give us gradient function let see pictur imagin two paramet lambda one lambda two depend set paramet get differ valu condit log likelihood map get kind likelihood surfac idea want go start lambda one lambda two set valu go calcul deriv partial deriv point partial deriv give us gradient direct steepest ascent likelihood surfac gonna walk direct littl calcul gradient walk direct gradient bit calcul gradient walk gonna head come maximum valu condit log likelihood function maxent model log likelihood surfac alway convex maximum look fairli easi maxim exampl real problem deal might hundr thousand million paramet consider difficult solv problem find good numer optim packag get find good paramet valu particular let note commonli packag includ one use program assign actual minim instead minim neg condit log likelihood equival mani numer optim techniqu simplest gradient descent say walk alway direct gradient var variant stochast version stochast gradient descent one actual quit effect still often use big problem sound earli work maxent model statist nlp peopl commonli use iter proport fit method like gener iter scale improv iter scale arent use much anymor though standard numer optim method like conjug gradient quit effect method choic that usual use day quasi newton method particular one well known algorithm lbfg one use assign code that good method use gener okay hope mean youv got good sens calcul partial deriv log likelihood function understand use find optim paramet model,[ 4  5 14 13 12]
164,Course2_W4-S2-L1_Introduction_to_Information_Extraction_9-18,hi segment im going introduce two tasks named entity recognition information extraction methods getting simple structured information text documents lets start information extraction goal information extraction systems find understand limited relevant parts text normally going run context system gathers information across many pieces text goal information gathering able produce sort structured representation relevant information think instances database relations arose database table knowledge base information extraction one two goals one goal directly help people organize information useful people possibility put information semantically precise form commonly computers downstream information processing task wont able perform input natural language text import various kinds relations database table easy subsequent data mining processing kind information ia systems extract clear actual information know kind kind information lets take look example couple examples ones gathering earnings profits board members headquarters kind factual information companies company reports idea system look text sees okay heres name company talking headquarters says headquarters located thats kind indicator headquarters location melbourne australia going get relation think inaudible database table thats one example lots examples another example one showed lots cases bio text mining learning drug gene product interactions learning sub cellular locus localizations lots relations interest scientist various ways worked information extraction systems consumer space information extraction available think popular various applications like either apple google mail used web indexing heres little example apple mail yeah theres date apple mail recognized date put little arrow box hold offering create calendar event thats added little help allowing kind task faster kind information extraction often fairly simple seems mainly based using kind regular expressions saw earlier including name lists heres one example companies starting surface information extraction search applications stick google hp bulletin headquarters well regular web search results actually giving write best guess headquarters melbourne london course argue little whether thats perfect answer theyre really kind two separate answers php inaudible actually kind complex case complicated jewel company structure inaudible office london office well call correct okay information extraction let go look particular sub task thats become important named entity recognition idea named entity recognition going particular look classify names text first step find names found name particular means finding limits name one two tokens name one token weve done going classify names going say name person name name organization date sequence person names fine one inaudible inaudible goes across line break say little bit precisely task sort phrase named entity recognition little bit codeword days reflects history idea entities made refer discrete things world entity stanford university entity something like sand air isnt entity theres specific delineated physical thing idea named entities names christopher maning name know chair im sitting record also entity discreet physical thing doesnt actually name attached named identity thats kind philosophical history practices wood use name identity recognition thats quite means cuz effectively using named entity recognition mean easily distinguishable names things texts pick particular things like dates times quantities normally regarded named entities although according know original starting point name something like date chemical protein isnt actually entity always include kinds things building named entity recognition systems okay use name identity recognition systems well name identities identify text theres something indexed linked note many companies make use various techniques taking entities web pages provide links entities biopages topic pages things sort whole bunch commercial products run web services allow kind thing general want crawling web picking sentiment discussed earlier detecting sentiment whether positive negative also need work sentiment sentiment point need picking company product names text thats task name entity recognition lot uses name entity recognition subcomponent larger task commonly youre information extraction lot youre actually identifying named entities working relationship well talk later inaudible questioning answering question answering various kinds questions often answers named entities something happen something helps lot question answering good named entity extraction another example also saw people use name inaudible recognition plays semantically interpreted information use various kinds things like calendaring applications youre making use semantic interpretation okay hope thats enough given guys good sense two tasks name density recognition simply picking concrete names objects people organizations etcetera quantities dates times things like bigger task information extraction goal pick particular relations inaudible database table pieces unstructured natural language text,Course2,W4-S2-L1,W4,S2,L1,Introduction,4,2,1,hi segment im go introduc two task name entiti recognit inform extract method get simpl structur inform text document let start inform extract goal inform extract system find understand limit relev part text normal go run context system gather inform across mani piec text goal inform gather abl produc sort structur represent relev inform think instanc databas relat aros databas tabl knowledg base inform extract one two goal one goal directli help peopl organ inform use peopl possibl put inform semant precis form commonli comput downstream inform process task wont abl perform input natur languag text import variou kind relat databas tabl easi subsequ data mine process kind inform ia system extract clear actual inform know kind kind inform let take look exampl coupl exampl one gather earn profit board member headquart kind factual inform compani compani report idea system look text see okay here name compani talk headquart say headquart locat that kind indic headquart locat melbourn australia go get relat think inaud databas tabl that one exampl lot exampl anoth exampl one show lot case bio text mine learn drug gene product interact learn sub cellular locu local lot relat interest scientist variou way work inform extract system consum space inform extract avail think popular variou applic like either appl googl mail use web index here littl exampl appl mail yeah there date appl mail recogn date put littl arrow box hold offer creat calendar event that ad littl help allow kind task faster kind inform extract often fairli simpl seem mainli base use kind regular express saw earlier includ name list here one exampl compani start surfac inform extract search applic stick googl hp bulletin headquart well regular web search result actual give write best guess headquart melbourn london cours argu littl whether that perfect answer theyr realli kind two separ answer php inaud actual kind complex case complic jewel compani structur inaud offic london offic well call correct okay inform extract let go look particular sub task that becom import name entiti recognit idea name entiti recognit go particular look classifi name text first step find name found name particular mean find limit name one two token name one token weve done go classifi name go say name person name name organ date sequenc person name fine one inaud inaud goe across line break say littl bit precis task sort phrase name entiti recognit littl bit codeword day reflect histori idea entiti made refer discret thing world entiti stanford univers entiti someth like sand air isnt entiti there specif delin physic thing idea name entiti name christoph mane name know chair im sit record also entiti discreet physic thing doesnt actual name attach name ident that kind philosoph histori practic wood use name ident recognit that quit mean cuz effect use name entiti recognit mean easili distinguish name thing text pick particular thing like date time quantiti normal regard name entiti although accord know origin start point name someth like date chemic protein isnt actual entiti alway includ kind thing build name entiti recognit system okay use name ident recognit system well name ident identifi text there someth index link note mani compani make use variou techniqu take entiti web page provid link entiti biopag topic page thing sort whole bunch commerci product run web servic allow kind thing gener want crawl web pick sentiment discuss earlier detect sentiment whether posit neg also need work sentiment sentiment point need pick compani product name text that task name entiti recognit lot use name entiti recognit subcompon larger task commonli your inform extract lot your actual identifi name entiti work relationship well talk later inaud question answer question answer variou kind question often answer name entiti someth happen someth help lot question answer good name entiti extract anoth exampl also saw peopl use name inaud recognit play semant interpret inform use variou kind thing like calendar applic your make use semant interpret okay hope that enough given guy good sens two task name densiti recognit simpli pick concret name object peopl organ etcetera quantiti date time thing like bigger task inform extract goal pick particular relat inaud databas tabl piec unstructur natur languag text,[13  4  9 14 12]
165,Course2_W4-S2-L2_Evaluation_of_Named_Entity_Recognition_6-34,okay let introduce evaluate named entity recognition named entity recognition task sequence word tokens going want predict entities going want predict organization two words two words person one word organization general entity names several tokens long want identify boundaries entity also class person think making classification token sequence way doesnt terribly make sense cuz really unit interest whole entities person organizations standard better task motivated evaluation used named entity recognition evaluate per entity per token working two two contingency table true positives heres system guess gonna level entities data three entities could imagine perhaps system identified one person name identified one organization name missed one well saying two true positives one false negative three tokens precision system percent every says right recall twothirds okay looks okay get details gets little bit trickier problem recall precision straightforward tasks like web search information retrieval text categorization theres one grain size youre putting classification document case putting classifications subsequences words precision recall f measures actually behave bit funnily happens heres example give good sense problems actually occur commonly systems heres piece text first bank chicago announced earnings correct entity right first bank chicago single organization name however system made little bit booboo system said bank chicago name organization means made boundary error got right boundary entity correct got left boundary entity wrong kind error ner systems make lot easy see case made error first also common noun start sentence perfectly reasonable common noun first apple announced microsoft announced intuitively might feel like really case name entity recognizer counted mostly correct identified organization name labeled three four tokens thats things work using set based measures false positives false negatives true positives true negatives youre working sequences say true annotation theres organization spans token one token four text whereas system guessed organization span token two token four text claims taken unit put set claims count number matching claims thats true positives count set differences directions gives us false positives false negatives end classification case false negative one false positive actually system scored made two errors actually system would scored better f evaluation named entity recognition labeled nothing easily seem kind wrong seemed wrong people various suggestions provide measures evaluating named entity recognition systems get partial credit things like getting entity almost right example muc score used prominent early evaluations named entity recognition algorithm gave partial credit cases like complicated questions much partial credit give cases exactly clear various arbitrary parameters really rest field hasnt gone ended using straightforward f measure name identity recognition despite complexities boundary errors ive tried illustrate okay give good sense measures precision recall fmeasure theyre useful use name identity recognition also slight sense little bit careful interpreting numbers case,Course2,W4-S2-L2,W4,S2,L2,Evaluation,4,2,2,okay let introduc evalu name entiti recognit name entiti recognit task sequenc word token go want predict entiti go want predict organ two word two word person one word organ gener entiti name sever token long want identifi boundari entiti also class person think make classif token sequenc way doesnt terribl make sens cuz realli unit interest whole entiti person organ standard better task motiv evalu use name entiti recognit evalu per entiti per token work two two conting tabl true posit here system guess gonna level entiti data three entiti could imagin perhap system identifi one person name identifi one organ name miss one well say two true posit one fals neg three token precis system percent everi say right recal twothird okay look okay get detail get littl bit trickier problem recal precis straightforward task like web search inform retriev text categor there one grain size your put classif document case put classif subsequ word precis recal f measur actual behav bit funnili happen here exampl give good sens problem actual occur commonli system here piec text first bank chicago announc earn correct entiti right first bank chicago singl organ name howev system made littl bit booboo system said bank chicago name organ mean made boundari error got right boundari entiti correct got left boundari entiti wrong kind error ner system make lot easi see case made error first also common noun start sentenc perfectli reason common noun first appl announc microsoft announc intuit might feel like realli case name entiti recogn count mostli correct identifi organ name label three four token that thing work use set base measur fals posit fals neg true posit true neg your work sequenc say true annot there organ span token one token four text wherea system guess organ span token two token four text claim taken unit put set claim count number match claim that true posit count set differ direct give us fals posit fals neg end classif case fals neg one fals posit actual system score made two error actual system would score better f evalu name entiti recognit label noth easili seem kind wrong seem wrong peopl variou suggest provid measur evalu name entiti recognit system get partial credit thing like get entiti almost right exampl muc score use promin earli evalu name entiti recognit algorithm gave partial credit case like complic question much partial credit give case exactli clear variou arbitrari paramet realli rest field hasnt gone end use straightforward f measur name ident recognit despit complex boundari error ive tri illustr okay give good sens measur precis recal fmeasur theyr use use name ident recognit also slight sens littl bit care interpret number case,[13  4  6  2 14]
166,Course2_W4-S2-L3_Sequence_Models_for_Named_Entity_Recognition_15-05,hi segment im gonna introduce machine learning sequence model approach named entity recognition kinds information extraction tasks im gonna say little bit structure approach things features use task next segment im gonna talk details using maximum entropy models sequence classifiers sound going use sequence model named entity recognition need supervised training data means examples training documents words labeled entity class steps going go first collecting representative set training documents contain entities interested context interested going go though word label token entity class class itll labeled normally denoted machine learning classifier side going design appropriate feature extractors identifying words classes going train sequence classifier whose job best job possible labeling token entity class part well talk next chunk want run classifier actual documents stuff thats often referred testing maybe call classifying train model get set testing documents modeled sound run sequence model inference document able tell us highest probability label token use labels output recognized entities probably becomes concrete show example okay document sequence words labeling done hand training documents automatically train model classification time putting word label representing either entity type column showing gets called io encoding inside short insideoutside obvious natural thing someone come named entity recognition sequence labeling taking fred labeling person taking showed labeling sue person name mengqiu person name huang part person name next three tokens theres catch labeling scheme actually one persons name another persons name cant represent io encoding say well take maximal sequences entities class call name entity recognize two entities whereas really three theres technical way fix problem thats known iob encoding way iob encoding youre prefixing class b beginning entity class continuation entity class see heres one person name heres second person name know stops another b right third person name two tokens long iob encoding isnt deficient solves problem comes bit cost suppose c entity classes io encoding need c plus one labels whereas iob encoding two c labels coming dont need distinguish bi even iob encoding well seems fairly small difference well see look sequence models sequence information youre raising order sequence model youre minimum squaring means ending things considerably slower runtime iob encoding sense clearly right thing since deficient representation lot people actually ill reveal little secret stanford named entity recognizer actually use ion coding reasons runs lot faster turns slight limits representation arent really problem practice two reasons problem practice one situations like rarely occur quite often get entities next theyre commonly entities different classes io encoding problem person followed organization see boundary perfectly well problem two entities class happens pretty rarely happen occasionally turns practice systems trained iob encoding rarely get right even though capable representing fact classes worsens sparseness fact simply hard tell one name ends next one begins find practice iob trained systems given data like tag nearly always tag one person name three tokens exactly classification extract practice io classification practice using works fine despite slight ugliness use okay let say moment features put sequence labeling information extraction named entity recognition problems obvious starting point put features words put feature current word class essentially works like learned dictionary words class also put features previous next words give us context features know things like words might likely locations used kinds linguistic processing know partofspeech tags theyll often also useful features might also throw current words partofspeech tag next words partofspeech tag previous words partofspeech tag features looking observed data could done straight classifier built word sequence model also put label context thats saying john smith think john smith think john person name quite likely next token also person name person names commonly one token long features model label sequence features type definitional making something sequence model sound get details sequence models id like mention couple kinds features back level little bit interesting using words features really useful models generalized better work better rare unseen words one kinds features character sub sequences character sub sequences word useful classificatory features gonna show neat example done student mine joseph smarr years ago classifying entities one five classes drug company movie place person asked indicative particular character subsequences couple examples data words trying classify give idea well werent words actually could multiword sequences like one lets ask questions particular character subsequences suppose know term classified oxa well turns x letter really strong marker drug names right thats drug names like xanax things like people kind particular semantic sound patterns name drugs data least saw letter oxa term percent time drug name thats purple categorical indicator feature thats extreme case arent like lots good features heres another good feature data terms saw colon term pretty much giveaway movie name exceptions ones almost categorical feature heres perhaps typical example place character subsequences still useful word ends field mean could anything couldnt drug name data could place could person say david copperfield could name movie cause theres movie david copperfield could name company semantic origins field ending ends field overwhelmingly location two thirds time ending field still good indicative feature way character substring features useful heres one kind feature turns quite complementary character subsequences call word shape sequences idea first suggested michael collins far im aware idea map words onto equivalence classes simplified representation encodes attributes something length words something capitalization use numerals internal punctuation things like many particular ways gives example general idea done biological entities precisely system way defined capital letter b etc mapped capital x little latin letter mapped little x number mapped little symbols like hyphen colon period mapped theres one trick used way shortening adjacent letters idea beginnings ends words important maybe pay less attention stuff middle worked first two letters last two letters encoded according system ive drawn means word four characters less thats picking length idea see length encoding another example something xp would encoded xx word longer four characters encoding everything remaining characters characters middle saying set character types occur set theres lower case letter theres dash symbol set written canonicalized order always actually dash comes x thats giving form lot details lot different ways imagine important part way defining word shaped equivalence class word much denser rare individual words kind good predictors behavior theyre recording important attributes like digit capitalized caps funny capitals end useful features classification okay hope introduced problem sequence models features use ner well get back bit details building maximum entry sequence model,Course2,W4-S2-L3,W4,S2,L3,Sequence,4,2,3,hi segment im gonna introduc machin learn sequenc model approach name entiti recognit kind inform extract task im gonna say littl bit structur approach thing featur use task next segment im gonna talk detail use maximum entropi model sequenc classifi sound go use sequenc model name entiti recognit need supervis train data mean exampl train document word label entiti class step go go first collect repres set train document contain entiti interest context interest go go though word label token entiti class class itll label normal denot machin learn classifi side go design appropri featur extractor identifi word class go train sequenc classifi whose job best job possibl label token entiti class part well talk next chunk want run classifi actual document stuff that often refer test mayb call classifi train model get set test document model sound run sequenc model infer document abl tell us highest probabl label token use label output recogn entiti probabl becom concret show exampl okay document sequenc word label done hand train document automat train model classif time put word label repres either entiti type column show get call io encod insid short insideoutsid obviou natur thing someon come name entiti recognit sequenc label take fred label person take show label sue person name mengqiu person name huang part person name next three token there catch label scheme actual one person name anoth person name cant repres io encod say well take maxim sequenc entiti class call name entiti recogn two entiti wherea realli three there technic way fix problem that known iob encod way iob encod your prefix class b begin entiti class continu entiti class see here one person name here second person name know stop anoth b right third person name two token long iob encod isnt defici solv problem come bit cost suppos c entiti class io encod need c plu one label wherea iob encod two c label come dont need distinguish bi even iob encod well seem fairli small differ well see look sequenc model sequenc inform your rais order sequenc model your minimum squar mean end thing consider slower runtim iob encod sens clearli right thing sinc defici represent lot peopl actual ill reveal littl secret stanford name entiti recogn actual use ion code reason run lot faster turn slight limit represent arent realli problem practic two reason problem practic one situat like rare occur quit often get entiti next theyr commonli entiti differ class io encod problem person follow organ see boundari perfectli well problem two entiti class happen pretti rare happen occasion turn practic system train iob encod rare get right even though capabl repres fact class worsen spars fact simpli hard tell one name end next one begin find practic iob train system given data like tag nearli alway tag one person name three token exactli classif extract practic io classif practic use work fine despit slight ugli use okay let say moment featur put sequenc label inform extract name entiti recognit problem obviou start point put featur word put featur current word class essenti work like learn dictionari word class also put featur previou next word give us context featur know thing like word might like locat use kind linguist process know partofspeech tag theyll often also use featur might also throw current word partofspeech tag next word partofspeech tag previou word partofspeech tag featur look observ data could done straight classifi built word sequenc model also put label context that say john smith think john smith think john person name quit like next token also person name person name commonli one token long featur model label sequenc featur type definit make someth sequenc model sound get detail sequenc model id like mention coupl kind featur back level littl bit interest use word featur realli use model gener better work better rare unseen word one kind featur charact sub sequenc charact sub sequenc word use classificatori featur gonna show neat exampl done student mine joseph smarr year ago classifi entiti one five class drug compani movi place person ask indic particular charact subsequ coupl exampl data word tri classifi give idea well werent word actual could multiword sequenc like one let ask question particular charact subsequ suppos know term classifi oxa well turn x letter realli strong marker drug name right that drug name like xanax thing like peopl kind particular semant sound pattern name drug data least saw letter oxa term percent time drug name that purpl categor indic featur that extrem case arent like lot good featur here anoth good featur data term saw colon term pretti much giveaway movi name except one almost categor featur here perhap typic exampl place charact subsequ still use word end field mean could anyth couldnt drug name data could place could person say david copperfield could name movi caus there movi david copperfield could name compani semant origin field end end field overwhelmingli locat two third time end field still good indic featur way charact substr featur use here one kind featur turn quit complementari charact subsequ call word shape sequenc idea first suggest michael collin far im awar idea map word onto equival class simplifi represent encod attribut someth length word someth capit use numer intern punctuat thing like mani particular way give exampl gener idea done biolog entiti precis system way defin capit letter b etc map capit x littl latin letter map littl x number map littl symbol like hyphen colon period map there one trick use way shorten adjac letter idea begin end word import mayb pay less attent stuff middl work first two letter last two letter encod accord system ive drawn mean word four charact less that pick length idea see length encod anoth exampl someth xp would encod xx word longer four charact encod everyth remain charact charact middl say set charact type occur set there lower case letter there dash symbol set written canonic order alway actual dash come x that give form lot detail lot differ way imagin import part way defin word shape equival class word much denser rare individu word kind good predictor behavior theyr record import attribut like digit capit cap funni capit end use featur classif okay hope introduc problem sequenc model featur use ner well get back bit detail build maximum entri sequenc model,[13  4  5  1  2]
167,Course2_W4-S2-L4_Maximum_Entropy_Sequence_Models_13-01,weve looked detail make maximum entropy classifiers want segment extend show build sequence models classifiers particular going make maximum entropy sequence models many problems nlp data sequence something might sequence characters sequence words sequence phrases sequence lines paragraphs sentences think task perhaps little coding one labeling item straightforward example part speech tagging looked words word going label particular part speech examples fairly straightforward well named entity recognition word going label entity one like organization entity going label cases things done sequence problems little bit subtle think encoding actually quite straightforward youve worked ideas piece chinese text want word segmentation word second word third word fourth word singlecharacter word use labeling capture whats called bi labeling distinguishing two states begin inside sufficient represent tasks like word segmentation says beginning new word next token says also beginning new word start word weve got class continue another class continue thats three character word go back b start new word two character word although really segmenting characters subsequences encode decisions make regarding problem sequence labeling task heres one slightly different example stretch text think text lines oldfashioned file hard line breaks sentences faq find commonly websites want automatically process work questions answers gonna regard sequence labeling task items representing line sentence encoding decisions using exactly kind twoclass classification line classified either question line answer line answer lines grouped together particular answer gonna maximum entropy models going put sequence model usually referred maximum entropy markov models memms conditional markov models classifier makes single decision time able conditioned evidence observations previous decisions showing picture middle part speech tagging weve already given part speech tags first three words proceeding left right giving part speech tag word idea features classification going able use features current word going able use features words wish also going allowed use features previously assigned part speech tag perhaps part speech tags two backwards influence classification thats whats shown features current word next word previous word previous tag previous two tags taken jointly feature also define features kind weve discussed hasdigit feature current word used generalize different numbers cause maybe dont know much anything number particular generalizing little picture move basic maxent classifier sequence model overall sequence data want classification sequence level individual words characters want assign classes way gonna going look decision individually going say okay theres particular classification interest one going make classification well gonna say particular classification theres data locally interest theres current word previous word previous class going going feature extraction data label trying predict see supervised training data weve got features observed data previous classifications building maximum entropy model point stuff weve talked optimization model smoothing end day build little local classifier makes individual decisions well well position well repeat thing next position well go along tagging sequence extremely easy see one way first decide label go decide label label label point use preceding labels help determine next one greedy sequence modeler thats deciding one sequence time moving many applications actually works quite well commonly people want explore search space little bit better sometimes although might decide one part speech tag best later youve looked would give reason think maybe chosen differently back couple methods commonly used one method beam inference beam inference position rather deciding likely label keep several possibilities might keep top k complete subsequences point far stage well consider partial subsequence extend one position beam inference also fast easy implement turns lot time beam sizes gives enough maintenance possibilities works almost well exact inference find best possible state sequence easy implement course thats necessarily true get guarantees youve found globally best part speech tag sequence whatever sequence problem certainly case possibilities would later shown good might fall beam get shown good better actually find best sequence states globally highest score model referred nlp viterbi inference since andrew viterbi invented lot algorithms finding best way things viterbi inference form dynamic programming also think memoization kind dynamic programming providing small window state influence example youre trying decide label depends words however wants depends say previous label one end sort fixed small window stuff need know make decisions nothing back affecting decision make providing thats true write dynamic programming algorithms find optimal state sequence obvious advantage youre guaranteed find best state sequence disadvantages requires bit work implement forces restrict use previous labels inference process fixed small window restriction lot time bad restriction practice hard get long distance interactions working effectively anyway even something like b model allow long distance interactions okay ive introduced maximum entropy markov models hope feel understand able build assignment end briefly mention conditional random fields conditional random fields another probabilistic sequence model sort look big picture math conditional random field boy equation looks exactly like equation weve staring recent slides difference conditional random fields probabilities terms entire sequences sequence classes sequence observed data values terms particular points space get whole sequence conditional model rather chaining local models looks would difficult deal cause space sequence cs exponential length space sequence data items represented features minimum huge perhaps even infinite turns providing fi features remain local permitting dynamic programming conditional sequence likelihood calculated exactly training somewhat slower crfs theoretical advantages avoiding certain causal competition biases occur maximum entropy markov models however explain details models wed go quite way afield looking general markov random field inference feel better topic courses let mention say days using crfs variants use max margin criterion coming svms seen state art method sequence models various bits software including stanford software named entity recognition download implement crfs thing know although crfs theoretically cleaner avoid problems memms practice youre building models rich features condition observed data practice tend performance cant really distinguished maximum entropy markov models theres really problem using maximum entropy markov models job sequence classification thats well use assignment okay hope guys concrete idea build maxent classifier incorporate system sequencing inference,Course2,W4-S2-L4,W4,S2,L4,Maximum,4,2,4,weve look detail make maximum entropi classifi want segment extend show build sequenc model classifi particular go make maximum entropi sequenc model mani problem nlp data sequenc someth might sequenc charact sequenc word sequenc phrase sequenc line paragraph sentenc think task perhap littl code one label item straightforward exampl part speech tag look word word go label particular part speech exampl fairli straightforward well name entiti recognit word go label entiti one like organ entiti go label case thing done sequenc problem littl bit subtl think encod actual quit straightforward youv work idea piec chines text want word segment word second word third word fourth word singlecharact word use label captur what call bi label distinguish two state begin insid suffici repres task like word segment say begin new word next token say also begin new word start word weve got class continu anoth class continu that three charact word go back b start new word two charact word although realli segment charact subsequ encod decis make regard problem sequenc label task here one slightli differ exampl stretch text think text line oldfashion file hard line break sentenc faq find commonli websit want automat process work question answer gonna regard sequenc label task item repres line sentenc encod decis use exactli kind twoclass classif line classifi either question line answer line answer line group togeth particular answer gonna maximum entropi model go put sequenc model usual refer maximum entropi markov model memm condit markov model classifi make singl decis time abl condit evid observ previou decis show pictur middl part speech tag weve alreadi given part speech tag first three word proceed left right give part speech tag word idea featur classif go abl use featur current word go abl use featur word wish also go allow use featur previous assign part speech tag perhap part speech tag two backward influenc classif that what shown featur current word next word previou word previou tag previou two tag taken jointli featur also defin featur kind weve discuss hasdigit featur current word use gener differ number caus mayb dont know much anyth number particular gener littl pictur move basic maxent classifi sequenc model overal sequenc data want classif sequenc level individu word charact want assign class way gonna go look decis individu go say okay there particular classif interest one go make classif well gonna say particular classif there data local interest there current word previou word previou class go go featur extract data label tri predict see supervis train data weve got featur observ data previou classif build maximum entropi model point stuff weve talk optim model smooth end day build littl local classifi make individu decis well well posit well repeat thing next posit well go along tag sequenc extrem easi see one way first decid label go decid label label label point use preced label help determin next one greedi sequenc model that decid one sequenc time move mani applic actual work quit well commonli peopl want explor search space littl bit better sometim although might decid one part speech tag best later youv look would give reason think mayb chosen differ back coupl method commonli use one method beam infer beam infer posit rather decid like label keep sever possibl might keep top k complet subsequ point far stage well consid partial subsequ extend one posit beam infer also fast easi implement turn lot time beam size give enough mainten possibl work almost well exact infer find best possibl state sequenc easi implement cours that necessarili true get guarante youv found global best part speech tag sequenc whatev sequenc problem certainli case possibl would later shown good might fall beam get shown good better actual find best sequenc state global highest score model refer nlp viterbi infer sinc andrew viterbi invent lot algorithm find best way thing viterbi infer form dynam program also think memoiz kind dynam program provid small window state influenc exampl your tri decid label depend word howev want depend say previou label one end sort fix small window stuff need know make decis noth back affect decis make provid that true write dynam program algorithm find optim state sequenc obviou advantag your guarante find best state sequenc disadvantag requir bit work implement forc restrict use previou label infer process fix small window restrict lot time bad restrict practic hard get long distanc interact work effect anyway even someth like b model allow long distanc interact okay ive introduc maximum entropi markov model hope feel understand abl build assign end briefli mention condit random field condit random field anoth probabilist sequenc model sort look big pictur math condit random field boy equat look exactli like equat weve stare recent slide differ condit random field probabl term entir sequenc sequenc class sequenc observ data valu term particular point space get whole sequenc condit model rather chain local model look would difficult deal caus space sequenc cs exponenti length space sequenc data item repres featur minimum huge perhap even infinit turn provid fi featur remain local permit dynam program condit sequenc likelihood calcul exactli train somewhat slower crf theoret advantag avoid certain causal competit bias occur maximum entropi markov model howev explain detail model wed go quit way afield look gener markov random field infer feel better topic cours let mention say day use crf variant use max margin criterion come svm seen state art method sequenc model variou bit softwar includ stanford softwar name entiti recognit download implement crf thing know although crf theoret cleaner avoid problem memm practic your build model rich featur condit observ data practic tend perform cant realli distinguish maximum entropi markov model there realli problem use maximum entropi markov model job sequenc classif that well use assign okay hope guy concret idea build maxent classifi incorpor system sequenc infer,[ 4  1  5 13 10]
168,Course2_W4-S3-L1_What_is_Relation_Extraction_9-47,welcome back weve introduced idea information extraction one core tasks named entity extraction wed like turn second task information extraction task extracting relations relation extraction consider following sentence company report international business machines incorporated state new york june sixteenth wed like extract relatively complex relation company founding wed like know founding event company ibm location new york date company originally named ctr kind facts large structured relation thats general task information extraction weve defined gonna define simpler task relation extraction gonna call task extracting relation triples instead complex events well extract thing like founding year ibm simple relation predicate two arguments ibm founded year founding year ibm location foundation location ibm new york series individual triples thatll task extracting triples text example imagine following complete page stanford university taking sentence like leland stanford junior university commonly referred stanford university fragments sentences wed like extract relations like following stanford located california get located stanford california founded inaudible founded university founder leland stanford leland stanford kind text job extract individual relations entities texts relation extraction important sorts applications time need kind structured knowledge knowledge originally form text easy application get knowledge structured database wed like extract textual facts structured form could new structured databases could adding words current databases like word net source well talk later freebase ddp well talk later today kind relations quite useful example tasks like question answering imagine following question thats modified question asked jeopardy show granddaughter actor starred movie et order answer question wed like know relations question include acted et want know acted et know someone actor someone somebody elses granddaughter kind relations help whats asked question finding answer raw text sound thats great relations extracting well one set relations comes automated contact extraction ace task seventeen relations grouped six classes defined example class persons social relations families thats gonna related parent brother relation people physical location something located place located near place affiliation organization wanna founder organization owner member organization might creator artifact manufacturer inventor artifact might corporate subsidiary might geographical sub part entity kinds relations example might physical located relation might hold person like kind geopolitical entity like state tennessee part whole subsidiary might hold two organizations first organization parent company second organization talked family relations holding two people wife husband related family relation might founder relation holds person organization relation steve jobs apple kind relations extracted using inaudible set relations course theres different sets relations every possible task bio medical information extraction might wanna use umls unified medical language system defines entities relations well example entities like injury physiological function related relations like disrupts location example might pharmacological substance drug might cause pathological function might cause disease cause problem might pathological sorry excuse pharmacological substance might pharmacological substance instead treats pathological function lots different entities case dataset relations hold application might different set relations might useful example sentence doppler echo choreography used diagnose left anterior descending artery stenosis might extract sentence relation diagnoses diagnosis technique stenosis lots existing data basis relations extracted public sources like wikipedia example take stanford university wikipedia page may notice right little structure part wikipedia page thats called wikipedia info box look detail fact set relations type private president john hennessy location stanford kinds relation names values relations look source see actual info box example see city university stanford state california motto die luft der freiheit weht extract relations directly info boxes state relation state relation model holding stanford california stanford motto databases draw wikipedia tend represent information order called resource description framework triples rdf triples rdf triple subject sort predicate object predicate calling relation location would predicate rdf golden gate park location san francisco location relation golden gate park san francisco databases like dbpedia draw wikipedia infoboxes create large databases triple fact billion triples million come say million come english wikipedia know databases like like free base lot kind relations common relation nationality person mentioned wikipedia profession classification biological entity locations inside locations paris france sound also extract ontological relations well talk talk thesauruses like wordnet every kinds relation databases includes word called isa hypernym subsumption relations know giraffe kind ruminant kind ungulate kind mammal isa hypernym relations important think kind relation extract similar kinds methods theres specific type kind relation instance particular entity like san francisco instance city relationships classes class giraffe subtype class ruminant instance san francisco instance class like city relations extracted text use augment create new databases affirm build relation extractors theres number methods like information extraction saw identity tagging hand write patterns well talk hand written patterns extracting relations supervised machine learning theres number popular methods semisupervised unsupervised learning extracting relations well talk well weve seen relation extraction important component information extraction least question answering building large knowledge bases text,Course2,W4-S3-L1,W4,S3,L1,What,4,3,1,welcom back weve introduc idea inform extract one core task name entiti extract wed like turn second task inform extract task extract relat relat extract consid follow sentenc compani report intern busi machin incorpor state new york june sixteenth wed like extract rel complex relat compani found wed like know found event compani ibm locat new york date compani origin name ctr kind fact larg structur relat that gener task inform extract weve defin gonna defin simpler task relat extract gonna call task extract relat tripl instead complex event well extract thing like found year ibm simpl relat predic two argument ibm found year found year ibm locat foundat locat ibm new york seri individu tripl thatll task extract tripl text exampl imagin follow complet page stanford univers take sentenc like leland stanford junior univers commonli refer stanford univers fragment sentenc wed like extract relat like follow stanford locat california get locat stanford california found inaud found univers founder leland stanford leland stanford kind text job extract individu relat entiti text relat extract import sort applic time need kind structur knowledg knowledg origin form text easi applic get knowledg structur databas wed like extract textual fact structur form could new structur databas could ad word current databas like word net sourc well talk later freebas ddp well talk later today kind relat quit use exampl task like question answer imagin follow question that modifi question ask jeopardi show granddaught actor star movi et order answer question wed like know relat question includ act et want know act et know someon actor someon somebodi els granddaught kind relat help what ask question find answer raw text sound that great relat extract well one set relat come autom contact extract ace task seventeen relat group six class defin exampl class person social relat famili that gonna relat parent brother relat peopl physic locat someth locat place locat near place affili organ wanna founder organ owner member organ might creator artifact manufactur inventor artifact might corpor subsidiari might geograph sub part entiti kind relat exampl might physic locat relat might hold person like kind geopolit entiti like state tennesse part whole subsidiari might hold two organ first organ parent compani second organ talk famili relat hold two peopl wife husband relat famili relat might founder relat hold person organ relat steve job appl kind relat extract use inaud set relat cours there differ set relat everi possibl task bio medic inform extract might wanna use uml unifi medic languag system defin entiti relat well exampl entiti like injuri physiolog function relat relat like disrupt locat exampl might pharmacolog substanc drug might caus patholog function might caus diseas caus problem might patholog sorri excus pharmacolog substanc might pharmacolog substanc instead treat patholog function lot differ entiti case dataset relat hold applic might differ set relat might use exampl sentenc doppler echo choreographi use diagnos left anterior descend arteri stenosi might extract sentenc relat diagnos diagnosi techniqu stenosi lot exist data basi relat extract public sourc like wikipedia exampl take stanford univers wikipedia page may notic right littl structur part wikipedia page that call wikipedia info box look detail fact set relat type privat presid john hennessi locat stanford kind relat name valu relat look sourc see actual info box exampl see citi univers stanford state california motto die luft der freiheit weht extract relat directli info box state relat state relat model hold stanford california stanford motto databas draw wikipedia tend repres inform order call resourc descript framework tripl rdf tripl rdf tripl subject sort predic object predic call relat locat would predic rdf golden gate park locat san francisco locat relat golden gate park san francisco databas like dbpedia draw wikipedia infobox creat larg databas tripl fact billion tripl million come say million come english wikipedia know databas like like free base lot kind relat common relat nation person mention wikipedia profess classif biolog entiti locat insid locat pari franc sound also extract ontolog relat well talk talk thesaurus like wordnet everi kind relat databas includ word call isa hypernym subsumpt relat know giraff kind rumin kind ungul kind mammal isa hypernym relat import think kind relat extract similar kind method there specif type kind relat instanc particular entiti like san francisco instanc citi relationship class class giraff subtyp class rumin instanc san francisco instanc class like citi relat extract text use augment creat new databas affirm build relat extractor there number method like inform extract saw ident tag hand write pattern well talk hand written pattern extract relat supervis machin learn there number popular method semisupervis unsupervis learn extract relat well talk well weve seen relat extract import compon inform extract least question answer build larg knowledg base text,[13  4  9 14 12]
169,Course2_W4-S3-L2_Using_Patterns_to_Extract_Relations_6-17,perhaps simplest way relation extraction using hand built patterns lets look intuition heres sentence agar substance prepared mixture red algae gelidium laboratory industrial use perhaps like didnt know word gelidium meant learn means sentence way know see phrase red algae gelidium tells us gelidium kind red algae intuition early paper marty hurst suggested theres fact lot patterns used suggest two entities inaudible relation example knowing saw red algae gelidium tells us gelidium kind red algae x kind might see patterns like gelidium red algae gelidium red algae red algae including gelidium case textual pattern holding two entities thats strong queue particular relation two entities sound hirsch showed kind patterns able learn isa relation new terms bombard dong inaudible bold learning author names learning country names could added database kind intuition using handwritten rules learn relations used rich relations weve looking richer isa hypenem intuition learning rules relationships often specific entities location located relation often holds organization location founding relation holds person organization addition coming clever string patterns might indicate relation also use facts actual entities involved help us learn kind relations might gonna start bentley tags thats gonna help us relation extraction many arent quite enough see lets look couple examples imagine entity drug entity disease kind relations hold entities well lot could drug could cure disease could prevent disease could even cause disease thats true kinds entities person organization person could founder organization could investor organization member organization employee organization could president organization kind relation may hold entities although constraints person cant cure cant cure disease cause kinds things theres constraints entities lots relations meet constraints gonna combine two intuitions using pattern using name entities combine two intuitions use named entity use specific patterns extract richer relations consider question holds office organization wed like extract sentence like george marshal secretary state united states person george marshal played role office secretary state organization united states government entity person entity position entity organization heres patter person comma position organization might extract sentence like position relation person organization might need another pattern sentences different form person truman appointed marshal secretary state might person appointed word like named chose person position might option preposition truman appointed marshal secretary state option truman appointed marshal secretary state combination patterns maybe list possible words might get thesaurus kind thing named entities entities finding another pattern george marshal named us secretary state person maybe form word optionally followed one named appointed words maybe optional preposition organization position combining named entity types words perhaps parts speech create patterns extract particular relations advantage pattern extraction algorithm relation extraction human patterns tend high precision look particular domain write rules rules usually pretty high precision tailored specific domains particular relation extraction task write rules get high precision minuses human patterns tend low recall often dont think particular way somebody might thought word certain sentence free text expresses relation also lot work especially lot relations youre gonna come specific patterns relation domain relation would nice also get higher accuracy get relations handbuilt patterns useful method relation extraction gonna look methods like supervised unsupervised methods well handwritten rules numentity patterns kinds words parts speech good way extract relations problems accuracy generalizing new domains going also introduce methods relation extraction like supervised unsupervised learning,Course2,W4-S3-L2,W4,S3,L2,Using,4,3,2,perhap simplest way relat extract use hand built pattern let look intuit here sentenc agar substanc prepar mixtur red alga gelidium laboratori industri use perhap like didnt know word gelidium meant learn mean sentenc way know see phrase red alga gelidium tell us gelidium kind red alga intuit earli paper marti hurst suggest there fact lot pattern use suggest two entiti inaud relat exampl know saw red alga gelidium tell us gelidium kind red alga x kind might see pattern like gelidium red alga gelidium red alga red alga includ gelidium case textual pattern hold two entiti that strong queue particular relat two entiti sound hirsch show kind pattern abl learn isa relat new term bombard dong inaud bold learn author name learn countri name could ad databas kind intuit use handwritten rule learn relat use rich relat weve look richer isa hypenem intuit learn rule relationship often specif entiti locat locat relat often hold organ locat found relat hold person organ addit come clever string pattern might indic relat also use fact actual entiti involv help us learn kind relat might gonna start bentley tag that gonna help us relat extract mani arent quit enough see let look coupl exampl imagin entiti drug entiti diseas kind relat hold entiti well lot could drug could cure diseas could prevent diseas could even caus diseas that true kind entiti person organ person could founder organ could investor organ member organ employe organ could presid organ kind relat may hold entiti although constraint person cant cure cant cure diseas caus kind thing there constraint entiti lot relat meet constraint gonna combin two intuit use pattern use name entiti combin two intuit use name entiti use specif pattern extract richer relat consid question hold offic organ wed like extract sentenc like georg marshal secretari state unit state person georg marshal play role offic secretari state organ unit state govern entiti person entiti posit entiti organ here patter person comma posit organ might extract sentenc like posit relat person organ might need anoth pattern sentenc differ form person truman appoint marshal secretari state might person appoint word like name chose person posit might option preposit truman appoint marshal secretari state option truman appoint marshal secretari state combin pattern mayb list possibl word might get thesauru kind thing name entiti entiti find anoth pattern georg marshal name us secretari state person mayb form word option follow one name appoint word mayb option preposit organ posit combin name entiti type word perhap part speech creat pattern extract particular relat advantag pattern extract algorithm relat extract human pattern tend high precis look particular domain write rule rule usual pretti high precis tailor specif domain particular relat extract task write rule get high precis minus human pattern tend low recal often dont think particular way somebodi might thought word certain sentenc free text express relat also lot work especi lot relat your gonna come specif pattern relat domain relat would nice also get higher accuraci get relat handbuilt pattern use method relat extract gonna look method like supervis unsupervis method well handwritten rule nument pattern kind word part speech good way extract relat problem accuraci gener new domain go also introduc method relat extract like supervis unsupervis learn,[13  4 10  0 14]
170,Course2_W4-S3-L3_Supervised_Relation_Extraction_10-51,supervised machine learning important way relation extraction algorithm works follows choose set relations wed like extract choose set entities wed like extract relationship presumes name entity tagger tag entities find data label choose representative corpus run name entity tagger label entities label hand small hand well label relationship entity relations interested gonna label corpus break corpus training development test like done past classification tasks train classifier training set test development test set efficiency reasons often modify algorithm slightly first find pairs name entities usually occurring sentence right near build one classifier makes yes decision two entities related way run second classifier classifies relation dont build two classifiers instead one usually lot data simple inaudible classifier says things probably related way run quickly trained fast run fast train lot data eliminate pairs entities sentences probably whatever relation looking use distinct feature sets specific deciding two things related deciding theyre particular relation might use relations example automated content extraction ace task remember six med relations seventeen sub type relations given set relations task classify relation two entities sentence imagine sentence american airlines unit amr immediately matched moves spokesman tim wagner said two entities american airlines tim wagner task decide relationship two entities might family citizen employment might nothing might unrelated could subsidiary founder venter features gonna use task lets imagine task deciding relationship two heres sentence two mentions mention one american airlines mention two tim wagner one important feature head words two mentions head word american airlines airlines well talk head words get parsing case airl american airlines kind airline head word tim wagner wagner airlines wagner might useful features create new feature two combined together sometimes thats gonna useful gonna see two heads together often enough feature might actually tells us information three features far airlines wagner airlines wagner might throw bag words even bag bigrams inaudible word american word airlines word tim word wagner words occur inaudible bigram american airlines bigram tim wagner occur two mentions might pick words bigrams particular positions left right two mentions example word mention two well call word minus one respect mention two word spokesman word mention two well call one respect mention two word said mention one counting punctuation first word comma counting punctuation first word word american airlines nil theres word american airlines words specific specific positions mention words two mentions example region unit amr immediately match word spokesman american airlines tim wagner throw bag words amr immediately sort thing fact enough compute power throw bags bigrams well pairs words two entities weve already said named entity type important relation extraction wanna know first entity organization american airlines organization second mentioned tim wagner person might create new feature concatenating two together new feature called orgperson concatenation two named entity types feature called orgperson might add whats called entity level two mentions entity level whether entity name nominal pronoun often names also get nominals pronouns acting name entities two names american airlines name tim wagner name instead would call pronoun nominal like company proper noun call nominal another feature use two mentions sound havent talked yet parsen use lots features related parse parse sentence extract lots useful parse features give intuition without going details parsing could extract whats called syntactic chunk sequence base chunk sequence theres couple noun phrases followed prepositional phrase noun phrase noun phrase preposition phrase verb phrase noun phrase sequence sympathetic chunks actually run parser flatten parse whats called constituent pass well talk work later basically saying see parse noun phrase whos parent noun phrase whos parent sentence whos parent another sentence way taking complex parse stream flattening dependency path example say verb said argument wagner argument matched matched argument airlines kind things used parse features relation extraction finally use gazetteer trigger word features trigger word list terms terms might useful particular domain example kinship terms obviously useful family relation word like parent wife husband grandparent obviously words gonna help finding family relation get online data bases like wordnet thesaurus places gazetteer feature list useful geographical geopolitical words might country name list gazetteer kinds kinds sub entities like names rivers lakes states cities thats gonna help us know san francisco california california united states often talk inaudible features might example detecting named entities like person names country name list isnt useful list common person names whatever language working might useful feature often call inaudible features even though name list isnt really inaudible list names sometimes use word inaudible mean inaudible long list useful proper nouns might help us name entity extraction summary sentence american airlines unit amr immediately matched moved spokesman tim wagner said might whole series features might entity type first mentioned org second one person head first one airlines head second one wagner concatenated type feature whose value org purse bag words words two entities word entity one isnt one would none nil word entity two said various parse features talked combine features extract training set extract test set standard classification course use classifier like talked weve talked max n classifier naive base classifier theres classifiers like svms whatever like case train classifier training set extract features deciding relation train classifier training set tune hyperparameters inaudible test unseen test set like kinds classification supervised relation extraction evaluated precision recall f saw kinds classification precision number correctly extracted relations total number relations extracted recall number correctly extracted relations total number true gold relations test thats hand labeled correct relations compute precision recall balance f pr pr summary supervised relation extraction lets us get high accuracies enough handlabeled data test set domain training set minuses supervised relation extraction expense making large training set general problem supervised models dont generalize well different genres know going run system similar genre training supervised good approach worried test sets going different training set able robust different genres probably going need unsupervised semisupervised methods supervise relation extraction important way relation extraction cases gonna afford label training set think test domain gonna similar training domain,Course2,W4-S3-L3,W4,S3,L3,Supervised,4,3,3,supervis machin learn import way relat extract algorithm work follow choos set relat wed like extract choos set entiti wed like extract relationship presum name entiti tagger tag entiti find data label choos repres corpu run name entiti tagger label entiti label hand small hand well label relationship entiti relat interest gonna label corpu break corpu train develop test like done past classif task train classifi train set test develop test set effici reason often modifi algorithm slightli first find pair name entiti usual occur sentenc right near build one classifi make ye decis two entiti relat way run second classifi classifi relat dont build two classifi instead one usual lot data simpl inaud classifi say thing probabl relat way run quickli train fast run fast train lot data elimin pair entiti sentenc probabl whatev relat look use distinct featur set specif decid two thing relat decid theyr particular relat might use relat exampl autom content extract ace task rememb six med relat seventeen sub type relat given set relat task classifi relat two entiti sentenc imagin sentenc american airlin unit amr immedi match move spokesman tim wagner said two entiti american airlin tim wagner task decid relationship two entiti might famili citizen employ might noth might unrel could subsidiari founder venter featur gonna use task let imagin task decid relationship two here sentenc two mention mention one american airlin mention two tim wagner one import featur head word two mention head word american airlin airlin well talk head word get pars case airl american airlin kind airlin head word tim wagner wagner airlin wagner might use featur creat new featur two combin togeth sometim that gonna use gonna see two head togeth often enough featur might actual tell us inform three featur far airlin wagner airlin wagner might throw bag word even bag bigram inaud word american word airlin word tim word wagner word occur inaud bigram american airlin bigram tim wagner occur two mention might pick word bigram particular posit left right two mention exampl word mention two well call word minu one respect mention two word spokesman word mention two well call one respect mention two word said mention one count punctuat first word comma count punctuat first word word american airlin nil there word american airlin word specif specif posit mention word two mention exampl region unit amr immedi match word spokesman american airlin tim wagner throw bag word amr immedi sort thing fact enough comput power throw bag bigram well pair word two entiti weve alreadi said name entiti type import relat extract wanna know first entiti organ american airlin organ second mention tim wagner person might creat new featur concaten two togeth new featur call orgperson concaten two name entiti type featur call orgperson might add what call entiti level two mention entiti level whether entiti name nomin pronoun often name also get nomin pronoun act name entiti two name american airlin name tim wagner name instead would call pronoun nomin like compani proper noun call nomin anoth featur use two mention sound havent talk yet parsen use lot featur relat pars pars sentenc extract lot use pars featur give intuit without go detail pars could extract what call syntact chunk sequenc base chunk sequenc there coupl noun phrase follow preposit phrase noun phrase noun phrase preposit phrase verb phrase noun phrase sequenc sympathet chunk actual run parser flatten pars what call constitu pass well talk work later basic say see pars noun phrase who parent noun phrase who parent sentenc who parent anoth sentenc way take complex pars stream flatten depend path exampl say verb said argument wagner argument match match argument airlin kind thing use pars featur relat extract final use gazett trigger word featur trigger word list term term might use particular domain exampl kinship term obvious use famili relat word like parent wife husband grandpar obvious word gonna help find famili relat get onlin data base like wordnet thesauru place gazett featur list use geograph geopolit word might countri name list gazett kind kind sub entiti like name river lake state citi that gonna help us know san francisco california california unit state often talk inaud featur might exampl detect name entiti like person name countri name list isnt use list common person name whatev languag work might use featur often call inaud featur even though name list isnt realli inaud list name sometim use word inaud mean inaud long list use proper noun might help us name entiti extract summari sentenc american airlin unit amr immedi match move spokesman tim wagner said might whole seri featur might entiti type first mention org second one person head first one airlin head second one wagner concaten type featur whose valu org purs bag word word two entiti word entiti one isnt one would none nil word entiti two said variou pars featur talk combin featur extract train set extract test set standard classif cours use classifi like talk weve talk max n classifi naiv base classifi there classifi like svm whatev like case train classifi train set extract featur decid relat train classifi train set tune hyperparamet inaud test unseen test set like kind classif supervis relat extract evalu precis recal f saw kind classif precis number correctli extract relat total number relat extract recal number correctli extract relat total number true gold relat test that hand label correct relat comput precis recal balanc f pr pr summari supervis relat extract let us get high accuraci enough handlabel data test set domain train set minus supervis relat extract expens make larg train set gener problem supervis model dont gener well differ genr know go run system similar genr train supervis good approach worri test set go differ train set abl robust differ genr probabl go need unsupervis semisupervis method supervis relat extract import way relat extract case gonna afford label train set think test domain gonna similar train domain,[13  4  5  0 14]
171,Course2_W4-S3-L4_Semi-Supervised_and_Unsupervised_Relation_Extraction_9-53,popular reason algorithms relation extraction semi supervised unsupervised algorithm lets look sound happens dont large training set instead maybe couple seed examples maybe couple highprecision patterns use seeds something useful seedbased bootstrapping approaches relation extraction use seeds directly learn populate relation intuition seed based method hersh gather set seed pairs relation iterate following find sentences pairs look context around pairs look words relation generalize context create patterns use patterns search pairs web searching web large corpus look corpus find pairs iterate take pairs find sentences find patterns generalize patterns iterative loop suppose looking famous authors buried know mark twain buried elmira new york might start single c tuple might grap google web environment c tuple might find sentences like mark twain buried elmira grave mark twain elmira elmira mark twains final resting place take actual entities create little variables learn x buried pattern grave xs ys pattern ys xs final resting place learn kind patterns take patterns grap mark tuple iterate approach first applied sergi brinn looked task extracting author book pairs might authors like isaac asimov robots dawn william shakespeare comedy errors first might find instances imagine comedy errors happen find four instances comedy errors william shakespeare comedy errors william shakespeare comedy errors one william shakespeares earliest attempts extract patterns one way group patterns whats middle two comma middle take longest common prefix suffix parts nothing comma well extract pattern saying x followed comma followed comma two patterns comma one middle nothing ans afterwards extract pattern x comma one thens iterate weve got new patterns find new seeds match pattern continually iterate brin approach improved snowball algorithm similar itera algorithm similar kinds grouped instances extract patterns extra intuition snowball requirement x named entities inaudible algorithm x could string gonna add intuition things particular named entity thats going help us know relation organization location extract words inaudible two patterns inaudible algorithm also computed confidence value pattern thats kind new intuition another semi supervised algorithm extends ideas combining booth trapping algorithms saw seed based methods supervised learning algorithms saw previous lecture instead five seeds distant supervision algorithm take large database get huge number seed examples huge number seeds big database could hundreds thousands examples create lots lots features instead iterating simply take features build big supervise classifier supervised facts know true large database like supervised classification distance supervision learning classifier lots features thats supervised detailed handcreated knowledge database complex inaudible expanding patterns saw c based method like unsupervised classification distance supervision lets use lots lots unlabeled data sensitive genre issues might supervised classification lets see works relation lets see trying distract born relation go tuple big database bornin relations lots bornin relations edward hubble born marshfield albert einstein born ulm find sentences large corpus lets say using web entities heres bunch sentences might find hubble born marshfield einstein born ulm hubbles birthplace marshfield lots sentences sentences different entities extract frequent features might parse sentences might use words might inaudible might part speech tags sort things extract lots amounts features take features exactly supervised classification ton features large training set train supervised classifier well need like supervised classifier need examples training set positive negative instances extract positive instances weve seen database person particular person albert einstein born inaudible positive instance supervised classifier get probability born relation particular data point condition sorts features extract sentence huge number features recently theres number algorithms unsupervised relation ex extraction often called open information extraction goal extract relations web training data realistic relation go web pull information heres inaudible algorithm called text runner algorithm sound first used parse data train classifier decide particular relation inaudible trustworthy small amount parse data use expensive parse features decide subject verb object likely relation train classifier inaudible relation walk large corpus lets say web single pass extract relation nps keep trustworthy classifier says likely relation two entities rank relations based redundancy relation occur lot times different websites guess real relation open information extraction algorithm extracts relations like fci specializes software development tesla invented coil transformer extract virtually infinite number possible relations entities see often enough times web evaluate semisupervised unsupervised relation extraction algorithms since extracting totally new relations web large corpus gold set correct instances cant prelabel web relations could wed done means cant compute precision dont know new relations weve extracted correct cant complete compute recall dont know ones missed compute approximate precision drawing random sample relations output system check precision manually take random sample pull relations sample measure many correct tell us estimate precision system also different levels recall could take lets say relations ranked weve got probability theres probably ranking could take top ranking compute precision sample take top compute precision sample get precision different levels recall case taking random sample theres way evaluate actual recall complete recall system without labeling entire database course cant semi supervised unsupervised algorithms entity extraction two exciting areas model research extracting relations general use rules entities extract relations use supervised machine learning new unsupervised approaches always solve important task relation extraction one core parts information,Course2,W4-S3-L4,W4,S3,L4,Semi-Supervised,4,3,4,popular reason algorithm relat extract semi supervis unsupervis algorithm let look sound happen dont larg train set instead mayb coupl seed exampl mayb coupl highprecis pattern use seed someth use seedbas bootstrap approach relat extract use seed directli learn popul relat intuit seed base method hersh gather set seed pair relat iter follow find sentenc pair look context around pair look word relat gener context creat pattern use pattern search pair web search web larg corpu look corpu find pair iter take pair find sentenc find pattern gener pattern iter loop suppos look famou author buri know mark twain buri elmira new york might start singl c tupl might grap googl web environ c tupl might find sentenc like mark twain buri elmira grave mark twain elmira elmira mark twain final rest place take actual entiti creat littl variabl learn x buri pattern grave xs ys pattern ys xs final rest place learn kind pattern take pattern grap mark tupl iter approach first appli sergi brinn look task extract author book pair might author like isaac asimov robot dawn william shakespear comedi error first might find instanc imagin comedi error happen find four instanc comedi error william shakespear comedi error william shakespear comedi error one william shakespear earliest attempt extract pattern one way group pattern what middl two comma middl take longest common prefix suffix part noth comma well extract pattern say x follow comma follow comma two pattern comma one middl noth an afterward extract pattern x comma one then iter weve got new pattern find new seed match pattern continu iter brin approach improv snowbal algorithm similar itera algorithm similar kind group instanc extract pattern extra intuit snowbal requir x name entiti inaud algorithm x could string gonna add intuit thing particular name entiti that go help us know relat organ locat extract word inaud two pattern inaud algorithm also comput confid valu pattern that kind new intuit anoth semi supervis algorithm extend idea combin booth trap algorithm saw seed base method supervis learn algorithm saw previou lectur instead five seed distant supervis algorithm take larg databas get huge number seed exampl huge number seed big databas could hundr thousand exampl creat lot lot featur instead iter simpli take featur build big supervis classifi supervis fact know true larg databas like supervis classif distanc supervis learn classifi lot featur that supervis detail handcreat knowledg databas complex inaud expand pattern saw c base method like unsupervis classif distanc supervis let use lot lot unlabel data sensit genr issu might supervis classif let see work relat let see tri distract born relat go tupl big databas bornin relat lot bornin relat edward hubbl born marshfield albert einstein born ulm find sentenc larg corpu let say use web entiti here bunch sentenc might find hubbl born marshfield einstein born ulm hubbl birthplac marshfield lot sentenc sentenc differ entiti extract frequent featur might pars sentenc might use word might inaud might part speech tag sort thing extract lot amount featur take featur exactli supervis classif ton featur larg train set train supervis classifi well need like supervis classifi need exampl train set posit neg instanc extract posit instanc weve seen databas person particular person albert einstein born inaud posit instanc supervis classifi get probabl born relat particular data point condit sort featur extract sentenc huge number featur recent there number algorithm unsupervis relat ex extract often call open inform extract goal extract relat web train data realist relat go web pull inform here inaud algorithm call text runner algorithm sound first use pars data train classifi decid particular relat inaud trustworthi small amount pars data use expens pars featur decid subject verb object like relat train classifi inaud relat walk larg corpu let say web singl pass extract relat np keep trustworthi classifi say like relat two entiti rank relat base redund relat occur lot time differ websit guess real relat open inform extract algorithm extract relat like fci special softwar develop tesla invent coil transform extract virtual infinit number possibl relat entiti see often enough time web evalu semisupervis unsupervis relat extract algorithm sinc extract total new relat web larg corpu gold set correct instanc cant prelabel web relat could wed done mean cant comput precis dont know new relat weve extract correct cant complet comput recal dont know one miss comput approxim precis draw random sampl relat output system check precis manual take random sampl pull relat sampl measur mani correct tell us estim precis system also differ level recal could take let say relat rank weve got probabl there probabl rank could take top rank comput precis sampl take top comput precis sampl get precis differ level recal case take random sampl there way evalu actual recal complet recal system without label entir databas cours cant semi supervis unsupervis algorithm entiti extract two excit area model research extract relat gener use rule entiti extract relat use supervis machin learn new unsupervis approach alway solv import task relat extract one core part inform,[13  4  5 14 12]
172,Course2_W5-S1-L1_The_Maximum_Entropy_Model_Presentation_12-14,ive referred models using maximum entropy models actually defined likelihood maximizing models defined certain exponential model form part want show models also referred maximum entropy models motivating intuition maximum entropy idea motivating intuition maximum entropy models general tons probability distributions spiked specific would tend overfit particular data items want find distribution thats uniform possible except places know theres reason believe probability distribution isnt uniform uniformity thought high entropy search distributions properties desire also high entropy kind embodying statement thomas jeffersons ignorance preferable error less remote truth believes nothing believes wrong wanting beliefs model havent particularly stated extent havent stated constraints model want probabilities uniform possible entropy quantity measures uncertainty distribution event x probability work surprise event taking log inverse probability see event think small probability happening surprise great thought zero probability happening surprise infinite hand event high probability happening surprise small particular think probability one surprise zero work entropy distribution taking expectation surprise written form gives us equation entropy heres example simple case flipping possibly weighted coin coin always comes heads always comes tails theres entropy distribution entropy maximized coin equally likely come heads tails let go concrete examples show happens try maximize entropy distribution ie minimize commitment starting beliefs ways going want probability distribution resembles reference distribution us going taking straight observed data going say model want maximize entropy subject feature based constraints precisely featurebased constraints say expectations values features model empirical expectations features observed data every time add features puts constraints model therefore lowers maximum entropy hand raises likelihood observed data takes distribution uniform brings distribution closer data simple example unconstrained entropy distribution maximum right put constraint said probability heads weve constrained distribution single point point constrained distribution lower maximum entropy example one dimension simple hard see much lets slightly complex example two dimensions two two probabilities probability heads probability tails lets assume two numbers zero one havent even modeled fact heads tails complementary distribution well model entropy surface find maximum entropy comes probability ph pt around well well thats components entropy distribution minus x log x optimum value one e something like thats normally see see entropy picture coin thats normally immediately put constraint saying heads tails complementary distribution sum probabilities add one weve constrained space saying somewhere along line situation entropy maximized probability heads equaling probability tails equaling half could sort stick one constraint say probability heads weve constrained distribution single feasible point point notice constraints maximum entropy model gone lower true maximum function sort away true maximum function still reasonably high entropy point weve wandered even maximum function maximum entropy going modeling facts situation world want model lets look concrete example thats little bit closer actual language problem lets suppose event space parts speech six parts speech nouns plural nouns proper nouns proper plural nouns two verb parts speech empirical data saw different events common things saw proper nouns actually two thirds data saw regular nouns saw verbs right probability events say maximize theyre sort p p p say maximize probabilities maximum entropy distribution setting value e thats want want say set categorical probabilities maximum entropy distribution say probabilities one sixth uniformity maximum entropy distribution categorical distributions thats uniform given happened data nouns much common verbs data gonna add feature value one tag noun zero otherwise well expected value feature eight ninths thats using data previous slide add constraint say whats maximum entropy distribution going set probabilities constraint satisfied sum probabilities equals within category probability mass still gonna distributed uniformly thats maximum entropy distribution similarly within verb classes youre gonna get remaining probability mass distributed uniformly point might notice proper nouns much frequent common nouns add second constraint feature value one tag proper noun expectation feature two thirds two thirds data proper nouns noted put model get expectation feature observed two thirds probability mass goes proper nouns distributed uniformly still feature probability mass goes kind noun remainder distributed evenly among nouns course could keep refining model could example say add feature say singular nouns comprised certain amount data singular nouns comprised eighteen thirtysixths data add constraint eventually added enough constraints wed force distribution exactly empirical distribution easy see maximum entropy models convex models whats idea convex function idea convex function f take function value weighted mean set points function value greater take function value points weight function values found kind thats distinguish something like nonconvex function could local minima function value average point beneath function value take average values two greater points convexity guarantees function single global maximum higher points function greedily reachable maximum entropy formulation easy see convex function start showing entropy function convex minus x log x convex function therefore take sum drew thats convex function take sum convex functions thats always convex adding constraints function feasible region constrained entropy function linear subspace also convex like put linear subspace constraint right entropy surface weve still got convex function coming im gonna show true original maximum likelihood presentation get convex function okay well hope understand name maxent models comes key idea maximum entropy principle,Course2,W5-S1-L1,W5,S1,L1,The,5,1,1,ive refer model use maximum entropi model actual defin likelihood maxim model defin certain exponenti model form part want show model also refer maximum entropi model motiv intuit maximum entropi idea motiv intuit maximum entropi model gener ton probabl distribut spike specif would tend overfit particular data item want find distribut that uniform possibl except place know there reason believ probabl distribut isnt uniform uniform thought high entropi search distribut properti desir also high entropi kind embodi statement thoma jefferson ignor prefer error less remot truth believ noth believ wrong want belief model havent particularli state extent havent state constraint model want probabl uniform possibl entropi quantiti measur uncertainti distribut event x probabl work surpris event take log invers probabl see event think small probabl happen surpris great thought zero probabl happen surpris infinit hand event high probabl happen surpris small particular think probabl one surpris zero work entropi distribut take expect surpris written form give us equat entropi here exampl simpl case flip possibl weight coin coin alway come head alway come tail there entropi distribut entropi maxim coin equal like come head tail let go concret exampl show happen tri maxim entropi distribut ie minim commit start belief way go want probabl distribut resembl refer distribut us go take straight observ data go say model want maxim entropi subject featur base constraint precis featurebas constraint say expect valu featur model empir expect featur observ data everi time add featur put constraint model therefor lower maximum entropi hand rais likelihood observ data take distribut uniform bring distribut closer data simpl exampl unconstrain entropi distribut maximum right put constraint said probabl head weve constrain distribut singl point point constrain distribut lower maximum entropi exampl one dimens simpl hard see much let slightli complex exampl two dimens two two probabl probabl head probabl tail let assum two number zero one havent even model fact head tail complementari distribut well model entropi surfac find maximum entropi come probabl ph pt around well well that compon entropi distribut minu x log x optimum valu one e someth like that normal see see entropi pictur coin that normal immedi put constraint say head tail complementari distribut sum probabl add one weve constrain space say somewher along line situat entropi maxim probabl head equal probabl tail equal half could sort stick one constraint say probabl head weve constrain distribut singl feasibl point point notic constraint maximum entropi model gone lower true maximum function sort away true maximum function still reason high entropi point weve wander even maximum function maximum entropi go model fact situat world want model let look concret exampl that littl bit closer actual languag problem let suppos event space part speech six part speech noun plural noun proper noun proper plural noun two verb part speech empir data saw differ event common thing saw proper noun actual two third data saw regular noun saw verb right probabl event say maxim theyr sort p p p say maxim probabl maximum entropi distribut set valu e that want want say set categor probabl maximum entropi distribut say probabl one sixth uniform maximum entropi distribut categor distribut that uniform given happen data noun much common verb data gonna add featur valu one tag noun zero otherwis well expect valu featur eight ninth that use data previou slide add constraint say what maximum entropi distribut go set probabl constraint satisfi sum probabl equal within categori probabl mass still gonna distribut uniformli that maximum entropi distribut similarli within verb class your gonna get remain probabl mass distribut uniformli point might notic proper noun much frequent common noun add second constraint featur valu one tag proper noun expect featur two third two third data proper noun note put model get expect featur observ two third probabl mass goe proper noun distribut uniformli still featur probabl mass goe kind noun remaind distribut evenli among noun cours could keep refin model could exampl say add featur say singular noun compris certain amount data singular noun compris eighteen thirtysixth data add constraint eventu ad enough constraint wed forc distribut exactli empir distribut easi see maximum entropi model convex model what idea convex function idea convex function f take function valu weight mean set point function valu greater take function valu point weight function valu found kind that distinguish someth like nonconvex function could local minima function valu averag point beneath function valu take averag valu two greater point convex guarante function singl global maximum higher point function greedili reachabl maximum entropi formul easi see convex function start show entropi function convex minu x log x convex function therefor take sum drew that convex function take sum convex function that alway convex ad constraint function feasibl region constrain entropi function linear subspac also convex like put linear subspac constraint right entropi surfac weve still got convex function come im gonna show true origin maximum likelihood present get convex function okay well hope understand name maxent model come key idea maximum entropi principl,[ 4  5  0 14 13]
173,Course2_W5-S1-L2_Feature_Overlap-Feature_Interaction_12-51,lets go examples show maximum entropy models behave particular see maximum entropy models dont double count features way naive bayes models examples assuming teeny empirical data set two features two values theres little big b theres little big weve got distribution six data points features going put models first feature say probability distribution put feature maximum entropy model say okay give quarter probability outcome second constraint going put model going say well wait minute lot big data second feature red feature expectation red feature empirical data twothirds add parameter model captures distribution get twothirds data column cells uniformly onethird onethird well suppose happens add second feature model actually looking exactly thing saying exp expectation getting big must two thirds well two parameter weights lambda prime lambda double prime gonna optimize model say well sum expectations column must twothirds gonna happen actually sum lambda prime lambda double prime going value old lambda get exactly probability distribution right effect maximum entropy models features duplicate evidence features tend get much weight heres example named entity model predicting named entity goes next token tagging along word time grace road weve said going want give named entity tag grace two candidates examples person location could reasonably think grace good word grace good indicator something person therefore high weight person look whats actually model well positive vote right grace person fairly weak positive vote lot reason weak positive vote actually knowing letters word starts g actually rather strong indicator something person current word feature special case general feature beginning word starts g weight goes theres little bit positive weight knowing current word okay lets look interesting example features overlap arent exactly time heres empirical data want model three points empirical data start constraining values probabilities put red feature whose expectation still twothirds empirical data gives us distribution parameter weight lambda okay want extra constraint model capture fact theres data well obvious thing notice capital b also expectation twothirds observed data lets add feature thats true data point capital b lets add model weight lambda b might hopeful would mean model data perfectly actually dont maximum entropy distribution one get th chance little b little weve smoothed model little maybe good dont get uniform distribution points get weights ths two cells ths cell youre expecting almost half probability mass got get big big b think terms parameter weights kind obvious whats happening case big big b features fire weight goes maxent formulation sum lambda lambda b therefore probability much higher probabilities assigned lambda lambda b alone theres weight might thought still probability zero remember observing empirical expectations model constraints model probabilities sum probabilities sum weve added constraint probability cell double probability cell work constraints maximum likelihood solution precisely one thats shown fourninths twoninths twoninths oneninth shows maximum entropy models dont model free statisticians call interaction terms want say something special culmination big big b behaves putting extra features model data start two features one way fix distribution add third feature true b capitalized well expectation feature observed data feature determines probability cell must one third well given feature constraints coming columns get probabilities one third cell exactly model empirical distribution including course get probability zero remaining cell mean course thats way could achieved could instead assigned different feature f true situations big big b big little b little big b three situations value one zero otherwise make model feature alone would also give exactly right distribution general thing take away lot time want put features model interaction terms model sets data mean particular natural language contexts commonly want features model natural classes something like feature character digit character upper case character letter e regardless whether lowercase uppercase kind natural classes make good features theyll cause model generalize good ways though find features want put model inside statistics youre looking logistic regression models maximum entropy models basically equivalent multiclass logistic regression models standard greedy stepwise search space possible interaction terms mean dont evaluate every possible subset features cause number possible subsets features exponential start null model one one add feature useful features arent yet model find good model works reasonably well traditional statistics cases maybe ten twenty features kind cases natural language processing commonly use templates generate thousands thousands features instance current word feature thats feature might values typical training set dont want feature want features like previous word next word often want higher order features like word pair previous word current word commonly get models millions features indeed models millions features optimize performance system well millions features cant affording train millions upon millions models try work whats roughly optimal set features even greedy fashion therefore nlp actually normal features put model interaction terms put model determined hand based linguistic intuition always work looking automatic ways find good feature interactions though even work fairly heuristic things make fact search space big case heres example showing feature interactions work example grace road previous class example previous class says previous class unlikely next word named entity strongly negatively weighted features thats really words arent named entities youve seen came wasnt named entity probably next word isnt named entity either hand see word capitalized done word shape signature features capitalized word well capitalized words english normally proper nouns proper nouns normally entities feature positive weights person location okay two features banging add terms well certainly case sum weights approximately zero youre actually getting particular evidence capitalized word something class going named entity thats wrong get model work better put interaction term models conjunction previous state capitalized word find interaction term votes quite strongly either word either person location features overlap actually quite subtle way weighting features works something thats important get sense understand maxent models work hope examples helped,Course2,W5-S1-L2,W5,S1,L2,Feature,5,1,2,let go exampl show maximum entropi model behav particular see maximum entropi model dont doubl count featur way naiv bay model exampl assum teeni empir data set two featur two valu there littl big b there littl big weve got distribut six data point featur go put model first featur say probabl distribut put featur maximum entropi model say okay give quarter probabl outcom second constraint go put model go say well wait minut lot big data second featur red featur expect red featur empir data twothird add paramet model captur distribut get twothird data column cell uniformli onethird onethird well suppos happen add second featur model actual look exactli thing say exp expect get big must two third well two paramet weight lambda prime lambda doubl prime gonna optim model say well sum expect column must twothird gonna happen actual sum lambda prime lambda doubl prime go valu old lambda get exactli probabl distribut right effect maximum entropi model featur duplic evid featur tend get much weight here exampl name entiti model predict name entiti goe next token tag along word time grace road weve said go want give name entiti tag grace two candid exampl person locat could reason think grace good word grace good indic someth person therefor high weight person look what actual model well posit vote right grace person fairli weak posit vote lot reason weak posit vote actual know letter word start g actual rather strong indic someth person current word featur special case gener featur begin word start g weight goe there littl bit posit weight know current word okay let look interest exampl featur overlap arent exactli time here empir data want model three point empir data start constrain valu probabl put red featur whose expect still twothird empir data give us distribut paramet weight lambda okay want extra constraint model captur fact there data well obviou thing notic capit b also expect twothird observ data let add featur that true data point capit b let add model weight lambda b might hope would mean model data perfectli actual dont maximum entropi distribut one get th chanc littl b littl weve smooth model littl mayb good dont get uniform distribut point get weight th two cell th cell your expect almost half probabl mass got get big big b think term paramet weight kind obviou what happen case big big b featur fire weight goe maxent formul sum lambda lambda b therefor probabl much higher probabl assign lambda lambda b alon there weight might thought still probabl zero rememb observ empir expect model constraint model probabl sum probabl sum weve ad constraint probabl cell doubl probabl cell work constraint maximum likelihood solut precis one that shown fourninth twoninth twoninth oneninth show maximum entropi model dont model free statistician call interact term want say someth special culmin big big b behav put extra featur model data start two featur one way fix distribut add third featur true b capit well expect featur observ data featur determin probabl cell must one third well given featur constraint come column get probabl one third cell exactli model empir distribut includ cours get probabl zero remain cell mean cours that way could achiev could instead assign differ featur f true situat big big b big littl b littl big b three situat valu one zero otherwis make model featur alon would also give exactli right distribut gener thing take away lot time want put featur model interact term model set data mean particular natur languag context commonli want featur model natur class someth like featur charact digit charact upper case charact letter e regardless whether lowercas uppercas kind natur class make good featur theyll caus model gener good way though find featur want put model insid statist your look logist regress model maximum entropi model basic equival multiclass logist regress model standard greedi stepwis search space possibl interact term mean dont evalu everi possibl subset featur caus number possibl subset featur exponenti start null model one one add featur use featur arent yet model find good model work reason well tradit statist case mayb ten twenti featur kind case natur languag process commonli use templat gener thousand thousand featur instanc current word featur that featur might valu typic train set dont want featur want featur like previou word next word often want higher order featur like word pair previou word current word commonli get model million featur inde model million featur optim perform system well million featur cant afford train million upon million model tri work what roughli optim set featur even greedi fashion therefor nlp actual normal featur put model interact term put model determin hand base linguist intuit alway work look automat way find good featur interact though even work fairli heurist thing make fact search space big case here exampl show featur interact work exampl grace road previou class exampl previou class say previou class unlik next word name entiti strongli neg weight featur that realli word arent name entiti youv seen came wasnt name entiti probabl next word isnt name entiti either hand see word capit done word shape signatur featur capit word well capit word english normal proper noun proper noun normal entiti featur posit weight person locat okay two featur bang add term well certainli case sum weight approxim zero your actual get particular evid capit word someth class go name entiti that wrong get model work better put interact term model conjunct previou state capit word find interact term vote quit strongli either word either person locat featur overlap actual quit subtl way weight featur work someth that import get sens understand maxent model work hope exampl help,[ 5  4 13 14 12]
174,Course2_W5-S1-L3_Conditional_Maxent_Models_for_Classification_4-11,models weve looking motivate maximum entropy models joint models weve data weve putting probability distribution observed data joint models probability x conditional model full probability c given building beforehand answer question think c cross space complex x particular applications working set classes generally small know maybe theres something like two topic classes part speech tags named entity labels whereas hand generally huge space possible documents minimally humungous possibly infinite principle build models joint space cd involve calculating expectations features cd shown equation equation sum joint space general thats impractical cause cant enumerate members x effectively far big space may huge infinite occur actual training data end day training million words training data million documents document classification system one million different often practice well quite less repeats something could try adding extra feature model constrain expectation match empirical data saying probability observed probability end giving probability mass observed documents saying rest entries pcd zero showing probability mass going observed rest given probability zero seems slightly crude thing clear practical benefit much easier sum sum cases saw data iterate different possible classes dont iterate possible data items weve constrained marginals way estimating model thing vary conditional distributions rewriting form saying probability observed probabilities maximize likelihood model gives data degree freedom left adjusting conditional probability distribution connection joint conditional maximum entropy exponential models conditional models thought joint models marginal constraints exactly match distribution observed data constricted model form maximizing joint likelihood conditional likelihood data actually equivalent joint likelihood constraint matching marginals maximizing conditional likelihood okay hope thats enough introduction made sense view exponential log linear models models maximize entropy,Course2,W5-S1-L3,W5,S1,L3,Conditional,5,1,3,model weve look motiv maximum entropi model joint model weve data weve put probabl distribut observ data joint model probabl x condit model full probabl c given build beforehand answer question think c cross space complex x particular applic work set class gener small know mayb there someth like two topic class part speech tag name entiti label wherea hand gener huge space possibl document minim humung possibl infinit principl build model joint space cd involv calcul expect featur cd shown equat equat sum joint space gener that impract caus cant enumer member x effect far big space may huge infinit occur actual train data end day train million word train data million document document classif system one million differ often practic well quit less repeat someth could tri ad extra featur model constrain expect match empir data say probabl observ probabl end give probabl mass observ document say rest entri pcd zero show probabl mass go observ rest given probabl zero seem slightli crude thing clear practic benefit much easier sum sum case saw data iter differ possibl class dont iter possibl data item weve constrain margin way estim model thing vari condit distribut rewrit form say probabl observ probabl maxim likelihood model give data degre freedom left adjust condit probabl distribut connect joint condit maximum entropi exponenti model condit model thought joint model margin constraint exactli match distribut observ data constrict model form maxim joint likelihood condit likelihood data actual equival joint likelihood constraint match margin maxim condit likelihood okay hope that enough introduct made sens view exponenti log linear model model maxim entropi,[ 4  2  5  1 14]
175,Course2_W5-S1-L4_Smoothing-Regularization-Priors_for_Maxent_Models_29-24,section im gonna talk smoothing maximum entropy models like models build natural language processing still issue models overfit want apply smoothing techniques parameters estimate dont lead spiky distributions overfit observed training data topic smoothing maximum entropy models often also described using prior distribution parameters regularization models issue smoothing prominent models build models build lots lots features typically logistic regression statistics class model might four eight twelve features theres enough data suitably estimate parameters features typically models well build natural language processing hundreds thousands millions even tens millions features one thing notice right simply storing array parameter values substantial memory cost statistical estimation point view importantly parameters features poorly estimated well limited data estimate parameter values lots issues sparsity overfitting training data easy need smoothing prevent many features saw happen see training time example might never see use model test time want give much weight features happen see training time reasons need smooth maxent models dont feature weights infinite iterative solvers use set parameter values take long time get infinities really want change model formulation optimal feature weights finite therefore easily found findable optimization procedure let motivate issue smoothing looking really simple example lets assume tossing coin distribution heads tails natural way thats formulated maximum entropy features kind weve talked two features one head one tail well following model distribution probability heads feature weight head probability tails feature weight tail normalization term cases makes look like two parameters really one degree freedom piece math shows maxent formulation connects normal formulation see twoclass logistic regression statistics textbook instead said really matters difference weight head tail parameter say probability heads instead written like weve taken equation weve multiplied term e minus lambda simplify math little get e lambda e lambda probability tails kind trickery comes e zero e lambda plus zero simplify thats one e lambda plus one simple form often see logistic regression model general thats true right whats important difference weights opposing parameters multi twoclass multiclass model okay graph lambda get classic logistic curve weight zero get half chance heads tails weight goes negative probability drops initially sharply slowly lambda becomes positive probability climbs initially quickly slowly well lets assume weve seen little bit data estimate lambda maximize likelihood observed data probability observed data number heads times log probability heads log likelihood observed data plus number tails times log likelihood probability tails using form previous equation previous slide comes like okay suppose toss two heads two tails surprisingly optimum value lambda comes zero corresponds probability heads tails half toss three heads one tail optimum value lambda parameter comes positive cause theres three quarters chance head comes problem suppose training data saw four heads tails well means categorical distribution observed data tails means bigger lambda gets higher conditional log likelihood observed data lambda value positive infinity corresponds categorical distribution probability head one probably tails equals zero thats immediately problematic reasons mentioned earlier firstly dont want estimate categorical models certain feature appears say theres probability one outcome want smoothing normally data sparse also asking optimization procedure optimize something optimum value pos infinite value parameter thats gonna difficult optimization procedure several ways problem dealt maximum entropy models ill mention several concentrating use prior distribution usually used method days case two problems first problem optimal value lambda infinity long trip optimization procedure find optimal value parameter indeed kind cant move large number second problem got smoothing learned distribution gonna spiked empirical one gonna commonly happen nlp features gonna throw million features model gonna turn gonna say words beginning gho well saw two training data cases word person name therefore gonna say anytime see something start gho person name might turn future data thats necessary case might organization name place name maybe even word starts gho like ghoul isnt proper name solve problem one crude way solve problem stop optimization early could run iterative solver say lets run twenty iterations stop value lambda definitely finite grown fairly large optimization wont run infinite loop commonly used early maxent work idea start optimization procedure tracking likelihood making lambda bigger simply stop iterations example might stop value lambda five end result probability distribution lambda equals five might something like percent chance head one percent chance tail satisfied goal smoothing distribution slightly longer categorical optimization procedure run finite amount time heres better way achieving goal thats whats referred using prior distribution talk using map estimation stands maximum postiori estimation idea suppose prior prior expectation parameter values shouldnt large strongly say prior expectation features irrelevant classification prior expectation feature weights zero see evidence theyre useful features well gradually increase weights parameters balance evidence observed data prior beliefs evidence never totally defeat prior parameters smoothed kept finite explicitly changing optimization objective maximum postiori likelihood thats going penalized log likelihood condition penalized conditional log likelihood observed data going sum prior probability parameter weights plus previous conditional log likelihood observed data evidence common way practice use gaussian also known quadratic l priors formalization say lets assume prior belief parameter distributed according gaussian mean mu variance sigma squared probability parameter different values given equation normal curve reason soon see also referred quadratic l priors term ends really mattering important part bit getting squared distance parameter value mean parameters penalized value drifting far mean practice usually take mean zero saying feature irrelevant classification zeroweight features influence classification theres extra parameter hyper parameter big variance going control easy parameter move away zero variance small optimization keep parameter values clustered close zero variance weak theyll allowed walk farther away starting point building models taking two sigma squared equal one commonly works rather well play value commonly youll find kind sparsely evidenced many feature models build making sigma much smaller one quickly becomes problematic make sigma two sigma squared quite bit larger commonly could sort somewhere range half models work fairly well heres graph sorta shows optimization happens youve got regularization term exactly four zero distribution heads tails set regularization differently take two sigma squared equal one fairly strong regularization maximum positive gonna say youre likely get head tail still gonna something like two thirds head one thirds tail take two sigma squared equal ten regularization weaker value lambda gonna come two thats gonna say percent chance head five percent chance tail making two sigma squared infinite equivalent regularization thats leads us back previous model optimal value parameter infinite use gaussian priors putting tradeoff expectation matching versus smaller parameter values means practice multiple features recruited explain certain observation common ones receive weight way think think model formulation pay penalty nonzero parameter weight whereas get gain nonzero parameter weight every data item parameter feature parameter weight useful therefore feature useful lot times explaining data items allowed high weight prior basically overwhelmed whereas feature help explaining small number observations weight greatly constrained prior good thing know gaussian priors putting improve accuracy classifier well see let go concretely happens equations use prior going use penalized conditional log likelihood well term adding term prior term prior log normal distribution parameter weights go back look equation take log going effectively constant going taking log exp go away identity function end subtracting term whereas k log constant ignore maximization penalized conditional likelihood actually working squared term far parameter values away mean distribution term scaled two sigma squared able see big make two sigma squared sort balances tradeoff two terms derivative conditional penalized conditional log likelihood taking derivative part saw difference actual weight predicted weight feature subtracting derivative thats easy quadratic form derivative going say make zero predicted weight predicted expectation feature true going little bit less actual expectation penalized amount simplify things one step practice almost always take mean prior distribution zero since position least commitment assume features irrelevant equations simplify one time get results shown lambdas big lambdas distance away zero thats determining using let go back example making maximum entropy named entity recognition model ive shown actually showing slides leaving detail parameter weights model actually estimated model used gaussian prior smoothing understand little bit see effect estimation parameters model two features feature conjunction feature present tag proper noun previous tag preposition feature necessarily going much less evidenced training data feature current part speech proper noun general expectation higher weight given common features carry much predictive information thats see high weight given general feature much smaller weights given specific feature thats even though evidence conjunction feature fact matter youre quite likely named entity part speech proper noun weight going general features also see effect feature classes talked two current word grace fairly specific rarely evidenced feature whereas fact word begins capital letter g much general feature seeing weight good evidence person rather location going much general feature look maybe see cases thing happening also realize lot extra information feature conjunction feature conjunction get lot weight still saw example two cases discussed since tokens classified entity simply know previous word entity likely next word entity well get highly negative weight entity classes know previous word entity also know current word capitalized case theres strong evidence classify things differently feature get significant weight even though conjunction feature involves previous feature get kind lot evidence knowing conjunction true okay heres example showing effects using gaussian smoothing estimating maximum entropy model example learning part speech tagger work christina toutanova others stanford okay example shows typical effects happens smoothed versus unsmoothed maximum entropy models find weve trained models iterations one thing find thats good know looking accuracy test data model smoothing performs lot better model without smoothing okay thats percent better part speech tagging generally highly accurate better thinking error reduction youre reducing error rate thats actually quite large error reduction okay see bit see model smoothing shows typical pattern used observe training maxent models train model initially accuracy improves strongly peak starts decline see overfitting model training data way actually means worse test time common practice stop training maxent models around iteration sort little bit unfair normally happened without smoothing early stopping youd really getting accuracy something like lets say nevertheless thats still well less accuracy smoothed model hand smoothed model smoothed models likelihood nicely increases converges optimization stops giving us performance theres actually second effect thats interesting focusing particular performance model unknown words unknown words estimated part speech tag distribution estimated using special features generalize words letters begin end nevertheless lot features pretty sparse find model without smoothing especially overfits tends badly unknown words model smoothing three percent better unknown words thats significant increase helpful applications cause performance previous words model wasnt trained especially important good performance applications okay smoothing good softens distributions pushes weight onto explanatory features allows dump features model without lot problems least dont early stopping speeds convergence models let give one terminology note talking priors maximum posteriori estimation language bayesian statistics frequentist statistics people instead talk regularization maximum entropy models particular gaussian priors called l regularization im really going get really need know math comes doesnt matter name choose let quickly mention two ways smoothing another way thinking smoothing smooth data parameters saw earlier talking language models distribution actually saw four heads zero tails smooth addone smoothing say say got five heads one tail well weve solved problem maximum conditional likelihood estimation setting value parameter something like got finite value dont problem works fine simple example like reason thats practical models millions parameters becomes almost impossible know create artificial data way every parameter non zero value without creating enormous amounts artificial data amount artificial data overwhelms amount real data final thing thats commonly done nlp models use count cutoffs features idea calculate features data item model look features empirical support training data simply say something like okay feature observed three times less training data im going dump model estimate model remaining features discussion smoothing weak indirect smoothing method effectively saying going estimate weight rare features zero also think assigning gaussian prior zero variance mean zero weights never move away zero dropping low counts remove features need smoothing speed estimation model reducing model size crude method smoothing message id like give count cutoffs generally hurt accuracy presence proper smoothing lot people got habit using count cutoffs days regularized models cause cases would usually help use count cutoffs got less overfitting model proper smoothing shouldnt need use count cutoffs get best possible model doesnt mean theres reason use count cutoffs common reason use count cutoffs practice want shrink size model ten million parameters take lot memory store might prefer build model parameters obviously want keep useful features normally basically ones significant frequency occurrence data okay thats end discussion smoothing priors maxent models section weve talked use gaussian l priors recent work quite bit discussion using priors particular theres common use l priors different way cutting number features model may seen youve seen things like machine learning im gonna discuss classes,Course2,W5-S1-L4,W5,S1,L4,Smoothing-Regularization-Priors,5,1,4,section im gonna talk smooth maximum entropi model like model build natur languag process still issu model overfit want appli smooth techniqu paramet estim dont lead spiki distribut overfit observ train data topic smooth maximum entropi model often also describ use prior distribut paramet regular model issu smooth promin model build model build lot lot featur typic logist regress statist class model might four eight twelv featur there enough data suitabl estim paramet featur typic model well build natur languag process hundr thousand million even ten million featur one thing notic right simpli store array paramet valu substanti memori cost statist estim point view importantli paramet featur poorli estim well limit data estim paramet valu lot issu sparsiti overfit train data easi need smooth prevent mani featur saw happen see train time exampl might never see use model test time want give much weight featur happen see train time reason need smooth maxent model dont featur weight infinit iter solver use set paramet valu take long time get infin realli want chang model formul optim featur weight finit therefor easili found findabl optim procedur let motiv issu smooth look realli simpl exampl let assum toss coin distribut head tail natur way that formul maximum entropi featur kind weve talk two featur one head one tail well follow model distribut probabl head featur weight head probabl tail featur weight tail normal term case make look like two paramet realli one degre freedom piec math show maxent formul connect normal formul see twoclass logist regress statist textbook instead said realli matter differ weight head tail paramet say probabl head instead written like weve taken equat weve multipli term e minu lambda simplifi math littl get e lambda e lambda probabl tail kind trickeri come e zero e lambda plu zero simplifi that one e lambda plu one simpl form often see logist regress model gener that true right what import differ weight oppos paramet multi twoclass multiclass model okay graph lambda get classic logist curv weight zero get half chanc head tail weight goe neg probabl drop initi sharpli slowli lambda becom posit probabl climb initi quickli slowli well let assum weve seen littl bit data estim lambda maxim likelihood observ data probabl observ data number head time log probabl head log likelihood observ data plu number tail time log likelihood probabl tail use form previou equat previou slide come like okay suppos toss two head two tail surprisingli optimum valu lambda come zero correspond probabl head tail half toss three head one tail optimum valu lambda paramet come posit caus there three quarter chanc head come problem suppos train data saw four head tail well mean categor distribut observ data tail mean bigger lambda get higher condit log likelihood observ data lambda valu posit infin correspond categor distribut probabl head one probabl tail equal zero that immedi problemat reason mention earlier firstli dont want estim categor model certain featur appear say there probabl one outcom want smooth normal data spars also ask optim procedur optim someth optimum valu po infinit valu paramet that gonna difficult optim procedur sever way problem dealt maximum entropi model ill mention sever concentr use prior distribut usual use method day case two problem first problem optim valu lambda infin long trip optim procedur find optim valu paramet inde kind cant move larg number second problem got smooth learn distribut gonna spike empir one gonna commonli happen nlp featur gonna throw million featur model gonna turn gonna say word begin gho well saw two train data case word person name therefor gonna say anytim see someth start gho person name might turn futur data that necessari case might organ name place name mayb even word start gho like ghoul isnt proper name solv problem one crude way solv problem stop optim earli could run iter solver say let run twenti iter stop valu lambda definit finit grown fairli larg optim wont run infinit loop commonli use earli maxent work idea start optim procedur track likelihood make lambda bigger simpli stop iter exampl might stop valu lambda five end result probabl distribut lambda equal five might someth like percent chanc head one percent chanc tail satisfi goal smooth distribut slightli longer categor optim procedur run finit amount time here better way achiev goal that what refer use prior distribut talk use map estim stand maximum postiori estim idea suppos prior prior expect paramet valu shouldnt larg strongli say prior expect featur irrelev classif prior expect featur weight zero see evid theyr use featur well gradual increas weight paramet balanc evid observ data prior belief evid never total defeat prior paramet smooth kept finit explicitli chang optim object maximum postiori likelihood that go penal log likelihood condit penal condit log likelihood observ data go sum prior probabl paramet weight plu previou condit log likelihood observ data evid common way practic use gaussian also known quadrat l prior formal say let assum prior belief paramet distribut accord gaussian mean mu varianc sigma squar probabl paramet differ valu given equat normal curv reason soon see also refer quadrat l prior term end realli matter import part bit get squar distanc paramet valu mean paramet penal valu drift far mean practic usual take mean zero say featur irrelev classif zeroweight featur influenc classif there extra paramet hyper paramet big varianc go control easi paramet move away zero varianc small optim keep paramet valu cluster close zero varianc weak theyll allow walk farther away start point build model take two sigma squar equal one commonli work rather well play valu commonli youll find kind spars evidenc mani featur model build make sigma much smaller one quickli becom problemat make sigma two sigma squar quit bit larger commonli could sort somewher rang half model work fairli well here graph sorta show optim happen youv got regular term exactli four zero distribut head tail set regular differ take two sigma squar equal one fairli strong regular maximum posit gonna say your like get head tail still gonna someth like two third head one third tail take two sigma squar equal ten regular weaker valu lambda gonna come two that gonna say percent chanc head five percent chanc tail make two sigma squar infinit equival regular that lead us back previou model optim valu paramet infinit use gaussian prior put tradeoff expect match versu smaller paramet valu mean practic multipl featur recruit explain certain observ common one receiv weight way think think model formul pay penalti nonzero paramet weight wherea get gain nonzero paramet weight everi data item paramet featur paramet weight use therefor featur use lot time explain data item allow high weight prior basic overwhelm wherea featur help explain small number observ weight greatli constrain prior good thing know gaussian prior put improv accuraci classifi well see let go concret happen equat use prior go use penal condit log likelihood well term ad term prior term prior log normal distribut paramet weight go back look equat take log go effect constant go take log exp go away ident function end subtract term wherea k log constant ignor maxim penal condit likelihood actual work squar term far paramet valu away mean distribut term scale two sigma squar abl see big make two sigma squar sort balanc tradeoff two term deriv condit penal condit log likelihood take deriv part saw differ actual weight predict weight featur subtract deriv that easi quadrat form deriv go say make zero predict weight predict expect featur true go littl bit less actual expect penal amount simplifi thing one step practic almost alway take mean prior distribut zero sinc posit least commit assum featur irrelev equat simplifi one time get result shown lambda big lambda distanc away zero that determin use let go back exampl make maximum entropi name entiti recognit model ive shown actual show slide leav detail paramet weight model actual estim model use gaussian prior smooth understand littl bit see effect estim paramet model two featur featur conjunct featur present tag proper noun previou tag preposit featur necessarili go much less evidenc train data featur current part speech proper noun gener expect higher weight given common featur carri much predict inform that see high weight given gener featur much smaller weight given specif featur that even though evid conjunct featur fact matter your quit like name entiti part speech proper noun weight go gener featur also see effect featur class talk two current word grace fairli specif rare evidenc featur wherea fact word begin capit letter g much gener featur see weight good evid person rather locat go much gener featur look mayb see case thing happen also realiz lot extra inform featur conjunct featur conjunct get lot weight still saw exampl two case discuss sinc token classifi entiti simpli know previou word entiti like next word entiti well get highli neg weight entiti class know previou word entiti also know current word capit case there strong evid classifi thing differ featur get signific weight even though conjunct featur involv previou featur get kind lot evid know conjunct true okay here exampl show effect use gaussian smooth estim maximum entropi model exampl learn part speech tagger work christina toutanova other stanford okay exampl show typic effect happen smooth versu unsmooth maximum entropi model find weve train model iter one thing find that good know look accuraci test data model smooth perform lot better model without smooth okay that percent better part speech tag gener highli accur better think error reduct your reduc error rate that actual quit larg error reduct okay see bit see model smooth show typic pattern use observ train maxent model train model initi accuraci improv strongli peak start declin see overfit model train data way actual mean wors test time common practic stop train maxent model around iter sort littl bit unfair normal happen without smooth earli stop youd realli get accuraci someth like let say nevertheless that still well less accuraci smooth model hand smooth model smooth model likelihood nice increas converg optim stop give us perform there actual second effect that interest focus particular perform model unknown word unknown word estim part speech tag distribut estim use special featur gener word letter begin end nevertheless lot featur pretti spars find model without smooth especi overfit tend badli unknown word model smooth three percent better unknown word that signific increas help applic caus perform previou word model wasnt train especi import good perform applic okay smooth good soften distribut push weight onto explanatori featur allow dump featur model without lot problem least dont earli stop speed converg model let give one terminolog note talk prior maximum posteriori estim languag bayesian statist frequentist statist peopl instead talk regular maximum entropi model particular gaussian prior call l regular im realli go get realli need know math come doesnt matter name choos let quickli mention two way smooth anoth way think smooth smooth data paramet saw earlier talk languag model distribut actual saw four head zero tail smooth addon smooth say say got five head one tail well weve solv problem maximum condit likelihood estim set valu paramet someth like got finit valu dont problem work fine simpl exampl like reason that practic model million paramet becom almost imposs know creat artifici data way everi paramet non zero valu without creat enorm amount artifici data amount artifici data overwhelm amount real data final thing that commonli done nlp model use count cutoff featur idea calcul featur data item model look featur empir support train data simpli say someth like okay featur observ three time less train data im go dump model estim model remain featur discuss smooth weak indirect smooth method effect say go estim weight rare featur zero also think assign gaussian prior zero varianc mean zero weight never move away zero drop low count remov featur need smooth speed estim model reduc model size crude method smooth messag id like give count cutoff gener hurt accuraci presenc proper smooth lot peopl got habit use count cutoff day regular model caus case would usual help use count cutoff got less overfit model proper smooth shouldnt need use count cutoff get best possibl model doesnt mean there reason use count cutoff common reason use count cutoff practic want shrink size model ten million paramet take lot memori store might prefer build model paramet obvious want keep use featur normal basic one signific frequenc occurr data okay that end discuss smooth prior maxent model section weve talk use gaussian l prior recent work quit bit discuss use prior particular there common use l prior differ way cut number featur model may seen youv seen thing like machin learn im gonna discuss class,[ 4  5 14 13 12]
176,Course2_W5-S2-L1_An_Intro_to_Parts_of_Speech_and_POS_Tagging_13-19,segment give brief introduction task part speech tagging well come back see discriminative sequence models used technique part speech tagging many linguistic traditions different parts globe west idea parts speech perhaps started aristotle fourth century common era suggested idea parts speech otherwise known lexical categories word classes tags pos common abbreviation think today parts speech normally attributed dionysius thrax alexandria proposed eight parts speech thats number thats still us today number parts speech people get taught school sometime though actually look details little eight parts speech dionysius thrax proposed arent us today normal list people taught schools english noun verb adjective adverb preposition conjunction pronoun interjection thrax actually little differences list course working ancient greek english beginning list starts note doesnt recognize adjective part speech greek latin adjectives nouns behave similarly recognize participle part speech difference separate part speech given articles words like whereas modern list get class interjections status articles always bit controversial current standard set eight parts speech articles seen case adjectives actually soon start looking grammar languages like english articles special behavior quite different regular adjectives example adjectives get multiple large green leather couch whereas cant things like book except kind speech hesitation article using part speech tagging computational linguistics normally work rather eight parts speech lot distinctions useful make multiplies number parts speech start indicate kinds parts speech get slide see various hierarchical groupings inside nouns divide proper nouns common nouns might also want divide singular plural would happen proper common nouns verbs much divide main verbs see registered versus various auxiliary modal verbs like adjectives adverbs note adjectives least short adjectives english several forms positive comparative superlative might want distinguish forms well neither lists last slide numbers seem fairly obvious practically useful part speech recognize similarly among closed classes ill say bit closed classes next slide dont category thats called articles category called determiners modern linguistic tradition group articles together certain demonstratives behave similarly words like categories prepositions familiar distinguishing class particles particles words normally prepositions take part english phrasal verb constructions things like made meaning invent top level distinction last slide open classes closed classes clear parts speech like determiners pronouns occupied small set words english pronouns theres first second person pronouns third person plural small set words arent new pronouns invented things like call closed class general closed classes dont get new members though know sometimes occasionally long term language change sometimes people suggested inventing new gender neutral pronouns english though nothings really taken contrast open classes nouns verbs adjectives adverbs classes vast number members beyond easy gain new members new protocol computer science create new noun like ssh quickly decide useful use verb well say sshed server fix configuration task part speech tagging say word part speech context running text reason thats trivial lots words one part speech example word back adjective modifying noun like back door noun referring body part back adverb phrase like win voters back also verb another political example promise back bill wanting see instance word back assign one parts speech task part speech tagging start text plays well others word consider possible parts speech plays plural noun verb well interjection adjective noun adverb preposition others plural noun words ambiguous going choose correct part speech context plays verb well adverb modifying plays get disambiguating parts speech finding end weve got know choices determining part speech tags depending part speech tag set commonly natural language processing applications somewhere twelve part speech tags differentiated see set part speech tags differentiating things like plural noun singular noun heres singular noun verb form verb third person singular plays opposed play particular partofspeech tag set seeing penn treebank partofspeech tags commonly used partofspeech tags english especially united states im embarrassed say ive business long enough actually tell heart penn partofspeech tags set cover word classes like punctuation good part speech tagging well letting answer high school english problems variety uses particular uses parts speech sufficient youre text speech want see instance word lead want know whether pronoun lead lead answer well whats part speech verb lead noun lead parts speech lets easily surface linguistic analysis lots applications useful pick call base noun phrases theyre noun noun compound possibly modified front determiner adjectives nearest school old green couch base noun phrases whereas big complex noun phrases relative clauses arent parts speech easily write regular expressions pick things like base noun phrases verbs followed particle phrasal verbs used lot applications want shallow processing nevertheless want able put give user multiword keywords something like part speech tagging commonly used input speed parser quite parsers work already part speech tagged text even ones part speech tagging often sped first process part speech tagging finally lot models build probability estimation sparse try work word level discussed various ways backing might back trigram bigram earlier another common way backing back word dont know much part speech category predict based knowing adjective part speech tagging performance normally measured accuracy percent tags get right answer best current taggers get percent words right sounds really good important realize part speech tagging sense easy task least task get high numbers theres baseline approach part speech tagging tag every word common tag know training data dont know training data tag noun turns fairly simple baseline method already gives performance thats turns get high baseline couple reasons one reason many words unambiguous see famous always adjective also way part speech tagging normally measured get points every token get points every instance unambiguous articles common like even get points getting part speech tags punctuation right doesnt mean parts speech decisions easy part speech decisions quite difficult even human beings examples word around around acting particle got around around straight verb time prepositional phrase around corner around preposition third example around following straight verb actually time modifier noun phrase around short around around adverb modifying number kind like adjective turns english corpus around eleven percent word types ambiguous regard part speech doesnt sound like many tend common words word word introduces subordinate clause penn treebank words tagged preposition might surprise theres linguistic argument determiner play adverb modifying far far adverb turns although eleven percent word types ambiguous part speech percent word tokens ambiguous actually lot decisions part speech tagger make okay hope thats given bit background information part speech tagging make sense task terms methods approaching,Course2,W5-S2-L1,W5,S2,L1,An,5,2,1,segment give brief introduct task part speech tag well come back see discrimin sequenc model use techniqu part speech tag mani linguist tradit differ part globe west idea part speech perhap start aristotl fourth centuri common era suggest idea part speech otherwis known lexic categori word class tag po common abbrevi think today part speech normal attribut dionysiu thrax alexandria propos eight part speech that number that still us today number part speech peopl get taught school sometim though actual look detail littl eight part speech dionysiu thrax propos arent us today normal list peopl taught school english noun verb adject adverb preposit conjunct pronoun interject thrax actual littl differ list cours work ancient greek english begin list start note doesnt recogn adject part speech greek latin adject noun behav similarli recogn participl part speech differ separ part speech given articl word like wherea modern list get class interject statu articl alway bit controversi current standard set eight part speech articl seen case adject actual soon start look grammar languag like english articl special behavior quit differ regular adject exampl adject get multipl larg green leather couch wherea cant thing like book except kind speech hesit articl use part speech tag comput linguist normal work rather eight part speech lot distinct use make multipli number part speech start indic kind part speech get slide see variou hierarch group insid noun divid proper noun common noun might also want divid singular plural would happen proper common noun verb much divid main verb see regist versu variou auxiliari modal verb like adject adverb note adject least short adject english sever form posit compar superl might want distinguish form well neither list last slide number seem fairli obviou practic use part speech recogn similarli among close class ill say bit close class next slide dont categori that call articl categori call determin modern linguist tradit group articl togeth certain demonstr behav similarli word like categori preposit familiar distinguish class particl particl word normal preposit take part english phrasal verb construct thing like made mean invent top level distinct last slide open class close class clear part speech like determin pronoun occupi small set word english pronoun there first second person pronoun third person plural small set word arent new pronoun invent thing like call close class gener close class dont get new member though know sometim occasion long term languag chang sometim peopl suggest invent new gender neutral pronoun english though noth realli taken contrast open class noun verb adject adverb class vast number member beyond easi gain new member new protocol comput scienc creat new noun like ssh quickli decid use use verb well say sshed server fix configur task part speech tag say word part speech context run text reason that trivial lot word one part speech exampl word back adject modifi noun like back door noun refer bodi part back adverb phrase like win voter back also verb anoth polit exampl promis back bill want see instanc word back assign one part speech task part speech tag start text play well other word consid possibl part speech play plural noun verb well interject adject noun adverb preposit other plural noun word ambigu go choos correct part speech context play verb well adverb modifi play get disambigu part speech find end weve got know choic determin part speech tag depend part speech tag set commonli natur languag process applic somewher twelv part speech tag differenti see set part speech tag differenti thing like plural noun singular noun here singular noun verb form verb third person singular play oppos play particular partofspeech tag set see penn treebank partofspeech tag commonli use partofspeech tag english especi unit state im embarrass say ive busi long enough actual tell heart penn partofspeech tag set cover word class like punctuat good part speech tag well let answer high school english problem varieti use particular use part speech suffici your text speech want see instanc word lead want know whether pronoun lead lead answer well what part speech verb lead noun lead part speech let easili surfac linguist analysi lot applic use pick call base noun phrase theyr noun noun compound possibl modifi front determin adject nearest school old green couch base noun phrase wherea big complex noun phrase rel claus arent part speech easili write regular express pick thing like base noun phrase verb follow particl phrasal verb use lot applic want shallow process nevertheless want abl put give user multiword keyword someth like part speech tag commonli use input speed parser quit parser work alreadi part speech tag text even one part speech tag often sped first process part speech tag final lot model build probabl estim spars tri work word level discuss variou way back might back trigram bigram earlier anoth common way back back word dont know much part speech categori predict base know adject part speech tag perform normal measur accuraci percent tag get right answer best current tagger get percent word right sound realli good import realiz part speech tag sens easi task least task get high number there baselin approach part speech tag tag everi word common tag know train data dont know train data tag noun turn fairli simpl baselin method alreadi give perform that turn get high baselin coupl reason one reason mani word unambigu see famou alway adject also way part speech tag normal measur get point everi token get point everi instanc unambigu articl common like even get point get part speech tag punctuat right doesnt mean part speech decis easi part speech decis quit difficult even human be exampl word around around act particl got around around straight verb time preposit phrase around corner around preposit third exampl around follow straight verb actual time modifi noun phrase around short around around adverb modifi number kind like adject turn english corpu around eleven percent word type ambigu regard part speech doesnt sound like mani tend common word word word introduc subordin claus penn treebank word tag preposit might surpris there linguist argument determin play adverb modifi far far adverb turn although eleven percent word type ambigu part speech percent word token ambigu actual lot decis part speech tagger make okay hope that given bit background inform part speech tag make sens task term method approach,[ 1  4  0 14 13]
177,Course2_W5-S2-L2_Some_Methods_and_Results_on_Sequence_Models_for_POS_Tagging_13-04,let return part speech taking say little bit sequence models perform kind features end used useful main sources information part speech tagging one source information knowledge neighboring words possible part speech tags sentence bill saw man yesterday lot part speech ambiguities bill either proper noun verb saw either noun verb either determiner whats tagged introducing complement clause man noun verb point start notice constraints neighborhoods cant case determiner followed verb bad sequence sequence information gives us ideas part speech tag things people started working part speech tags people thought main source evidence turns thats really case easiest biggest source evidence statistics knowledge often words different parts speech also get long way saying man rarely used verb therefore without even looking context probably tag take part speech tag noun man latter source information proves useful thats mean first source information isnt useful gonna wanna start putting better features maximum entropy tagger producing feature based sequence model one part features define word knowing word useful turns get lot value especially rare unknown words also putting features pick properties words looking word lowercase form maybe many words havent seen capitalized beginning sentence know lowercase form parts speech help us knowing beginning word know word starts un gives bit information part speech knowing suffix word know word ends ly almost certainly adverb english capitalization useful english capital letter youre away beginning sentence good clue youve got proper noun heres interesting different kind feature weve made lot use stanford systems call word shape features idea first suggested michael collins different way making features words creates natural equivalence classes useful generalization idea youre mapping word one small set equivalence classes represents shape different exact schemes use say digit character also digit character lets collapse equivalence classing sequence characters theres hyphen weve got sequence lower case letter characters give word shape dx number digits followed hyphen followed number lower case letters kind feature proves useful feature quite discriminative models heres headline good news get clever using kinds features build model looks current word assigns tag turns without looking context whatsoever build part speech tagger overall gets percent words right actually quite well unknown words well commonly pull separately accuracy unknown words always lower distinctively different accuracy known words figures give picture kind accuracies expecting part speech taggers different features impact classification discussed idea baseline method giving frequent tag words calling everything else noun overall gives gets half unknown words right next set models people looked hmm models hidden markov models im really gonna cover class sort mid state art part speech tagging got around percent words right little bit better unknown words saw previous slide straight classifier sequence model rich carefully chosen features actually works rather well much better unknown words doesnt actually perform badly known words people gone idea could use features better job able incorporated hidden markov models well thorsten brants produced high performance hidden markov model based tagger used feature based ideas predicting unknown words performed rather nicely gets around percent overall competitive performance unknown words keep building features maximum entropy markov model including using context features didnt first model go sort standard maximum entropy markov model tagger kind weve discussed might getting almost words starting bit better unknown words work pushed several places including stanford stanford model thats like almost like maximum entropy model allows bidirectional dependencies thats pushing overall accuracy percent pushing unknown word accuracy thats getting close good expect part speech taggers perform reasons humans sure right answer part speech tagging problems also gold standards errors sometimes human goof put wrong part speech tag even seems like correct answer clear seems like around percent upper bound could possibly hope score kind test sets part speech tagging one wants keep working improving things normally way one goes staring hard output part speech tagger whatever looking makes errors thinking ways could encode information features would let detect something gone wrong get system prefer configuration im gonna give example error part speech tagger theres word part speech tagger chosen preposition tag whereas chosen adverb tag cause soon modifying soon well could fix well seems like information want use next word soon thats really good clue functioning adverb fix error adding feature looks next word using next words well previous words previous part speech tags part speech tagger intrinsic labeled proper noun thats common error part speech tagger sees capitalized word never saw training data normally first best guess say thats proper noun tha ts necessarily true beginning sentence words get capitalized maybe actually know word intrinsic seen several times training data know adjective could utilize knowledge wed able get case right well put feature whats word lower cased feature would argue word tagged adjective might able hope fix error note features didnt require sequence information looking features word right whats lower case form word thats interesting general observation early work sequence models people fixated sequence models using sort sequence labelings sequence tags predict tags turns many problems conventionally done sequence labeling sequence information barely useful normally gives fraction extra performance little already saw case part speech tagging using model using lot features current word predict tag performs nicely gave us performance tokens unknown tokens lot better suggested previous slide also considering things like whats next word using influence tag whats previous word using influence tag thats whats referred three words model youre features independently three words influencing tag actually quite stunning well performs gives total accuracy percent unknown word accuracy percent thing notice performance actually quite similar performance get sequence models go back performance see performance close performance thats listed mem tagger fractionally exceedingly close performance okay weve learned part speech tagging changing generative discriminative models doesnt give big boost part speech tagging performance big boost comes able put lots features observations combine together good way things like suffix analysis word shape features lower casing things like additional power rich features shown result major improvements accuracy sequence models cost cost youll probably find assignment training discriminative models much slower training generative models remember back language models well estimate language models essentially counting data whereas discriminative models processes numerical optimization much time intensive completes discussion part speech taggers position understand something part speech tagging general apply maximum entropy sequence models part speech tagging similar sequence problems,Course2,W5-S2-L2,W5,S2,L2,Some,5,2,2,let return part speech take say littl bit sequenc model perform kind featur end use use main sourc inform part speech tag one sourc inform knowledg neighbor word possibl part speech tag sentenc bill saw man yesterday lot part speech ambigu bill either proper noun verb saw either noun verb either determin what tag introduc complement claus man noun verb point start notic constraint neighborhood cant case determin follow verb bad sequenc sequenc inform give us idea part speech tag thing peopl start work part speech tag peopl thought main sourc evid turn that realli case easiest biggest sourc evid statist knowledg often word differ part speech also get long way say man rare use verb therefor without even look context probabl tag take part speech tag noun man latter sourc inform prove use that mean first sourc inform isnt use gonna wanna start put better featur maximum entropi tagger produc featur base sequenc model one part featur defin word know word use turn get lot valu especi rare unknown word also put featur pick properti word look word lowercas form mayb mani word havent seen capit begin sentenc know lowercas form part speech help us know begin word know word start un give bit inform part speech know suffix word know word end ly almost certainli adverb english capit use english capit letter your away begin sentenc good clue youv got proper noun here interest differ kind featur weve made lot use stanford system call word shape featur idea first suggest michael collin differ way make featur word creat natur equival class use gener idea your map word one small set equival class repres shape differ exact scheme use say digit charact also digit charact let collaps equival class sequenc charact there hyphen weve got sequenc lower case letter charact give word shape dx number digit follow hyphen follow number lower case letter kind featur prove use featur quit discrimin model here headlin good news get clever use kind featur build model look current word assign tag turn without look context whatsoev build part speech tagger overal get percent word right actual quit well unknown word well commonli pull separ accuraci unknown word alway lower distinct differ accuraci known word figur give pictur kind accuraci expect part speech tagger differ featur impact classif discuss idea baselin method give frequent tag word call everyth els noun overal give get half unknown word right next set model peopl look hmm model hidden markov model im realli gonna cover class sort mid state art part speech tag got around percent word right littl bit better unknown word saw previou slide straight classifi sequenc model rich care chosen featur actual work rather well much better unknown word doesnt actual perform badli known word peopl gone idea could use featur better job abl incorpor hidden markov model well thorsten brant produc high perform hidden markov model base tagger use featur base idea predict unknown word perform rather nice get around percent overal competit perform unknown word keep build featur maximum entropi markov model includ use context featur didnt first model go sort standard maximum entropi markov model tagger kind weve discuss might get almost word start bit better unknown word work push sever place includ stanford stanford model that like almost like maximum entropi model allow bidirect depend that push overal accuraci percent push unknown word accuraci that get close good expect part speech tagger perform reason human sure right answer part speech tag problem also gold standard error sometim human goof put wrong part speech tag even seem like correct answer clear seem like around percent upper bound could possibl hope score kind test set part speech tag one want keep work improv thing normal way one goe stare hard output part speech tagger whatev look make error think way could encod inform featur would let detect someth gone wrong get system prefer configur im gonna give exampl error part speech tagger there word part speech tagger chosen preposit tag wherea chosen adverb tag caus soon modifi soon well could fix well seem like inform want use next word soon that realli good clue function adverb fix error ad featur look next word use next word well previou word previou part speech tag part speech tagger intrins label proper noun that common error part speech tagger see capit word never saw train data normal first best guess say that proper noun tha ts necessarili true begin sentenc word get capit mayb actual know word intrins seen sever time train data know adject could util knowledg wed abl get case right well put featur what word lower case featur would argu word tag adject might abl hope fix error note featur didnt requir sequenc inform look featur word right what lower case form word that interest gener observ earli work sequenc model peopl fixat sequenc model use sort sequenc label sequenc tag predict tag turn mani problem convent done sequenc label sequenc inform bare use normal give fraction extra perform littl alreadi saw case part speech tag use model use lot featur current word predict tag perform nice gave us perform token unknown token lot better suggest previou slide also consid thing like what next word use influenc tag what previou word use influenc tag that what refer three word model your featur independ three word influenc tag actual quit stun well perform give total accuraci percent unknown word accuraci percent thing notic perform actual quit similar perform get sequenc model go back perform see perform close perform that list mem tagger fraction exceedingli close perform okay weve learn part speech tag chang gener discrimin model doesnt give big boost part speech tag perform big boost come abl put lot featur observ combin togeth good way thing like suffix analysi word shape featur lower case thing like addit power rich featur shown result major improv accuraci sequenc model cost cost youll probabl find assign train discrimin model much slower train gener model rememb back languag model well estim languag model essenti count data wherea discrimin model process numer optim much time intens complet discuss part speech tagger posit understand someth part speech tag gener appli maximum entropi sequenc model part speech tag similar sequenc problem,[ 4  1  5 14 13]
178,Course2_W5-S3-L1_Syntactic_Structure-_Constituency_vs_Dependency_8-46,okay section course gonna start talking parse text using statistical models let introduce two views syntactic structure well explore parsing models first view whats called constituency parsing idea constituency parsing organize words nested constituents let give bit example long ago sentence fed raises interest rates parts speech weve seen proper noun verb nouns sense correct parse wanted say nouns go together form noun phrase verb noun phrase go together form verb phrase everything goes together form sentence things constituents claim part sentence thats also sometimes shown square brackets interest rates constituent unit sentence know constituent well thats complex issue ideas use one big idea looking words stay together go together phrases move around various syntactic constructions example weve got two three words children drugs well something youll notice actually swap around opposite order thats fine john talked drugs children could things example maybe could take one front certain discourse contexts could say children john talked drugs evidence groups words constituent hand randomly reorder words sentence normally get something doesnt work ive tried take word drugs end thought hmm maybe take stick talked doesnt work cant say john talked john talked drugs children lots tests another test idea phrasal expansion substitution phrase box make bigger unit like right top box make smaller unit like general apply substitutions place phrase box appear lots syntactic tests people use constituency really want learn lot take linguistics course youll see detail hope thats enough give rough idea simple sentences kind thing happens youve got big real sentences sentence analyst said mr stronach wants resume influential role running company idea constituency structure phrase structure units constituents heres constituent influential role heres another constituent name mr stronach different constituents bottom running company constituent also bigger running company also constituent constituents nest together kind parse trees nlp people actually spend time dealing well talk note one attribute well actual pronounced words theyre postulating unpronounced units well called empty elements straightforward way model phrase structural constituency structure context free grammar assumed youve seen somewhere computer science idea youve got rules says rewrites bcd apply lets say b goes ef apply get phrase structure rules bcd goes ef gives constituency structure kind rules write arbitrary symbols actually look happens natural languages theres always lot structure idea gets called xbar theory says natural languages large phrases headed particular kinds words take modifiers dependents around informally call things verb phrase precisely verb middle call things noun phrase precisely noun central element noun phrase something like man corner centrally noun man modifiers sitting around although according context free grammar arbitrary symbols practice give names reflect headed structure phrase structure true categories well like adjective phrases adverb phrases theyre large part category inventory kinds phrases sentences various kinds inverted questioning sentences basically correspond subject noun phrase verb phrase appear bigger complement clauses sbar sbarq also something like introducing sentence various miscellaneous stuff happens well learn lot linguistics syntax class idea headed phrase structure connection phrase structure big approach syntactic structure want show thats dependency structure dependency structure idea directly show words sentence words depend modify arguments lets look example boy put put tortoise rug head whole sentence putting thats central word sentence thats sometimes represented dependency arc coming edge pointing whole head whole sentence okay put two arguments puts arguments boy im sorry got three arguments tortoise rug things put put okay point sort say well boy modifiers arguments modifier tortoise modifiers arguments modifier preposition takes complement argument complement rug rug head modifier rug set arrows ive drawn dependency analysis sentence form dependency analysis major syntactic representation well use okay hope thats enough give idea representations use syntactic structure two representations connect,Course2,W5-S3-L1,W5,S3,L1,Syntactic,5,3,1,okay section cours gonna start talk pars text use statist model let introduc two view syntact structur well explor pars model first view what call constitu pars idea constitu pars organ word nest constitu let give bit exampl long ago sentenc fed rais interest rate part speech weve seen proper noun verb noun sens correct pars want say noun go togeth form noun phrase verb noun phrase go togeth form verb phrase everyth goe togeth form sentenc thing constitu claim part sentenc that also sometim shown squar bracket interest rate constitu unit sentenc know constitu well that complex issu idea use one big idea look word stay togeth go togeth phrase move around variou syntact construct exampl weve got two three word children drug well someth youll notic actual swap around opposit order that fine john talk drug children could thing exampl mayb could take one front certain discours context could say children john talk drug evid group word constitu hand randomli reorder word sentenc normal get someth doesnt work ive tri take word drug end thought hmm mayb take stick talk doesnt work cant say john talk john talk drug children lot test anoth test idea phrasal expans substitut phrase box make bigger unit like right top box make smaller unit like gener appli substitut place phrase box appear lot syntact test peopl use constitu realli want learn lot take linguist cours youll see detail hope that enough give rough idea simpl sentenc kind thing happen youv got big real sentenc sentenc analyst said mr stronach want resum influenti role run compani idea constitu structur phrase structur unit constitu here constitu influenti role here anoth constitu name mr stronach differ constitu bottom run compani constitu also bigger run compani also constitu constitu nest togeth kind pars tree nlp peopl actual spend time deal well talk note one attribut well actual pronounc word theyr postul unpronounc unit well call empti element straightforward way model phrase structur constitu structur context free grammar assum youv seen somewher comput scienc idea youv got rule say rewrit bcd appli let say b goe ef appli get phrase structur rule bcd goe ef give constitu structur kind rule write arbitrari symbol actual look happen natur languag there alway lot structur idea get call xbar theori say natur languag larg phrase head particular kind word take modifi depend around inform call thing verb phrase precis verb middl call thing noun phrase precis noun central element noun phrase someth like man corner central noun man modifi sit around although accord context free grammar arbitrari symbol practic give name reflect head structur phrase structur true categori well like adject phrase adverb phrase theyr larg part categori inventori kind phrase sentenc variou kind invert question sentenc basic correspond subject noun phrase verb phrase appear bigger complement claus sbar sbarq also someth like introduc sentenc variou miscellan stuff happen well learn lot linguist syntax class idea head phrase structur connect phrase structur big approach syntact structur want show that depend structur depend structur idea directli show word sentenc word depend modifi argument let look exampl boy put put tortois rug head whole sentenc put that central word sentenc that sometim repres depend arc come edg point whole head whole sentenc okay put two argument put argument boy im sorri got three argument tortois rug thing put put okay point sort say well boy modifi argument modifi tortois modifi argument modifi preposit take complement argument complement rug rug head modifi rug set arrow ive drawn depend analysi sentenc form depend analysi major syntact represent well use okay hope that enough give idea represent use syntact structur two represent connect,[ 0 14  4 10 13]
179,Course2_W5-S3-L2_Empirical-Data-Driven_Approach_to_Parsing_7-11,segment im going talk nlp researchers adopted empirical approach using data statistics making progress problem parsing structure human language sentences situation existed well classic way parsing mean people would write grammar might phrase structure grammar context free grammar two terms equivalent might complex format lexicon something like baby example ive shown grammar sort use grammar parsing system proof system find allowed parses string words problem approach scaled badly kind sentences people typically write say didnt give suitable solution problem let try give indication problem consider full version sentence weve used example fed raises interest rates percent effort control inflation linguist write grammar thats completely crazy smallest possible grammar parse sentence say parser parse sentence find grammar already allows parses sentence write slightly general grammar allow things commonly happen natural language sentences well suddenly grammar allows parses sentence look kind broad coverage grammars actually used statistical parsers well talk soon try parse sentence find sentence millions parses see weve got problem thats kind control classical parsing stuck two problems one hand could try put lot constraint grammars limit ability generate weird unlikely parses sentences problem extent grammar became nonrobust quite common traditional systems even gave well edited text like newswire articles know something like third sentences would parse whatsoever hand made grammar looser could parse sentences found even extremely simple sentences started getting unlikely weird parses didnt good way choose need system grammar thats flexible enough deal flexible ways humans use language daily life predictive enough allow us choose correct likely parse sentence thats precisely statistical parsing systems well look lectures allow us build statistical parsing system thats topic data comes huge enabling factor able build statistical parsing systems development treebanks im showing example sentence penn treebank earliest widely available treebank still famous weve shown examples treebank sentences actually get treebank kind structures sentence tree structure indicated parentheses nested show constituency youre old time lisp programmer look like indeed lisp expressions giving structure sentence noun phrases verb phrases various things markings also indicate empty elements various kinds also sort various functional annotations says subject sentence kind annotated data position use machine learning techniques train parsers say likely parse sentence also much better position build robust parsers lot sentences give us kind indication kind flexibility structure need admit languages able parse typical language use revelation youre starting actually seems like building treebank lot work low payoff cause whole power writing rules grammar write one rule generalizes thousands millions fact infinite number possible sentences whereas building treebank youre taking one sentence time parsing seems slower less useful actually treebank gives us many many useful things gives us reusability normally parser one person completely used developing parser another person done completely independently weve able build many many parsers part speech taggers etc penn treebank data also wider uses example many linguists use penn treebank source testing developing hypotheses language penntree bank gives us something broad coverage gives us statistics thatll allow us choose parses gives us another thing thats proved incredibly important gives us way evaluate parsers common testbed good data parsers better worth parsers course days penn treebank isnt parser dozens treebanks treebanks different languages treebanks different genres original penn treebank newswire extended genres treebanks cover things like well biomedical english one popular domain also things like looking various kinds informal texts specialized kinds english usage treebanks questions okay hope thats motivated important take empirical approach developing parsing systems human languages great value weve able get data resources like treebanks,Course2,W5-S3-L2,W5,S3,L2,Empirical-Data-Driven,5,3,2,segment im go talk nlp research adopt empir approach use data statist make progress problem pars structur human languag sentenc situat exist well classic way pars mean peopl would write grammar might phrase structur grammar context free grammar two term equival might complex format lexicon someth like babi exampl ive shown grammar sort use grammar pars system proof system find allow pars string word problem approach scale badli kind sentenc peopl typic write say didnt give suitabl solut problem let tri give indic problem consid full version sentenc weve use exampl fed rais interest rate percent effort control inflat linguist write grammar that complet crazi smallest possibl grammar pars sentenc say parser pars sentenc find grammar alreadi allow pars sentenc write slightli gener grammar allow thing commonli happen natur languag sentenc well suddenli grammar allow pars sentenc look kind broad coverag grammar actual use statist parser well talk soon tri pars sentenc find sentenc million pars see weve got problem that kind control classic pars stuck two problem one hand could tri put lot constraint grammar limit abil gener weird unlik pars sentenc problem extent grammar becam nonrobust quit common tradit system even gave well edit text like newswir articl know someth like third sentenc would pars whatsoev hand made grammar looser could pars sentenc found even extrem simpl sentenc start get unlik weird pars didnt good way choos need system grammar that flexibl enough deal flexibl way human use languag daili life predict enough allow us choos correct like pars sentenc that precis statist pars system well look lectur allow us build statist pars system that topic data come huge enabl factor abl build statist pars system develop treebank im show exampl sentenc penn treebank earliest wide avail treebank still famou weve shown exampl treebank sentenc actual get treebank kind structur sentenc tree structur indic parenthes nest show constitu your old time lisp programm look like inde lisp express give structur sentenc noun phrase verb phrase variou thing mark also indic empti element variou kind also sort variou function annot say subject sentenc kind annot data posit use machin learn techniqu train parser say like pars sentenc also much better posit build robust parser lot sentenc give us kind indic kind flexibl structur need admit languag abl pars typic languag use revel your start actual seem like build treebank lot work low payoff caus whole power write rule grammar write one rule gener thousand million fact infinit number possibl sentenc wherea build treebank your take one sentenc time pars seem slower less use actual treebank give us mani mani use thing give us reusabl normal parser one person complet use develop parser anoth person done complet independ weve abl build mani mani parser part speech tagger etc penn treebank data also wider use exampl mani linguist use penn treebank sourc test develop hypothes languag penntre bank give us someth broad coverag give us statist thatll allow us choos pars give us anoth thing that prove incred import give us way evalu parser common testb good data parser better worth parser cours day penn treebank isnt parser dozen treebank treebank differ languag treebank differ genr origin penn treebank newswir extend genr treebank cover thing like well biomed english one popular domain also thing like look variou kind inform text special kind english usag treebank question okay hope that motiv import take empir approach develop pars system human languag great valu weve abl get data resourc like treebank,[ 0  4 14  8 13]
180,Course2_W5-S3-L3_The_Exponential_Problem_in_Parsing_14-30,segment im gonna talk huge number parses human languages sentences comes might go solving statistical parsing systems key part parsing human language sentences problem whats called attachment psychological literature various kinds phrases decide modify time prepositional phrases adverbial participial phrases infinitives coordinations things like attachment decision working modifying lets work example get idea problem sentence board approved acquisition royal trust co limited toronto share monthly meeting look sentence well find find well subject board verb approved object acquisition look find actually four prepositional phrases row royal trust co ltd toronto cents share monthly meeting need decide modifying general prepositional phrase modify verb modify preceding noun phrase got two targets actually prepositional phrases actually includes noun phrase get later prepositional phrases therell things one choose modify okay lets go one turn right royal trust co thats modifying acquisition approval acquisition royal trust co thats dependency format relationship okay toronto modifying approval acquisition rather royal trust co limited toronto prepositional phrase modifying royal trust co limited okay go third one cents share thats modifying toronto royal trust co modifying acquisition acquisition cents share last prepositional phrase monthly meeting thats presumably referring acquisition monthly meeting things rather approval monthly meeting prepositional phrase modifies first verb okay kind complex though human beings read sentence understand means attachment decisions without apparent effort time thing want think moment many possible attachments well sort think know one two three four things could attach bunch points one chosen completely freely fairly clear well going get exponential number possibilities exponential number prepositional phrases precisely cause huge number parses saw previously like parses sentence came really making number decisions things like category attachment lot fairly freely combine multiply ambiguities get huge number however case examples like youre completely free attach anything anywhere royal trust co could certainly choose attach either preceding noun verb syntactically attached toronto three choices could attach royal trust co preceding noun phrase preceding verb phrase syntactically possible youve made higher attachments means earlier sentence things become restricted particular decided attach share modifier acquisition restricts attachments possible prepositional phrase could still attach share could attach acquisition could attach approved ones kind hidden behind attachment share acquisition ones become possible anymore leaves kind little puzzle well add prepositional phrase attachments sentence many possible attachments observe constraint corresponds dependencies crossing tree structure sentence phrase structure grammar well turns answer question well worked piece math answer found series catalan numbers sequence turns lot places wherever youve got kind treelike context example even turns probabilistic graphical models important thing know purposes growth catalan numbers still exponential sequence get exponential number parses unless youre math geek working factorials probably isnt useful thing rather probably much valuable linguistically actually look possible structures simple sentence get sense many thats well ask quiz question follows okay hopefully worked quiz answer two prepositional phrases number possible structures five rather focusing number wanted start focusing two problems come exponential possible number parses even quite simple sentences problem number one parse naively means use simple kinds top bottom parses normally used contexts like programming languages theyre used less ambiguous languages would mean parses end exponential amount work thats good wed like parses parse quickly exponential amount work come well thing notice generating exponential number trees comes small number basic ambiguities silly example two pps cats claws choosing attach therere really two decisions multiplying exponential number parses course happens lot repeated work look parse structures see keep building piece structure heres little pp claws build parse build parse build parse build every parse always going pp generate structures building true larger bits structure heres bigger pp cats claws also appears parse completely duplicated work goes heres verb phrase scratch people generate parse generate parse places build pieces structure get repeated heres noun phrase people cats heres piece noun phrase people cats verb phrase scratch people cats heres verb phrase scratch people cats generated second time lot repeated work secret building efficient parsers getting away exponential amount work avoid work twice want find possible pieces structure sentence precisely well able turn parsing exponential problem polynomial time problem problem choose correct parse several likely parses many many possible parses sentence well lets look simple example classic heres verb object noun phrase prepositional phrase attachment following could modifies verb looking telescope saw man telescope could modifies noun man telescope saw person well correct well answer could either whats commonly referred ai complete problem knowing correct could depend arbitrary knowledge world situation thats observed prior discourse context really complex ways really hard tell well thats true turns lot cases pretty sure whats correct without understanding lot looking words involved nouns verbs prepositions get really good idea attachment correct even without anything like full understanding sentence lets look couple examples illustrate okay real text examples exactly structure weve got verb noun phrase prepositional phrase moscow sent soldiers afghanistan one possibility sending afghanistan verb attachment possibility soldiers afghanistan syntactically either possible soldiers afghanistan would structure phrase like whatever students lady gaga something like possible syntactic structure really really unlikely youre unlikely soldiers afghanistan almost certainly structure want choose afghanistan modifying sent thats common idiom send something environment choose parse lets look next example example structure pp new south wales head dependent breached breached new south wales health dependent agreement agreement new south wales head well breaching new south wales head isnt likely dont say breached friend doesnt sound good hand agreement new south wales health thats really common normal sounding cause make agreements somebody attachment choose didnt look hard thats exactly well statistical parsers try exploit statistics word combination kind preposition likely nouns verbs able choose correct parses sentences without full understanding okay hope thats given sense parsing human language sentences really bad problem also idea start developing methods solve problem exponential number parses causing exponential amount work choose likely parses sentences,Course2,W5-S3-L3,W5,S3,L3,The,5,3,3,segment im gonna talk huge number pars human languag sentenc come might go solv statist pars system key part pars human languag sentenc problem what call attach psycholog literatur variou kind phrase decid modifi time preposit phrase adverbi participi phrase infinit coordin thing like attach decis work modifi let work exampl get idea problem sentenc board approv acquisit royal trust co limit toronto share monthli meet look sentenc well find find well subject board verb approv object acquisit look find actual four preposit phrase row royal trust co ltd toronto cent share monthli meet need decid modifi gener preposit phrase modifi verb modifi preced noun phrase got two target actual preposit phrase actual includ noun phrase get later preposit phrase therel thing one choos modifi okay let go one turn right royal trust co that modifi acquisit approv acquisit royal trust co that depend format relationship okay toronto modifi approv acquisit rather royal trust co limit toronto preposit phrase modifi royal trust co limit okay go third one cent share that modifi toronto royal trust co modifi acquisit acquisit cent share last preposit phrase monthli meet that presum refer acquisit monthli meet thing rather approv monthli meet preposit phrase modifi first verb okay kind complex though human be read sentenc understand mean attach decis without appar effort time thing want think moment mani possibl attach well sort think know one two three four thing could attach bunch point one chosen complet freeli fairli clear well go get exponenti number possibl exponenti number preposit phrase precis caus huge number pars saw previous like pars sentenc came realli make number decis thing like categori attach lot fairli freeli combin multipli ambigu get huge number howev case exampl like your complet free attach anyth anywher royal trust co could certainli choos attach either preced noun verb syntact attach toronto three choic could attach royal trust co preced noun phrase preced verb phrase syntact possibl youv made higher attach mean earlier sentenc thing becom restrict particular decid attach share modifi acquisit restrict attach possibl preposit phrase could still attach share could attach acquisit could attach approv one kind hidden behind attach share acquisit one becom possibl anymor leav kind littl puzzl well add preposit phrase attach sentenc mani possibl attach observ constraint correspond depend cross tree structur sentenc phrase structur grammar well turn answer question well work piec math answer found seri catalan number sequenc turn lot place wherev youv got kind treelik context exampl even turn probabilist graphic model import thing know purpos growth catalan number still exponenti sequenc get exponenti number pars unless your math geek work factori probabl isnt use thing rather probabl much valuabl linguist actual look possibl structur simpl sentenc get sens mani that well ask quiz question follow okay hope work quiz answer two preposit phrase number possibl structur five rather focus number want start focus two problem come exponenti possibl number pars even quit simpl sentenc problem number one pars naiv mean use simpl kind top bottom pars normal use context like program languag theyr use less ambigu languag would mean pars end exponenti amount work that good wed like pars pars quickli exponenti amount work come well thing notic gener exponenti number tree come small number basic ambigu silli exampl two pp cat claw choos attach therer realli two decis multipli exponenti number pars cours happen lot repeat work look pars structur see keep build piec structur here littl pp claw build pars build pars build pars build everi pars alway go pp gener structur build true larger bit structur here bigger pp cat claw also appear pars complet duplic work goe here verb phrase scratch peopl gener pars gener pars place build piec structur get repeat here noun phrase peopl cat here piec noun phrase peopl cat verb phrase scratch peopl cat here verb phrase scratch peopl cat gener second time lot repeat work secret build effici parser get away exponenti amount work avoid work twice want find possibl piec structur sentenc precis well abl turn pars exponenti problem polynomi time problem problem choos correct pars sever like pars mani mani possibl pars sentenc well let look simpl exampl classic here verb object noun phrase preposit phrase attach follow could modifi verb look telescop saw man telescop could modifi noun man telescop saw person well correct well answer could either what commonli refer ai complet problem know correct could depend arbitrari knowledg world situat that observ prior discours context realli complex way realli hard tell well that true turn lot case pretti sure what correct without understand lot look word involv noun verb preposit get realli good idea attach correct even without anyth like full understand sentenc let look coupl exampl illustr okay real text exampl exactli structur weve got verb noun phrase preposit phrase moscow sent soldier afghanistan one possibl send afghanistan verb attach possibl soldier afghanistan syntact either possibl soldier afghanistan would structur phrase like whatev student ladi gaga someth like possibl syntact structur realli realli unlik your unlik soldier afghanistan almost certainli structur want choos afghanistan modifi sent that common idiom send someth environ choos pars let look next exampl exampl structur pp new south wale head depend breach breach new south wale health depend agreement agreement new south wale head well breach new south wale head isnt like dont say breach friend doesnt sound good hand agreement new south wale health that realli common normal sound caus make agreement somebodi attach choos didnt look hard that exactli well statist parser tri exploit statist word combin kind preposit like noun verb abl choos correct pars sentenc without full understand okay hope that given sens pars human languag sentenc realli bad problem also idea start develop method solv problem exponenti number pars caus exponenti amount work choos like pars sentenc,[ 0  4 14 13 12]
181,Course2_W6-S1-L1_CFGs_and_PCFGs_15-29,segment im going introduce contextfree grammars extension probabilities probabilistic contextfree grammars linguist calls phrase structure grammar also known computer science contextfree grammar thing various rules category rewrites sequence categories eventually writes called terminal symbols words using grammar produce sentences start start symbol expand using rules grammar goes np vp theres rule says noun phrase goes noun noun go say people verb phrase go verb noun phrase verb go say fish noun phrase go noun noun go say tanks using grammar make sentences language two sentences language saw one sentence language another sentence language look carefully play around bit youll see actually ambiguous grammar sentences like people people people people people fish fish fish also sentences language okay context free grammar formally context free grammar formally four tuple consisting set terminal symbols words like fish people set nonterminal symbols ones like np noun phrase vp verb phrase start symbol one non terminal symbols set rules productions form nonterminal rewrites sequence nonterminals terminals like vp goes v np pp thats sequence gives us grammar well say grammar generates language language l sentences produced process rewriting start symbol sequence terminals many cases theres complexity size language infinite wont actually include possible strings thats formal definition context free grammar practice working linguistics computational linguistics always make couple refinements arent super theoretically interesting give us natural form linguistic purposes lets quickly look heres nlp phrase structure grammar different different weve introduced set preterminal symbols practice linguistic grammars almost always rules nonterminals like noun phrase goes determiner noun oh part often refer grammar lexicon words terminal symbols belong categories determiner rewrite nn rewrite man first definition preterminals lexical categories special status wed kind like thats say categories nouns verbs preterminals terminals words nonterminals really become phrasal categories things like noun phrase okay nothing else really changes start symbol lexicon rules form preterminal rewrites terminal grammar set rewrite rules rewrite sequence left hand side always phrasal nonterminal right hand side include phrasal nonterminals lexical categories convention examples often start symbol taken actually cause also stand sentence let note lot nlp purposes including kind tree banks look always symbol cause sometimes find things text arent full sentences might fragment like prepositional phrase might whole noun phrase something always symbol rewrite pp fragment whatever commonly thats taken named either root top one fine point right hand size gamma sequence phrasal lexical categories actually includes empty sequence often seems bit funny nothing right hand side might confusing people people commonly write italic e mean right hand side actually empty lets see conventions work phrase structure grammar okay noun phrase rewrites empty linguists often use empty categories grammar seems useful describe things missing something example well sentence people fish tanks make imperative fish tanks unspecified object say people fish dont want get linguistic analysis lot certainly isnt linguistically refined grammar idea way might want think explaining things like say well actually subject np sentence empty object np also empty unexpressed thats grammars often rules like empties well come back often talk ways getting rid nlp bit later okay empty creatures things called unary rules one category rewrites another category bunch things binary rules youre also able rules something rewrites three things heres case verb phrase rewrites sequence verb noun phrase prepositional phrase also things often want go get rid nlp grammars ill explain bit okay grammar rules lexicon okay given us phrase structure grammar context free grammar wanting probabilities grammar go well really actually simple extension far stuff like context free grammar add saying theres one extra thing probability function idea probability function takes rule gives probability maps real number zero one well dont let map number completely unconstrained add constraint nonterminal sums probabilities rewrites add one probability distribution say noun phrase rewrites make condition actually couple technical assumptions practice estimating grammars tree banks arent always true wont explain end result get get grammar produces language model technical sense introduced beforehand talking language models look language l thats generated grammar ive expressed know consider sequences terminals work probability sum probabilities probabilities sum one language model sense language models heres example pcfg like except next rule giving probability theres one rewrite probability one meet condition gave take interesting example three rewrites np sum three rewrites sum one required grammar showed actually made one change deleted noun phrase goes empty rule reason already really ambiguous grammar left example im show would get way ambiguous complex could try work slightly simplified okay using pcfg lets go example working probabilities two things want look first draw tree work probability tree thats actually pretty easy actually take probabilities grammar lexicon used generate tree multiply together slightly trickier thing want language model question want know probability string words well consider possible tree structures could generated string probability sequence words sum possible possible trees tree parse sentence since tree includes sentence bottom thats sum trees condition lets look concrete example sentence people fish tanks rods heres one parse grammar generates ive done write next parent category probability says vp goes v np pp rule probability get parse get prepositional phrase modifying verb weve seen also get parse prepositional phrase modifying noun write parse well less ambiguous grammar without empty two parses sentence work probability tree probability tree multiply together probabilities rule expansion get always little number probability tree noun attachment work probability sequence words sentence people fish tanks rods simply sum probabilities parses gives us probability language model score look well getting language model score also see parse pcfg would choose likely parse sentence answer one chooses verb attachment seems like natural reading sentence english might interesting guys look think moment chooses parse start looking youll see lot probabilities tree exactly parses theres really one area difference vp attach use rewrite rule maybe draw sort bigger include little sub tree whereas np attach vp rewrite rule extra np rewrite rule bit different well look found unique one sort piece times unique one versus compare end result vp verb attach three third times likely np attach purely accounted difference could even make little bit suspicious pcfgs something well come back later cause sort looks like ones getting lower probability theres depth tree rewrites done pcfgs tend slightly odd effects like well stop exploration well content saying pcfg choosing right parse sentence case feeling hopefully guys understand context free grammar probabilistic extension,Course2,W6-S1-L1,W6,S1,L1,CFGs,6,1,1,segment im go introduc contextfre grammar extens probabl probabilist contextfre grammar linguist call phrase structur grammar also known comput scienc contextfre grammar thing variou rule categori rewrit sequenc categori eventu write call termin symbol word use grammar produc sentenc start start symbol expand use rule grammar goe np vp there rule say noun phrase goe noun noun go say peopl verb phrase go verb noun phrase verb go say fish noun phrase go noun noun go say tank use grammar make sentenc languag two sentenc languag saw one sentenc languag anoth sentenc languag look care play around bit youll see actual ambigu grammar sentenc like peopl peopl peopl peopl peopl fish fish fish also sentenc languag okay context free grammar formal context free grammar formal four tupl consist set termin symbol word like fish peopl set nontermin symbol one like np noun phrase vp verb phrase start symbol one non termin symbol set rule product form nontermin rewrit sequenc nontermin termin like vp goe v np pp that sequenc give us grammar well say grammar gener languag languag l sentenc produc process rewrit start symbol sequenc termin mani case there complex size languag infinit wont actual includ possibl string that formal definit context free grammar practic work linguist comput linguist alway make coupl refin arent super theoret interest give us natur form linguist purpos let quickli look here nlp phrase structur grammar differ differ weve introduc set pretermin symbol practic linguist grammar almost alway rule nontermin like noun phrase goe determin noun oh part often refer grammar lexicon word termin symbol belong categori determin rewrit nn rewrit man first definit pretermin lexic categori special statu wed kind like that say categori noun verb pretermin termin word nontermin realli becom phrasal categori thing like noun phrase okay noth els realli chang start symbol lexicon rule form pretermin rewrit termin grammar set rewrit rule rewrit sequenc left hand side alway phrasal nontermin right hand side includ phrasal nontermin lexic categori convent exampl often start symbol taken actual caus also stand sentenc let note lot nlp purpos includ kind tree bank look alway symbol caus sometim find thing text arent full sentenc might fragment like preposit phrase might whole noun phrase someth alway symbol rewrit pp fragment whatev commonli that taken name either root top one fine point right hand size gamma sequenc phrasal lexic categori actual includ empti sequenc often seem bit funni noth right hand side might confus peopl peopl commonli write ital e mean right hand side actual empti let see convent work phrase structur grammar okay noun phrase rewrit empti linguist often use empti categori grammar seem use describ thing miss someth exampl well sentenc peopl fish tank make imper fish tank unspecifi object say peopl fish dont want get linguist analysi lot certainli isnt linguist refin grammar idea way might want think explain thing like say well actual subject np sentenc empti object np also empti unexpress that grammar often rule like empti well come back often talk way get rid nlp bit later okay empti creatur thing call unari rule one categori rewrit anoth categori bunch thing binari rule your also abl rule someth rewrit three thing here case verb phrase rewrit sequenc verb noun phrase preposit phrase also thing often want go get rid nlp grammar ill explain bit okay grammar rule lexicon okay given us phrase structur grammar context free grammar want probabl grammar go well realli actual simpl extens far stuff like context free grammar add say there one extra thing probabl function idea probabl function take rule give probabl map real number zero one well dont let map number complet unconstrain add constraint nontermin sum probabl rewrit add one probabl distribut say noun phrase rewrit make condit actual coupl technic assumpt practic estim grammar tree bank arent alway true wont explain end result get get grammar produc languag model technic sens introduc beforehand talk languag model look languag l that gener grammar ive express know consid sequenc termin work probabl sum probabl probabl sum one languag model sens languag model here exampl pcfg like except next rule give probabl there one rewrit probabl one meet condit gave take interest exampl three rewrit np sum three rewrit sum one requir grammar show actual made one chang delet noun phrase goe empti rule reason alreadi realli ambigu grammar left exampl im show would get way ambigu complex could tri work slightli simplifi okay use pcfg let go exampl work probabl two thing want look first draw tree work probabl tree that actual pretti easi actual take probabl grammar lexicon use gener tree multipli togeth slightli trickier thing want languag model question want know probabl string word well consid possibl tree structur could gener string probabl sequenc word sum possibl possibl tree tree pars sentenc sinc tree includ sentenc bottom that sum tree condit let look concret exampl sentenc peopl fish tank rod here one pars grammar gener ive done write next parent categori probabl say vp goe v np pp rule probabl get pars get preposit phrase modifi verb weve seen also get pars preposit phrase modifi noun write pars well less ambigu grammar without empti two pars sentenc work probabl tree probabl tree multipli togeth probabl rule expans get alway littl number probabl tree noun attach work probabl sequenc word sentenc peopl fish tank rod simpli sum probabl pars give us probabl languag model score look well get languag model score also see pars pcfg would choos like pars sentenc answer one choos verb attach seem like natur read sentenc english might interest guy look think moment choos pars start look youll see lot probabl tree exactli pars there realli one area differ vp attach use rewrit rule mayb draw sort bigger includ littl sub tree wherea np attach vp rewrit rule extra np rewrit rule bit differ well look found uniqu one sort piec time uniqu one versu compar end result vp verb attach three third time like np attach pure account differ could even make littl bit suspici pcfg someth well come back later caus sort look like one get lower probabl there depth tree rewrit done pcfg tend slightli odd effect like well stop explor well content say pcfg choos right pars sentenc case feel hope guy understand context free grammar probabilist extens,[ 0  4 14 13 12]
182,Course2_W6-S1-L2_Grammar_Transforms_12-05,segment im going tell kinds grammar transforms use efficient parsing pcfgs frequently known grammar transformation chomsky normal form idea chomsky normal form cfg rules restricted two simple forms theyre either x goes yz x z nonterminals theyre simply form nonterminal x rewrites terminal w take cfg transform chomsky normal form derived normal form produce another cfg recognizes language doesnt necessarily give tree structures strings part part language way going series transforms get rid empty rules unary rules divide rules two things left side lets quickly go example heres ugly phrase structure grammar bad cases first lets epsilon removal theres one rule epsilon going get rid one way going every time rule np right hand side going split two rules one one notes noun phrase empty says go vp okay keep places np turns get grammar point see lot unary rules gonna want start getting rid way pick first one say work consequences downwards saying go straight vp means look theres vp left hand side since immediately go vp well keeping rule going add another one says go v np make change grammar unfortunately changes mean weve introduced new unary rule left hand side unary removal recursively every unary disappeared goes v well v appear left hand side happens lexicon get rid rule going add new lexical entries saying go people cases gets us state okay point keep going going get rid vp goes v means going look places v appears left hand side going increase size lexicon still unaries keep going first one np goes np thats unary doesnt add anything apart perhaps ideas linguistic structure erase without changing language thats recognized np goes n gonna start looking n appears left hand side well find case theres nowhere grammar n appears righthand side rule dont actually split lexical entries actually rename put np one keep going couple unary rules get rid get grammar looks like next step say well gee weve still got rules three things left hand side gonna change binary rules way introduce extra categories lets take one gonna say vp goes first thing v going introduce new category say x gonna say x goes np v np pp called x make little bit simpler gonna use systematic unwieldy names call vp underscore v way think name nonterminal like nonterminal treat atom weve given systematic way introduce make changes final grammar chomsky normal form linguist hands grammar like steps hand back say chomsky normal form theyre likely like see made real mess structure grammar thats really something worry regard internal representation inside system allow efficient parsing isnt really designed structure language linguist sees system lets us grammar transformations efficient parsing havent exhibited extra bookkeeping symbol names kind transformations still able reconstruct original trees would made without grammar transformation nevertheless practice full chomsky normal form pain able see way cons deconstructed rules three things left hand side turning back nary rules binary rules thats gonna straightforward reconstructing empties unaries trickier point actually divide thing want know binarization grammar rules absolutely essential algorithms going show allow cubic time cfg parsing cubic time parsing arbitrary context free grammars system efficient polynomial time parsing context free grammars somewhere theres binarization might done advance like example going show cky algorithm explicitly transform grammar parse forms parsing binarization gets hidden inside workings parser binarization always done somewhere hand getting rid unaries empties optional want neatest version algorithms need want unaries empties well leaving unaries empties around doesnt change asymptotic complexity algorithms commonly convenient leave well demonstrate real examples weve discussed grammar transform normally actually statistical parsing days read trees tree bank stuff count sub trees become grammar gonna going say large reading trees tree bank look sub tree going something grammar sub tree gonna something grammar well dont want subtrees three things beneath ruins efficient parsability property read trees going transform turn binary trees weve done way discussed introducing new nonterminal dividing rule three children may want things empties unaries lets discuss little bit actual tree penn tree bank headline newspaper article headline atone way thats analyzed saying thats sentence imperative sentence heres imperative verb imperative verb unexpressed np subject empty okay says headline functional tags saying subject normally process treebank tree kind things first strip functional tags deal basic categories gets us commonly parsing algorithms dont explicitly deal empties also delete empty node everything thats empty node maps us onto tree tree lot unaries possible also get rid unaries use algorithm showed get rid unaries keeping highest node kept truly highest node root node youd even get rid would root goes atone start symbol grammar would rewrite word commonly dont well least keep rewrite start symbol different types things get sentence noun phrase keep high node well goes straight word atone normally dont want like keep lexicon parts speech rewriting words usual keep low end unary chain get rid higher things case want unique start symbol definitely keep start symbol allow start symbol rewrite unarily either nonterminal phrasal category preterminal turns dont actually stuff perfectly okay leave unaries grammar use algorithms going show makes bit complex messier terms parsing algorithm makes much easier reconstruct original parse trees way parsing algorithms show next segment actually work representation like still unary rules weve deleted functional tags empty elements okay hope sense grammar rules transform get frameworks allow efficient cleaner parsing algorithms well go look particular parsing algorithm works binary unary cfg rules,Course2,W6-S1-L2,W6,S1,L2,Grammar,6,1,2,segment im go tell kind grammar transform use effici pars pcfg frequent known grammar transform chomski normal form idea chomski normal form cfg rule restrict two simpl form theyr either x goe yz x z nontermin theyr simpli form nontermin x rewrit termin w take cfg transform chomski normal form deriv normal form produc anoth cfg recogn languag doesnt necessarili give tree structur string part part languag way go seri transform get rid empti rule unari rule divid rule two thing left side let quickli go exampl here ugli phrase structur grammar bad case first let epsilon remov there one rule epsilon go get rid one way go everi time rule np right hand side go split two rule one one note noun phrase empti say go vp okay keep place np turn get grammar point see lot unari rule gonna want start get rid way pick first one say work consequ downward say go straight vp mean look there vp left hand side sinc immedi go vp well keep rule go add anoth one say go v np make chang grammar unfortun chang mean weve introduc new unari rule left hand side unari remov recurs everi unari disappear goe v well v appear left hand side happen lexicon get rid rule go add new lexic entri say go peopl case get us state okay point keep go go get rid vp goe v mean go look place v appear left hand side go increas size lexicon still unari keep go first one np goe np that unari doesnt add anyth apart perhap idea linguist structur eras without chang languag that recogn np goe n gonna start look n appear left hand side well find case there nowher grammar n appear righthand side rule dont actual split lexic entri actual renam put np one keep go coupl unari rule get rid get grammar look like next step say well gee weve still got rule three thing left hand side gonna chang binari rule way introduc extra categori let take one gonna say vp goe first thing v go introduc new categori say x gonna say x goe np v np pp call x make littl bit simpler gonna use systemat unwieldi name call vp underscor v way think name nontermin like nontermin treat atom weve given systemat way introduc make chang final grammar chomski normal form linguist hand grammar like step hand back say chomski normal form theyr like like see made real mess structur grammar that realli someth worri regard intern represent insid system allow effici pars isnt realli design structur languag linguist see system let us grammar transform effici pars havent exhibit extra bookkeep symbol name kind transform still abl reconstruct origin tree would made without grammar transform nevertheless practic full chomski normal form pain abl see way con deconstruct rule three thing left hand side turn back nari rule binari rule that gonna straightforward reconstruct empti unari trickier point actual divid thing want know binar grammar rule absolut essenti algorithm go show allow cubic time cfg pars cubic time pars arbitrari context free grammar system effici polynomi time pars context free grammar somewher there binar might done advanc like exampl go show cki algorithm explicitli transform grammar pars form pars binar get hidden insid work parser binar alway done somewher hand get rid unari empti option want neatest version algorithm need want unari empti well leav unari empti around doesnt chang asymptot complex algorithm commonli conveni leav well demonstr real exampl weve discuss grammar transform normal actual statist pars day read tree tree bank stuff count sub tree becom grammar gonna go say larg read tree tree bank look sub tree go someth grammar sub tree gonna someth grammar well dont want subtre three thing beneath ruin effici parsabl properti read tree go transform turn binari tree weve done way discuss introduc new nontermin divid rule three children may want thing empti unari let discuss littl bit actual tree penn tree bank headlin newspap articl headlin aton way that analyz say that sentenc imper sentenc here imper verb imper verb unexpress np subject empti okay say headlin function tag say subject normal process treebank tree kind thing first strip function tag deal basic categori get us commonli pars algorithm dont explicitli deal empti also delet empti node everyth that empti node map us onto tree tree lot unari possibl also get rid unari use algorithm show get rid unari keep highest node kept truli highest node root node youd even get rid would root goe aton start symbol grammar would rewrit word commonli dont well least keep rewrit start symbol differ type thing get sentenc noun phrase keep high node well goe straight word aton normal dont want like keep lexicon part speech rewrit word usual keep low end unari chain get rid higher thing case want uniqu start symbol definit keep start symbol allow start symbol rewrit unarili either nontermin phrasal categori pretermin turn dont actual stuff perfectli okay leav unari grammar use algorithm go show make bit complex messier term pars algorithm make much easier reconstruct origin pars tree way pars algorithm show next segment actual work represent like still unari rule weve delet function tag empti element okay hope sens grammar rule transform get framework allow effici cleaner pars algorithm well go look particular pars algorithm work binari unari cfg rule,[ 0  4 14 13 12]
183,Course2_W6-S1-L3_CKY_Parsing_23-25,segment im going introduce way parsing contextfree grammars exactly polynomial time particular itll give us means finding probable parse sentence according pcfg polynomial time method im going introduce famous parsing method called cky algorithm remember goal constituency parsing start sentence example going use fish people fish tanks grammar probabilities rules pcfg want find sentence structure thats licensed grammar pcfg case normally probable sentence structure remember crucially wanted without exponential amount work explore everything exhaustively since exponential number parses exponential number work cky algorithm gives us cubic time algorithm terms length sentence size grammar terms number nonterminals allows us compact representation use dynamic programming like edit distance algorithm first week lets see goes secret uses data structure gets referred parse triangle also quite commonly chart way works cell going describe things build fish square going describe things build people square going describe things build fish people see since basically half square number squares order n squared gonna show fill square order n resulting algorithm runs cubic time secret cky algorithm fill chart working layers according size span first thing fill cells single words fill second row chart cells second row chart precisely cells describe two words fill third row chart constituents three words length finally fill top cell give us categories parse enti cover entire sentence lets look detail fill one cells chart assuming weve already filled two cells actually according grammar im using havent filled exhaustively put enough get general idea want fill cell want fill cell describes two word constituent people fish way gonna build things binary fashion saying lets take constituent constituent grammar rule thats able combine build thing left hand side grammar rule probability comes multiplying three probabilities going build going build noun phrase built two noun phrases probability going times times probability rule multiply together comes okay thats thing build cell also take verb noun phrase build verb phrase verb noun phrase gonna write possibility well gonna vp goes v np probability gonna times times equals okay point keep going see else build well another thing build take np vp combine rule right top build np vp probability times times goes np vp product also different number ones weve seen okay weve run every binary rule possibly could one possibility collisions well find multiple ways building vp dont actually example going illustrate thing extend unary rules grammar also unary rules rule like goes vp right rule applied inside cell since build vp right also build goes vp probability probability calculated times probability rule though multiplied gets okay point though cubic time parsing algorithm im wanting store two ways making cell want recognize build two word span pcfg im trying find probable parse want record simply best way making span precisely independence assumptions pcfg way constituent used probable parse sentence well using best way making span case compare two ways making probabilities versus goes np vp higher probability way making keep one dont store one okay filled cell mean actually strictly partially higher cell repeating moving chart heart cky algorithm original cky algorithm chomsky normalform grammars showed earlier indicated easily extend cky algorithm handle unary rules makes algorithm kind messier doesnt increase algorithmic complexity turns also easily extend cky algorithm handle empties lets look quickly sentence people fish tanks well way things called words word numbers one two three build cky chart cell stuff word one cell cell cell call one one cell measuring constituent spans first word contain last word contain inclusive ends instead want work empties grammar use trick fenceposts see various places computer science sentence people fish tanks instead put numbers words build chart numbers four points drawn like hm guess need move three little okay becomes cell store empties put similarly become cell cell cell store empty constituents actual words span one stretches position zero one gonna put actual words second row chart theyll entries point continue entries entry things span whole sentence position zero three could possibly include empties empties empties points im going go algorithm detail something could work exercise see work exactly way want emphasize idea binarization vital way get cubic time parsing algorithms cfgs binarization means rule two things right hand side allows cubic time algorithm soon allow two things right hand side youre minimally n higher order polynomial algorithm make limits long right hand side well algorithm exponential look literature cfg pcfg parsing sometimes see parsing algorithms explicitly work binarized grammars thats gonna case cky algorithm tell going transform grammar first feed cky algorithm grammars arent like actually misspelling theres e binarization internal grammar early algorithm looks like works arbitrary pcfg actually binarization internal workings parser okay heres cky algorithm going use next segment well go detail works let point things going array scores store probabilities things build build span two span indices beginning end nonterminal going record probability building constituent span certain type cant build certain nonterminal span well say probability zero secondly recording back pointers saying nonterminal span recording pointer best way building nonterminal span make dont actually store one time space tradeoffs parsing faster store cause get end know best way building every constituent every span easily write resulting parse tree would actually prefer conserve space dont store example stanford parser dont actually store explicitly reconstruct best parse simply knowing scores score table thats actually fairly good trade cause greatly reduces space requirements getting final parse actually still quite quick okay first part algorithm part part lexicon filling nonterminals words rewrite pretty simple going word saying weve got rule nonterminal goes tanks something going put score cell chart might probability thats main part cky algorithm everything dealing unary rules dealing unary rules bit complex things cell unary rules like b goes weve put cell well also able build b span problem might also rule c goes b well able build c span fact might even rule goes c means weve found another way build span actually keep applying unary rules stop discovering new constituents build span better probabilities thats theres little check whether found better way building nonterminal individual iteration loop consider unary rules work probability assign category left hand side better probability thats right store back trace say weve done work providing weve done work well going another iteration right loop checking possible unary rules okay still actually easy part lexicon go actually build rest chart thats part remember ordering work size constituent span start two word span constituents build length sentence go across cells left right start two word sp constituents starting position zero heading right position parse triangle works end crucial part algorithm saying okay maybe well filling cell one seven going build binary rules means built way built something one four four seven alternatively could built something one two two seven try different possibilities thats choice split point weve decided split point going consider grammar rules exist work probability building left hand side imagine rule goes b c probability building span terms build b c work three probabilities multiply together okay found better way building span record thats heart cky algorithm binary rules working higher chart rest handling unaries exactly way span say well also rule says goes also build repeat weve stopped able build anything better okay one cell chart finishing loops different beginning points working across rows chart different spans working parse triangle go across kind head like okay done found best way build every constituent every nonterminal every span weve particular found build right top cell start category know want build start category point use chart look find highest probability parse sentence simply return back user theres obviously another bit algorithm im omitting think fairly clear theres lot grasp think take seeing concrete example next segment makes sense algorithm give easy way see algorithm cubic length sentence number nonterminals grammar look loop span size order length sentence loop beginning order length sentence loop split point order length sentence n cubed terms length sentence start exploring grammar rules considering triples nonterminals order g cubed number nonterminals grammar im saying size set nonterminals equals g let mention guys implementing version simplistic way polynomial algorithm right minimal order complexity practice want fast pcfg parser dont want naively rather want checking indexing make things go fast kind thing want naively iterate triples nonterminals instead say well ive got know im building say zero four four seven building zero seven know constituents could build zero four actually want say things list okay tell grammar rules one left corner perhaps would goes np vp might also np goes np pp rules np left corner precisely rules going ones work work category clever indexing make things quite lot faster okay thats cky algorithm im sure kind confusing hope itll become lot clearer work example next segment,Course2,W6-S1-L3,W6,S1,L3,CKY,6,1,3,segment im go introduc way pars contextfre grammar exactli polynomi time particular itll give us mean find probabl pars sentenc accord pcfg polynomi time method im go introduc famou pars method call cki algorithm rememb goal constitu pars start sentenc exampl go use fish peopl fish tank grammar probabl rule pcfg want find sentenc structur that licens grammar pcfg case normal probabl sentenc structur rememb crucial want without exponenti amount work explor everyth exhaust sinc exponenti number pars exponenti number work cki algorithm give us cubic time algorithm term length sentenc size grammar term number nontermin allow us compact represent use dynam program like edit distanc algorithm first week let see goe secret use data structur get refer pars triangl also quit commonli chart way work cell go describ thing build fish squar go describ thing build peopl squar go describ thing build fish peopl see sinc basic half squar number squar order n squar gonna show fill squar order n result algorithm run cubic time secret cki algorithm fill chart work layer accord size span first thing fill cell singl word fill second row chart cell second row chart precis cell describ two word fill third row chart constitu three word length final fill top cell give us categori pars enti cover entir sentenc let look detail fill one cell chart assum weve alreadi fill two cell actual accord grammar im use havent fill exhaust put enough get gener idea want fill cell want fill cell describ two word constitu peopl fish way gonna build thing binari fashion say let take constitu constitu grammar rule that abl combin build thing left hand side grammar rule probabl come multipli three probabl go build go build noun phrase built two noun phrase probabl go time time probabl rule multipli togeth come okay that thing build cell also take verb noun phrase build verb phrase verb noun phrase gonna write possibl well gonna vp goe v np probabl gonna time time equal okay point keep go see els build well anoth thing build take np vp combin rule right top build np vp probabl time time goe np vp product also differ number one weve seen okay weve run everi binari rule possibl could one possibl collis well find multipl way build vp dont actual exampl go illustr thing extend unari rule grammar also unari rule rule like goe vp right rule appli insid cell sinc build vp right also build goe vp probabl probabl calcul time probabl rule though multipli get okay point though cubic time pars algorithm im want store two way make cell want recogn build two word span pcfg im tri find probabl pars want record simpli best way make span precis independ assumpt pcfg way constitu use probabl pars sentenc well use best way make span case compar two way make probabl versu goe np vp higher probabl way make keep one dont store one okay fill cell mean actual strictli partial higher cell repeat move chart heart cki algorithm origin cki algorithm chomski normalform grammar show earlier indic easili extend cki algorithm handl unari rule make algorithm kind messier doesnt increas algorithm complex turn also easili extend cki algorithm handl empti let look quickli sentenc peopl fish tank well way thing call word word number one two three build cki chart cell stuff word one cell cell cell call one one cell measur constitu span first word contain last word contain inclus end instead want work empti grammar use trick fencepost see variou place comput scienc sentenc peopl fish tank instead put number word build chart number four point drawn like hm guess need move three littl okay becom cell store empti put similarli becom cell cell cell store empti constitu actual word span one stretch posit zero one gonna put actual word second row chart theyll entri point continu entri entri thing span whole sentenc posit zero three could possibl includ empti empti empti point im go go algorithm detail someth could work exercis see work exactli way want emphas idea binar vital way get cubic time pars algorithm cfg binar mean rule two thing right hand side allow cubic time algorithm soon allow two thing right hand side your minim n higher order polynomi algorithm make limit long right hand side well algorithm exponenti look literatur cfg pcfg pars sometim see pars algorithm explicitli work binar grammar that gonna case cki algorithm tell go transform grammar first feed cki algorithm grammar arent like actual misspel there e binar intern grammar earli algorithm look like work arbitrari pcfg actual binar intern work parser okay here cki algorithm go use next segment well go detail work let point thing go array score store probabl thing build build span two span indic begin end nontermin go record probabl build constitu span certain type cant build certain nontermin span well say probabl zero secondli record back pointer say nontermin span record pointer best way build nontermin span make dont actual store one time space tradeoff pars faster store caus get end know best way build everi constitu everi span easili write result pars tree would actual prefer conserv space dont store exampl stanford parser dont actual store explicitli reconstruct best pars simpli know score score tabl that actual fairli good trade caus greatli reduc space requir get final pars actual still quit quick okay first part algorithm part part lexicon fill nontermin word rewrit pretti simpl go word say weve got rule nontermin goe tank someth go put score cell chart might probabl that main part cki algorithm everyth deal unari rule deal unari rule bit complex thing cell unari rule like b goe weve put cell well also abl build b span problem might also rule c goe b well abl build c span fact might even rule goe c mean weve found anoth way build span actual keep appli unari rule stop discov new constitu build span better probabl that there littl check whether found better way build nontermin individu iter loop consid unari rule work probabl assign categori left hand side better probabl that right store back trace say weve done work provid weve done work well go anoth iter right loop check possibl unari rule okay still actual easi part lexicon go actual build rest chart that part rememb order work size constitu span start two word span constitu build length sentenc go across cell left right start two word sp constitu start posit zero head right posit pars triangl work end crucial part algorithm say okay mayb well fill cell one seven go build binari rule mean built way built someth one four four seven altern could built someth one two two seven tri differ possibl that choic split point weve decid split point go consid grammar rule exist work probabl build left hand side imagin rule goe b c probabl build span term build b c work three probabl multipli togeth okay found better way build span record that heart cki algorithm binari rule work higher chart rest handl unari exactli way span say well also rule say goe also build repeat weve stop abl build anyth better okay one cell chart finish loop differ begin point work across row chart differ span work pars triangl go across kind head like okay done found best way build everi constitu everi nontermin everi span weve particular found build right top cell start categori know want build start categori point use chart look find highest probabl pars sentenc simpli return back user there obvious anoth bit algorithm im omit think fairli clear there lot grasp think take see concret exampl next segment make sens algorithm give easi way see algorithm cubic length sentenc number nontermin grammar look loop span size order length sentenc loop begin order length sentenc loop split point order length sentenc n cube term length sentenc start explor grammar rule consid tripl nontermin order g cube number nontermin grammar im say size set nontermin equal g let mention guy implement version simplist way polynomi algorithm right minim order complex practic want fast pcfg parser dont want naiv rather want check index make thing go fast kind thing want naiv iter tripl nontermin instead say well ive got know im build say zero four four seven build zero seven know constitu could build zero four actual want say thing list okay tell grammar rule one left corner perhap would goe np vp might also np goe np pp rule np left corner precis rule go one work work categori clever index make thing quit lot faster okay that cki algorithm im sure kind confus hope itll becom lot clearer work exampl next segment,[ 0  4  7 12 14]
184,Course2_W6-S1-L4_CKY_Example_21-52,hard really get sense cky algorithm staring pseudo code think work example itll actually seem pretty natural straightforward lets starting heres grammar going use similar ones ive shown grammar transformations grammar unaries empties okay theres one conceptual leap make get work showed parse triangles natural way make sense turns really hard write text diagonal diamonds large everyone decided much much easier get human beings tilt necks degrees right see parse triangle rather actually print things way conceptually think weve fish people fish tanks first diagonal single words second diagonal pairs words triples words four word constituents weve actually stuck make boxes rectangles holding head side look providing make past obstacle ready go okay need get going sentence parse fish people fish tanks second thing need grammar weve done start things particular going start filling lexical entries start cell say okay word fish noun verb put make noun fish probability make verb fish probability keep cells along diagonal people noun probability people verb probability keep going get okay grammar also unaries rewrite categories also apply unary rules way well cell well find unary rules apply create category isnt one higher probability well put chart case relevant unary rules weve got one unary rule actually creating different categories well adding new things chart well say look also make vp probability im taking multiplying make np probability times first round thats well weve rules nonterminals say well added new things added true well whole loop need also discover apply second layer unary rules put build span well also build probability times probability vp thatll okay going apply unary rules cells lexicon thatll give us point ready start heart cky algorithm building constituents bigger spans first applying binary rules well unaries top okay point cell next want fill one cell thats covering words position zero two version algorithm ive written using fence posts even though particular way im arent empty elements okay build binary constituent way build something zero one something one two well things zero one contained cell things one two contained cell going consider ways combining one make constituent licensed providing theres rule grammar x goes z something z something sort well theres binary rules n right hand side unary rules wont able anything v look sorry wont able anything n look v could made rule thats something make look weve got vp goes v np means make vp cell probability times times probability rule point youre starting stretch ability mental arithmetic thats times thats five carry one also things build binary rules binary rules weve got rule goes np vp well certainly apply weve got np vp build span similarly ones np goes np np well weve got np np build np span wont try work probabilities apply end probabilities turns constituents built binary phase theres one way making wasnt contention well able see contention next unary phase rule contention always discover multiple ways make category span keep one highest probability next phase unaries cells look unary rules give way making something higher probability put chart say weve done work done work lead us whole loop case able build chains unary rules particular case unary rules np right hand side cant anything unary rule vp right hand side discover theres another way make make goes vp well probability actually going x actually lot probability rule going record better way making back pointers going record probability new best way making always keep constituents make best probability way making storing back pointers way best way making go cells see make better ways unary rules find thats possible precisely two places one also find better way making update two second row okay gonna go third row building three word constituents fish people fish cell people fish tanks cell point things get little bit interesting building binary constituents going build things cell things well going build binary constituents two ways either take something zero two combine something two three else take something zero one combine something one three thats part algorithm choose split point dividing constituent two exploit binary rules get cubic time algorithm iterate across split points another operation consider rules apply split point part first going start saying okay going combine stuff zero two thats cell something two three thats cell well see build well np np allow us build bigger np according rule np vp thatll allow us build using rule well seems like cant actually build vp span things way okay lets consider possibility well time build np combining np looking one looking cell time using two cells use np np allow us build np umm use verb np allow us build vp use np vp allow us build build things span going want say okay definitely build np span want find two highest probability one way making np keep similarly going want find two highest probability way making keep actually work math case theres little fine point np case look np case purple one x probability np rule constant go green one reflecting fact ways using word fish twice world people actually ways making np identical probability practice ties like parsers choose one analysis first one come across sort say choose one analysis randomly okay fills cell get numbers point would ply apply unary rules unary rule thats active level one cause two unary rules work n v arent applicable cell like little grammar wed also find new way making making vp probability would times number would less way weve already discovered making well keep highest probability analysis okay cell one four cell two split points ways making could either make something one two three four else could make something one three three four explore ways combining things fill cell okay almost end fill final cell chart thing apart iterate across split points three ways make things put either taking things zero one followed two four things zero two plus things three four things zero three plus things three four explore ways combining things choose highest probability way making noun phrase vp three constituent types build span time youve gone far math much difficult real time front ive worked advance fill get get ask question whats highest probability parse sentence regardless category choose answer parsing sentence nominated start category like instead say want find analysis regardless also return analysis okay say weve finished actually well parse tree answer parse tree find calling build tree routine traces backwards tree know first thing built top taking starting point goes np vp point confess actually left little bit whats written table keep things written larger rather simply writing rule used youll actually record cells built actually constituents spans actually noun phrase built zero two followed verb phrase built two four okay means look zero two cell find noun phrase look three four cell find verb phrase point well start zero four zero two two four part start recurse downwards highest probability way building noun phrase using noun phrase goes noun phrase noun phrase rule since binary rule necessarily know zero one one two highest way making verb phrase using verb noun phrase rule goes two three goes three four okay means building noun phrase noun phrase cell building noun phrase noun phrase cell building verb verb cell building noun phrase noun phrase cell point say well whats highest way building noun phrase oh making noun zero one well whats highest way making noun zero one span unary rule well realizing word fish word sentence similarly cells okay one two way building noun phrase highest probability building noun unary rule way building noun highest probability using word people thats terminal sentence highest probability way building verb two three directly making word fish terminal sentence highest probability way making noun phrase three four making noun three four via unary rule highest probability way making noun three four making using word tanks terminal point weve able back follow back traces back show highest probability parse sentence phew bit get well hope see actually fairly straightforward algorithm actually much easier computers human beings filling squares rectangle hope kind feel like understand could go write program kind loops,Course2,W6-S1-L4,W6,S1,L4,CKY,6,1,4,hard realli get sens cki algorithm stare pseudo code think work exampl itll actual seem pretti natur straightforward let start here grammar go use similar one ive shown grammar transform grammar unari empti okay there one conceptu leap make get work show pars triangl natur way make sens turn realli hard write text diagon diamond larg everyon decid much much easier get human be tilt neck degre right see pars triangl rather actual print thing way conceptu think weve fish peopl fish tank first diagon singl word second diagon pair word tripl word four word constitu weve actual stuck make box rectangl hold head side look provid make past obstacl readi go okay need get go sentenc pars fish peopl fish tank second thing need grammar weve done start thing particular go start fill lexic entri start cell say okay word fish noun verb put make noun fish probabl make verb fish probabl keep cell along diagon peopl noun probabl peopl verb probabl keep go get okay grammar also unari rewrit categori also appli unari rule way well cell well find unari rule appli creat categori isnt one higher probabl well put chart case relev unari rule weve got one unari rule actual creat differ categori well ad new thing chart well say look also make vp probabl im take multipli make np probabl time first round that well weve rule nontermin say well ad new thing ad true well whole loop need also discov appli second layer unari rule put build span well also build probabl time probabl vp thatll okay go appli unari rule cell lexicon thatll give us point readi start heart cki algorithm build constitu bigger span first appli binari rule well unari top okay point cell next want fill one cell that cover word posit zero two version algorithm ive written use fenc post even though particular way im arent empti element okay build binari constitu way build someth zero one someth one two well thing zero one contain cell thing one two contain cell go consid way combin one make constitu licens provid there rule grammar x goe z someth z someth sort well there binari rule n right hand side unari rule wont abl anyth v look sorri wont abl anyth n look v could made rule that someth make look weve got vp goe v np mean make vp cell probabl time time probabl rule point your start stretch abil mental arithmet that time that five carri one also thing build binari rule binari rule weve got rule goe np vp well certainli appli weve got np vp build span similarli one np goe np np well weve got np np build np span wont tri work probabl appli end probabl turn constitu built binari phase there one way make wasnt content well abl see content next unari phase rule content alway discov multipl way make categori span keep one highest probabl next phase unari cell look unari rule give way make someth higher probabl put chart say weve done work done work lead us whole loop case abl build chain unari rule particular case unari rule np right hand side cant anyth unari rule vp right hand side discov there anoth way make make goe vp well probabl actual go x actual lot probabl rule go record better way make back pointer go record probabl new best way make alway keep constitu make best probabl way make store back pointer way best way make go cell see make better way unari rule find that possibl precis two place one also find better way make updat two second row okay gonna go third row build three word constitu fish peopl fish cell peopl fish tank cell point thing get littl bit interest build binari constitu go build thing cell thing well go build binari constitu two way either take someth zero two combin someth two three els take someth zero one combin someth one three that part algorithm choos split point divid constitu two exploit binari rule get cubic time algorithm iter across split point anoth oper consid rule appli split point part first go start say okay go combin stuff zero two that cell someth two three that cell well see build well np np allow us build bigger np accord rule np vp thatll allow us build use rule well seem like cant actual build vp span thing way okay let consid possibl well time build np combin np look one look cell time use two cell use np np allow us build np umm use verb np allow us build vp use np vp allow us build build thing span go want say okay definit build np span want find two highest probabl one way make np keep similarli go want find two highest probabl way make keep actual work math case there littl fine point np case look np case purpl one x probabl np rule constant go green one reflect fact way use word fish twice world peopl actual way make np ident probabl practic tie like parser choos one analysi first one come across sort say choos one analysi randomli okay fill cell get number point would pli appli unari rule unari rule that activ level one caus two unari rule work n v arent applic cell like littl grammar wed also find new way make make vp probabl would time number would less way weve alreadi discov make well keep highest probabl analysi okay cell one four cell two split point way make could either make someth one two three four els could make someth one three three four explor way combin thing fill cell okay almost end fill final cell chart thing apart iter across split point three way make thing put either take thing zero one follow two four thing zero two plu thing three four thing zero three plu thing three four explor way combin thing choos highest probabl way make noun phrase vp three constitu type build span time youv gone far math much difficult real time front ive work advanc fill get get ask question what highest probabl pars sentenc regardless categori choos answer pars sentenc nomin start categori like instead say want find analysi regardless also return analysi okay say weve finish actual well pars tree answer pars tree find call build tree routin trace backward tree know first thing built top take start point goe np vp point confess actual left littl bit what written tabl keep thing written larger rather simpli write rule use youll actual record cell built actual constitu span actual noun phrase built zero two follow verb phrase built two four okay mean look zero two cell find noun phrase look three four cell find verb phrase point well start zero four zero two two four part start recurs downward highest probabl way build noun phrase use noun phrase goe noun phrase noun phrase rule sinc binari rule necessarili know zero one one two highest way make verb phrase use verb noun phrase rule goe two three goe three four okay mean build noun phrase noun phrase cell build noun phrase noun phrase cell build verb verb cell build noun phrase noun phrase cell point say well what highest way build noun phrase oh make noun zero one well what highest way make noun zero one span unari rule well realiz word fish word sentenc similarli cell okay one two way build noun phrase highest probabl build noun unari rule way build noun highest probabl use word peopl that termin sentenc highest probabl way build verb two three directli make word fish termin sentenc highest probabl way make noun phrase three four make noun three four via unari rule highest probabl way make noun three four make use word tank termin point weve abl back follow back trace back show highest probabl pars sentenc phew bit get well hope see actual fairli straightforward algorithm actual much easier comput human be fill squar rectangl hope kind feel like understand could go write program kind loop,[ 0  4 12  7 14]
185,Course2_W6-S1-L5_Constituency_Parser_Evaluation_9-45,segment briefly discuss evaluate parsing results go evaluating whether parsers good job one measure would say parse produce exactly right parse according tree bank thats possible standard actually close probabilistic models rather tough job get entire structure sentence right used evaluation measures divided pieces parse get partial credit right lets go works idea start gold standard parse tree correct parse sentence run parser parser going choose parse tree sentence return example parser mostly right got noun phrase sales executives got verb structure examining noun phrase figures right actually made one little mistake end yesterday kind weird kind temporal noun phrase english bare noun phrases like yesterday next week express time otherwise expressed prepositional phrase like sunday spring parser wrongly stuck extra noun end noun phrase great care thats error gonna go measuring well way gonna going look individual constituency claims noun phrase gonna say goes position zero two sentence im putting fence post marker numbers words sentence look guessed parse say well also noun phrase goes zero two sentence particular constituency claim gold tree correctly captured parse tree way cases convert gold tree set constituency claims leave root node ive written claims across root node constituency claims sentence thats going zero eleven various constituency claims exactly thing candidate parse produced suggested parser also suggested whole thing sentence zero eleven also suggested np zero two got set constituency claims simply treat unit atom either get right wrong use exactly precision recall f measure youve already seen several times gold standard label bracketings candidate label bracketings ive shown bold three candidate agrees correct parse total eight bracketings gold parse seven proposed parse labeled precision percent recall percent f combines usual way comes partly means extra nodes parts speech talked dont parsing measure evalu evaluate part speech tagging even though parsers part speech tagging part work thats reported separately example ive got tagging completely correct thats something might notice scores actually pretty low worth thinking bit theyre low find even though candidate parse mostly correct actually get lot brackets wrong happened thing wrong candidate parse noun yesterday wrongly fit noun phrase rather sentential modifier temporal np consequence making mistake parser scored lot things wrong verb phrase scored wrong wrongly extending cover yesterday whereas shouldnt verb phrase wrong exactly reason prepositional phrase wrong reason noun phrase labeled precision recall measure suffers cascading errors whenever attach something low high vice versa many people think actually good measure parser performance people including argued instead use dependency measures parsing performance even constituency parsers isnt current practice see everywhere research measure see labeled precision labeled recall f thats measure weve presented ill briefly mention also unlabeled version measures simply look constituency claim sequence words without worrying label thats much less used okay know evaluate pcfgs well well train pcfg penn treebank run penn treebank test data evaluate measure answer get percent f measure thats terrible get quite lot brackets right remember counting bracket separately youre getting quarter brackets wrong means youre mak tending make several bad attachment errors every sentence thats great result well talk improve soon lets summarize though little bit properties pcfgs weve seen good things pcfgs theyre robust normally estimating pcfg tree bank get result smoothed probabilities every possible string words included grammar theres categorical grammar constraint whatsoever anymore action probabilities whats high probability low probability thats useful parser robust give anything itll best job give likely sentence parse another good thing pcfgs give us least part solution grammar ambiguity like example people fishing showed earlier pcfg tells parse sentence choose least sometimes chooses correct one gives ideas plausibility parse well see later already got hint strong independence assumptions pcfg actually good third good property pcfgs give us probabilistic language model say likely unlikely sentences language important point shouldnt go run think ahha ive got grammar ill use pcfg better language model bigrams trigrams saw earlier actually pcfg weve shown far doesnt work well bigram trigram language model tasks like spelling correction speech recognition reason seems pcfgs lack lexicalization trigram model mean well meant youre looking pcfg pcfg nature vp rewrites vbd vp rules expanding without reference words actually used sentence fact rewrite rules consider words ones rewrite preterminal word limits ability plain pcfg act effective language model thats problem well address soon moment know go evaluating constituency parsers,Course2,W6-S1-L5,W6,S1,L5,Constituency,6,1,5,segment briefli discuss evalu pars result go evalu whether parser good job one measur would say pars produc exactli right pars accord tree bank that possibl standard actual close probabilist model rather tough job get entir structur sentenc right use evalu measur divid piec pars get partial credit right let go work idea start gold standard pars tree correct pars sentenc run parser parser go choos pars tree sentenc return exampl parser mostli right got noun phrase sale execut got verb structur examin noun phrase figur right actual made one littl mistak end yesterday kind weird kind tempor noun phrase english bare noun phrase like yesterday next week express time otherwis express preposit phrase like sunday spring parser wrongli stuck extra noun end noun phrase great care that error gonna go measur well way gonna go look individu constitu claim noun phrase gonna say goe posit zero two sentenc im put fenc post marker number word sentenc look guess pars say well also noun phrase goe zero two sentenc particular constitu claim gold tree correctli captur pars tree way case convert gold tree set constitu claim leav root node ive written claim across root node constitu claim sentenc that go zero eleven variou constitu claim exactli thing candid pars produc suggest parser also suggest whole thing sentenc zero eleven also suggest np zero two got set constitu claim simpli treat unit atom either get right wrong use exactli precis recal f measur youv alreadi seen sever time gold standard label bracket candid label bracket ive shown bold three candid agre correct pars total eight bracket gold pars seven propos pars label precis percent recal percent f combin usual way come partli mean extra node part speech talk dont pars measur evalu evalu part speech tag even though parser part speech tag part work that report separ exampl ive got tag complet correct that someth might notic score actual pretti low worth think bit theyr low find even though candid pars mostli correct actual get lot bracket wrong happen thing wrong candid pars noun yesterday wrongli fit noun phrase rather sententi modifi tempor np consequ make mistak parser score lot thing wrong verb phrase score wrong wrongli extend cover yesterday wherea shouldnt verb phrase wrong exactli reason preposit phrase wrong reason noun phrase label precis recal measur suffer cascad error whenev attach someth low high vice versa mani peopl think actual good measur parser perform peopl includ argu instead use depend measur pars perform even constitu parser isnt current practic see everywher research measur see label precis label recal f that measur weve present ill briefli mention also unlabel version measur simpli look constitu claim sequenc word without worri label that much less use okay know evalu pcfg well well train pcfg penn treebank run penn treebank test data evalu measur answer get percent f measur that terribl get quit lot bracket right rememb count bracket separ your get quarter bracket wrong mean your mak tend make sever bad attach error everi sentenc that great result well talk improv soon let summar though littl bit properti pcfg weve seen good thing pcfg theyr robust normal estim pcfg tree bank get result smooth probabl everi possibl string word includ grammar there categor grammar constraint whatsoev anymor action probabl what high probabl low probabl that use parser robust give anyth itll best job give like sentenc pars anoth good thing pcfg give us least part solut grammar ambigu like exampl peopl fish show earlier pcfg tell pars sentenc choos least sometim choos correct one give idea plausibl pars well see later alreadi got hint strong independ assumpt pcfg actual good third good properti pcfg give us probabilist languag model say like unlik sentenc languag import point shouldnt go run think ahha ive got grammar ill use pcfg better languag model bigram trigram saw earlier actual pcfg weve shown far doesnt work well bigram trigram languag model task like spell correct speech recognit reason seem pcfg lack lexic trigram model mean well meant your look pcfg pcfg natur vp rewrit vbd vp rule expand without refer word actual use sentenc fact rewrit rule consid word one rewrit pretermin word limit abil plain pcfg act effect languag model that problem well address soon moment know go evalu constitu parser,[ 0  4 14  1 13]
186,Course2_W6-S2-L1_Lexicalization_of_PCFGs_7-03,segment im going introduce idea lexiclizing pcfgs lets look basic pcfg see probabilities rules actually work pcfg local trees corresponding rules verb phrase rewrites vbb pp probability attached like maybe probability would zero three three percent verb phrases expand past tense verb prepositional phrase well similarly rules sentence goes np vp rule would much higher probability maybe probability something like really important thing notice rules make reference whatsoever actual words saying overall three percent verb phrases consist past tense verb prepositional phrase whether thats likely depends awful lot verb dealing example word verb walked walked kind motion verb really really likely pp following whereas another verb example verb saw would past tense verb would really unlikely prepositional phrase coming something like saw mirror something like youd normally get noun phrase object saw first seems like really come good probability estimates know words sentence thats precisely idea lexicalization idea lexicalization lets define category way finding head noun phrase well say last noun whether proper noun common noun declared head noun phrase well say head sue noun phrase well say head word store last noun noun phrase well apply ideas verb phrase head verb phrase verb inside head verb phrase walked prepositional phrase head preposition inside well say head sentence head verb phrase put way lexical items represent head phrase next non terminal grammar let get neatly printed version okay well happens well find weve got categories like swalked combination old nonterminals plus lexical item weve enormously enormously expanded effective space nonterminals something like twenty nonterminals something like words vocabulary well weve got nonterminals grammar suggests need start specialized engineering able well lets worry moment lets think terms probabilities allow us well neat thing allow us looking whats probability sub tree wont saying whats probability verb expanding past tense verb phrase expanding past tense verb pp well saying whats probability verb phrase headed walked taking pp particular pp thats headed gonna capturing inside rule two things well capturing vp walk likely take pp capturing relationship heads captur capturing relationship reasonable someone walk something lot much richer probabilistic conditioning captured grammar extra information really really useful resolving various kinds ambiguities classic example prepositional phrase attachment want work prepositional phrase whether modifying noun precedes modifying verb precedes well capture quite lot inside pcfg lexicalize well rule nprates taking righthand side ppfor ask whether likely thing happen say vpannounce taking expanding righthand side ppin reasonable thing happen use information better model pp attachments could vanilla pcfg doesnt mean capture everything pp attachments model though actually want could think little things youd like know attaching prepositional phrase arent yet captured model head word pinned every phrase like nevertheless kind headlexicalization captures additional things youd like know make various parsing decisions sentence also useful things like coordination scope knowing complementation patterns verbs indeed case lexicalization pcfgs seen parsing breakthrough late thats really useful notion know idea lexicalizing pcfgs capture necessary probabilistic conditioning information make parsing decisions,Course2,W6-S2-L1,W6,S2,L1,Lexicalization,6,2,1,segment im go introduc idea lexicl pcfg let look basic pcfg see probabl rule actual work pcfg local tree correspond rule verb phrase rewrit vbb pp probabl attach like mayb probabl would zero three three percent verb phrase expand past tens verb preposit phrase well similarli rule sentenc goe np vp rule would much higher probabl mayb probabl someth like realli import thing notic rule make refer whatsoev actual word say overal three percent verb phrase consist past tens verb preposit phrase whether that like depend aw lot verb deal exampl word verb walk walk kind motion verb realli realli like pp follow wherea anoth verb exampl verb saw would past tens verb would realli unlik preposit phrase come someth like saw mirror someth like youd normal get noun phrase object saw first seem like realli come good probabl estim know word sentenc that precis idea lexic idea lexic let defin categori way find head noun phrase well say last noun whether proper noun common noun declar head noun phrase well say head sue noun phrase well say head word store last noun noun phrase well appli idea verb phrase head verb phrase verb insid head verb phrase walk preposit phrase head preposit insid well say head sentenc head verb phrase put way lexic item repres head phrase next non termin grammar let get neatli print version okay well happen well find weve got categori like swalk combin old nontermin plu lexic item weve enorm enorm expand effect space nontermin someth like twenti nontermin someth like word vocabulari well weve got nontermin grammar suggest need start special engin abl well let worri moment let think term probabl allow us well neat thing allow us look what probabl sub tree wont say what probabl verb expand past tens verb phrase expand past tens verb pp well say what probabl verb phrase head walk take pp particular pp that head gonna captur insid rule two thing well captur vp walk like take pp captur relationship head captur captur relationship reason someon walk someth lot much richer probabilist condit captur grammar extra inform realli realli use resolv variou kind ambigu classic exampl preposit phrase attach want work preposit phrase whether modifi noun preced modifi verb preced well captur quit lot insid pcfg lexic well rule nprate take righthand side ppfor ask whether like thing happen say vpannounc take expand righthand side ppin reason thing happen use inform better model pp attach could vanilla pcfg doesnt mean captur everyth pp attach model though actual want could think littl thing youd like know attach preposit phrase arent yet captur model head word pin everi phrase like nevertheless kind headlexic captur addit thing youd like know make variou pars decis sentenc also use thing like coordin scope know complement pattern verb inde case lexic pcfg seen pars breakthrough late that realli use notion know idea lexic pcfg captur necessari probabilist condit inform make pars decis,[ 0  4 14 13 12]
187,Course2_W6-S2-L2_Charniaks_Model_18-23,lets look particular model realizing lexicalized pcfgs model going look model eugene charniak charniak isnt recent model im choosing simplest straightforward way building lexicalized pcfg hopefully easy guys get sense works give bit context im explaining youre actually parsing charniaks parsing model way hes parsing bottomup way somewhat similar way cky parsing earlier segment plain vanilla pcfg probabilistic conditioning topdown youve got probability righthand side given lefthand side ie probability stuff given stuff segment im basically gonna show probability distributions rather actually concretely going parsing algorithm actual parsing algorithm youre using probabilities applying working upwards tree idea charniaks algorithm works example im going assume starting point already partway building tree node know headword whole sentence going rose sentence rewriting noun phrase verb phrase well since verb phrase head sentence know automatically headword also going rose point havent yet decided expand noun phrase dont know head noun phrase charniaks model two probability distributions used expand sentence well see turn first one gee choose headword noun phrase choosing headword going condition several things going condition headword parent category noun phrase parent category probability distribution going use one going probability different headword choices given three conditioning things category parent category parent head word asking noun phrase context node head whole rose likely nouns choose head noun phrase might think something like balloon rose say balloon temperature temperature rose might think tempers something like since actually dealing financial newspaper wall street journal actually likely example profits rose okay noun phrase noun phrase headed profits dont actually know expands going second probability distribution work going want rewrite rule thats working expands going condition headword profits category noun phrase parent category thats going give us second probability distribution probability rule expands us one level tree given three conditioning things category parent category headword okay gonna choose rule rule could np goes noun plural noun would probability could something else example actual rule thats chosen generate adjective plural noun thats given probability well weve generated adjective plural noun notion head phrase know deterministically noun phrase headed profits well plural noun must profits head noun phrase since actually level part speech tag means actually word profits bottom tree okay point weve completed element expansion according two probability distributions point going keep going going say well category actually nonterminal category going expand working headword going use probability distribution going choose word headword corporate point nonterminal well know thats word sentence vp headed rose going expanded making use probability distribution going expand choose expansion phrase maybe itll go past tense verb prepositional phrase well start expanding choosing headwords expand well get rose automatically course go word rose choose head word start expanding pretty much done thing really need way get started need slightly varied probability distribution beginning going say root category top needs headword point going probability headword given category equals root things dont exist chance different words head whole sentence weve done start expanding downwards root head headed rose well look rule expand looking expansion point dont yet parent category weve got category head need couple special probability distributions get started beginning basic recursion ive talked achieve putting words grammar like statistics show go bit detail talked previously different expansions verb phrase rule choice different verbs head illustrates systematically chance different expansions verb phrase vary enormously depending verb youre choosing verb like come find getting pp verb enormously enormously common happens onethird time verb verb phrase happens ten percent time many kinds complements like np complements really really rare verb come thats exactly also tran noun phrase verbs facts different verb take well take normally takes object took nap took book took ticket anything like third time get vp goes v np though course get things well move think think sort sentential complement verb youre saying thought fact nearly always think almost three quarters time youre getting sbar complement thats something like thinks dishonest thinks dishonest either sbar complement regardless whether theres overt final example illustrated verb want also takes complements takes infinitive complements want go store seventy percent time get essentially notice theyre extremely different probabilities different expansions verb phrase dependent knowing head verb precisely kind information captured charniak parse lexicalized parsing model one final thing note sometimes referred monolexical probabilities looking expansion categories terms categories knowing one lexical item head verb thats slight contrast main way lexicalized pcfgs useful thats predicting dependencies things like prepositional phrases bilexical probabilities examples like go man deciding likely connection preposition noun verb involve two words time charniak model also bilexical probabilities chances choosing head noun noun phrase plural noun phrase given amount information might include information noun phrase also information parent category headword sentence whole sentence find wall street journal isnt typical english plural noun bit one percent noun prices know subject inside noun phrase node ie subject sentence well chances prices become two half percent really makes big difference know verb fell well thats information tells kind verb could easily go prices probability becomes far higher almost fifteen percent chance one seven chance head noun phrase gonna prices capturing lot probabilistic information might wondering gee really estimate probabilities answer general cant estimate probabilities probabilities youd like estimate become far sparse ill illustrate next slide way dealt charniaks model complicated scheme linear interpolation different models less precise reminiscent saw language models second week class want estimate probability distribution weve seen choosing headword based parents headword current category parents category way taking linear interpolation bunch different distributions one maximum likelihood estimation conditioning everything distributions first leave parent headword also leave even parent category point youre choosing headword based category saying noun phrase whats chance certain head charniaks model different distributions weighted deterministic way depending much youd expect seen certain kinds evidence making use language modeling like techniques essential build kind lexicalized pcfgs data sparse next slide shows different distributions combined together linear interpolation one right first find youve got noun phrase headed profits inside theres adjective youre wanting ask adjective wall street journal one quarter time corporate profits okay thats precise fully conditioned estimate whereas start erasing information get coarser still nonzero estimates havent actually made sure second line method charniak used get kind coarse semantic classes try keep information parent head word getting rid entirely look dont really lot one look two see youre conditioning knowing noun phrase noun phrase chance corporate dropping almost two orders magnitude youre half percent okay good case calculate probability rich conditioning helps lot quite commonly doesnt work say well verb sentence rose ive got subject noun phrase noun phrase noun well particular sentence data parsed actual noun profits turns training data profits never occurred noun heading noun phrase subject rose despite fact sounds perfectly normal profits rose last quarter profits rose throughout economy sentence like maximum likelihood estimate mle zero way getting nonzero probability estimate using probabilities condition less stuff even getting low probabilities right probability noun profits noun phrase sort one th percent thats best kind estimate although youd like use rich estimates time lexicalized pcfg parsing youre actually fall back rather coarser estimates cant get conditioning information youd like fairly small supervised tree banks train particular bilexical probabilities youre trying condition two lexical items probabilities tend get backed amount information estimate extremely extremely sparse youre space even consider categories youre something thats kind like bigram probability estimates hard estimate word bigram probabilities million words text thats text handconstructed tree banks okay details needed smoothing end hope main thing could take away segment fairly straightforward system two probability distributions charniak able use realize lexicalized pcfg model,Course2,W6-S2-L2,W6,S2,L2,Charniaks,6,2,2,let look particular model realiz lexic pcfg model go look model eugen charniak charniak isnt recent model im choos simplest straightforward way build lexic pcfg hope easi guy get sens work give bit context im explain your actual pars charniak pars model way he pars bottomup way somewhat similar way cki pars earlier segment plain vanilla pcfg probabilist condit topdown youv got probabl righthand side given lefthand side ie probabl stuff given stuff segment im basic gonna show probabl distribut rather actual concret go pars algorithm actual pars algorithm your use probabl appli work upward tree idea charniak algorithm work exampl im go assum start point alreadi partway build tree node know headword whole sentenc go rose sentenc rewrit noun phrase verb phrase well sinc verb phrase head sentenc know automat headword also go rose point havent yet decid expand noun phrase dont know head noun phrase charniak model two probabl distribut use expand sentenc well see turn first one gee choos headword noun phrase choos headword go condit sever thing go condit headword parent categori noun phrase parent categori probabl distribut go use one go probabl differ headword choic given three condit thing categori parent categori parent head word ask noun phrase context node head whole rose like noun choos head noun phrase might think someth like balloon rose say balloon temperatur temperatur rose might think temper someth like sinc actual deal financi newspap wall street journal actual like exampl profit rose okay noun phrase noun phrase head profit dont actual know expand go second probabl distribut work go want rewrit rule that work expand go condit headword profit categori noun phrase parent categori that go give us second probabl distribut probabl rule expand us one level tree given three condit thing categori parent categori headword okay gonna choos rule rule could np goe noun plural noun would probabl could someth els exampl actual rule that chosen gener adject plural noun that given probabl well weve gener adject plural noun notion head phrase know determinist noun phrase head profit well plural noun must profit head noun phrase sinc actual level part speech tag mean actual word profit bottom tree okay point weve complet element expans accord two probabl distribut point go keep go go say well categori actual nontermin categori go expand work headword go use probabl distribut go choos word headword corpor point nontermin well know that word sentenc vp head rose go expand make use probabl distribut go expand choos expans phrase mayb itll go past tens verb preposit phrase well start expand choos headword expand well get rose automat cours go word rose choos head word start expand pretti much done thing realli need way get start need slightli vari probabl distribut begin go say root categori top need headword point go probabl headword given categori equal root thing dont exist chanc differ word head whole sentenc weve done start expand downward root head head rose well look rule expand look expans point dont yet parent categori weve got categori head need coupl special probabl distribut get start begin basic recurs ive talk achiev put word grammar like statist show go bit detail talk previous differ expans verb phrase rule choic differ verb head illustr systemat chanc differ expans verb phrase vari enorm depend verb your choos verb like come find get pp verb enorm enorm common happen onethird time verb verb phrase happen ten percent time mani kind complement like np complement realli realli rare verb come that exactli also tran noun phrase verb fact differ verb take well take normal take object took nap took book took ticket anyth like third time get vp goe v np though cours get thing well move think think sort sententi complement verb your say thought fact nearli alway think almost three quarter time your get sbar complement that someth like think dishonest think dishonest either sbar complement regardless whether there overt final exampl illustr verb want also take complement take infinit complement want go store seventi percent time get essenti notic theyr extrem differ probabl differ expans verb phrase depend know head verb precis kind inform captur charniak pars lexic pars model one final thing note sometim refer monolex probabl look expans categori term categori know one lexic item head verb that slight contrast main way lexic pcfg use that predict depend thing like preposit phrase bilex probabl exampl like go man decid like connect preposit noun verb involv two word time charniak model also bilex probabl chanc choos head noun noun phrase plural noun phrase given amount inform might includ inform noun phrase also inform parent categori headword sentenc whole sentenc find wall street journal isnt typic english plural noun bit one percent noun price know subject insid noun phrase node ie subject sentenc well chanc price becom two half percent realli make big differ know verb fell well that inform tell kind verb could easili go price probabl becom far higher almost fifteen percent chanc one seven chanc head noun phrase gonna price captur lot probabilist inform might wonder gee realli estim probabl answer gener cant estim probabl probabl youd like estim becom far spars ill illustr next slide way dealt charniak model complic scheme linear interpol differ model less precis reminisc saw languag model second week class want estim probabl distribut weve seen choos headword base parent headword current categori parent categori way take linear interpol bunch differ distribut one maximum likelihood estim condit everyth distribut first leav parent headword also leav even parent categori point your choos headword base categori say noun phrase what chanc certain head charniak model differ distribut weight determinist way depend much youd expect seen certain kind evid make use languag model like techniqu essenti build kind lexic pcfg data spars next slide show differ distribut combin togeth linear interpol one right first find youv got noun phrase head profit insid there adject your want ask adject wall street journal one quarter time corpor profit okay that precis fulli condit estim wherea start eras inform get coarser still nonzero estim havent actual made sure second line method charniak use get kind coars semant class tri keep inform parent head word get rid entir look dont realli lot one look two see your condit know noun phrase noun phrase chanc corpor drop almost two order magnitud your half percent okay good case calcul probabl rich condit help lot quit commonli doesnt work say well verb sentenc rose ive got subject noun phrase noun phrase noun well particular sentenc data pars actual noun profit turn train data profit never occur noun head noun phrase subject rose despit fact sound perfectli normal profit rose last quarter profit rose throughout economi sentenc like maximum likelihood estim mle zero way get nonzero probabl estim use probabl condit less stuff even get low probabl right probabl noun profit noun phrase sort one th percent that best kind estim although youd like use rich estim time lexic pcfg pars your actual fall back rather coarser estim cant get condit inform youd like fairli small supervis tree bank train particular bilex probabl your tri condit two lexic item probabl tend get back amount inform estim extrem extrem spars your space even consid categori your someth that kind like bigram probabl estim hard estim word bigram probabl million word text that text handconstruct tree bank okay detail need smooth end hope main thing could take away segment fairli straightforward system two probabl distribut charniak abl use realiz lexic pcfg model,[ 0  4 14 13 12]
188,Course2_W6-S2-L3_PCFG_Independence_Assumptions_9-44,segment gonna dig little deeper independence assumptions pcfgs means practice symbols pcfg define independence assumptions pair rules like goes noun phrase verb phrase noun phrase goes determiner noun youre expanding thing know youve got noun phrase youve got probabilities expands right way think categories pcfg kind choke points information flow outside tree weve expanding noun phrase know nothing outside tree know expanding noun phrase looked reverse working parse choose dont know anything went know noun phrase number words need work happens precisely point theres independence assumption assumption work probabilities things inside knowing noun phrase work probabilities things knowing noun phrase thats strong independence assumption see clearly strong looking various cases heres one example say chances expanding noun phrase overall well chance noun phrase expanding noun phrase prepositional phrase penn tree bank percent chances expanding pronoun something like noun phrase goes pronoun thats six percent overall statistics lets suppose ask particular categories suppose know noun phrase thats subject noun phrase sentence find statistics enormously different chance expanding np pp go little whats really dramatic chances becoming pronoun go enormously three half times contrast know noun phrase verb phrase chance pronoun drops chances np pp go lot double bit background linguistics would say moment well course thats exactly youd expect based knowledge information structure language subjects normally used topics established discourse reference therefore highly appropriate pronoun whereas inside verb phrase youre normally introducing new information therefore needs explained little bit therefore would quite common noun phrase prepositional phrase hanging thats one ways express information say something like man railway railroad tracks something like youre giving descriptive content thinking moment pcfg well finding say expanding noun phrase losing additional information would let us give much better probabilities expand another way see independence assumptions strong run basic pcfg find wrong thing choosing rules look wrong shouldnt used somehow choosing use heres example noun phrase like big board composite training noun phrase structure youre meant choose penn tree bank flat structure general penn tree bank uses lot flat structures compound nouns like train simple pcfg run sentence structure actually get one put extra noun phrase node around big board well shouldnt done first reaction well compound nouns penn tree bank kind flat structure didnt learn reason able theres different structure penn tree bank get np nodes start np happens possessives possessive np noun phrase going noun phrase adjective noun youd like say well rule okay use possessives noun phrase isnt possessive thats precisely problem cause nodes marked noun phrase recording information one possessive noun phrase somehow want get information category say possessive noun phrase parser would know cant use expansion rule non possessive noun phrase like big board thats want relax independence assumptions pcfg encoding information nonterminal symbols process thats often referred statesplitting one first proposals statesplitting showing successful mark johnson noticed could improve probabilistic context free grammar quite lot encoding part nonterminal also parent category remember also saw idea conditioning charniak parser preceding segment johnsons observation nothing changed nonterminals also record parent nonterminal effectively number nonterminals squared space nonterminals change alone train pcfg regular manner makes big difference pushes parsing numbers couple percent thats useful idea things might want slightly different character example previous slide want rule big board bad parse seems like also want know possessive noun phrases possessive noun phrases ones like fidelitys apostrophe end different nature regular noun phrases wasnt recorded category symbol called noun phrase dont state splitting say oh gonna split noun phrases possessives call nppos information also captured grammar well accurate pcfg course theres danger much much state splitting start getting nonterminals information nonterminals becomes sparser sparser particular model gonna show next segment actually sm smoothing rule rewrite probabilities much state splitting youd end zero probabilities things havent seen unlike word probabilities basically smooth reasonable amount state splitting categories sort way deal smoothing going look well ways splitting categories useful capturing probabilistic dependence thats needed good job pcfg parsing okay hope gives concrete sense pcfg independence assumptions hurt also solve,Course2,W6-S2-L3,W6,S2,L3,PCFG,6,2,3,segment gonna dig littl deeper independ assumpt pcfg mean practic symbol pcfg defin independ assumpt pair rule like goe noun phrase verb phrase noun phrase goe determin noun your expand thing know youv got noun phrase youv got probabl expand right way think categori pcfg kind choke point inform flow outsid tree weve expand noun phrase know noth outsid tree know expand noun phrase look revers work pars choos dont know anyth went know noun phrase number word need work happen precis point there independ assumpt assumpt work probabl thing insid know noun phrase work probabl thing know noun phrase that strong independ assumpt see clearli strong look variou case here one exampl say chanc expand noun phrase overal well chanc noun phrase expand noun phrase preposit phrase penn tree bank percent chanc expand pronoun someth like noun phrase goe pronoun that six percent overal statist let suppos ask particular categori suppos know noun phrase that subject noun phrase sentenc find statist enorm differ chanc expand np pp go littl what realli dramat chanc becom pronoun go enorm three half time contrast know noun phrase verb phrase chanc pronoun drop chanc np pp go lot doubl bit background linguist would say moment well cours that exactli youd expect base knowledg inform structur languag subject normal use topic establish discours refer therefor highli appropri pronoun wherea insid verb phrase your normal introduc new inform therefor need explain littl bit therefor would quit common noun phrase preposit phrase hang that one way express inform say someth like man railway railroad track someth like your give descript content think moment pcfg well find say expand noun phrase lose addit inform would let us give much better probabl expand anoth way see independ assumpt strong run basic pcfg find wrong thing choos rule look wrong shouldnt use somehow choos use here exampl noun phrase like big board composit train noun phrase structur your meant choos penn tree bank flat structur gener penn tree bank use lot flat structur compound noun like train simpl pcfg run sentenc structur actual get one put extra noun phrase node around big board well shouldnt done first reaction well compound noun penn tree bank kind flat structur didnt learn reason abl there differ structur penn tree bank get np node start np happen possess possess np noun phrase go noun phrase adject noun youd like say well rule okay use possess noun phrase isnt possess that precis problem caus node mark noun phrase record inform one possess noun phrase somehow want get inform categori say possess noun phrase parser would know cant use expans rule non possess noun phrase like big board that want relax independ assumpt pcfg encod inform nontermin symbol process that often refer statesplit one first propos statesplit show success mark johnson notic could improv probabilist context free grammar quit lot encod part nontermin also parent categori rememb also saw idea condit charniak parser preced segment johnson observ noth chang nontermin also record parent nontermin effect number nontermin squar space nontermin chang alon train pcfg regular manner make big differ push pars number coupl percent that use idea thing might want slightli differ charact exampl previou slide want rule big board bad pars seem like also want know possess noun phrase possess noun phrase one like fidel apostroph end differ natur regular noun phrase wasnt record categori symbol call noun phrase dont state split say oh gonna split noun phrase possess call nppo inform also captur grammar well accur pcfg cours there danger much much state split start get nontermin inform nontermin becom sparser sparser particular model gonna show next segment actual sm smooth rule rewrit probabl much state split youd end zero probabl thing havent seen unlik word probabl basic smooth reason amount state split categori sort way deal smooth go look well way split categori use captur probabilist depend that need good job pcfg pars okay hope give concret sens pcfg independ assumpt hurt also solv,[ 0 10  4 14 13]
189,Course2_W6-S2-L4_The_Return_of_Unlexicalized_PCFGs_20-53,much better understanding role independence assumptions pcfgs segment im gonna show make much much better pcfg without lexicalization work done accurate unlexicalized parsing paper dan klein first mean unlexicalized pcfg thats gonna mean general grammar rules systematically specified level lexical items gonna able lexicalized categories like lexicalized pcfg like npstocks vprose hand going allowed things like parent annotation categories note things makeup categories like saying noun phrase coordinated noun phrase drill teeny bit particular wish make distinction closed versus open class words long tradition linguistics syntax make use function words features markers categories selection english verbs act verbal auxiliaries things like running eaten okay recognize function words special role treat specially sentential complement okay know whether complement versus whether complement kind conditioning features allowed state splits theyre fundamentally different idea marking semantic heads phrases working things like prepositional phrase attachment really youre using lexical heads kind proxy semantics thesis large percent need accurate parsing indeed much actually captured lexicalized pcfgs wasnt actually anything probabilistic dependencies captured content words actually really basic grammatical features like verb form finiteness presence verbal auxiliary well known people working traditional grammars particular people whod worked feature based grammars languages way went investigating penn tree bank wall street journal chose small development set could whole bunch experiments days computers much slower files wall street journal seemed like itd take long time used twenty files wall street journal development section wed run wed make manual state splits grammar try improve performance breaking wrong independence assumptions couple statistics one performance level usual f labelled precision recall one size grammar make state splits grammar bigger terms number nonterminals number gets big thats dan dangerous two reasons itll slow parser well start get problems sparseness smoothing rewrites pcfg except lexical level rewriting words goal state split sparingly possible want make limited number state splits give bang buck terms capturing necessary probabilistic dependencies lets look examples motivation part looked done lexicalized pcfgs turns things done lexicalized pcfg models prominent ones days eugene charniaks mike collinss didnt actually anything content word lexicalization theyd things one idea split contextfree grammar rules long right hand sides limited conditioning things right hand side flat wall lots penn tree bank weve already discussed needing binarize theres sort straightforward way binarization point preserve left context probabilities expanding preserving left context saying probabilities expanding go youre conditioning stuff might make much difference youre also going places penn tree bank noun phrase expands five proper nouns row something like greater duluth investment advancement committee something like youre going get five noun proper nouns row seems like point wanna know youre expanding bunch proper nouns many beforehand doesnt matter get rid history way language models markovize say lets use entire preceding context lets use bit preceding context preceding context terms categories wed seen previously righthand side keep expanding thing saw recently righthand side get rid prior conditioning context two states become grammar actually get smaller perspective naive binarization flat rules actually get big grammar parse percent number weve talked plain vanilla pcfg turns condition less context like two preceding tags one preceding thing right hand side grammar gets enormously smaller actually performance goes particular condition two previous categories expansion right hand side performance goes little bit found little bit better could condition sometimes one piece preceding context sometimes two bits preceding context depending often youd seen expansions category common nonterminal okay thats horizontal markovization idea using parent categories think vertical markovization saying okay expanding verb phrase could say well lets also look thing tree marking category looking thing tree kind looking history going upwards think whole history going upwards progressively deleting thats form vertical markovization perspective standard pcfg youre expanding category youre looking nothing youre vertical markov order one slightly percent performance level johnson noticed simply nothing else also knowing parent category gives lot value thats pushing parsing numbers four percent turns put grandparents well push numbers even little bit problem start bigger bigger grammar non terminals model used basis linguistic state splits actually take parent nothing model using parent category time always certain rare nonterminals theyre ones saw mention occasionally theyre things like special non terminals fragments reduced relative clauses things like occur rarely okay took vertical horizontal markovization level v one two get pcfg accuracy percent f number non terminals grammar seven half thousand thatll basis introducing linguistic state splits okay well problems independent assumptions pcfg cause easy way find run pcfg see makes parse errors development set scratch head think choosing wrong rule information could encode non terminals would cause stop heres example find basic pcfg unary rules used often make easy change one category another high category rule like verb phrase going verb ending ing noun phrase gonna want use finds change sentence verb phrase fairly high probability thats high probability unary rule use theres theres problem unary rewrite isnt appropriate higher level rule higher level rule expecting finite sentence subject therefore shouldnt expand unary rule way capture saying well lets mark unary rewrites training data whenever unary rule applied well stick unary parent category know kinds places unary rewrites occur point itll decide parsing time isnt place unary rewrites occur itll choose different structure sentence decide choose structure actually modifier phrase modifying noun phrase thats right answer making little change immediately already move parsing numbers half percent lets keep going another problem independent assumptions various places part speech tags coarse one worst examples tag penn tree bank tag set thats used three sentential complementizers sentence complements words like whether subordinating conjunctions like true prepositions words like well causes wrong parses heres example causes wrong parse sentential complement advertising works sentence advertising works instead chosen give regular prepositional phrase analysis despite fact preposition really true preposition one sentential complementizers okay well way deal split tag different kinds words say noun phrase preposition want kind appears noun phrase complement well whats going happen going learn example instead example kind appears sentential complement appears sbar construction means parser going choose different correct construction going say part sbar splitting tag way big change gives extra two percent parsing accuracy keep lets look couple examples sometimes also want refine phrasal categories verb phrase makes fair bit difference knowing kind verb phrase finite verb nonfinite infinitive verb one way capture knowing part speech tag verb finite tag verb infinitive non finite tag verb represent information also verb phrase category find structure bad although verb take vp complements wont take infinitive bare infinitive vp complements like panic instead going take things like participial progressives like running things like annotate extra information parser know isnt good structure choose different structure choosing make panic buying noun phrase correctly okay thats idea splitting verb phrase categories mentioned earlier wanted split possessive noun phrase categories worth quite bit parsing accuracy getting almost percent possessives getting two percent knowing kind phrase dealing theres one category splits introduced somehow want know something attachment sites prepositional phrases even though lexical conditioning want kind idea prepositional phrases tend attach low tend attach high put features capture mark whether phrase contains verb tends capture whether already something big sentence sized also mark noun phrases whether theyve already things attached whether theyre still base noun phrase hasnt anything attached ideas also push performance numbers little indeed development set weve gotten f end day weve constructed transformed training data manual rules done state splits produce richer set nonterminals verb phrase thats scategory finite verb phrase contains verb categories build straightforward pcfg categories parse well well answer unlexicalized pcfg suddenly actually rather well results two earliest lexicalized pcfg models got almost handbuilt unlexicalized pcfg actually little bit better isnt quite well model charniak showed earlier indeed collinss slightly later model ones getting know little bit rate shows considering know started plain pcfg look gave mileage get looks like well know thirteen fourteen percent mileage coming ideas like knowing context parent annotation refining critical categories knowing possessive noun phrases knowing type verb phrases participial infinitive finite verb phrases amount better models capturing true lexicalization really rather small somewhere one three percent changed orientation somewhat better understanding isnt important able choose right parses okay showed although idea lexicalization clearly important capturing certain kinds parsing decisions prepositional phrase attachments lot little bit linguistic modeling rather actually going whole way massive space lexicalized pcfgs kind modeling actually kind familiar work featurebased unificationbased models explored linguistics,Course2,W6-S2-L4,W6,S2,L4,The,6,2,4,much better understand role independ assumpt pcfg segment im gonna show make much much better pcfg without lexic work done accur unlexic pars paper dan klein first mean unlexic pcfg that gonna mean gener grammar rule systemat specifi level lexic item gonna abl lexic categori like lexic pcfg like npstock vprose hand go allow thing like parent annot categori note thing makeup categori like say noun phrase coordin noun phrase drill teeni bit particular wish make distinct close versu open class word long tradit linguist syntax make use function word featur marker categori select english verb act verbal auxiliari thing like run eaten okay recogn function word special role treat special sententi complement okay know whether complement versu whether complement kind condit featur allow state split theyr fundament differ idea mark semant head phrase work thing like preposit phrase attach realli your use lexic head kind proxi semant thesi larg percent need accur pars inde much actual captur lexic pcfg wasnt actual anyth probabilist depend captur content word actual realli basic grammat featur like verb form finit presenc verbal auxiliari well known peopl work tradit grammar particular peopl whod work featur base grammar languag way went investig penn tree bank wall street journal chose small develop set could whole bunch experi day comput much slower file wall street journal seem like itd take long time use twenti file wall street journal develop section wed run wed make manual state split grammar tri improv perform break wrong independ assumpt coupl statist one perform level usual f label precis recal one size grammar make state split grammar bigger term number nontermin number get big that dan danger two reason itll slow parser well start get problem spars smooth rewrit pcfg except lexic level rewrit word goal state split sparingli possibl want make limit number state split give bang buck term captur necessari probabilist depend let look exampl motiv part look done lexic pcfg turn thing done lexic pcfg model promin one day eugen charniak mike collinss didnt actual anyth content word lexic theyd thing one idea split contextfre grammar rule long right hand side limit condit thing right hand side flat wall lot penn tree bank weve alreadi discuss need binar there sort straightforward way binar point preserv left context probabl expand preserv left context say probabl expand go your condit stuff might make much differ your also go place penn tree bank noun phrase expand five proper noun row someth like greater duluth invest advanc committe someth like your go get five noun proper noun row seem like point wanna know your expand bunch proper noun mani beforehand doesnt matter get rid histori way languag model markov say let use entir preced context let use bit preced context preced context term categori wed seen previous righthand side keep expand thing saw recent righthand side get rid prior condit context two state becom grammar actual get smaller perspect naiv binar flat rule actual get big grammar pars percent number weve talk plain vanilla pcfg turn condit less context like two preced tag one preced thing right hand side grammar get enorm smaller actual perform goe particular condit two previou categori expans right hand side perform goe littl bit found littl bit better could condit sometim one piec preced context sometim two bit preced context depend often youd seen expans categori common nontermin okay that horizont markov idea use parent categori think vertic markov say okay expand verb phrase could say well let also look thing tree mark categori look thing tree kind look histori go upward think whole histori go upward progress delet that form vertic markov perspect standard pcfg your expand categori your look noth your vertic markov order one slightli percent perform level johnson notic simpli noth els also know parent categori give lot valu that push pars number four percent turn put grandpar well push number even littl bit problem start bigger bigger grammar non termin model use basi linguist state split actual take parent noth model use parent categori time alway certain rare nontermin theyr one saw mention occasion theyr thing like special non termin fragment reduc rel claus thing like occur rare okay took vertic horizont markov level v one two get pcfg accuraci percent f number non termin grammar seven half thousand thatll basi introduc linguist state split okay well problem independ assumpt pcfg caus easi way find run pcfg see make pars error develop set scratch head think choos wrong rule inform could encod non termin would caus stop here exampl find basic pcfg unari rule use often make easi chang one categori anoth high categori rule like verb phrase go verb end ing noun phrase gonna want use find chang sentenc verb phrase fairli high probabl that high probabl unari rule use there there problem unari rewrit isnt appropri higher level rule higher level rule expect finit sentenc subject therefor shouldnt expand unari rule way captur say well let mark unari rewrit train data whenev unari rule appli well stick unari parent categori know kind place unari rewrit occur point itll decid pars time isnt place unari rewrit occur itll choos differ structur sentenc decid choos structur actual modifi phrase modifi noun phrase that right answer make littl chang immedi alreadi move pars number half percent let keep go anoth problem independ assumpt variou place part speech tag coars one worst exampl tag penn tree bank tag set that use three sententi complement sentenc complement word like whether subordin conjunct like true preposit word like well caus wrong pars here exampl caus wrong pars sententi complement advertis work sentenc advertis work instead chosen give regular preposit phrase analysi despit fact preposit realli true preposit one sententi complement okay well way deal split tag differ kind word say noun phrase preposit want kind appear noun phrase complement well what go happen go learn exampl instead exampl kind appear sententi complement appear sbar construct mean parser go choos differ correct construct go say part sbar split tag way big chang give extra two percent pars accuraci keep let look coupl exampl sometim also want refin phrasal categori verb phrase make fair bit differ know kind verb phrase finit verb nonfinit infinit verb one way captur know part speech tag verb finit tag verb infinit non finit tag verb repres inform also verb phrase categori find structur bad although verb take vp complement wont take infinit bare infinit vp complement like panic instead go take thing like participi progress like run thing like annot extra inform parser know isnt good structur choos differ structur choos make panic buy noun phrase correctli okay that idea split verb phrase categori mention earlier want split possess noun phrase categori worth quit bit pars accuraci get almost percent possess get two percent know kind phrase deal there one categori split introduc somehow want know someth attach site preposit phrase even though lexic condit want kind idea preposit phrase tend attach low tend attach high put featur captur mark whether phrase contain verb tend captur whether alreadi someth big sentenc size also mark noun phrase whether theyv alreadi thing attach whether theyr still base noun phrase hasnt anyth attach idea also push perform number littl inde develop set weve gotten f end day weve construct transform train data manual rule done state split produc richer set nontermin verb phrase that scategori finit verb phrase contain verb categori build straightforward pcfg categori pars well well answer unlexic pcfg suddenli actual rather well result two earliest lexic pcfg model got almost handbuilt unlexic pcfg actual littl bit better isnt quit well model charniak show earlier inde collinss slightli later model one get know littl bit rate show consid know start plain pcfg look gave mileag get look like well know thirteen fourteen percent mileag come idea like know context parent annot refin critic categori know possess noun phrase know type verb phrase participi infinit finit verb phrase amount better model captur true lexic realli rather small somewher one three percent chang orient somewhat better understand isnt import abl choos right pars okay show although idea lexic clearli import captur certain kind pars decis preposit phrase attach lot littl bit linguist model rather actual go whole way massiv space lexic pcfg kind model actual kind familiar work featurebas unificationbas model explor linguist,[ 0  4  1 10 14]
190,Course2_W6-S2-L5_Latent_Variable_PCFGs_12-07,following work accurate unlexicalized parsing extension led latent variable pcfgs im gonna go detail want briefly introduce idea show works model looking previously accurate unlexicalized pcfgs everything hand done someone staring sentences parsed deciding better split categories build parser would work better youre interested machine learning thinking gee maybe could automatically rather manually thats idea latent variable pcfgs starting point still tree bank train bracketing sentences training data known base categories known noun phrase node thats context already know saying well maybe theres way wed like annotate information say kind noun phrase parse accurately concretely way thats done splitting noun phrase category certain number subcategories one given number np learning task choose number subcategories group noun phrases tra training data particular subcategories something common nps good predictions expand words contain im going go algorithm detail youve seen things classes like machine learning probabilistic graphical models form em algorithm like forwardbackward algorithm used hmms constrained preexisting tree structure youre preexisting tree structure black youre wanting probability distribution different latent substates category simplest way would say okay category ten subcategories learn probability distribution choice different subcategories state tried previously found work well far slav petrov introduced cleverer split merge algorithm category started single category said okay lets try split two categories good way split capture conditioning information yes lets keep split okay well try lets take split two four subcategories maybe point discover one splits useful one wasnt get rid one go back three subcategories youd repeat number times youd progressively split merge subcategories come good number sets different categories yeah im gonna go algorithm detail lets look turns shows subcategories learned part speech tags cause theyre easiest ones interpret proper nouns find proper nouns divided groups way theyre divided isnt purely syntactic like feature based grammar anymore theyre kind syntacticosemantic class based models subcategory proper nouns abbreviations months another one thats first names another one thats initials another one thats last names people two corresponding states location names multiword first word last word kind interesting semantic subclasses nouns something similar happening also personal pronouns ones nominative subject pronouns ones accusative object pronouns also distinguishing subject ones whether theyre capitalized maybe capturing something beginning sentence word isnt restricted appear one category rewrite multiple categories appears places used nominative accusative pattern looking overall pattern state splits phrases actually works interestingly linguistically effectively common categories like noun phrase verb phrase prepositional phrase equally ones basic categories penn tree bank crude thats found previously hand states splits found wanted distinguish possessive noun phrases wanted distinguish verb phrases depending whether theyre infinitive twoverb phrases whether theyre ing verb phrases whether theyre finite verb phrases well places split merge algorithm learns lot subcategories learning subcategories hand rare weird categories unlike constituent phrase right recursive clauses fragments learning subcategories fact ones making splits single tree bank category okay building grammar making split arent purely syntactic also got semantically flavored splits still much coarser actually head word lexicalization models im showing maximum possible number splits could happen category practice reasonable number survived practice youre always getting less see example level state splitting well make parser work answer turned could make parser worked amazingly well slide shows current parsing results parsers around im saying around start basic pcfg around percent f working im showing two columns results lot early parser results shown sentences words whereas recently common show results sentences length okay baseline unlexicalized parser klein manning sentences lengths getting little bit percent matsuzaki et al tried simple idea splitting category latent states assuming number latent states category didnt really work better particular parsers still noticeably could get lexicalized pcfgs head words best charniak collins family lexicalized pcfgs recent model eugene charniaks getting half percent seemed like maybe three half percent gap value getting lexicalization beyond got basic state splitting latent variable pcfg showed well actually could get value without actually modeling particular head words using fairly coarse semantic classes words latent variable pcfg actually little bit better getting extra half percent kind cant explain absolute versions better really using less conditioning information charniaks parser reason better probability estimation better better actually lot easier much smaller number categories grammar youre complicated smoothing head words people gone done work lexicalized pcfgs particular eugene charniak mark johnson worked together add discriminative reranker top charniak parser discriminative reranker essentially using maxent model kind saw week applying choose different parses sentence candidate parses generated charniaks generative parser feeding well maxent model able kind better forward probabilistic conditioning definitely helped adding two percent performance charniak parser particular adding little bit still level petrov klein parser still noticeably knowing individual particular head words still giving percent performance finally always machine learning contexts bunch systems bunch classifiers things always get even better results combining together people shown fossum knight combined various constituent parsers performance aggregate model able get extra percent performance state art moment performance probabilistic parsing actually extremely high level accuracy probabilistic parsers actually give right parses parts sentences quite reliably okay shows idea latent states allow defined maximize parse performance get syntactico semantic states really allow parse rather well still quite compact grammar,Course2,W6-S2-L5,W6,S2,L5,Latent,6,2,5,follow work accur unlexic pars extens led latent variabl pcfg im gonna go detail want briefli introduc idea show work model look previous accur unlexic pcfg everyth hand done someon stare sentenc pars decid better split categori build parser would work better your interest machin learn think gee mayb could automat rather manual that idea latent variabl pcfg start point still tree bank train bracket sentenc train data known base categori known noun phrase node that context alreadi know say well mayb there way wed like annot inform say kind noun phrase pars accur concret way that done split noun phrase categori certain number subcategori one given number np learn task choos number subcategori group noun phrase tra train data particular subcategori someth common np good predict expand word contain im go go algorithm detail youv seen thing class like machin learn probabilist graphic model form em algorithm like forwardbackward algorithm use hmm constrain preexist tree structur your preexist tree structur black your want probabl distribut differ latent substat categori simplest way would say okay categori ten subcategori learn probabl distribut choic differ subcategori state tri previous found work well far slav petrov introduc clever split merg algorithm categori start singl categori said okay let tri split two categori good way split captur condit inform ye let keep split okay well tri let take split two four subcategori mayb point discov one split use one wasnt get rid one go back three subcategori youd repeat number time youd progress split merg subcategori come good number set differ categori yeah im gonna go algorithm detail let look turn show subcategori learn part speech tag caus theyr easiest one interpret proper noun find proper noun divid group way theyr divid isnt pure syntact like featur base grammar anymor theyr kind syntacticosemant class base model subcategori proper noun abbrevi month anoth one that first name anoth one that initi anoth one that last name peopl two correspond state locat name multiword first word last word kind interest semant subclass noun someth similar happen also person pronoun one nomin subject pronoun one accus object pronoun also distinguish subject one whether theyr capit mayb captur someth begin sentenc word isnt restrict appear one categori rewrit multipl categori appear place use nomin accus pattern look overal pattern state split phrase actual work interestingli linguist effect common categori like noun phrase verb phrase preposit phrase equal one basic categori penn tree bank crude that found previous hand state split found want distinguish possess noun phrase want distinguish verb phrase depend whether theyr infinit twoverb phrase whether theyr ing verb phrase whether theyr finit verb phrase well place split merg algorithm learn lot subcategori learn subcategori hand rare weird categori unlik constitu phrase right recurs claus fragment learn subcategori fact one make split singl tree bank categori okay build grammar make split arent pure syntact also got semant flavor split still much coarser actual head word lexic model im show maximum possibl number split could happen categori practic reason number surviv practic your alway get less see exampl level state split well make parser work answer turn could make parser work amazingli well slide show current pars result parser around im say around start basic pcfg around percent f work im show two column result lot earli parser result shown sentenc word wherea recent common show result sentenc length okay baselin unlexic parser klein man sentenc length get littl bit percent matsuzaki et al tri simpl idea split categori latent state assum number latent state categori didnt realli work better particular parser still notic could get lexic pcfg head word best charniak collin famili lexic pcfg recent model eugen charniak get half percent seem like mayb three half percent gap valu get lexic beyond got basic state split latent variabl pcfg show well actual could get valu without actual model particular head word use fairli coars semant class word latent variabl pcfg actual littl bit better get extra half percent kind cant explain absolut version better realli use less condit inform charniak parser reason better probabl estim better better actual lot easier much smaller number categori grammar your complic smooth head word peopl gone done work lexic pcfg particular eugen charniak mark johnson work togeth add discrimin rerank top charniak parser discrimin rerank essenti use maxent model kind saw week appli choos differ pars sentenc candid pars gener charniak gener parser feed well maxent model abl kind better forward probabilist condit definit help ad two percent perform charniak parser particular ad littl bit still level petrov klein parser still notic know individu particular head word still give percent perform final alway machin learn context bunch system bunch classifi thing alway get even better result combin togeth peopl shown fossum knight combin variou constitu parser perform aggreg model abl get extra percent perform state art moment perform probabilist pars actual extrem high level accuraci probabilist parser actual give right pars part sentenc quit reliabl okay show idea latent state allow defin maxim pars perform get syntactico semant state realli allow pars rather well still quit compact grammar,[ 0  4 10 14 13]
191,Course2_W6-S3-L1_Dependency_Parsing_Introduction_10-25,segment im going return dependency parsing right first segment introduced idea dependency syntax lets look worked idea dependency syntax connect words sentence putting arrows show relationships modifiers arguments words example weve got head whole sentence submitted got dependents got bills submitted somebody also auxiliary verb necessarily quite commonly arrows typed name grammatic relation see weve got subject passive verbal auxiliary prepositional relationship things know bit terminology firstly arrow always thing head governor submitted thing dependent modifier inferior various words used see bills theyre two ends governor governed thing dependency beyond theres actually inconsistency things done slides original dependency grammar work tesnière arrows run governor dependent absolutely also find work points arrows way actually like youre using height tree show whats dependent actually dont need arrows could draw lines without arrowhead okay example normally find dependencies form tree theres root node everything heads words single head nice acyclic manner mention also actually quite common add sort one pseudo node top often called root wall points head sentence actually makes things lot cleaner terms parsing algorithms also terms things like evaluation representation get property every word sentence including root dependent one thing think assignment process working governor word sentence dependency grammar relate kind phrase structure grammar weve concentrated far well central innovation really dependency grammar built around notion heads dependents whereas basic case contextfree grammar theres notion head whatsoever actually things moved look modern ex modern linguistic theory means things like xbar grammar linguists modern statistical parsers whether charniakcollins stanford parser notion head use extensively example parsers theres notion head rules itll identify category head larger category soon head rules kind discussed well straightforwardly get dependencies phrase structure representation basically kind spine head chains everywhere something coming thats dependent dependency walked sue dependency walked another head chain weve got another dependency store head chain dependency store basis dependency representation inside phrase structured tree heads represented go opposite direction try go dependencies phrase structure reconstruct phrase structure tree taking closure dependencies word saying represent constituent slightly changes representation normally see phrase structure trees particular situation like cant vp node actually sue dependents walked therefore three must flat phrase structure representation three head two dependents sue people go dependency parsing whole variety methods used dependency parsing one method dynamic programming algorithm like cky algorithm saw phrase structure parsing naively adding heads end something similar lexicalized probabilistic context free grammars saw earlier end big n fifth algorithm theres clever reformulation parse items due jason eisner makes complexity dependency parsing also n cubed kind youd hope thinking nature operation whole bunch methods people directly used graph algorithms dependency parsing one idea algorithms literature construct maximum spanning tree sentence since want words connected together dependent something means build tree spans words sentence thats idea thats used well known mst parser ideas constraint satisfaction start dense set edges words eliminate ones dont satisfy hard constraints final trained independency parsing actually going focus way dependency parsing head left right sentence make greedy decisions based machine learning classifiers words connect words dependents well known example framework maltparser im going concentrate partly different approach phrase structure parsing looked depth also shown kind method dependency parsing actually works extremely well work accurately exceedingly quickly good thing know different point space matter dependency parsing need sources information let us choose possible analyses words take dependents words heres list main sources information people use obvious source information bilexical dependencies something like dependency issues well look word thats head look word thats dependent say likely thats similar bilexical dependencies earlier lexicalized pcfgs dont want use source information partly lexical information sparse several good sources information lets go one distance head dependent look picture youll see dependencies short theyre nearby words couple exceptions dependency pretty long one pretty short sources information intervening material general dependencies dont cross verbs commonly dont cross punctuation exceptions commas quite often crossed looking words give information whether dependency likely final source information looking valency heads thats saying particular word kind dependents typically take word like typically takes dependents left dependents right case hand word say noun take dependents like adjectives articles left wont take kind dependents right though take kinds words dependents right example take prepositional phrase modifiers relative clauses dependents right develop quite rich typology kind dependents words take okay give better sense dependency representations look like big picture go parsing next segment well introduce concrete algorithm dependency parsing,Course2,W6-S3-L1,W6,S3,L1,Dependency,6,3,1,segment im go return depend pars right first segment introduc idea depend syntax let look work idea depend syntax connect word sentenc put arrow show relationship modifi argument word exampl weve got head whole sentenc submit got depend got bill submit somebodi also auxiliari verb necessarili quit commonli arrow type name grammat relat see weve got subject passiv verbal auxiliari preposit relationship thing know bit terminolog firstli arrow alway thing head governor submit thing depend modifi inferior variou word use see bill theyr two end governor govern thing depend beyond there actual inconsist thing done slide origin depend grammar work tesnièr arrow run governor depend absolut also find work point arrow way actual like your use height tree show what depend actual dont need arrow could draw line without arrowhead okay exampl normal find depend form tree there root node everyth head word singl head nice acycl manner mention also actual quit common add sort one pseudo node top often call root wall point head sentenc actual make thing lot cleaner term pars algorithm also term thing like evalu represent get properti everi word sentenc includ root depend one thing think assign process work governor word sentenc depend grammar relat kind phrase structur grammar weve concentr far well central innov realli depend grammar built around notion head depend wherea basic case contextfre grammar there notion head whatsoev actual thing move look modern ex modern linguist theori mean thing like xbar grammar linguist modern statist parser whether charniakcollin stanford parser notion head use extens exampl parser there notion head rule itll identifi categori head larger categori soon head rule kind discuss well straightforwardli get depend phrase structur represent basic kind spine head chain everywher someth come that depend depend walk sue depend walk anoth head chain weve got anoth depend store head chain depend store basi depend represent insid phrase structur tree head repres go opposit direct tri go depend phrase structur reconstruct phrase structur tree take closur depend word say repres constitu slightli chang represent normal see phrase structur tree particular situat like cant vp node actual sue depend walk therefor three must flat phrase structur represent three head two depend sue peopl go depend pars whole varieti method use depend pars one method dynam program algorithm like cki algorithm saw phrase structur pars naiv ad head end someth similar lexic probabilist context free grammar saw earlier end big n fifth algorithm there clever reformul pars item due jason eisner make complex depend pars also n cube kind youd hope think natur oper whole bunch method peopl directli use graph algorithm depend pars one idea algorithm literatur construct maximum span tree sentenc sinc want word connect togeth depend someth mean build tree span word sentenc that idea that use well known mst parser idea constraint satisfact start dens set edg word elimin one dont satisfi hard constraint final train independ pars actual go focu way depend pars head left right sentenc make greedi decis base machin learn classifi word connect word depend well known exampl framework maltpars im go concentr partli differ approach phrase structur pars look depth also shown kind method depend pars actual work extrem well work accur exceedingli quickli good thing know differ point space matter depend pars need sourc inform let us choos possibl analys word take depend word here list main sourc inform peopl use obviou sourc inform bilex depend someth like depend issu well look word that head look word that depend say like that similar bilex depend earlier lexic pcfg dont want use sourc inform partli lexic inform spars sever good sourc inform let go one distanc head depend look pictur youll see depend short theyr nearbi word coupl except depend pretti long one pretti short sourc inform interven materi gener depend dont cross verb commonli dont cross punctuat except comma quit often cross look word give inform whether depend like final sourc inform look valenc head that say particular word kind depend typic take word like typic take depend left depend right case hand word say noun take depend like adject articl left wont take kind depend right though take kind word depend right exampl take preposit phrase modifi rel claus depend right develop quit rich typolog kind depend word take okay give better sens depend represent look like big pictur go pars next segment well introduc concret algorithm depend pars,[14  0  4 13 12]
192,Course2_W6-S3-L2_Greedy_Transition-Based_Parsing_31-05,segment im going look greedy transition based parsing approach dependency parsing particular im going describe model thats used maltparser bestknown example framework idea maltparser maybe could parsing making simple greedy decisions attach word comes along particular make decisions well use discriminative machine learning classifier parser sequence actions working bottom kind like shift reduce parser youve seen either cfg parsing programming language literature parser stack written top right start root symbol introduced last time buffer stuff weve yet look top written left input sentence go along well build set dependency arcs starts empty well set actions natural way think transition based dependencies parser start configuration stack buffer empty set dependency arcs essentially moves shift word across buffer stack like shift operation cfg parsing reduce operation dependency parsing framework end two reducer operations depending take whether taking word left word right head word word j take word right head get left arc means kind creating arc points like added set arcs right arc operation dependency goes way word buffer dependent word stack untyped dependency parsing two operations want type dependency parsing also reduction say label youre using connect two words means twenty labels actually end different actions dependencyi parser finish youve exhausted buffer something note seen shift reduce parsing theres something thats slightly unusual presentation normally shift reduce parsing cfgs first put things stack reduction done fully stack model thats show reductions either direction youre actually redu reduction one thing thats stack one thing thats buffer thats convention thats adopted standard say dependency parsing literature claim makes little bit cleaner formulate things way though im sure makes big difference ive gone cause standardly see simple way things standard thing people actually im gonna move next common method see transition based dependency parsers let explain simple way little bit problematic sentence sort um sue tried open door cellar well youre going dependency tried open soon youve gotten input least obvious youre going dependency tried open youre using kind three action model dependency parsing ive shown cant construct arc immediately instead fact door dependent open cellar dependent door construct material construct arc closure dependencies dependent constructed hook higher head dependencies constructed first well means shifted words input decide anything dependency found amount look ahead actually makes harder machine learning based greedy classifiers work well instead people wanted move able greedy fashion hook local things right even know dependents thats whats done different way formulating actions thats referred arc eager dependency parsing arceager dependency parser exactly start finish configurations operations sort similar theres shift operation theres left arc right arc steps though add one new operation reduce operation number differences subtleties works lets sort look moment left arc operation thing constructing left dependency head thats buffer top word stack result constructing new dependency arc add preconditions need precondition wi isnt already dependent word cause allowed arc put wed get analysis two arrows would pointing word precisely arc eager character words still already stack even though theyve made dependent word right arc operation bit different right arc operation starting point exactly weve going want make arrow like head stack dependent beginning buffer add dependency difference rather getting rid wj push stack means keeping word right take dependents later thats precisely get word stack already declared dependent word okay fact necessitates extra reduce operation cause weve found dependents word eventually get rid go back finding dependents words higher syntax tree thats reduce operation reduce operation also precondition precondition get rid word stack made dependent word words introduced way dependent word later point reduce closed thats bit confusing lets go concrete example think itll make lot sense okay sentence im going work happy children like play friends heres cheat sheet operations going able perform starting start heres stack root symbol buffer sentence weve found dependencies situation four operations could apply thought happy head whole sentence could immediately right arc operation doesnt seem right shift stack situation okay well situation children going dependent happy next thing want construct dependency sort means left arc operation particular introducing adjectival modifier dependent left arc operation add set dependencies weve found get rid dependent stack disappears right okay point children isnt head sentence either another shift operation well point ready another left arc operation children subject like introduce noun subject dependency add set dependencies set two dependencies weve already built okay well point weve actually found head whole sentence like head whole sentence means connect root right arc operation add root dependency whole sentence remember right arc operation havent yet found dependents like right therefore adding stack something still find dependents right indeed point immedi going able start play going dependent like first get past shift onto stack infinitive marker going dependent verb play thats going another left attach auxiliary modifier okay point say like take first right dependent like play thats done right attachment getting right attach xcomp adding set dependencies okay point making progress still got far play well play also going take argument play another right attach play means moves stack weve attached weve made dependency play hasnt found arguments right placed stack going able find arguments withs argument going friends get shift introduce left arc friends attach remember formally apply left arc rule time check precondition precondition hasnt made dependent word hadnt precondition satisfied okay another right arc operation hook together friends object preposition okay making good progress point weve got things weve finished friends never right doesnt right dependents found right dependent friends play found right dependent friends point start using reduce operation reduce friends remember precondition precondition checking friends hooked dependent something indeed reduce get rid reduce get rid play okay point weve got period deal say dependent main verb introduce also right attach operation point buffer empty way defined finished state soon buffer empty stop could want say well geez cant sort reduce reduce pop things back root know could defined things like wouldnt harm actually unnecessary buffer empty cant construct anymore dependencies operation introduces dependency taking one thing stack one thing buffer theres way formulation things reappear buffer theyve moved stack know done weve found complete set dependencies kind work back though list make set dependencies weve introduced okay thats model parser operates operations step step step multiple things could done could chosen shift could chosen make left arc right arc chooses make left arc right arc label dependency introduces one many dependency labels well way thats done using form discriminative classifier support vector machines svms commonly used practice could equally another kind discriminative classifier maxent classifier classifiers choosing set moves untyped dependency parser four moves arc eager configuration typed twice number dependency types plus two classes choose finite set classes youre choosing features use dependency parser well definitely use whats word top stack whats part speech whats first word buffer whats part speech let choose operations like shifting kind dependency label choose arent operations good dependency parsers cause remember things knew could important like length dependency arc thats proposed knowing whats intervening also might want know words top stack well dependents already influence likely get dependencies put things features discriminative classifier kind like saw looking maxent classifiers simplest form transition based dependency parser hence absolutely search whatsoever point youre making greedy decision youve got stack buffer state run classifier decides likely next action take thats approach thats strongly pursued maltparser framework see good job manner really good classifiers good choosing next move mean course dont could kind beam search explore different possibilities rather stunning result form completely greedy transition based parsing really well build dependency parsers work almost well best lexicalized probabilistic context free grammars kind complete cky parsing style search possibilities saw previously taking converting lpcfgs dependency representations evaluate ill say little bit evaluation moment well close state art would good dramatic reason people really liked parsers theyve become extremely widely used theyre greedy transition based parsers theyre super super fast style parsers work way faster kind dynamic program parser pursuing every possible alternative analysis evaluate dependency parsers standard way via accuracy since word going choose something governor head say many decisions didnt get right lets look example heres simple sentence going use saw video lecture commonly well number word sentence including giving number zero root add sentence analysis showing correct dependencies lay chart word word number say word index head word particular word head whole sentence saw saying word index governor zero root symbol additionally want also label dependencies happens okay built parser transition based dependency parser parser tries parse sentence gets results look starts well says saw root sentence subject saw actually makes bit booboo says saw something lecturing saw dependent lecture verb video analyzed subject verb lecture well evaluate accuracy work accuracy taking number correct dependencies total number dependencies two ways one ignoring labels grammatic relations standardly gets referred unlabeled attachment score look pretty well one correct one correct one correct one correct place different word chosen governor word right dependent lecture wrong analysis say dependent video however look labeled accuracy score wrong parse isnt well gets one right gets one right everything else gets wrong gets wrong two reasons case chooses wrong word governor video lecture although chooses right word governor chooses wrong function thinking lecture verb complement clause see thats wrong direct object analyzing video subject lecture thats wrong well really compound noun structure getting two five right couple numbers give sense well people dependency parsers want look dependency parse evaluation one good source information conll conference conference natural language learning held shared task used dependency parsing collection thirteen different languages many dependency parsers took part see scores look results maltparser competition evaluated primarily labeled attachment score scores range best languages got percent dependencies right worst languages got percent right huge range languages think intrinsically harder others think also issues quality tree banks much better others thats also reflected numbers try connect saw earlier constituency parsing let also give numbers english dependency numbers time looking unlabeled attachment score since always get unlabeled attachments constituency parser notion heads would producing labels convert output charniak collins models generative constituency parsing lexicalized pcfg parsers accuracy dependencies untyped percent yamada matsumotos parser another transition based parser kind like maltparser bit worse percent heres different style dependency parser minimum spanning tree graph based style dependency parser almost almost well part know people gone research partly dependency parsers often much simpler theres quite lot work looking combinations dependency parsers example heres one dependency parser getting results little bit two constituency parsers results even years date hasnt recent careful comparison constituencies dependency parsers think big picture take away greedy transition based dependency parsers perhaps little bit worse performance dynamic programmed pcfg parsers little bit performance characteristics make extremely extremely attractive finishing presentation dependency parsing techniques theres one issue mention issue projectivity take dependencies cfg tree using heads get projective dependency parse means dependencies regarded something everything nests together like constituency nest together without crossing dependencies theories dependency grammar allow crossing dependencies ie nonprojective structures indeed cant get semantics certain constructions right without nonprojective dependencies lets look example heres sentence bill buy coffee yesterday dependencies nest together giving us kind tree structure want hook want say well object preposition put dependency dependency cross two dependencies non projective dependency structure question handle kind non projectivity parsing methods particular transition based arceager algorithm presented builds projective dependency trees active area investigation dependency literature im gonna cover detail briefly mention range possible analyses pursued one possibility declare defeat non projective arcs languages like english quite non projective arcs basically reason context free grammars work well english large everything tree structured work getting almost everything right non projective parser second possibility use dependency formalism projective representations youll see one next segment thats kind analogous context free grammars actually represent context free grammar also doesnt represent connections words cant done inside tree structure effectively something like tree bank representations saw phrase structure grammar failed represent connections words arent within tree representation hook word somewhere higher structure methods people pursued third method people used projective parsing run kind postprocessor perhaps picks classes labels indicate hooking analysis really moved somewhere else work resolve nonprojective links well something else actually add extra operations transition based dependency parser model handle least common cases nonprojectivity things like question words moved front sentence english well finally move parsing method doesnt assume projectivity particular minimum spanning treebased graphbased parsing methods dont make assumptions projectivity directly build nonprojective structures way appealing way seems like possibly going general situation youre going situation allowing projective structures making special requirement projective structures look natural languages find theyre mostly projective theyre particular constructions like things like wh movement moves question words beginning sentence various kinds right displacements like afterthoughts create nonprojectivity mst parser shouldnt non projectivity space machine learning classifier deciding dependencies likely construct okay hope thats given concrete sense transition based dependency parsers implemented maybe feel like could go implement one,Course2,W6-S3-L2,W6,S3,L2,Greedy,6,3,2,segment im go look greedi transit base pars approach depend pars particular im go describ model that use maltpars bestknown exampl framework idea maltpars mayb could pars make simpl greedi decis attach word come along particular make decis well use discrimin machin learn classifi parser sequenc action work bottom kind like shift reduc parser youv seen either cfg pars program languag literatur parser stack written top right start root symbol introduc last time buffer stuff weve yet look top written left input sentenc go along well build set depend arc start empti well set action natur way think transit base depend parser start configur stack buffer empti set depend arc essenti move shift word across buffer stack like shift oper cfg pars reduc oper depend pars framework end two reduc oper depend take whether take word left word right head word word j take word right head get left arc mean kind creat arc point like ad set arc right arc oper depend goe way word buffer depend word stack untyp depend pars two oper want type depend pars also reduct say label your use connect two word mean twenti label actual end differ action dependencyi parser finish youv exhaust buffer someth note seen shift reduc pars there someth that slightli unusu present normal shift reduc pars cfg first put thing stack reduct done fulli stack model that show reduct either direct your actual redu reduct one thing that stack one thing that buffer that convent that adopt standard say depend pars literatur claim make littl bit cleaner formul thing way though im sure make big differ ive gone caus standardli see simpl way thing standard thing peopl actual im gonna move next common method see transit base depend parser let explain simpl way littl bit problemat sentenc sort um sue tri open door cellar well your go depend tri open soon youv gotten input least obviou your go depend tri open your use kind three action model depend pars ive shown cant construct arc immedi instead fact door depend open cellar depend door construct materi construct arc closur depend depend construct hook higher head depend construct first well mean shift word input decid anyth depend found amount look ahead actual make harder machin learn base greedi classifi work well instead peopl want move abl greedi fashion hook local thing right even know depend that what done differ way formul action that refer arc eager depend pars arceag depend parser exactli start finish configur oper sort similar there shift oper there left arc right arc step though add one new oper reduc oper number differ subtleti work let sort look moment left arc oper thing construct left depend head that buffer top word stack result construct new depend arc add precondit need precondit wi isnt alreadi depend word caus allow arc put wed get analysi two arrow would point word precis arc eager charact word still alreadi stack even though theyv made depend word right arc oper bit differ right arc oper start point exactli weve go want make arrow like head stack depend begin buffer add depend differ rather get rid wj push stack mean keep word right take depend later that precis get word stack alreadi declar depend word okay fact necessit extra reduc oper caus weve found depend word eventu get rid go back find depend word higher syntax tree that reduc oper reduc oper also precondit precondit get rid word stack made depend word word introduc way depend word later point reduc close that bit confus let go concret exampl think itll make lot sens okay sentenc im go work happi children like play friend here cheat sheet oper go abl perform start start here stack root symbol buffer sentenc weve found depend situat four oper could appli thought happi head whole sentenc could immedi right arc oper doesnt seem right shift stack situat okay well situat children go depend happi next thing want construct depend sort mean left arc oper particular introduc adjectiv modifi depend left arc oper add set depend weve found get rid depend stack disappear right okay point children isnt head sentenc either anoth shift oper well point readi anoth left arc oper children subject like introduc noun subject depend add set depend set two depend weve alreadi built okay well point weve actual found head whole sentenc like head whole sentenc mean connect root right arc oper add root depend whole sentenc rememb right arc oper havent yet found depend like right therefor ad stack someth still find depend right inde point immedi go abl start play go depend like first get past shift onto stack infinit marker go depend verb play that go anoth left attach auxiliari modifi okay point say like take first right depend like play that done right attach get right attach xcomp ad set depend okay point make progress still got far play well play also go take argument play anoth right attach play mean move stack weve attach weve made depend play hasnt found argument right place stack go abl find argument with argument go friend get shift introduc left arc friend attach rememb formal appli left arc rule time check precondit precondit hasnt made depend word hadnt precondit satisfi okay anoth right arc oper hook togeth friend object preposit okay make good progress point weve got thing weve finish friend never right doesnt right depend found right depend friend play found right depend friend point start use reduc oper reduc friend rememb precondit precondit check friend hook depend someth inde reduc get rid reduc get rid play okay point weve got period deal say depend main verb introduc also right attach oper point buffer empti way defin finish state soon buffer empti stop could want say well geez cant sort reduc reduc pop thing back root know could defin thing like wouldnt harm actual unnecessari buffer empti cant construct anymor depend oper introduc depend take one thing stack one thing buffer there way formul thing reappear buffer theyv move stack know done weve found complet set depend kind work back though list make set depend weve introduc okay that model parser oper oper step step step multipl thing could done could chosen shift could chosen make left arc right arc choos make left arc right arc label depend introduc one mani depend label well way that done use form discrimin classifi support vector machin svm commonli use practic could equal anoth kind discrimin classifi maxent classifi classifi choos set move untyp depend parser four move arc eager configur type twice number depend type plu two class choos finit set class your choos featur use depend parser well definit use what word top stack what part speech what first word buffer what part speech let choos oper like shift kind depend label choos arent oper good depend parser caus rememb thing knew could import like length depend arc that propos know what interven also might want know word top stack well depend alreadi influenc like get depend put thing featur discrimin classifi kind like saw look maxent classifi simplest form transit base depend parser henc absolut search whatsoev point your make greedi decis youv got stack buffer state run classifi decid like next action take that approach that strongli pursu maltpars framework see good job manner realli good classifi good choos next move mean cours dont could kind beam search explor differ possibl rather stun result form complet greedi transit base pars realli well build depend parser work almost well best lexic probabilist context free grammar kind complet cki pars style search possibl saw previous take convert lpcfg depend represent evalu ill say littl bit evalu moment well close state art would good dramat reason peopl realli like parser theyv becom extrem wide use theyr greedi transit base parser theyr super super fast style parser work way faster kind dynam program parser pursu everi possibl altern analysi evalu depend parser standard way via accuraci sinc word go choos someth governor head say mani decis didnt get right let look exampl here simpl sentenc go use saw video lectur commonli well number word sentenc includ give number zero root add sentenc analysi show correct depend lay chart word word number say word index head word particular word head whole sentenc saw say word index governor zero root symbol addit want also label depend happen okay built parser transit base depend parser parser tri pars sentenc get result look start well say saw root sentenc subject saw actual make bit booboo say saw someth lectur saw depend lectur verb video analyz subject verb lectur well evalu accuraci work accuraci take number correct depend total number depend two way one ignor label grammat relat standardli get refer unlabel attach score look pretti well one correct one correct one correct one correct place differ word chosen governor word right depend lectur wrong analysi say depend video howev look label accuraci score wrong pars isnt well get one right get one right everyth els get wrong get wrong two reason case choos wrong word governor video lectur although choos right word governor choos wrong function think lectur verb complement claus see that wrong direct object analyz video subject lectur that wrong well realli compound noun structur get two five right coupl number give sens well peopl depend parser want look depend pars evalu one good sourc inform conll confer confer natur languag learn held share task use depend pars collect thirteen differ languag mani depend parser took part see score look result maltpars competit evalu primarili label attach score score rang best languag got percent depend right worst languag got percent right huge rang languag think intrins harder other think also issu qualiti tree bank much better other that also reflect number tri connect saw earlier constitu pars let also give number english depend number time look unlabel attach score sinc alway get unlabel attach constitu parser notion head would produc label convert output charniak collin model gener constitu pars lexic pcfg parser accuraci depend untyp percent yamada matsumoto parser anoth transit base parser kind like maltpars bit wors percent here differ style depend parser minimum span tree graph base style depend parser almost almost well part know peopl gone research partli depend parser often much simpler there quit lot work look combin depend parser exampl here one depend parser get result littl bit two constitu parser result even year date hasnt recent care comparison constitu depend parser think big pictur take away greedi transit base depend parser perhap littl bit wors perform dynam program pcfg parser littl bit perform characterist make extrem extrem attract finish present depend pars techniqu there one issu mention issu project take depend cfg tree use head get project depend pars mean depend regard someth everyth nest togeth like constitu nest togeth without cross depend theori depend grammar allow cross depend ie nonproject structur inde cant get semant certain construct right without nonproject depend let look exampl here sentenc bill buy coffe yesterday depend nest togeth give us kind tree structur want hook want say well object preposit put depend depend cross two depend non project depend structur question handl kind non project pars method particular transit base arceag algorithm present build project depend tree activ area investig depend literatur im gonna cover detail briefli mention rang possibl analys pursu one possibl declar defeat non project arc languag like english quit non project arc basic reason context free grammar work well english larg everyth tree structur work get almost everyth right non project parser second possibl use depend formal project represent youll see one next segment that kind analog context free grammar actual repres context free grammar also doesnt repres connect word cant done insid tree structur effect someth like tree bank represent saw phrase structur grammar fail repres connect word arent within tree represent hook word somewher higher structur method peopl pursu third method peopl use project pars run kind postprocessor perhap pick class label indic hook analysi realli move somewher els work resolv nonproject link well someth els actual add extra oper transit base depend parser model handl least common case nonproject thing like question word move front sentenc english well final move pars method doesnt assum project particular minimum span treebas graphbas pars method dont make assumpt project directli build nonproject structur way appeal way seem like possibl go gener situat your go situat allow project structur make special requir project structur look natur languag find theyr mostli project theyr particular construct like thing like wh movement move question word begin sentenc variou kind right displac like afterthought creat nonproject mst parser shouldnt non project space machin learn classifi decid depend like construct okay hope that given concret sens transit base depend parser implement mayb feel like could go implement one,[14  4  0 13 12]
193,Course2_W6-S3-L3_Dependencies_Encode_Relational_Structure_7-20,segment im going show dependency syntax natural representation relation extraction applications one domain lot work done relation extraction biomedical text domain example sentence “the results demonstrated kaic interacts rhythmically sasa kaia kaib” we’d like get protein interaction event here’s “interacts” indicates relation proteins involved bunch proteins involved well well point get kind dependency syntax easy starting follow along arguments subject preposition “with” easily see relation we’d like get little bit cleverer also follow along conjunction relations see kaic also interacting two proteins thats something lot people worked particular one representation that’s widely used relation extraction applications biomedicine stanford dependencies representation basic form representation projective dependency tree designed way could easily generated postprocessing phrase structure trees notion headedness phrase structure tree stanford dependency software provides set matching pattern rules type dependency relations give stanford dependency tree stanford dependencies also increasingly generated directly dependency parsers maltparser looked recently okay roughly representation looks like saw words connected type dependency arcs something explored stanford dependencies framework starting basic dependencies representation let’s make changes facilitate relation extraction applications idea emphasize relationships content words useful relation extraction applications let give couple examples one example commonly you’ll content word like “based” company based—los angeles— it’s separated preposition “in” function word think function words really functioning like case markers lot languages it’d seem useful directly connected “based” “la” introduced relationship “prepuin” u that’s simplify structure places better job representing semantics modifications graph structure particular place coordination relationships directly got “bell makes products” we’d also like get bell distributes products one way could recognizing “and” relationship saying “okay well means ‘bell’ also subject ‘distributing’ distribute ‘products’” similarly recognize they’re computer products well electronic products make changes graph get reduced graph representation things simple particular look structure it’s longer dependency tree multiple arcs pointing node multiple arcs pointing node hand relations we’d like extract represented much directly let show one graph gives indication graph originally put together jari björne et al team bionlp shared tasks relation extraction using representational substrate stanford dependencies wanted illustrate graph much effective dependency structures linking words wanted extract relation simply looking words linear context distance measured either counting words left right counting number dependency arcs follow percent time occurred see look linear distance lots times arguments relations want connect four five six seven eight words away fact there’s even pretty large residue well ten percent linear distance away words greater ten words hand though trying identify relate arguments relations looking dependency distance you’d discover vast majority arguments closeby neighbors terms dependency distance percent direct dependencies another percent distance take together that’s greater three quarters dependencies want find number trails away quickly virtually arguments relations aren’t fairly close together dependency distance it’s precisely reason get lot mileage relation extraction representationlike dependency syntax okay hope that’s given idea knowing syntax useful want various semantic tasks natural language processing,Course2,W6-S3-L3,W6,S3,L3,Dependencies,6,3,3,segment im go show depend syntax natur represent relat extract applic one domain lot work done relat extract biomed text domain exampl sentenc “the result demonstr kaic interact rhythmic sasa kaia kaib” we’d like get protein interact event here’ “interacts” indic relat protein involv bunch protein involv well well point get kind depend syntax easi start follow along argument subject preposit “with” easili see relat we’d like get littl bit clever also follow along conjunct relat see kaic also interact two protein that someth lot peopl work particular one represent that’ wide use relat extract applic biomedicin stanford depend represent basic form represent project depend tree design way could easili gener postprocess phrase structur tree notion headed phrase structur tree stanford depend softwar provid set match pattern rule type depend relat give stanford depend tree stanford depend also increasingli gener directli depend parser maltpars look recent okay roughli represent look like saw word connect type depend arc someth explor stanford depend framework start basic depend represent let’ make chang facilit relat extract applic idea emphas relationship content word use relat extract applic let give coupl exampl one exampl commonli you’ll content word like “based” compani based—lo angeles— it’ separ preposit “in” function word think function word realli function like case marker lot languag it’d seem use directli connect “based” “la” introduc relationship “prepuin” u that’ simplifi structur place better job repres semant modif graph structur particular place coordin relationship directli got “bell make products” we’d also like get bell distribut product one way could recogn “and” relationship say “okay well mean ‘bell’ also subject ‘distributing’ distribut ‘products’” similarli recogn they’r comput product well electron product make chang graph get reduc graph represent thing simpl particular look structur it’ longer depend tree multipl arc point node multipl arc point node hand relat we’d like extract repres much directli let show one graph give indic graph origin put togeth jari björne et al team bionlp share task relat extract use represent substrat stanford depend want illustr graph much effect depend structur link word want extract relat simpli look word linear context distanc measur either count word left right count number depend arc follow percent time occur see look linear distanc lot time argument relat want connect four five six seven eight word away fact there’ even pretti larg residu well ten percent linear distanc away word greater ten word hand though tri identifi relat argument relat look depend distanc you’d discov vast major argument closebi neighbor term depend distanc percent direct depend anoth percent distanc take togeth that’ greater three quarter depend want find number trail away quickli virtual argument relat aren’t fairli close togeth depend distanc it’ precis reason get lot mileag relat extract representationlik depend syntax okay hope that’ given idea know syntax use want variou semant task natur languag process,[14 13  4 12 11]
194,Course2_W7-S1-L1_Introduction_to_Information_Retrieval_9-16,hello segment going introduce task information retrieval including particular whats dominant form web search task information retrieval maybe defined follows goal finding material usually documents unstructured nature usually text satisfies information need thats person looking information within large collections usually stored computers theres lots mentions prototypical cases kinds information retrieval exist things like music information retrieval sounds text documents effectively going talk case usually clauses hold many scenarios information retrieval one people think first days almost invariably web search many others searching email searching contents laptop computer finding stuff companys knowledge base legal information retrieval find relevant cases legal context something like always case large percentage human knowledge information stored form human language documents yet theres also long time something paradox kind quite real graph give flavor things dont really believe numbers lefthand side exactly mean showing mid already case looked volume data data structured forms mean things like relational databases spreadsheets already vastly data companies organizations around peoples homes unstructured form form human language text however despite mid structured data management retrieval developed field already large database companies field unstructured data management little teeny little companies various kinds corporate document retrieval things like situation completely changed around turn millennium look today situation like data volumes gotten larger sides particular theyve gotten larger unstructured side massive outpouring blogs tweets forums places store massive amounts information theres also turn around corporate side huge companies addressing problems unstructured information retrieval major web search giants lets start say little bit base basic framework information retrieval start assuming weve got collection documents going retrieval moment going assume static collection later well deal documents get added deleted collection go find something like web search scenario goal retrieve documents relevant users information need helps user complete task let go little bit slowly picture start user task want perform lets take concrete example suppose want get rid mice garage im kind person doesnt really want kill poison thats user task want achieve achieve task feel need information information need want know getting rid mice without killing information need thing respect assess information retrieval system cant take someones information need stick straight computer able stick computer translate information need something goes search box thats whats happened query heres attempt query trap mice alive thats taken information need ive made attempt realize particular query query goes search engine leads interrogates document collection leads results coming back may end day sometimes im satisfied information retrieval session working might take evidence got go back come better query might decide alive bad word put something like without killing see works better okay go wrong well couple stages interpretation first initial task made decisions kind information need could got something wrong could misconceptions maybe getting rid mice important issue whether kill whether humanely gonna deal issue much interested translation information need query lots ways could go wrong misformulation query might choose wrong words express query might make use query search operators like inverted commas might good bad effect query actually works choices query formulation arent altering information need whatsoever thats important talk evaluate information retrieval systems thats topic well say later let give first rough sense cause well need everything whenever make query information retrieval system going get documents back want know whether results good basic way gonna see whether good follows going think two measures complementary one precision precision fraction retrieved documents system relevant users information needs thats whether look results seems like one document ten relevant information need whether seven documents ten thats assessing whether mix stuff youre getting back lot bad results corres complementary measure one recall recall measuring much good information thats document collection system succeeding finding fraction relevant docs collection retrieved well give precise definitions measurements information retrieval later one thing want express right measures make sense evaluated respect users information need certain kinds queries ones well look following segments documents get returned deterministic terms query submitted going going evaluate gets returned respect users query wed necessarily say precision thats think users information need precision results assessed relative particular look back moment theres misformulation query user didnt come good query seen lowering precision returned results okay beginning looking information retrieval hope youve already got sense think task information retrieval roughly conceive whether search engine good bad job,Course2,W7-S1-L1,W7,S1,L1,Introduction,7,1,1,hello segment go introduc task inform retriev includ particular what domin form web search task inform retriev mayb defin follow goal find materi usual document unstructur natur usual text satisfi inform need that person look inform within larg collect usual store comput there lot mention prototyp case kind inform retriev exist thing like music inform retriev sound text document effect go talk case usual claus hold mani scenario inform retriev one peopl think first day almost invari web search mani other search email search content laptop comput find stuff compani knowledg base legal inform retriev find relev case legal context someth like alway case larg percentag human knowledg inform store form human languag document yet there also long time someth paradox kind quit real graph give flavor thing dont realli believ number lefthand side exactli mean show mid alreadi case look volum data data structur form mean thing like relat databas spreadsheet alreadi vastli data compani organ around peopl home unstructur form form human languag text howev despit mid structur data manag retriev develop field alreadi larg databas compani field unstructur data manag littl teeni littl compani variou kind corpor document retriev thing like situat complet chang around turn millennium look today situat like data volum gotten larger side particular theyv gotten larger unstructur side massiv outpour blog tweet forum place store massiv amount inform there also turn around corpor side huge compani address problem unstructur inform retriev major web search giant let start say littl bit base basic framework inform retriev start assum weve got collect document go retriev moment go assum static collect later well deal document get ad delet collect go find someth like web search scenario goal retriev document relev user inform need help user complet task let go littl bit slowli pictur start user task want perform let take concret exampl suppos want get rid mice garag im kind person doesnt realli want kill poison that user task want achiev achiev task feel need inform inform need want know get rid mice without kill inform need thing respect assess inform retriev system cant take someon inform need stick straight comput abl stick comput translat inform need someth goe search box that what happen queri here attempt queri trap mice aliv that taken inform need ive made attempt realiz particular queri queri goe search engin lead interrog document collect lead result come back may end day sometim im satisfi inform retriev session work might take evid got go back come better queri might decid aliv bad word put someth like without kill see work better okay go wrong well coupl stage interpret first initi task made decis kind inform need could got someth wrong could misconcept mayb get rid mice import issu whether kill whether human gonna deal issu much interest translat inform need queri lot way could go wrong misformul queri might choos wrong word express queri might make use queri search oper like invert comma might good bad effect queri actual work choic queri formul arent alter inform need whatsoev that import talk evalu inform retriev system that topic well say later let give first rough sens caus well need everyth whenev make queri inform retriev system go get document back want know whether result good basic way gonna see whether good follow go think two measur complementari one precis precis fraction retriev document system relev user inform need that whether look result seem like one document ten relev inform need whether seven document ten that assess whether mix stuff your get back lot bad result corr complementari measur one recal recal measur much good inform that document collect system succeed find fraction relev doc collect retriev well give precis definit measur inform retriev later one thing want express right measur make sens evalu respect user inform need certain kind queri one well look follow segment document get return determinist term queri submit go go evalu get return respect user queri wed necessarili say precis that think user inform need precis result assess rel particular look back moment there misformul queri user didnt come good queri seen lower precis return result okay begin look inform retriev hope youv alreadi got sens think task inform retriev roughli conceiv whether search engin good bad job,[ 2  4 13 14 12]
195,Course2_W7-S1-L2_Term-Document_Incidence_Matrices_8-59,hello section im going introduce important idea term document matrix also im going explain isnt actually practical data structure information retrieval system well take example information retrieval works william shakespeare lets suppose concrete question plays shakespeare contain words brutus caesar calpurnia well youre starting basic level text searching commands first way youd think solve problem using searching text documents exhaustively whats known unix world grepping could kind first grep plays contain brutus caesar know grep command well give flag files match get ones dont contain calpurnia days works size william shakespeare kind query grepping perfectly satisfactory solution disk drives computers sufficiently fast could use method takes time find answer nevertheless isnt good answer full information retrieval problem falls flat number ways corpus becomes large means something like everything hard disk even world wide web cant afford linear scan documents every time query parts like part become less trivial implement finding things even part well complex queries like finding uses word romans near countrymen cant grep command even big thing thats happening information retrieval idea ranking finding best documents return query thats something cant get linear scan model finding things match well talk issues way theyre handled modern information retrieval systems later lectures lets first go idea term document matrix term document matrix rows matrix words often theyre also called information retrieval terms columns matrix documents simple thing simply saying lets fill every cell boolean matrix whether word appears play antony appears antony cleopatra calpurnia appear antony cleopatra matrix represents appearance words documents matrix straightforward answer boolean queries example queries documents contain brutus caesar calpurnia lets go concretely going going take vectors terms query going put together boolean operations first take row referring brutus goes take row caeser finally take row calpurnia complement stick calpurnia appears julius caesar complement vector everything one apart julius caesar point three vectors together answer one weve able information retrieval successfully tell query satisfied documents antony cleopatra hamlet indeed go document collection confirm case antony cleopatra antony found julius caesar dead cried almost roaring wept philippi found brutus slain similarly find words occurring hamlet okay suggests could information retrieval simply working term document matrix important thing realize doesnt really work go sensible size collections lets go minute lets go sensible size still small collection suppose documents well often use n refer number documents average words long okay mean terms size document collection terms size matrix average six bytes per word including spaces punctuation amount data talking six gigabytes thats teeny fraction one modern hard disk laptop let us suppose try work many distinct terms document collection need know number distinct terms corresponds number columns matrix lets suppose thatll typical number documents ill often refer number different terms well mean well means even size document collection cant build term document matrix youll rows million columns thats half trillion already huge probably bigger space store document collection gets bigger million documents things gonna get worse really important observation although matrix half trillion zeros ones actually almost entries zero document one billion would good guys stop think fraction second one billion answer well one million documents average document thousand words long said last time actual number word tokens even assume every word every document different could entries likely far less cause well common words like occurring many many times document therefore key observation matrix dealing sparse central question design information retrieval data structures taking advantage sparcity coming better data representation secret efficient storage mechanism want record positions hold one positions hold zero okay hope thats given understanding term document matrix important conceptual data structure keep coming back talk various kinds algorithms think terms matrix youll see actually come storage computer systems also see never want actually store documents information retrieval representation form,Course2,W7-S1-L2,W7,S1,L2,Term-Document,7,1,2,hello section im go introduc import idea term document matrix also im go explain isnt actual practic data structur inform retriev system well take exampl inform retriev work william shakespear let suppos concret question play shakespear contain word brutu caesar calpurnia well your start basic level text search command first way youd think solv problem use search text document exhaust what known unix world grep could kind first grep play contain brutu caesar know grep command well give flag file match get one dont contain calpurnia day work size william shakespear kind queri grep perfectli satisfactori solut disk drive comput suffici fast could use method take time find answer nevertheless isnt good answer full inform retriev problem fall flat number way corpu becom larg mean someth like everyth hard disk even world wide web cant afford linear scan document everi time queri part like part becom less trivial implement find thing even part well complex queri like find use word roman near countrymen cant grep command even big thing that happen inform retriev idea rank find best document return queri that someth cant get linear scan model find thing match well talk issu way theyr handl modern inform retriev system later lectur let first go idea term document matrix term document matrix row matrix word often theyr also call inform retriev term column matrix document simpl thing simpli say let fill everi cell boolean matrix whether word appear play antoni appear antoni cleopatra calpurnia appear antoni cleopatra matrix repres appear word document matrix straightforward answer boolean queri exampl queri document contain brutu caesar calpurnia let go concret go go take vector term queri go put togeth boolean oper first take row refer brutu goe take row caeser final take row calpurnia complement stick calpurnia appear juliu caesar complement vector everyth one apart juliu caesar point three vector togeth answer one weve abl inform retriev success tell queri satisfi document antoni cleopatra hamlet inde go document collect confirm case antoni cleopatra antoni found juliu caesar dead cri almost roar wept philippi found brutu slain similarli find word occur hamlet okay suggest could inform retriev simpli work term document matrix import thing realiz doesnt realli work go sensibl size collect let go minut let go sensibl size still small collect suppos document well often use n refer number document averag word long okay mean term size document collect term size matrix averag six byte per word includ space punctuat amount data talk six gigabyt that teeni fraction one modern hard disk laptop let us suppos tri work mani distinct term document collect need know number distinct term correspond number column matrix let suppos thatll typic number document ill often refer number differ term well mean well mean even size document collect cant build term document matrix youll row million column that half trillion alreadi huge probabl bigger space store document collect get bigger million document thing gonna get wors realli import observ although matrix half trillion zero one actual almost entri zero document one billion would good guy stop think fraction second one billion answer well one million document averag document thousand word long said last time actual number word token even assum everi word everi document differ could entri like far less caus well common word like occur mani mani time document therefor key observ matrix deal spars central question design inform retriev data structur take advantag sparciti come better data represent secret effici storag mechan want record posit hold one posit hold zero okay hope that given understand term document matrix import conceptu data structur keep come back talk variou kind algorithm think term matrix youll see actual come storag comput system also see never want actual store document inform retriev represent form,[ 2  4  9 14 13]
196,Course2_W7-S1-L3_The_Inverted_Index_10-42,hello segment gonna talk inverted index constructed inverted index key data structure underlies modern information retrieval systems systems running single laptop running biggest commercial search engines inverted index data structure exploits sparsity term document matrix talked preceding segment allows inefficient sorry allows efficient retrieval essentially without peer data structure used information retrieval systems let go whats inverted index term word must store list documents contain word lets identify document docid document serial number think starting first document called one two three et cetera question data structure use mean one idea might use fixed arrays like vectors term document matrix thats inefficient words theyll appear lot documents words appear documents moreover perhaps problems thing dynamic index documents added later documents changed well difficult things adjusting vector sizes reasons one way another need use variable sized lists store documents word occurs standard information retrieval terminology lists called postings lists postings lists traditionally usually stored disks may case big search engines youre storing postings lists disks right way store one continuous run postings gives efficient method able load disk back memory youre interested postings particular word memory postings list represented data structure like link list variable lengths arrays obvious tradeoffs size versus ease insertion data structure end inverted index like one im showing terms documents term weve got got pointer postings list giving different documents described document id occurs okay one occurrence worddocument pair referred posting sum postings lists referred postings overall parts lefthand side dictionary righthand side postings property postings theyre sorted document id soon well explain thats essential two data structures dictionary postings somewhat different statuses global size dictionary relatively small normally essential memory whereas postings large least something like small scale enterprise search engine normally stored disc let move inverted index constructed starting point bunch documents indexed documents well think sequence characters well assume weve already dealt perhaps someone elses software conversion pdf microsoft word documents things like going go first pre processing steps need tokenizer turns document sequence word tokens basic units indexing often dont index exactly words contained document might various linguistic modules way modify tokens put kind canonical form instance saying friends lower cased stemmed remove plural ending okay modified tokens fed indexer thing builds inverted index talking heres inverted index step indexer main thing want talk let first briefly mention initial stages text processing fraction detail things happen initial stages firstly tokenizations thats decide cut character sequence word tokens various issues punctuation come words treat possessives hyphenated terms kind stuff talk detail normalization issue certain things like usa without dots probably want treat term map text query terms form match might want kinds mapping stemming authorize authorization mapped stem straightforwardly match query finally may want index common words traditionally many search engines left common words like indexing clear modern world amount storage vast thats good idea queries youd might like song really need stop words turns modern indexes inefficient store okay lets go detail indexer goes sequence perhaps normalized tokens building inverted index example assuming two documents doc one doc two key sequence steps go input sequence tokens first document order come text sequence doc tokens second document order come text first step sort sort primary key terms putting alphabetical order alphabetical list terms term appearing multiple documents secondary sort document id word caesar appears twice document id one twice document id two sorting secondarily document id thats core expensive indexing step weve got far essentially consolidation weve found right take multiple entries single document merged thats two instances caesar theyre treated one also merge instances particular term rerepresent say dictionary entry caesar record total frequency collection ill come back bit later build postings list list documents occurs straightforwardly consequence sort previous step postings list going sorted document id thinking size inverted index think minute pay storage pay amount list terms counts number terms relatively modest example beforehand terms pay pointer identifies postings lists thats order things pay actual postings lists postings lists far biggest part even theyre bounded size number tokens collection examp example million documents average length words still less one billion items storage manageable actually building efficient ir system implementation think questions think make index efficient possible retrieval minimize storage sides side side terms various sorts compression gonna get details hope start see inverted index gives efficient basis retrieval operations thats something well talk detail next segment rate know underlying data structure really complex underlies modern information retrieval systems,Course2,W7-S1-L3,W7,S1,L3,The,7,1,3,hello segment gonna talk invert index construct invert index key data structur underli modern inform retriev system system run singl laptop run biggest commerci search engin invert index data structur exploit sparsiti term document matrix talk preced segment allow ineffici sorri allow effici retriev essenti without peer data structur use inform retriev system let go what invert index term word must store list document contain word let identifi document docid document serial number think start first document call one two three et cetera question data structur use mean one idea might use fix array like vector term document matrix that ineffici word theyll appear lot document word appear document moreov perhap problem thing dynam index document ad later document chang well difficult thing adjust vector size reason one way anoth need use variabl size list store document word occur standard inform retriev terminolog list call post list post list tradit usual store disk may case big search engin your store post list disk right way store one continu run post give effici method abl load disk back memori your interest post particular word memori post list repres data structur like link list variabl length array obviou tradeoff size versu eas insert data structur end invert index like one im show term document term weve got got pointer post list give differ document describ document id occur okay one occurr worddocu pair refer post sum post list refer post overal part lefthand side dictionari righthand side post properti post theyr sort document id soon well explain that essenti two data structur dictionari post somewhat differ status global size dictionari rel small normal essenti memori wherea post larg least someth like small scale enterpris search engin normal store disc let move invert index construct start point bunch document index document well think sequenc charact well assum weve alreadi dealt perhap someon els softwar convers pdf microsoft word document thing like go go first pre process step need token turn document sequenc word token basic unit index often dont index exactli word contain document might variou linguist modul way modifi token put kind canon form instanc say friend lower case stem remov plural end okay modifi token fed index thing build invert index talk here invert index step index main thing want talk let first briefli mention initi stage text process fraction detail thing happen initi stage firstli token that decid cut charact sequenc word token variou issu punctuat come word treat possess hyphen term kind stuff talk detail normal issu certain thing like usa without dot probabl want treat term map text queri term form match might want kind map stem author author map stem straightforwardli match queri final may want index common word tradit mani search engin left common word like index clear modern world amount storag vast that good idea queri youd might like song realli need stop word turn modern index ineffici store okay let go detail index goe sequenc perhap normal token build invert index exampl assum two document doc one doc two key sequenc step go input sequenc token first document order come text sequenc doc token second document order come text first step sort sort primari key term put alphabet order alphabet list term term appear multipl document secondari sort document id word caesar appear twice document id one twice document id two sort secondarili document id that core expens index step weve got far essenti consolid weve found right take multipl entri singl document merg that two instanc caesar theyr treat one also merg instanc particular term rerepres say dictionari entri caesar record total frequenc collect ill come back bit later build post list list document occur straightforwardli consequ sort previou step post list go sort document id think size invert index think minut pay storag pay amount list term count number term rel modest exampl beforehand term pay pointer identifi post list that order thing pay actual post list post list far biggest part even theyr bound size number token collect examp exampl million document averag length word still less one billion item storag manag actual build effici ir system implement think question think make index effici possibl retriev minim storag side side side term variou sort compress gonna get detail hope start see invert index give effici basi retriev oper that someth well talk detail next segment rate know underli data structur realli complex underli modern inform retriev system,[ 2  4 14 13 12]
197,Course2_W7-S1-L4_Query_Processing_with_the_Inverted_Index_6-43,segment gonna keep looking inverted index see efficient data structure query operations ir system particular well step detail perform common kind query query two terms starting well look details query processing well later segment well talk even detail kind queries process suppose want process query suppose query brutus caesar well let even simpler example suppose first kind query want look query brutus well totally straightforward locate brutus dictionary return postings list look say okay set documents brutus occurs dont need anything else lets go fraction complicated case well going locate brutus caesar going locate words dictionary look postings lists wed like work documents contain brutus caesar putting together standardly referred merging two postings lists term actually misleading query actually intersecting two sets documents find documents words occur whereas merging suggests kind putting together union operation term merge used actually cases merge algorithm family refers family algorithms step pair sorted lists various boolean operations lets look concrete detail happens okay way merge operation brutus caesar like start pointer points head lists going wanting work whats intersection way ask two pointers pointing equal doc id answer advance pointer smaller doc id two pointers like say two pointers pointing document id answer yes put result list weve done advance pointers say pointers pointing doc id first list greater first thing pointed first list pointer greater thing pointed second list pointer advance bottom pointer one say doc id pointed two pointers equal advance smaller one equal advance smaller one point theyre pointing doc id add result set advance pointers advance smaller one advance smaller one advance smaller one advance smaller one point try advance smaller one one lists exhausted items intersection stop return document set documents two eight contained brutus caesar hope went carefully enough see lists lengths x merge algorithm takes big x plus time linear sum lengths two postings lists also seen whats crucial make operation linear whats crucial making linear fact postings lists sorted order document id precisely could linear scan two postings lists hadnt case would turned n squared algorithm okay heres postings list intersection algorithm one time real algorithm hopefully see exactly hand start answer set zero going loop postings lists equal nil cause soon ones nil stop thats operation okay step ask whether two document id two pointers add answer sorry advance pointers work doc id smaller advance pointer either one one exactly soon one document lists runs return answer set okay hope made sense feel like write code intersection postings lists using merge algorithm,Course2,W7-S1-L4,W7,S1,L4,Query,7,1,4,segment gonna keep look invert index see effici data structur queri oper ir system particular well step detail perform common kind queri queri two term start well look detail queri process well later segment well talk even detail kind queri process suppos want process queri suppos queri brutu caesar well let even simpler exampl suppos first kind queri want look queri brutu well total straightforward locat brutu dictionari return post list look say okay set document brutu occur dont need anyth els let go fraction complic case well go locat brutu caesar go locat word dictionari look post list wed like work document contain brutu caesar put togeth standardli refer merg two post list term actual mislead queri actual intersect two set document find document word occur wherea merg suggest kind put togeth union oper term merg use actual case merg algorithm famili refer famili algorithm step pair sort list variou boolean oper let look concret detail happen okay way merg oper brutu caesar like start pointer point head list go want work what intersect way ask two pointer point equal doc id answer advanc pointer smaller doc id two pointer like say two pointer point document id answer ye put result list weve done advanc pointer say pointer point doc id first list greater first thing point first list pointer greater thing point second list pointer advanc bottom pointer one say doc id point two pointer equal advanc smaller one equal advanc smaller one point theyr point doc id add result set advanc pointer advanc smaller one advanc smaller one advanc smaller one advanc smaller one point tri advanc smaller one one list exhaust item intersect stop return document set document two eight contain brutu caesar hope went care enough see list length x merg algorithm take big x plu time linear sum length two post list also seen what crucial make oper linear what crucial make linear fact post list sort order document id precis could linear scan two post list hadnt case would turn n squar algorithm okay here post list intersect algorithm one time real algorithm hope see exactli hand start answer set zero go loop post list equal nil caus soon one nil stop that oper okay step ask whether two document id two pointer add answer sorri advanc pointer work doc id smaller advanc pointer either one one exactli soon one document list run return answer set okay hope made sens feel like write code intersect post list use merg algorithm,[ 2  4  9 14 13]
198,Course2_W7-S1-L5_Phrase_Queries_and_Positional_Indexes_19-45,segment i’m going introduce phrase queries practice important kind extended boolean query examine need extend inverted index data structure able handle phrase queries often we’d like query individual words also multiword units many useful things want query multiword units information retrieval multiword unit many many things like organization names stanford university want mechanism say lets match pair words unit syntax thats standardly used modern web search engines put inverted quotes like kind phrase query going say document says went university stanford doesnt match phrase query notion putting things inverted commas proven easily understood users something information retrieval system designers generally lament people design advanced search functionality systems almost nobody uses people like academics people like regular users dont use really notion phrase querying sort nearest thing exception fairly widely understood used ill point well overt phrase queries like theres actually another interesting phenomenon many queries implicit phrase queries someone give query san francisco something like well really want interpreted phrase didnt bother know put inverted quotes around modern web search engines active area research identify implicit phrases take advantage knowing implicit phrase alter documents get returned going leave aside moment going deal explicit phrases work good job matching information retrieval system okay well first clear longer suffices store term postings list documents term easily find documents contain two terms idea whether terms adjacent phrase document way could exhaustive postprocessing candidate documents see actually contain phrase would extremely slow wed back linear scanning documents one way solve problem idea biword index biword index index every consecutive pair terms phrase example text friends romans countrymen generate biwords adjacent words friends romans generate biword romans countrymen biwords dictionary term mean well means wed say friends romans occurs lets say document maybe also occurs document romans countrymen occurs document might occur document well build word phrase querying immediate want find documents contain friends romans look dictionary entry grab postings list returned consider complex cases even basically handle handle longer phrase queries breaking phrase query stanford university palo alto break query stanford university university palo palo alto use biword index find documents contain three bigrams thats going perfect without linear scan docs cant sure documents returning actually continuous phrase stanford university palo alto seems highly likely checked bigrams occurring theres small chance false positives seems like pretty good state issues using biword index well noted false positives whats returned maybe thats big problem big problem theres enormous blowup index dictionary gotten much massive means know practical triword quadword indices actually exactly match long queries even biwords going sort potentially space words squared possible dictionary entries biword indices standard solution handling phrase searching ill show wasnt useless explained ill show end important component solution standard solution standard solution move positional index idea positional index postings store documents contain term also position document term appears organization dictionary term document frequency term point postings lists instead list document ids list document ids document list positions term occurs lets look concretely example word occurs almost million documents document occurs words etc document occurs two word positions doesnt occur document document occurs bunch places able check whether phrases occur seeing whether words occur adjacent get idea could consider little question four documents could contain based document positions obviously havent seen postings lists words looking occurs document candidate kind data structure handle phrase queries using merge algorithm much showed little bit hairier rather intersection document ids sort level algorithm intersect document ids also check compatible positions words occurring phrase means need deal bit equality example wanting find instances phrase information retrieval want word information occurs position certain document want retrieval appearing position document going slightly detail process phrase query lets assume phrase want find inverted commas find get postings lists individual terms going going progressively intersect start going start beginning postings merge saying well document cant candidate doesnt appear postings list document cant candidate doesnt appear postings list point weve got document doc ids match point positional query recurse another merge algorithm positions within postings lists start time rather equality check wanting see find place occurs token number one larger well step along well say well heres one candidate heres second candidate two candidate matches inside document well returning candidate matches separately cause actually finding positions documents phrase query matching well need refine query words coming ahead point revert back oh wait sorry im wrong theres one candidate sorry three candidates right weve exhausted positions one document revert higher level postings merge say less gets crossed advance pointer proceed along hope see actually method works phrase queries words offset could actually use exactly method proximity queries saw earlier westlaw service could ask one word within three words another word something like right examples like limit within words statute within words federal within words tort could use kind techniques optimizing query started one condition close together token position indices count match within document clearly positional indices used kind proximity queries biword indices nothing ive sort said algorithm something might want work concretely change linear postings merge algorithm handle proximity queries particular think get work value k actually little bit tricky correctly efficiently within k words matching either side word keep track things right number words side would good try work also see one example answer figure introduction information retrieval book find free web look positional index great allows us answer phrase queries proximity queries theres cost positional index postings lists got lot larger rather storing document ids theyre storing document ids offsets tokens within document thats major factor even though indices compressed nevertheless completely standard use positional index power flexibility get handling phrase proximity queries whether theyre used explicitly terms kind phrase queries within queries whether theyre exploited improve ranking system looking implicit phrases said positional index gets much larger estimating size positional index note entry occurrence word document means size index depends average length documents fairly short documents big deal cause actually words occur document occur twice long documents blows size positional index rather example consider word common word frequency word occurs word thousand average well document size average length kind getting blowout really going positional index therell one position recorded document documents really long might getting times blowout size positional index everything depends give rough rules thumb know sense typical documents like web pages might expect positional index somewhere around times large nonpositional index particular size positional index remains smaller starts approach volume size original text heading sort third half original size text thats indexed much larger case nonpositional index might something like ir system doesnt actually take lot space storing text first place mentioned mentioned biword indices arent useless idea even though theyre normally solution let come back two approaches profitably combined look query stream high query volume services like web search engines tend queries keep turning things like person names popular people well treat know regular postings list intersections positional phrase query posting list intersection wed keep intersection every time someone sends question bad cases like michael jackson less bad youve got rare names like britney spears cause presumably posting lists much shorter intersection roughly individual postings list intersection every time especially bad common words like youre searching band whats going happen youre going retrieve two enormous postings lists intersection end short postings list phrase query paper group melbourne thats well known information retrieval group investigated sophisticated mixed indexing system happened common phrases know like examples build biwords index biwords rarer phrases done positional index able show typical web query mixture could execute one quarter time positional index alone making use also indexed biwords cost taking space positional index alone makes bit look fairly appealing tradeoff augment positional index common bigrams well one model work advance common bigrams index standard inverted index practice happens lot modern systems people try bit dynamically keep cache commonly queried phrase queries result postings list intersection thats like added biwords inverted index done bit dynamically okay thats introduced whats useful extension classic boolean retrieval model support phrase queries introduced method handling two methods particular looked positional indices handle phrase queries also proximity queries saw earlier,Course2,W7-S1-L5,W7,S1,L5,Phrase,7,1,5,segment i’m go introduc phrase queri practic import kind extend boolean queri examin need extend invert index data structur abl handl phrase queri often we’d like queri individu word also multiword unit mani use thing want queri multiword unit inform retriev multiword unit mani mani thing like organ name stanford univers want mechan say let match pair word unit syntax that standardli use modern web search engin put invert quot like kind phrase queri go say document say went univers stanford doesnt match phrase queri notion put thing invert comma proven easili understood user someth inform retriev system design gener lament peopl design advanc search function system almost nobodi use peopl like academ peopl like regular user dont use realli notion phrase queri sort nearest thing except fairli wide understood use ill point well overt phrase queri like there actual anoth interest phenomenon mani queri implicit phrase queri someon give queri san francisco someth like well realli want interpret phrase didnt bother know put invert quot around modern web search engin activ area research identifi implicit phrase take advantag know implicit phrase alter document get return go leav asid moment go deal explicit phrase work good job match inform retriev system okay well first clear longer suffic store term post list document term easili find document contain two term idea whether term adjac phrase document way could exhaust postprocess candid document see actual contain phrase would extrem slow wed back linear scan document one way solv problem idea biword index biword index index everi consecut pair term phrase exampl text friend roman countrymen gener biword adjac word friend roman gener biword roman countrymen biword dictionari term mean well mean wed say friend roman occur let say document mayb also occur document roman countrymen occur document might occur document well build word phrase queri immedi want find document contain friend roman look dictionari entri grab post list return consid complex case even basic handl handl longer phrase queri break phrase queri stanford univers palo alto break queri stanford univers univers palo palo alto use biword index find document contain three bigram that go perfect without linear scan doc cant sure document return actual continu phrase stanford univers palo alto seem highli like check bigram occur there small chanc fals posit seem like pretti good state issu use biword index well note fals posit what return mayb that big problem big problem there enorm blowup index dictionari gotten much massiv mean know practic triword quadword indic actual exactli match long queri even biword go sort potenti space word squar possibl dictionari entri biword indic standard solut handl phrase search ill show wasnt useless explain ill show end import compon solut standard solut standard solut move posit index idea posit index post store document contain term also posit document term appear organ dictionari term document frequenc term point post list instead list document id list document id document list posit term occur let look concret exampl word occur almost million document document occur word etc document occur two word posit doesnt occur document document occur bunch place abl check whether phrase occur see whether word occur adjac get idea could consid littl question four document could contain base document posit obvious havent seen post list word look occur document candid kind data structur handl phrase queri use merg algorithm much show littl bit hairier rather intersect document id sort level algorithm intersect document id also check compat posit word occur phrase mean need deal bit equal exampl want find instanc phrase inform retriev want word inform occur posit certain document want retriev appear posit document go slightli detail process phrase queri let assum phrase want find invert comma find get post list individu term go go progress intersect start go start begin post merg say well document cant candid doesnt appear post list document cant candid doesnt appear post list point weve got document doc id match point posit queri recurs anoth merg algorithm posit within post list start time rather equal check want see find place occur token number one larger well step along well say well here one candid here second candid two candid match insid document well return candid match separ caus actual find posit document phrase queri match well need refin queri word come ahead point revert back oh wait sorri im wrong there one candid sorri three candid right weve exhaust posit one document revert higher level post merg say less get cross advanc pointer proceed along hope see actual method work phrase queri word offset could actual use exactli method proxim queri saw earlier westlaw servic could ask one word within three word anoth word someth like right exampl like limit within word statut within word feder within word tort could use kind techniqu optim queri start one condit close togeth token posit indic count match within document clearli posit indic use kind proxim queri biword indic noth ive sort said algorithm someth might want work concret chang linear post merg algorithm handl proxim queri particular think get work valu k actual littl bit tricki correctli effici within k word match either side word keep track thing right number word side would good tri work also see one exampl answer figur introduct inform retriev book find free web look posit index great allow us answer phrase queri proxim queri there cost posit index post list got lot larger rather store document id theyr store document id offset token within document that major factor even though indic compress nevertheless complet standard use posit index power flexibl get handl phrase proxim queri whether theyr use explicitli term kind phrase queri within queri whether theyr exploit improv rank system look implicit phrase said posit index get much larger estim size posit index note entri occurr word document mean size index depend averag length document fairli short document big deal caus actual word occur document occur twice long document blow size posit index rather exampl consid word common word frequenc word occur word thousand averag well document size averag length kind get blowout realli go posit index therel one posit record document document realli long might get time blowout size posit index everyth depend give rough rule thumb know sens typic document like web page might expect posit index somewher around time larg nonposit index particular size posit index remain smaller start approach volum size origin text head sort third half origin size text that index much larger case nonposit index might someth like ir system doesnt actual take lot space store text first place mention mention biword indic arent useless idea even though theyr normal solut let come back two approach profit combin look queri stream high queri volum servic like web search engin tend queri keep turn thing like person name popular peopl well treat know regular post list intersect posit phrase queri post list intersect wed keep intersect everi time someon send question bad case like michael jackson less bad youv got rare name like britney spear caus presum post list much shorter intersect roughli individu post list intersect everi time especi bad common word like your search band what go happen your go retriev two enorm post list intersect end short post list phrase queri paper group melbourn that well known inform retriev group investig sophist mix index system happen common phrase know like exampl build biword index biword rarer phrase done posit index abl show typic web queri mixtur could execut one quarter time posit index alon make use also index biword cost take space posit index alon make bit look fairli appeal tradeoff augment posit index common bigram well one model work advanc common bigram index standard invert index practic happen lot modern system peopl tri bit dynam keep cach commonli queri phrase queri result post list intersect that like ad biword invert index done bit dynam okay that introduc what use extens classic boolean retriev model support phrase queri introduc method handl two method particular look posit indic handl phrase queri also proxim queri saw earlier,[ 2  4  0 10  6]
199,Course2_W7-S2-L1_Introducing_Ranked_Retrieval_4-27,hello segment im going start talking ranked retrieval far queries boolean give query like ocean liner search engine gonna return precisely documents contain words ocean liner others documents either match dont good expert users precise understanding needs collection good information retrieval system component larger application system consume thousands results adjust query needed system turns actually good vast majority users users incapable writing good boolean queries even write think far much work write particular boolean systems often produce thousands results users dont want wade thousands results particularly true web search applications general theres problem boolean search feast famine problem boolean queries often result either returned results one two even zero documents dont precisely satisfy search request else may result many results order thousands example heres system giving results give result standard user dlink returns results try make query specific ask standard user dlink card found get zero results takes lot skill come query produces manageable number hits basic problem youre putting words get results put words get many results part solution development ranked retrieval models idea rather set documents satisfying query ranked retrieval models system returns ordering top documents collection respect query going along adoption free text queries rather explicit query language like boolean retrieval model ors nots instead users query words human language principle two separate choices could manipulated separately practice ranked retrieval models normally associated free text queries opposite boolean retrieval model feast famine problem doesnt exist ranked retrieval system produces large result set users dont really notice indeed size result set basically isnt issue normally system start showing use top results overwhelm user total number results something probably wont even know notice depends ranking algorithm works well though top results good results basis ranked retrieval good system scoring need able return documents likely useful searcher raises question rank order documents respect query method well look idea assign score say number zero one document score measures well document query match need way assigning score querydocument pair lets start one term query well query term doesnt occur document score document zero beyond probably want say frequently query term appears document higher score exactly score documents isnt quite clear upcoming segments well look number alternatives okay hope thats given idea ranked retrieval models differ boolean retrieval model,Course2,W7-S2-L1,W7,S2,L1,Introducing,7,2,1,hello segment im go start talk rank retriev far queri boolean give queri like ocean liner search engin gonna return precis document contain word ocean liner other document either match dont good expert user precis understand need collect good inform retriev system compon larger applic system consum thousand result adjust queri need system turn actual good vast major user user incap write good boolean queri even write think far much work write particular boolean system often produc thousand result user dont want wade thousand result particularli true web search applic gener there problem boolean search feast famin problem boolean queri often result either return result one two even zero document dont precis satisfi search request els may result mani result order thousand exampl here system give result give result standard user dlink return result tri make queri specif ask standard user dlink card found get zero result take lot skill come queri produc manag number hit basic problem your put word get result put word get mani result part solut develop rank retriev model idea rather set document satisfi queri rank retriev model system return order top document collect respect queri go along adopt free text queri rather explicit queri languag like boolean retriev model or not instead user queri word human languag principl two separ choic could manipul separ practic rank retriev model normal associ free text queri opposit boolean retriev model feast famin problem doesnt exist rank retriev system produc larg result set user dont realli notic inde size result set basic isnt issu normal system start show use top result overwhelm user total number result someth probabl wont even know notic depend rank algorithm work well though top result good result basi rank retriev good system score need abl return document like use searcher rais question rank order document respect queri method well look idea assign score say number zero one document score measur well document queri match need way assign score querydocu pair let start one term queri well queri term doesnt occur document score document zero beyond probabl want say frequent queri term appear document higher score exactli score document isnt quit clear upcom segment well look number altern okay hope that given idea rank retriev model differ boolean retriev model,[ 2  4  9 14 13]
200,Course2_W7-S2-L2_Scoring_with_the_Jaccard_Coefficient_5-06,first simple example ranked retrieval model lets consider scoring jaccard coefficient jaccard coefficient commonly used measure overlap two sets b simply take number items intersection b divide number items union b take jaccard coefficient set set size size also size intersection union ratio one jaccard coefficient one two sets disjoint members common numerator jaccard coefficient zero jaccard coefficient zero two sets dont size able see jaccard coefficient always assign number zero one cause intersection large union suppose decided make query document match score jaccard coefficient computed sets words two documents contain idea lets suppose query ides march three words two documents say three different words document one caesar doesnt occur died doesnt occur doesnt occur march occur size intersection one word total number words six second document well doesnt occur query long doesnt occur query march occur query time jaccard coefficient going one divided number words time five okay document going win higher jaccard score course difference may seem significant essentially document winning shorter imagined different example maybe word ides second document wed get jaccard coefficient two overlapping words six maybe makes sense youre getting overlaps jaccard score higher idea else equal shorter document preferred common one well see ir models okay jaccard scoring good idea retrieval model general felt couple issues one doesnt consider term frequency uses set words document ignores many times words occur document thats typically information want well look models minute deal term frequency theres also second finer point rare terms collection informative frequent terms evaluating query thats something well also want factor models theres one aspect jaccard coefficient turns work well way normalization dividing union isnt necessarily quite right mean particular later segments well introduce idea using cosine similarity well go math general case youve seen want kind come back work cosine similarity score one zero bag words work cosine score itll turn like jaccard coefficient except weve got slight difference denominator taking square root union actually turns better form length normalization okay introduced jaccard coefficient simple example ranked retrieval model think hope also way show issues need deal good retrieval model factor frequency terms rareness words normalize score different document lengths,Course2,W7-S2-L2,W7,S2,L2,Scoring,7,2,2,first simpl exampl rank retriev model let consid score jaccard coeffici jaccard coeffici commonli use measur overlap two set b simpli take number item intersect b divid number item union b take jaccard coeffici set set size size also size intersect union ratio one jaccard coeffici one two set disjoint member common numer jaccard coeffici zero jaccard coeffici zero two set dont size abl see jaccard coeffici alway assign number zero one caus intersect larg union suppos decid make queri document match score jaccard coeffici comput set word two document contain idea let suppos queri ide march three word two document say three differ word document one caesar doesnt occur die doesnt occur doesnt occur march occur size intersect one word total number word six second document well doesnt occur queri long doesnt occur queri march occur queri time jaccard coeffici go one divid number word time five okay document go win higher jaccard score cours differ may seem signific essenti document win shorter imagin differ exampl mayb word ide second document wed get jaccard coeffici two overlap word six mayb make sens your get overlap jaccard score higher idea els equal shorter document prefer common one well see ir model okay jaccard score good idea retriev model gener felt coupl issu one doesnt consid term frequenc use set word document ignor mani time word occur document that typic inform want well look model minut deal term frequenc there also second finer point rare term collect inform frequent term evalu queri that someth well also want factor model there one aspect jaccard coeffici turn work well way normal divid union isnt necessarili quit right mean particular later segment well introduc idea use cosin similar well go math gener case youv seen want kind come back work cosin similar score one zero bag word work cosin score itll turn like jaccard coeffici except weve got slight differ denomin take squar root union actual turn better form length normal okay introduc jaccard coeffici simpl exampl rank retriev model think hope also way show issu need deal good retriev model factor frequenc term rare word normal score differ document length,[ 2  4 14 13 12]
201,Course2_W7-S2-L3_Term_Frequency_Weighting_5-59,next thing id like introduce term frequency weighting one components kind document scores regularly used information retrieval systems lets go back began term document incidence matrix matrix recorded number either one zero cell matrix depending whether word occurred document think representation document well vector binary vector dimensionality vector size vocabulary recording dont limit binary vector like obvious alternative instead move count vector still vector document rather simply putting putting number times word occurs document weve still got vector size vocabulary vector natural number vector space previously boolean retrieval model looking set words occurred document set operations like count model weve moved commonly used bag words model bag words model considering ordering words document considering many times word occurs document word bag commonly used extension sets record often word used bag words model huge limitations john quicker mary mary quicker john exactly vectors theres differentiation thats obviously limitations sense step back earlier introduced positional indices able distinguish two documents either proximity phrase queries well want get back well look later recovering positional information going develop bag words model used vector space retrieval models quantity term frequency term document number times occurs question use retrieval score thinking little hope convinced raw term frequency perhaps really want idea underlying making use term frequency im searching something like squirrels prefer document word squirrel three times one word squirrel hand find document word squirrel times clear prefer times much document mentions squirrel suggestion relevance goes number mentions linearly want come way scaling term frequencies relative frequency less linear go outline measure let highlight one last point talk term frequency word frequency actually two usages one rate something occurs frequency burglaries sense one thats always used information retrieval talk frequency information retrieval frequency means count count word document okay standardly done term frequency take log term frequency term frequency zero word doesnt occur document well log zero negative infinity thats slightly problematic standard fix two case construction add one term frequency term occur document occurs value become one log zero well add one return answer zero word doesnt occur means going little use base ten logarithms see getting less linear growth word occurs twice document gets weight little occurs ten times gets weight two times weight four order score document query pair gonna sum terms word query document sufficient take intersection words query document cause everything else contribute nothing score terms gonna calculate quantity sum note particular score indeed still zero none query terms present document okay thats idea term frequency weighting used give score documents particular query used rank documents returned,Course2,W7-S2-L3,W7,S2,L3,Term,7,2,3,next thing id like introduc term frequenc weight one compon kind document score regularli use inform retriev system let go back began term document incid matrix matrix record number either one zero cell matrix depend whether word occur document think represent document well vector binari vector dimension vector size vocabulari record dont limit binari vector like obviou altern instead move count vector still vector document rather simpli put put number time word occur document weve still got vector size vocabulari vector natur number vector space previous boolean retriev model look set word occur document set oper like count model weve move commonli use bag word model bag word model consid order word document consid mani time word occur document word bag commonli use extens set record often word use bag word model huge limit john quicker mari mari quicker john exactli vector there differenti that obvious limit sens step back earlier introduc posit indic abl distinguish two document either proxim phrase queri well want get back well look later recov posit inform go develop bag word model use vector space retriev model quantiti term frequenc term document number time occur question use retriev score think littl hope convinc raw term frequenc perhap realli want idea underli make use term frequenc im search someth like squirrel prefer document word squirrel three time one word squirrel hand find document word squirrel time clear prefer time much document mention squirrel suggest relev goe number mention linearli want come way scale term frequenc rel frequenc less linear go outlin measur let highlight one last point talk term frequenc word frequenc actual two usag one rate someth occur frequenc burglari sens one that alway use inform retriev talk frequenc inform retriev frequenc mean count count word document okay standardli done term frequenc take log term frequenc term frequenc zero word doesnt occur document well log zero neg infin that slightli problemat standard fix two case construct add one term frequenc term occur document occur valu becom one log zero well add one return answer zero word doesnt occur mean go littl use base ten logarithm see get less linear growth word occur twice document get weight littl occur ten time get weight two time weight four order score document queri pair gonna sum term word queri document suffici take intersect word queri document caus everyth els contribut noth score term gonna calcul quantiti sum note particular score inde still zero none queri term present document okay that idea term frequenc weight use give score document particular queri use rank document return,[ 2  4  5 14 13]
202,Course2_W7-S2-L4_Inverse_Document_Frequency_Weighting_10-16,segment im going introduce another score thats used ranking matches documents query make use notion document frequency particular always use reverse normally referred inverse document frequency weighting idea behind making use document frequency rare terms informative frequent terms remember earlier talked stop words know words like idea words common semantically empty didnt include information retrieval system effect good match document query well thats maybe quite true theres truth particular seems like general common words arent determinative matching document query whereas rare words important consider term query rare collection perhaps something like arachnocentric well someone typed word query find document contains word arachnocentric likely document user would interested seeing want give high weight match score rare terms like arachnocentric hand frequent terms less informative rare terms consider term frequent collection like high increase line might occur lots documents well document containing term likely relevant document doesnt query contained one terms sure indicator relevance frequent terms want give positive weights document matching term query lower weights rare terms way going go making use notion document frequency scores exactly well document frequency term number documents contain term means looking entire collection maybe collection million documents ten documents word saying document frequency ten thats counting number documents occurs regardless number times occurs thats something ill come back document frequency inverse measure informative informativeness term also note document frequency term smaller number documents collection putting together gives us measure inverse document frequency start document frequency use denominator numerator n number documents word appears one document part n word appears every document value one value one n take log log used dampen effect inverse document frequency idea use absolute score would strong vector computation see ive used log base ten thats commonly used actually turns use base log isnt really important okay lets go concrete example going suppose size document collection documents take extremely rare word like calpurnia lets say occurs one document well going going taking number documents divided one taking log means log base ten would six take somewhat common doc word occurs may documents going get inverse frequency four work progressively common words inverse document frequency countdown particular final case assume word occurred every one documents well weve got million divided million one take log get answer zero result actually get word occurs every document weight zero according idf score effect ordering words retrieval makes sense occurs every document discriminatory value documents gets weight zero see numbers overall though inverse document frequency weighting give small multiplier pay attention words rarer words rather common words another thing note idf values arent things change query theres precisely one idf value term collection thats going regardless query youre issuing collection okay heres yesno question guys idf effect ranking one term queries like one answer doesnt idf effect one term queries term one query youre going one terms n document frequency itll worked going scaling factor since theres one idf value term applied every document therefore wont affect ranking way get effect idf multiple terms query example query capricious person well situation capricious much rarer word idf say pay much attention documents contain word capricious documents contain word person ranking retrieval results theres another measure reflects frequency term indeed might wondering using measure information retrieval people refer collection frequency term collection frequency term total number times appears collection counting multiple occurrences thats measure weve using places measure using build unigram language models working spam classifiers something like usually used information retrieval ranking systems next example maybe help explain two words insurance try picked two words virtually identical collection frequency overall occur somewhat times collection lets look document frequency word try occurs odd documents stands contrast insurance occurs slightly documents mean means try occurs document tends occur try widely distributed across documents hand insurance occurs document tends occur several times tends occur two three times reflect reflects fact tend documents insurance mention insurance several times dont really tend documents trying mean terms coming score retrieval systems words matching seems suggest giving higher weighting instances word insurance appearing kind imagine kind query like try buy insurance important word make sure finding documents match query insurance probably second important word buy try coming third place near stop word thats idea correctly captured looking document frequency see captured collection frequency would score try insurance equally okay hope know document frequency weighting people usually use retrieval ranking score rather collection frequency,Course2,W7-S2-L4,W7,S2,L4,Inverse,7,2,4,segment im go introduc anoth score that use rank match document queri make use notion document frequenc particular alway use revers normal refer invers document frequenc weight idea behind make use document frequenc rare term inform frequent term rememb earlier talk stop word know word like idea word common semant empti didnt includ inform retriev system effect good match document queri well that mayb quit true there truth particular seem like gener common word arent determin match document queri wherea rare word import consid term queri rare collect perhap someth like arachnocentr well someon type word queri find document contain word arachnocentr like document user would interest see want give high weight match score rare term like arachnocentr hand frequent term less inform rare term consid term frequent collect like high increas line might occur lot document well document contain term like relev document doesnt queri contain one term sure indic relev frequent term want give posit weight document match term queri lower weight rare term way go go make use notion document frequenc score exactli well document frequenc term number document contain term mean look entir collect mayb collect million document ten document word say document frequenc ten that count number document occur regardless number time occur that someth ill come back document frequenc invers measur inform inform term also note document frequenc term smaller number document collect put togeth give us measur invers document frequenc start document frequenc use denomin numer n number document word appear one document part n word appear everi document valu one valu one n take log log use dampen effect invers document frequenc idea use absolut score would strong vector comput see ive use log base ten that commonli use actual turn use base log isnt realli import okay let go concret exampl go suppos size document collect document take extrem rare word like calpurnia let say occur one document well go go take number document divid one take log mean log base ten would six take somewhat common doc word occur may document go get invers frequenc four work progress common word invers document frequenc countdown particular final case assum word occur everi one document well weve got million divid million one take log get answer zero result actual get word occur everi document weight zero accord idf score effect order word retriev make sens occur everi document discriminatori valu document get weight zero see number overal though invers document frequenc weight give small multipli pay attent word rarer word rather common word anoth thing note idf valu arent thing chang queri there precis one idf valu term collect that go regardless queri your issu collect okay here yesno question guy idf effect rank one term queri like one answer doesnt idf effect one term queri term one queri your go one term n document frequenc itll work go scale factor sinc there one idf valu term appli everi document therefor wont affect rank way get effect idf multipl term queri exampl queri caprici person well situat caprici much rarer word idf say pay much attent document contain word caprici document contain word person rank retriev result there anoth measur reflect frequenc term inde might wonder use measur inform retriev peopl refer collect frequenc term collect frequenc term total number time appear collect count multipl occurr that measur weve use place measur use build unigram languag model work spam classifi someth like usual use inform retriev rank system next exampl mayb help explain two word insur tri pick two word virtual ident collect frequenc overal occur somewhat time collect let look document frequenc word tri occur odd document stand contrast insur occur slightli document mean mean tri occur document tend occur tri wide distribut across document hand insur occur document tend occur sever time tend occur two three time reflect reflect fact tend document insur mention insur sever time dont realli tend document tri mean term come score retriev system word match seem suggest give higher weight instanc word insur appear kind imagin kind queri like tri buy insur import word make sure find document match queri insur probabl second import word buy tri come third place near stop word that idea correctli captur look document frequenc see captur collect frequenc would score tri insur equal okay hope know document frequenc weight peopl usual use retriev rank score rather collect frequenc,[ 2  4 14 13 12]
203,Course2_W7-S2-L5_TF-IDF_Weighting_3-42,weve introduced two weights terms documents use information retrieval ranking process term frequency inverse document frequency segment going put together get tfidf weight terms tfidf weight term document right simply product tf weight scaled log term discussed times inverse document frequency weight best known weighting scheme terms information retrieval theres lot research many others know one one know note particular one fine point little dash hyphen tfidf weighting thats hyphen minus sign taking product sometimes people indicate explicitly using dot using multiplication sign features tfidf weighting tfidf weighting increases number times term occurs document tfidf weight query term depends document independent document tfidf weight term also goes rarity term collection thats idf weighting using find ranking documents query work score query document taking terms appear query document rest weighting working tdidf weight terms summing get score document respect query done weve done weve gradually moved first binary vectors original model boolean information retrieval count vectors used unscaled term frequency weight vectors document hence weight matrix terms documents thats see document represented realvalued vector example document julius caesar represented vector document vector space real valued numbers dimensionality number different terms collection okay bunch vectors document collection term term document matrix real valued matrix well say little bit later properties hopefully seeing kind term document matrix real numbers enough see ranking documents according query according tfidf scores weve assigned term document thats tfidf weighting one central concepts information retrieval systems,Course2,W7-S2-L5,W7,S2,L5,TF-IDF,7,2,5,weve introduc two weight term document use inform retriev rank process term frequenc invers document frequenc segment go put togeth get tfidf weight term tfidf weight term document right simpli product tf weight scale log term discuss time invers document frequenc weight best known weight scheme term inform retriev there lot research mani other know one one know note particular one fine point littl dash hyphen tfidf weight that hyphen minu sign take product sometim peopl indic explicitli use dot use multipl sign featur tfidf weight tfidf weight increas number time term occur document tfidf weight queri term depend document independ document tfidf weight term also goe rariti term collect that idf weight use find rank document queri work score queri document take term appear queri document rest weight work tdidf weight term sum get score document respect queri done weve done weve gradual move first binari vector origin model boolean inform retriev count vector use unscal term frequenc weight vector document henc weight matrix term document that see document repres realvalu vector exampl document juliu caesar repres vector document vector space real valu number dimension number differ term collect okay bunch vector document collect term term document matrix real valu matrix well say littl bit later properti hope see kind term document matrix real number enough see rank document accord queri accord tfidf score weve assign term document that tfidf weight one central concept inform retriev system,[ 2  5  4 14 13]
204,Course2_W7-S2-L6_The_Vector_Space_Model_16-22,hi okay weve already laid ground work notions like term frequency inverse document frequency segment want introduce probably retrieval model vector space model one commonly used models information retrieval real systems saw previous segment turn documents real valued vectors vdimensional vector space v number words vocabulary terms words axes space documents think either points space vectors origin pointing points high dimensional space tens millions dimensions real system apply web search engine crucial property vectors theyre sparse vectors entries zero individual document typically hundred thousand words vector space documents handle querying query comes key idea treat queries exactly way theyre also going vectors space rank documents according proximity query space proximity corresponds similarity vectors therefore roughly reverse distance want get away youre either boolean model relative score well document matches query going rank relevant documents higher less relevant documents lets try make bit precise formalize proximity vector space first attempt take distance two points distance end points vectors standard way vector space would euclidean distance points turns euclidean distance isnt actually good idea thats euclidean distance large vectors different lengths let explain mean lets suppose vector space well finding distance large particular larger either distance distance actually think terms information retrieval problem look whats space seems wrong teeny example two word axes shown gossip jealous query query would come precisely query gossip jealous words occurring equal weight well look documents find document one seems lot gossiping nothing jealousy document three lot jealousy nothing gossiping whereas document two seems kind document want get one lot gossiping jealously terms document similar ones q want saying actually similar document suggests way solve problem move forward rather talking distance want start looking angle vector space idea use angle instead distance lets particular motivate considering thought experiment suppose take document append giving us document prime clearly semantically prime content cover information working regular vector space euclidean distance distance two documents quite large thats vector vector vector prime would twice long pointing quite large distance two vectors dont want instead want notice two vectors line angle two vectors zero corresponding maximal similarity idea going rank documents according angle document query following two documents equivalent ranking documents decreasing order angle query document ranking documents increasing order cosine angle query document ill go little bit detail youll often hear phrase cosine similarity introducing secret notice cosine monotonically decreasing function angles interval zero degrees heres cosine remember angle zero cosine one perpendicular degrees cosine zero keep going right degrees cosine continuing descend minus one essentially need observe cosine monotonically decreasing function range zero therefore cosine score serves kind inverse angle well might still make seem reas rather strange thing use mean could taken reciprocal angle negative angle wouldve also turned things around got measure closeness documents similarity measure turns cosine measure actually standard theres actually efficient way evaluate cosine angle documents using vector arithmetic dont actually use transcendental functions like cosine would take long time compute starting point going getting idea length vector normalize length vector vector vector x work length vector summing components squared taking square root around outside something like vector thats going take three squared nine four squared sixteen take add gives take square root gives five thats length vector like standard pythagorean triangle okay take vector divide length get unit length vector think vector touches surface unit hypersphere around origin go back example earlier two documents appended give prime see documents theyre length normalized go back exactly position length normalize vectors long short documents comparable weights secret cosine measure length normalization heres cosine similarity two documents cosine angle two documents way numerator calculate dot product taking individual components vector component component multiplying taking sum way weve got denominator considering lengths vectors write like actually equivalent taking vector length normalizing taking dot product whole thing sort two parts factor apart wish fill written full length normalizations bottom summed dot product top okay elements qi tfidf weight term query di tfidf weight term document particular might want actually length normalize document vectors advance length normalize cosine length normalize query vector query comes cosine similarity measure simply dot product lengthnormalized vectors simply taking sum vector space discussed reality wont elements vector well terms vect terms vocabulary intersection ones appear q document going back kind picture vector space showing two axes keep viewable poor rich take document vector map unit length length normalization get document vectors vectors touch surface unit hypersphere circle two dimensions want order documents similarity query take query working angle cosine angle documents particular cosine highest small angles well order documents terms cosine angle document rank first itll okay lets go concretely example example three novels jane austens going represent vector space length normalized going work cosine similarity different novels words example isnt actually query vector working similarity different novels documents starting point starting term frequency count vectors different novels see affection one jane austins favorite words appears frequently every novel word wuthering occurs wuthering heights words like jealous jealous gossip occur occasionally going vocabulary example give going want take term frequency vectors turn length normalized vectors unit hypersphere example im going use term frequency weighting going leave idf weighting keep bit simpler lets see happens next slide okay weve done log frequency weighting kind saw zeros stay zero mapping getting weighting three number times affection appeared sense sensibility vectors arent yet length clearly longest vectors next step length normalize length normalized vectors three documents see vector gotten much shorter scaling property vectors length normalized took quantity squared plus quantity squared plus quantity squared would get one therefore square root sum would also one theyre length one vectors given theyre length one vectors calculate cosine similarities simply dot product vectors lets see happens okay cosine similarity sense sensibility pride prejudice taking pairwise products summing together gives us cosine similarity theyre similar cases see sense sensibility wuthering heights final pair two thing might wonder cosine similarity sense sensibility pride prejudice higher sense sensibility wuthering heights try look going comparing one two see part wuthering heights vector doesnt help producing similarity sense sensibility biggest component sense sensibility vector one generates lot similarity pride prejudice also word prominently represented word less represented wuthering heights therefore dot product term dot product much larger get greater similarity see sort ratio occurrence different words document big effect measuring overall similarity okay hope example helped make specific good idea vector space model information retrieval idea rank documents retrieval based similarity angles high dimensional vector space,Course2,W7-S2-L6,W7,S2,L6,The,7,2,6,hi okay weve alreadi laid ground work notion like term frequenc invers document frequenc segment want introduc probabl retriev model vector space model one commonli use model inform retriev real system saw previou segment turn document real valu vector vdimension vector space v number word vocabulari term word axe space document think either point space vector origin point point high dimension space ten million dimens real system appli web search engin crucial properti vector theyr spars vector entri zero individu document typic hundr thousand word vector space document handl queri queri come key idea treat queri exactli way theyr also go vector space rank document accord proxim queri space proxim correspond similar vector therefor roughli revers distanc want get away your either boolean model rel score well document match queri go rank relev document higher less relev document let tri make bit precis formal proxim vector space first attempt take distanc two point distanc end point vector standard way vector space would euclidean distanc point turn euclidean distanc isnt actual good idea that euclidean distanc larg vector differ length let explain mean let suppos vector space well find distanc larg particular larger either distanc distanc actual think term inform retriev problem look what space seem wrong teeni exampl two word axe shown gossip jealou queri queri would come precis queri gossip jealou word occur equal weight well look document find document one seem lot gossip noth jealousi document three lot jealousi noth gossip wherea document two seem kind document want get one lot gossip jealous term document similar one q want say actual similar document suggest way solv problem move forward rather talk distanc want start look angl vector space idea use angl instead distanc let particular motiv consid thought experi suppos take document append give us document prime clearli semant prime content cover inform work regular vector space euclidean distanc distanc two document quit larg that vector vector vector prime would twice long point quit larg distanc two vector dont want instead want notic two vector line angl two vector zero correspond maxim similar idea go rank document accord angl document queri follow two document equival rank document decreas order angl queri document rank document increas order cosin angl queri document ill go littl bit detail youll often hear phrase cosin similar introduc secret notic cosin monoton decreas function angl interv zero degre here cosin rememb angl zero cosin one perpendicular degre cosin zero keep go right degre cosin continu descend minu one essenti need observ cosin monoton decreas function rang zero therefor cosin score serv kind invers angl well might still make seem rea rather strang thing use mean could taken reciproc angl neg angl wouldv also turn thing around got measur close document similar measur turn cosin measur actual standard there actual effici way evalu cosin angl document use vector arithmet dont actual use transcendent function like cosin would take long time comput start point go get idea length vector normal length vector vector vector x work length vector sum compon squar take squar root around outsid someth like vector that go take three squar nine four squar sixteen take add give take squar root give five that length vector like standard pythagorean triangl okay take vector divid length get unit length vector think vector touch surfac unit hyperspher around origin go back exampl earlier two document append give prime see document theyr length normal go back exactli posit length normal vector long short document compar weight secret cosin measur length normal here cosin similar two document cosin angl two document way numer calcul dot product take individu compon vector compon compon multipli take sum way weve got denomin consid length vector write like actual equival take vector length normal take dot product whole thing sort two part factor apart wish fill written full length normal bottom sum dot product top okay element qi tfidf weight term queri di tfidf weight term document particular might want actual length normal document vector advanc length normal cosin length normal queri vector queri come cosin similar measur simpli dot product lengthnorm vector simpli take sum vector space discuss realiti wont element vector well term vect term vocabulari intersect one appear q document go back kind pictur vector space show two axe keep viewabl poor rich take document vector map unit length length normal get document vector vector touch surfac unit hyperspher circl two dimens want order document similar queri take queri work angl cosin angl document particular cosin highest small angl well order document term cosin angl document rank first itll okay let go concret exampl exampl three novel jane austen go repres vector space length normal go work cosin similar differ novel word exampl isnt actual queri vector work similar differ novel document start point start term frequenc count vector differ novel see affect one jane austin favorit word appear frequent everi novel word wuther occur wuther height word like jealou jealou gossip occur occasion go vocabulari exampl give go want take term frequenc vector turn length normal vector unit hyperspher exampl im go use term frequenc weight go leav idf weight keep bit simpler let see happen next slide okay weve done log frequenc weight kind saw zero stay zero map get weight three number time affect appear sens sensibl vector arent yet length clearli longest vector next step length normal length normal vector three document see vector gotten much shorter scale properti vector length normal took quantiti squar plu quantiti squar plu quantiti squar would get one therefor squar root sum would also one theyr length one vector given theyr length one vector calcul cosin similar simpli dot product vector let see happen okay cosin similar sens sensibl pride prejudic take pairwis product sum togeth give us cosin similar theyr similar case see sens sensibl wuther height final pair two thing might wonder cosin similar sens sensibl pride prejudic higher sens sensibl wuther height tri look go compar one two see part wuther height vector doesnt help produc similar sens sensibl biggest compon sens sensibl vector one gener lot similar pride prejudic also word promin repres word less repres wuther height therefor dot product term dot product much larger get greater similar see sort ratio occurr differ word document big effect measur overal similar okay hope exampl help make specif good idea vector space model inform retriev idea rank document retriev base similar angl high dimension vector space,[ 2  5  4 12 14]
205,Course2_W7-S2-L7_Calculating_TF-IDF_Cosine_Scores_12-47,okay let tell little bit tfidf scores cosine similarity measure get used together ranked ir retrieval system im going get lot details making systems practical efficient least give little bit sense first thing might already started notice tfidf weighting isnt really one thing theres really family measures lets look little bit detail first term frequency term frequency could natural term frequency suggested usually muted something like log weighting thats indeed common thing method thats used bunch methods suggested normalizing term frequency move document frequency use document frequency weighting could use kind log inverse document frequency weighting extremely common things people tried put two things together tfidf weighting giving us vector well may well want normalize vectors way better similarity computations discussed using cosine length normalization turns advantages disadvantages things people tried including ones came recently things like pivoted length normalization general kind broad menu choices beginning column ive given letter names choices choices choices developed context smart information retrieval system famous pioneering information retrieval system developed cornell jerry sultan really father lot modern information retrieval choices could given names giving letters using system weve mainly talking would coming ltc logarithm logarithmic idf cosine weighting kind weighting turns used queries documents differently lets go little bit comes different weightings queries versus documents follow smart notation standard way represented things six letters dot middle document weighting scheme followed query weighting scheme various variants one quite standard coming smart work one well mention one little bit detail query part first find theres log query normalization actually makes difference long queries might mention words multiple times really short queries word mentioned youre going getting weight one words appear zero words dont appear theres idf weighting query terms cosine normalization treatment documents except theres actually idf normalization documents thats something might want think moment bad idea theres reasons want one well youve already put idf factor words query cause remember youre going get nonzero scores words occur query document advantages terms efficiency compressing indices youre putting idf lets take weighting scheme go concrete example gonna working score precisely one document one query using weighting scheme well great depth okay document car insurance auto insurance bit fake document wanted something short query best car insurance go query first best car insurance raw weights gonna scale logarithmic scaling since word occurred stays one get document frequency words map onto inverse document frequencies rarer words like insurance getting highest weighting multiply column column ends looking like document frequency score except word didnt occur turn unit vector cosine normalization final representation query vector move document document term frequencies arent zero one reduce term frequency weightings look like case idf component document weights go exactly coming term frequency cosine normalization gives us final document vector okay work score document query working cosine similarity simply dot product two length normalized vectors thats vector bottom two components nonzero add overall score document good match query though mean remember youre looking cosine similarities fact cosine kind sort flat top know flat descending means tend get fairly similar documents cosine scores sort biased fairly high important remember ordering precise values okay shows evaluated document wed want evaluate bunch documents wed want rank according cosine similarity scores little exercise might like based example well know idf scores document frequencies actually able work number documents used basis example okay lets go work cosine scores vector space retrieval system document collection rough kind algorithm going want use gonna assume query typical short weblike query going thinking words either occurring occurring query also gonna skip one step actually going length normalization query part reason situation like length normalization query actually unnecessary query vector length whatever effective length normalization would rescaling applies query document calculations wouldnt change final result okay given background start scores array documents set zero going accumulate score document different query terms scores often also referred accumulators okay also going another array lengths different documents go term query say well query term actually going one fetch postings list query document postings list term frequency document may want scale something like log weighting something like give us document weight term components dot product summing scores array essence kind outer iteration query term working components cosine score query term accumulating scores array havent actually done length normalization documents either yet next step work length document divide scores length document length normalization different document sizes given assumptions mentioned beginning query vector one zero dont need length normalize something ordered length normalized cosine similarity score documents ranking wanted return number k documents ids representation highest value scores think little isnt quite yet practical algorithm document collection huge wouldnt actually want build array cell every document say might know twenty billion documents something like systems use methods work likely documents accumulators documents similarly end good way find relevant documents simply linear scan scores array efficient data structures hope thats given general idea build cosine similarity scoring ranked retrieval engine summarize essence weve covered vector space retrieval following steps query represented tfidf vector document also weighted tfidf document also represented tfidf vector score pair query document working cosine similarity scores straightforwardly use rank documents well first instance return top ks example top ten documents according score user initial results ask show okay thats general idea start build tfidf ranked retrieval system,Course2,W7-S2-L7,W7,S2,L7,Calculating,7,2,7,okay let tell littl bit tfidf score cosin similar measur get use togeth rank ir retriev system im go get lot detail make system practic effici least give littl bit sens first thing might alreadi start notic tfidf weight isnt realli one thing there realli famili measur let look littl bit detail first term frequenc term frequenc could natur term frequenc suggest usual mute someth like log weight that inde common thing method that use bunch method suggest normal term frequenc move document frequenc use document frequenc weight could use kind log invers document frequenc weight extrem common thing peopl tri put two thing togeth tfidf weight give us vector well may well want normal vector way better similar comput discuss use cosin length normal turn advantag disadvantag thing peopl tri includ one came recent thing like pivot length normal gener kind broad menu choic begin column ive given letter name choic choic choic develop context smart inform retriev system famou pioneer inform retriev system develop cornel jerri sultan realli father lot modern inform retriev choic could given name give letter use system weve mainli talk would come ltc logarithm logarithm idf cosin weight kind weight turn use queri document differ let go littl bit come differ weight queri versu document follow smart notat standard way repres thing six letter dot middl document weight scheme follow queri weight scheme variou variant one quit standard come smart work one well mention one littl bit detail queri part first find there log queri normal actual make differ long queri might mention word multipl time realli short queri word mention your go get weight one word appear zero word dont appear there idf weight queri term cosin normal treatment document except there actual idf normal document that someth might want think moment bad idea there reason want one well youv alreadi put idf factor word queri caus rememb your go get nonzero score word occur queri document advantag term effici compress indic your put idf let take weight scheme go concret exampl gonna work score precis one document one queri use weight scheme well great depth okay document car insur auto insur bit fake document want someth short queri best car insur go queri first best car insur raw weight gonna scale logarithm scale sinc word occur stay one get document frequenc word map onto invers document frequenc rarer word like insur get highest weight multipli column column end look like document frequenc score except word didnt occur turn unit vector cosin normal final represent queri vector move document document term frequenc arent zero one reduc term frequenc weight look like case idf compon document weight go exactli come term frequenc cosin normal give us final document vector okay work score document queri work cosin similar simpli dot product two length normal vector that vector bottom two compon nonzero add overal score document good match queri though mean rememb your look cosin similar fact cosin kind sort flat top know flat descend mean tend get fairli similar document cosin score sort bias fairli high import rememb order precis valu okay show evalu document wed want evalu bunch document wed want rank accord cosin similar score littl exercis might like base exampl well know idf score document frequenc actual abl work number document use basi exampl okay let go work cosin score vector space retriev system document collect rough kind algorithm go want use gonna assum queri typic short weblik queri go think word either occur occur queri also gonna skip one step actual go length normal queri part reason situat like length normal queri actual unnecessari queri vector length whatev effect length normal would rescal appli queri document calcul wouldnt chang final result okay given background start score array document set zero go accumul score document differ queri term score often also refer accumul okay also go anoth array length differ document go term queri say well queri term actual go one fetch post list queri document post list term frequenc document may want scale someth like log weight someth like give us document weight term compon dot product sum score array essenc kind outer iter queri term work compon cosin score queri term accumul score array havent actual done length normal document either yet next step work length document divid score length document length normal differ document size given assumpt mention begin queri vector one zero dont need length normal someth order length normal cosin similar score document rank want return number k document id represent highest valu score think littl isnt quit yet practic algorithm document collect huge wouldnt actual want build array cell everi document say might know twenti billion document someth like system use method work like document accumul document similarli end good way find relev document simpli linear scan score array effici data structur hope that given gener idea build cosin similar score rank retriev engin summar essenc weve cover vector space retriev follow step queri repres tfidf vector document also weight tfidf document also repres tfidf vector score pair queri document work cosin similar score straightforwardli use rank document well first instanc return top ks exampl top ten document accord score user initi result ask show okay that gener idea start build tfidf rank retriev system,[ 2  4 10 14 13]
206,Course2_W7-S2-L8_Evaluating_Search_Engines_9-02,section ill tell little bit evaluating quality search engine many measures quality search engine technical ones fast index fast search look things like expressiveness query language whether theyre able express complex informational needs things like phrase queries negations disjunctions people desires like uncluttered ui system doesnt cost lot use measures measurable quantify get kind score goodness practice measures important tend dominated another measure user happiness user happy using search engine speed response size index certainly factors blindingly fast useless answers wont make user happy huge part user happiness results returned results want thats metric relevance results users information need mentioned right beginning reiterate evaluating ir system evaluate respect information need information need translated query thats ir system actually runs relevance assessed relative information need query example information need persons looking information whether drinking red wine effective white wine reducing risks heart attacks theyll come query example might wine red white heart attack effective submitted search engine evaluating effectiveness search engine returning relevant results asking results search engine returns documents simply words page rather saying documents address users information need okay well go well search engine returns set results well evaluation start three things benchmark collection documents use evaluation benchmark set queries sense representative information needs interest weve gathered third thing assessor judgments whether particular documents relevant particular queries commonly practice cant assembled exhaustively certainly document collection large least particular set documents returned particular search engines get assessor judge whether documents relevant queries well result set three things business use exactly measures looked previously precision recall f measure combines suitable good measures exactly reason good measures talking things like named entity recognition normally documents relevant particular query measure much better looking measures precision recall weve moved search engine returns ranked results cant totally straightforwardly use measures precision recall f measure system return number results fact number returns normally depends often keep clicking asking sort look initial subset results work precision recall subset putting together come precision recall curve lets look works first ten results search engine weve labeled result either relevant relevant according assessors judgment take initial sub sequence documents work recall precision first document system got right relevant document lets assume overall ten relevant documents collection gotten one ten relevant documents recall well since document relevant system right first answer precision one point well next document relevant recall first two documents precision another relevant document precision im sorry recall still precision dropped look set top four documents weve found two ten relevant ones recall precision gone back fifth one also relevant recall precision three five keep going maybe guys could work entries measure want mention one recently used recent measures mean average precision look ranked retrieval results ordered way give bit room first document returned relevant second one relevant say third one relevant relevant one another relevant one relevant relevant relevant lets say top eight results youre mean average precision first youre working average precision one query way saying lets work precision point relevant document returned cause thats youre increasing recall precision one four documents precision half five documents precision seven documents four relevant thats around guys correct arithmetic eight documents five relevant thats work mean average precision kind keeping calculating numbers practice normally arent calculated exh exhaustively theyre calculated point lets say youre calculating average function numbers thats average precision one query calculate average precision queries benchmark query collection would take average gives mean average precision particular whats referred macroaveraging query counts equally calculation mean average precision good measure evaluates extent precision different recall levels still weighted precision like top returned documents level across different queries giving equal weight different queries tends useful thing always interested systems return work queries rare terms well queries common terms pretty good measure think using evaluating information retrieval systems okay theres even methods could talk thats probably good sense go evaluating performance ranked retrieval system,Course2,W7-S2-L8,W7,S2,L8,Evaluating,7,2,8,section ill tell littl bit evalu qualiti search engin mani measur qualiti search engin technic one fast index fast search look thing like express queri languag whether theyr abl express complex inform need thing like phrase queri negat disjunct peopl desir like unclutt ui system doesnt cost lot use measur measur quantifi get kind score good practic measur import tend domin anoth measur user happi user happi use search engin speed respons size index certainli factor blindingli fast useless answer wont make user happi huge part user happi result return result want that metric relev result user inform need mention right begin reiter evalu ir system evalu respect inform need inform need translat queri that ir system actual run relev assess rel inform need queri exampl inform need person look inform whether drink red wine effect white wine reduc risk heart attack theyll come queri exampl might wine red white heart attack effect submit search engin evalu effect search engin return relev result ask result search engin return document simpli word page rather say document address user inform need okay well go well search engin return set result well evalu start three thing benchmark collect document use evalu benchmark set queri sens repres inform need interest weve gather third thing assessor judgment whether particular document relev particular queri commonli practic cant assembl exhaust certainli document collect larg least particular set document return particular search engin get assessor judg whether document relev queri well result set three thing busi use exactli measur look previous precis recal f measur combin suitabl good measur exactli reason good measur talk thing like name entiti recognit normal document relev particular queri measur much better look measur precis recal weve move search engin return rank result cant total straightforwardli use measur precis recal f measur system return number result fact number return normal depend often keep click ask sort look initi subset result work precis recal subset put togeth come precis recal curv let look work first ten result search engin weve label result either relev relev accord assessor judgment take initi sub sequenc document work recal precis first document system got right relev document let assum overal ten relev document collect gotten one ten relev document recal well sinc document relev system right first answer precis one point well next document relev recal first two document precis anoth relev document precis im sorri recal still precis drop look set top four document weve found two ten relev one recal precis gone back fifth one also relev recal precis three five keep go mayb guy could work entri measur want mention one recent use recent measur mean averag precis look rank retriev result order way give bit room first document return relev second one relev say third one relev relev one anoth relev one relev relev relev let say top eight result your mean averag precis first your work averag precis one queri way say let work precis point relev document return caus that your increas recal precis one four document precis half five document precis seven document four relev that around guy correct arithmet eight document five relev that work mean averag precis kind keep calcul number practic normal arent calcul exh exhaust theyr calcul point let say your calcul averag function number that averag precis one queri calcul averag precis queri benchmark queri collect would take averag give mean averag precis particular what refer macroaverag queri count equal calcul mean averag precis good measur evalu extent precis differ recal level still weight precis like top return document level across differ queri give equal weight differ queri tend use thing alway interest system return work queri rare term well queri common term pretti good measur think use evalu inform retriev system okay there even method could talk that probabl good sens go evalu perform rank retriev system,[ 2  4  9 14 13]
207,Course2_W8-S1-L1_Word_Senses_and_Word_Relations_11-50,welcome back lecture begin talk word meaning decide two words meaning lets look fundamentals may remember earlier lecture talked difference lemmas word forms lemma citation form stem part speech rough semantics word form inflected word appears text example word form banks lemma bank word form sung lemma sing course true languages spanish word duermes lemma dormir sleep sleep lemma multiple meanings example word lemma bank sentence like bank hold investments east bank kind financial institution kind sloping land call difference sense say sense word sense discrete representation aspects word meaning wanna say lemma bank two senses least two senses sense one financial institution sense two sloping land word homonymy means situation words share form distinct unrelated meanings call two words homonyms homonyms homonymy bank one financial institution bank two sloping land homonyms similarly bat one club hitting ball bat two nocturnal flying mammal homonyms homonyms come two forms homographs thats words spelled whether pronounced differently bank bat homographs homophones write right sound theyre spelled differently sometimes homonyms homographs homophones bank bat homographs homophones sometimes theyre pronounced spelled differently sometimes theyre spelled pronounced differently homonymy causes number problems applications natural language processing information retrieval ask bat care dont know pet flying mammal youre baseball fan machine translation similar problem say english word bat spanish might mean flying mammal might mean baseball club text speech instrument stringed instrument called bass pronounced differently fish called bass going read word loud text speech know word talking consider two senses word bank bank constructed local red brick withdrew money bank sense might call sense two financial institution withdrew money financial institution sense one something building say bank constructed really mean building associated bank two senses financial institution building belonging financial institution related two senses homonymous word related call word polysemous polysemous word related multiple related meanings nonrare words multiple meanings polysemy quite common meanings related words systematic call systematic polysemy sometimes metonymy lots polysemy systematic example words like school university hospital polysemy saw bank mean institution mean building say theres systematic relationship buildings organizations theres kinds systematic polysemy might refer author jane austen wrote emma works love jane austen meaning love books writes jane austen stored shelf trees might talk blossoms plums fruit plum ate yesterday systematic polysemy authors works author trees fruit lots kinds systematic polysemy like know word one sense one test often use zeugma test consider two senses word serve flights serve breakfast lufthansa serve philadelphia mean give meal passenger mean airline flies particular city zeugma test says lets form sentence using senses word serve see bad sounds lufthansa serve breakfast san jose well sounds funny since conjunction sounds weird say two different senses serve say two words synonyms meaning contexts nut called hazelnut people filbert others means thing couch sofa words big large automobile car say two lexemes synonyms substituted situations thats case say propositional meaning happens perhaps examples perfect synonymy even many aspects meaning two words identical still may preserve acceptability depending politeness slang register genres water ho technically mean thing practice gonna use ho describe beautiful day mountains looking running stream even big large seem like might synonyms different meanings different situations lets look might say big plane would flying large small plane sounds like senses big large seems like synonyms refer measure size large scale size consider sentences miss nelson became kind big sister benjamin miss nelson became kind large sister benjamin big large mean different things big sister means older sister large sister simply cant meaning means physically larger sister funny kind meaning want say big big sense large simply lacks big sense theres sense big mean older grown large lacks sense sometimes even two words seem similar meaning senses may different two words tells us synonymy really relation senses rather relationship words say one senses big one meaning physically large one senses large meaning physically large hard define isnt senses senses synonymy relationship entire words another important relationship word senses antonymy antonyms senses opposites respect one feature meaning otherwise similar dark light short long fast slow words similar meanings short long mean im property length short means im end scale long means im end scale formally say antonyms define binary opposition opposite ends scale two kinds antonymy long short fast slow binary opposition reversives verb like rise fall words theyre different ends scale theyre different directions say theyre reversive directions two kinds antonymy relationships senses specific kinds relationships senses perhaps important relationship senses well see hyponymy hypernymy say one sense hyponym another first sense specific denotes subclass say car hyponym vehicle mango hyponym fruit another word hyponym might subclass subordinate conversely hypernym superordinate try remember saying hyper super vehicle hypernym car meaning supertype car car subtype vehicle vehicle superordinate car fruit hypernym mango ive given examples superordinate like vehicle subordinate like car hypernyms hyponyms hypernyms hyponyms superordinate subordinate synonyms formally say class denoted superordinate hypernym extensionally includes class denoted hyponym define hyp hyponymy also entailment say sense hyponym excuse sense hyponym sense b entails b hyponymy usually transitive hyponym b b hyponym c entails hyponym c another common name hyponymy hierarchy isa hierarchy intuitively say isa b b sometimes without dash also sometimes say b subsumes well see terms isa hyponymy subsumption hierarchy proceed course often make distinction hyponyms instances wordnet thesaurus well talk bit classes instances instance individual proper noun unique entity say san francisco instance instance class city instance relationship individual class city class say city hyponym municipality location kinds classes hyponomy subsumption relationship class another class instance relationship individual class fundamental definitions well need talking extracting word meaning,Course2,W8-S1-L1,W8,S1,L1,Word,8,1,1,welcom back lectur begin talk word mean decid two word mean let look fundament may rememb earlier lectur talk differ lemma word form lemma citat form stem part speech rough semant word form inflect word appear text exampl word form bank lemma bank word form sung lemma sing cours true languag spanish word duerm lemma dormir sleep sleep lemma multipl mean exampl word lemma bank sentenc like bank hold invest east bank kind financi institut kind slope land call differ sens say sens word sens discret represent aspect word mean wanna say lemma bank two sens least two sens sens one financi institut sens two slope land word homonymi mean situat word share form distinct unrel mean call two word homonym homonym homonymi bank one financi institut bank two slope land homonym similarli bat one club hit ball bat two nocturn fli mammal homonym homonym come two form homograph that word spell whether pronounc differ bank bat homograph homophon write right sound theyr spell differ sometim homonym homograph homophon bank bat homograph homophon sometim theyr pronounc spell differ sometim theyr spell pronounc differ homonymi caus number problem applic natur languag process inform retriev ask bat care dont know pet fli mammal your basebal fan machin translat similar problem say english word bat spanish might mean fli mammal might mean basebal club text speech instrument string instrument call bass pronounc differ fish call bass go read word loud text speech know word talk consid two sens word bank bank construct local red brick withdrew money bank sens might call sens two financi institut withdrew money financi institut sens one someth build say bank construct realli mean build associ bank two sens financi institut build belong financi institut relat two sens homonym word relat call word polysem polysem word relat multipl relat mean nonrar word multipl mean polysemi quit common mean relat word systemat call systemat polysemi sometim metonymi lot polysemi systemat exampl word like school univers hospit polysemi saw bank mean institut mean build say there systemat relationship build organ there kind systemat polysemi might refer author jane austen wrote emma work love jane austen mean love book write jane austen store shelf tree might talk blossom plum fruit plum ate yesterday systemat polysemi author work author tree fruit lot kind systemat polysemi like know word one sens one test often use zeugma test consid two sens word serv flight serv breakfast lufthansa serv philadelphia mean give meal passeng mean airlin fli particular citi zeugma test say let form sentenc use sens word serv see bad sound lufthansa serv breakfast san jose well sound funni sinc conjunct sound weird say two differ sens serv say two word synonym mean context nut call hazelnut peopl filbert other mean thing couch sofa word big larg automobil car say two lexem synonym substitut situat that case say proposit mean happen perhap exampl perfect synonymi even mani aspect mean two word ident still may preserv accept depend polit slang regist genr water ho technic mean thing practic gonna use ho describ beauti day mountain look run stream even big larg seem like might synonym differ mean differ situat let look might say big plane would fli larg small plane sound like sens big larg seem like synonym refer measur size larg scale size consid sentenc miss nelson becam kind big sister benjamin miss nelson becam kind larg sister benjamin big larg mean differ thing big sister mean older sister larg sister simpli cant mean mean physic larger sister funni kind mean want say big big sens larg simpli lack big sens there sens big mean older grown larg lack sens sometim even two word seem similar mean sens may differ two word tell us synonymi realli relat sens rather relationship word say one sens big one mean physic larg one sens larg mean physic larg hard defin isnt sens sens synonymi relationship entir word anoth import relationship word sens antonymi antonym sens opposit respect one featur mean otherwis similar dark light short long fast slow word similar mean short long mean im properti length short mean im end scale long mean im end scale formal say antonym defin binari opposit opposit end scale two kind antonymi long short fast slow binari opposit revers verb like rise fall word theyr differ end scale theyr differ direct say theyr revers direct two kind antonymi relationship sens specif kind relationship sens perhap import relationship sens well see hyponymi hypernymi say one sens hyponym anoth first sens specif denot subclass say car hyponym vehicl mango hyponym fruit anoth word hyponym might subclass subordin convers hypernym superordin tri rememb say hyper super vehicl hypernym car mean supertyp car car subtyp vehicl vehicl superordin car fruit hypernym mango ive given exampl superordin like vehicl subordin like car hypernym hyponym hypernym hyponym superordin subordin synonym formal say class denot superordin hypernym extension includ class denot hyponym defin hyp hyponymi also entail say sens hyponym excus sens hyponym sens b entail b hyponymi usual transit hyponym b b hyponym c entail hyponym c anoth common name hyponymi hierarchi isa hierarchi intuit say isa b b sometim without dash also sometim say b subsum well see term isa hyponymi subsumpt hierarchi proceed cours often make distinct hyponym instanc wordnet thesauru well talk bit class instanc instanc individu proper noun uniqu entiti say san francisco instanc instanc class citi instanc relationship individu class citi class say citi hyponym municip locat kind class hyponomi subsumpt relationship class anoth class instanc relationship individu class fundament definit well need talk extract word mean,[ 4 14 13 12 11]
208,Course2_W8-S1-L2_WordNet_and_Other_Online_Thesauri_6-23,thesaurus one important repositories word meaning lecture well talk wordnet one important thesauri thesauri ontologies used sorts applications information extraction retrieval question answering lots bioinformatic applications machine translations play role sorts places one commonly used ones wordnet hierarchically organized lexical database online thesaurus also aspects dictionary wordnet english theres lots languages either available development wordnet nouns verbs also adjectives adverbs gonna definitions ontological relations example take word bass bass wordnet nouns also verbs heres list nouns senses adjectives senses bass bass wordnet sea bass fresh water bass bass voice basso wordnet sense defined whats called synset synonym set thats set near synonyms similar words instantiates sense concept also gloss example take chump noun gloss person gullible easy take advantage sense chump shared nine words chump one fool two second sense fool ninth sense word mark first sense word fallguy senses gloss every sense words example gull another sense sense two kind aquatic bird synset set near synonyms meaning particular senses words similar meaning heres wordnet hypernym hierarchy bass bass one sense bass adult male singer lowest voice see bass kind singer kind musician kind performer kind person kind organism way physical entity entity hyp hypernym hierarchy goes way top wordnet lot relations weve talked relations hypernyms hyponyms wordnet also relationships like member relation hasmember example group like faculty members like professors instances talked instances relationship composers general one composer bach one composer johann sebastian bach course theres one bach meronyms holonyms part whole relationships individuals table part leg course part meal relationships parts whole lots relationships occur wordnet antonymy well look web wordnet look word bass bass sure enough theres senses bass click one lets click one fish go ask whole set inherited hypernyms theres bass kind bass excuse kind freshwater fish kind seafood kind food way way entity bottom heres whole hypernym superordinate hierarchy standard libraries accessing word wordnet python libraries java libraries weve given pointers another important thesaurus ontology mesh medical subject headings thesaurus national library medicine almost couple hundred thousand entries corresponding lot headings heres one hemoglobin set called entry terms hemoglobin ferrous hemoglobin dont even know pronounce word syns corresponds synsets wordnet set synonyms defining hemoglobins gloss well heres mesh hierarchy top sixteen top level categories might wanna look inside one lets say chemicals drugs inside might wanna look inside amino acids look inside proteins theres hemoglobins theres hierarchy mesh ontology used provide synonyms called entry terms mesh terminology provide hypernyms hierarchy like wordnet mesh used indexing journal articles national library medicines bibliographic database medlinepubmed theres twenty million journal articles articles handassigned ten twenty mesh terms useful ontology weve seen wordnet mesh two commonly used thesauri ontologies important place get representations word meaning word relations like hyponomy synonomy,Course2,W8-S1-L2,W8,S1,L2,WordNet,8,1,2,thesauru one import repositori word mean lectur well talk wordnet one import thesauri thesauri ontolog use sort applic inform extract retriev question answer lot bioinformat applic machin translat play role sort place one commonli use one wordnet hierarch organ lexic databas onlin thesauru also aspect dictionari wordnet english there lot languag either avail develop wordnet noun verb also adject adverb gonna definit ontolog relat exampl take word bass bass wordnet noun also verb here list noun sens adject sens bass bass wordnet sea bass fresh water bass bass voic basso wordnet sens defin what call synset synonym set that set near synonym similar word instanti sens concept also gloss exampl take chump noun gloss person gullibl easi take advantag sens chump share nine word chump one fool two second sens fool ninth sens word mark first sens word fallguy sens gloss everi sens word exampl gull anoth sens sens two kind aquat bird synset set near synonym mean particular sens word similar mean here wordnet hypernym hierarchi bass bass one sens bass adult male singer lowest voic see bass kind singer kind musician kind perform kind person kind organ way physic entiti entiti hyp hypernym hierarchi goe way top wordnet lot relat weve talk relat hypernym hyponym wordnet also relationship like member relat hasmemb exampl group like faculti member like professor instanc talk instanc relationship compos gener one compos bach one compos johann sebastian bach cours there one bach meronym holonym part whole relationship individu tabl part leg cours part meal relationship part whole lot relationship occur wordnet antonymi well look web wordnet look word bass bass sure enough there sens bass click one let click one fish go ask whole set inherit hypernym there bass kind bass excus kind freshwat fish kind seafood kind food way way entiti bottom here whole hypernym superordin hierarchi standard librari access word wordnet python librari java librari weve given pointer anoth import thesauru ontolog mesh medic subject head thesauru nation librari medicin almost coupl hundr thousand entri correspond lot head here one hemoglobin set call entri term hemoglobin ferrou hemoglobin dont even know pronounc word syn correspond synset wordnet set synonym defin hemoglobin gloss well here mesh hierarchi top sixteen top level categori might wanna look insid one let say chemic drug insid might wanna look insid amino acid look insid protein there hemoglobin there hierarchi mesh ontolog use provid synonym call entri term mesh terminolog provid hypernym hierarchi like wordnet mesh use index journal articl nation librari medicin bibliograph databas medlinepubm there twenti million journal articl articl handassign ten twenti mesh term use ontolog weve seen wordnet mesh two commonli use thesauri ontolog import place get represent word mean word relat like hyponomi synonomi,[ 4 13 14 12 11]
209,Course2_W8-S1-L3_Word_Similarity_and_Thesaurus_Methods_16-17,hi lets turn compute whether two words similar well start section methods use thesaurus compute word similarity synonymy binary relation two words either synonymous theyre often want looser definition similarity distance says two words similar share features meaning meaning without requiring absolute synonyms similarity like synonymy relationship senses words dont say word bank similar word slope say sense one bank similar word fund actually sense three fund actually sense two bank similar slope remember two senses bank even though technically similarity property senses well find ways compute similarity words well either taking max similarity senses summing various ways word similarity plays role lots applications might want know two words similar grab set synonyms similar words query someone asks information retrieval answer question answering lots times translation word youll look similar words help find translation essay grading students write essay need know similar words correct word concept comes lot applications word similarity well often distinguish technically word similarity word relatedness similar word near synonym related word related way car bicycle might similar maybe theyre synonyms car automobile synonyms car bicycle quite close close therere similar car gasoline clearly related gasoline something goes cars similar cars way generally looking similarity occasionally algorithms give instead words related might might useful depending upon application two classes similarity algorithms thesaurus based algorithms well talk section words nearby hypernym hierarchy words similar glosses well use hierarchy glosses ways defining similarity distributional algorithms well talk next section words similar distributional contexts simplest thesaurus based similarity algorithms called path based similarity im giving little picture wordnet hierarchy say two concepts two senses synsets well call concepts similar theyre near thesaurus hierarchy near mean short path well define path length somewhat unusually well say concept path length one two nearest neighbor word concept nickel path length one nickel two coin three dime goes one nickel two coin three dime path length nickel coinage similarly three way money six coin coinage currency medium exchange money nickel even richter scale goes way standard richter scale path length formally one plus number edges shortest path hypernym graph sense nodes c c use turn path length distance metric simpath similarity metric simply taking one distance well take path length invert get similarity metric turn sensebased metric simpath metric similarity two senses concepts c c turn metric words taking maximum similarity among pairs senses senses word one senses word two take similarity word one senses word two senses take maximum similarity pairs thats similarity words returning sensebased similarity weve got metric path based similarity one path length nickel coin path length remember adding one number edges fund budget similarly one two nickel currency one two three edges one four nickel money six distance one six similarly coinage richter scale similarity one six matter nickel standard also one six theres problem basic pathbased similarity assumed every link represented uniform distance nickel money somehow seems us ought closer nickel standard thats nodes high hierarchy abstract wed like metric says nodes whose link going way top hierarchy probably similar words wed like represent cost edge independently common solution problem use information content similarity metrics first proposed resnik define concept p c probability concept c probability randomly selected word corpus instance concept formally mean theres distinct random variable ranges words associated concept every node hierarchy random variable concept every observed noun either member concept probability pc member concept probability one minus pc every word member root node might called entity might called something else different versions wordnet hierarchy means probability whatever root node one probability nodes right root node going high lower get hierarchy lower probability lets see works heres little piece hierarchy weve got entity theres actually something hierarchy top entity geological formation leaf notes hill ridge grotto coast gonna train information content similarity first training probability pc every instance word like hill counts towards frequency parents obviously also natural elevation geological formation way entity define concept words c words c set words children node c say plus c well words natural elevation hill ridge natural elevation words geological formation hill ridge grotto coast shore cave natural elevation geological formation take concept sum words concept children sum counts words normalize total number words corpus tells us probability concept probability random word instance concept weve computed probabilities associate hierarchy heres probabilities computed dekang lin say concept coast probability go hierarchy entity probability whatever particular version wordnet entity probability one probability need two things well define information content concept negative log probability concept following information theoretic definition information well define lowest common subsumer node lowest node hierarchy subsumes naturally lowest common subsumer hill coast think geological formation lowest common subsumer coast shore shore going use information content similarity metric number methods resnik method say similarity two words related much information common common similar resnik say whats common two words information content lowest common subsumer two concepts whats common thing share inherited thing common measure amount information fact common negative log probability least common subsumer thats similarity well define metric resnik similarity metric way alternative metric dealing information theoretic similarity lin metric resnik two things common similar new intuition differences b less similar measure commonality introducing predicate common proposition stating commonalities b ic informa amount information contained proposition measure difference b say total description b sum everything know sum commonalities plus differences get differences take description subtract commonalities roughly speaking b similar ic common high ic description low lin similarity two concepts b higher common less theres lot things dont common lin modifies resnik defining information content commonality two twice information lowest common subsumer given two concepts heirarchy lin similarity two times log probability least common subsumer sum log probabilities two concepts total description know two concepts lets look example small sample hierarchy want know lin similarity concepts hill coast look lowest common subsumer geological formation take twice log probability divided sum log probabilities two items hill coast gives us lin similarity hill coast final thesaurus based similarity metric called lesk algorithm michael lesk invented often called lesk extended lesk method instead using hierarchy looks glosses words dictionary thesaurus intuition two concepts similar glosses contain similar words two wordnet words drawing paper decal lots similar words paper especially prepared use drafting transferring designs especially prepared paper blah blah blah words specially prepared common paper definitions nword phrase thats glosses lesk algorithm adds score n squared paper glosses thats length one well add score one specially prepared length two well add score two squared four total lesk similarity drawing paper decal five fact versions lesk similarity dont look glosses two words look glosses words hypernyms hyponyms add sum words sometimes max words similarity summary weve seen three classes thesaurus based similarity path length similarity two words similar theres short path hierarchy information theoretic similarity weve seen two methods resnik lin theres third one called jiang conrath information theoretic similarity looking least common subsumer node measuring probability turning information measure weve seen lesk similarity given two concepts take gloss compute overlap words kind weighting talked might look glosses words words relations concepts like hyponyms hypernyms sum specified set relations gives us lesk similarity extended lesk similarity lots libraries computing various thesaurus based methods python nltk methods pythonbased tools like wordnetsimilarity theres even nice web based interface check similarity two words based different meth methods go take look evaluate similarity like many nlp algorithms two different ways intrinsic evaluation look metric say similar numbers metric gives humans would give similar task get similarity metric two words get humans give number similar two words compare thats intrinsic evaluation functional extrinsic task based endtoend evaluations application put similarity metric application see well improves application application could words sense disambiguation spelling error correction essay grading common simple extrinsic test thats used taking test english foreign language toefl multiple choice vocabulary test questions like levied closest meaning four words simply take similarity metric compute similarity levied imposed levied believed levied requested levied correlated see metric returns right answer similar word correct answer imposed thesaurus based methods word similarity useful way telling two words similar theyre functional languages like english lots thesauruses either general text wordnet medical text mesh work less well working particular genres might right information thesaurus languages thesaurus thesauruses available applications well turn distributional methods well see next,Course2,W8-S1-L3,W8,S1,L3,Word,8,1,3,hi let turn comput whether two word similar well start section method use thesauru comput word similar synonymi binari relat two word either synonym theyr often want looser definit similar distanc say two word similar share featur mean mean without requir absolut synonym similar like synonymi relationship sens word dont say word bank similar word slope say sens one bank similar word fund actual sens three fund actual sens two bank similar slope rememb two sens bank even though technic similar properti sens well find way comput similar word well either take max similar sens sum variou way word similar play role lot applic might want know two word similar grab set synonym similar word queri someon ask inform retriev answer question answer lot time translat word youll look similar word help find translat essay grade student write essay need know similar word correct word concept come lot applic word similar well often distinguish technic word similar word related similar word near synonym relat word relat way car bicycl might similar mayb theyr synonym car automobil synonym car bicycl quit close close therer similar car gasolin clearli relat gasolin someth goe car similar car way gener look similar occasion algorithm give instead word relat might might use depend upon applic two class similar algorithm thesauru base algorithm well talk section word nearbi hypernym hierarchi word similar gloss well use hierarchi gloss way defin similar distribut algorithm well talk next section word similar distribut context simplest thesauru base similar algorithm call path base similar im give littl pictur wordnet hierarchi say two concept two sens synset well call concept similar theyr near thesauru hierarchi near mean short path well defin path length somewhat unusu well say concept path length one two nearest neighbor word concept nickel path length one nickel two coin three dime goe one nickel two coin three dime path length nickel coinag similarli three way money six coin coinag currenc medium exchang money nickel even richter scale goe way standard richter scale path length formal one plu number edg shortest path hypernym graph sens node c c use turn path length distanc metric simpath similar metric simpli take one distanc well take path length invert get similar metric turn sensebas metric simpath metric similar two sens concept c c turn metric word take maximum similar among pair sens sens word one sens word two take similar word one sens word two sens take maximum similar pair that similar word return sensebas similar weve got metric path base similar one path length nickel coin path length rememb ad one number edg fund budget similarli one two nickel currenc one two three edg one four nickel money six distanc one six similarli coinag richter scale similar one six matter nickel standard also one six there problem basic pathbas similar assum everi link repres uniform distanc nickel money somehow seem us ought closer nickel standard that node high hierarchi abstract wed like metric say node whose link go way top hierarchi probabl similar word wed like repres cost edg independ common solut problem use inform content similar metric first propos resnik defin concept p c probabl concept c probabl randomli select word corpu instanc concept formal mean there distinct random variabl rang word associ concept everi node hierarchi random variabl concept everi observ noun either member concept probabl pc member concept probabl one minu pc everi word member root node might call entiti might call someth els differ version wordnet hierarchi mean probabl whatev root node one probabl node right root node go high lower get hierarchi lower probabl let see work here littl piec hierarchi weve got entiti there actual someth hierarchi top entiti geolog format leaf note hill ridg grotto coast gonna train inform content similar first train probabl pc everi instanc word like hill count toward frequenc parent obvious also natur elev geolog format way entiti defin concept word c word c set word children node c say plu c well word natur elev hill ridg natur elev word geolog format hill ridg grotto coast shore cave natur elev geolog format take concept sum word concept children sum count word normal total number word corpu tell us probabl concept probabl random word instanc concept weve comput probabl associ hierarchi here probabl comput dekang lin say concept coast probabl go hierarchi entiti probabl whatev particular version wordnet entiti probabl one probabl need two thing well defin inform content concept neg log probabl concept follow inform theoret definit inform well defin lowest common subsum node lowest node hierarchi subsum natur lowest common subsum hill coast think geolog format lowest common subsum coast shore shore go use inform content similar metric number method resnik method say similar two word relat much inform common common similar resnik say what common two word inform content lowest common subsum two concept what common thing share inherit thing common measur amount inform fact common neg log probabl least common subsum that similar well defin metric resnik similar metric way altern metric deal inform theoret similar lin metric resnik two thing common similar new intuit differ b less similar measur common introduc predic common proposit state common b ic informa amount inform contain proposit measur differ b say total descript b sum everyth know sum common plu differ get differ take descript subtract common roughli speak b similar ic common high ic descript low lin similar two concept b higher common less there lot thing dont common lin modifi resnik defin inform content common two twice inform lowest common subsum given two concept heirarchi lin similar two time log probabl least common subsum sum log probabl two concept total descript know two concept let look exampl small sampl hierarchi want know lin similar concept hill coast look lowest common subsum geolog format take twice log probabl divid sum log probabl two item hill coast give us lin similar hill coast final thesauru base similar metric call lesk algorithm michael lesk invent often call lesk extend lesk method instead use hierarchi look gloss word dictionari thesauru intuit two concept similar gloss contain similar word two wordnet word draw paper decal lot similar word paper especi prepar use draft transfer design especi prepar paper blah blah blah word special prepar common paper definit nword phrase that gloss lesk algorithm add score n squar paper gloss that length one well add score one special prepar length two well add score two squar four total lesk similar draw paper decal five fact version lesk similar dont look gloss two word look gloss word hypernym hyponym add sum word sometim max word similar summari weve seen three class thesauru base similar path length similar two word similar there short path hierarchi inform theoret similar weve seen two method resnik lin there third one call jiang conrath inform theoret similar look least common subsum node measur probabl turn inform measur weve seen lesk similar given two concept take gloss comput overlap word kind weight talk might look gloss word word relat concept like hyponym hypernym sum specifi set relat give us lesk similar extend lesk similar lot librari comput variou thesauru base method python nltk method pythonbas tool like wordnetsimilar there even nice web base interfac check similar two word base differ meth method go take look evalu similar like mani nlp algorithm two differ way intrins evalu look metric say similar number metric give human would give similar task get similar metric two word get human give number similar two word compar that intrins evalu function extrins task base endtoend evalu applic put similar metric applic see well improv applic applic could word sens disambigu spell error correct essay grade common simpl extrins test that use take test english foreign languag toefl multipl choic vocabulari test question like levi closest mean four word simpli take similar metric comput similar levi impos levi believ levi request levi correl see metric return right answer similar word correct answer impos thesauru base method word similar use way tell two word similar theyr function languag like english lot thesaurus either gener text wordnet medic text mesh work less well work particular genr might right inform thesauru languag thesauru thesaurus avail applic well turn distribut method well see next,[ 4 14 13 12 11]
210,Course2_W8-S1-L4_Word_Similarity-_Distributional_Similarity_I_13-14,alright lets turn second important method computing word similarity distributional similarity thesauri problems computing similarity dont always thesaurus particular language even thesauri problem recall words missing phrases missing connections senses may missing general thesauri dont work well verbs adjectives less structured hyponymy relations reasons often use distributional models meaning often called vectorspace models meaning tend give higher recall handbuilt thesauri although might lower precision intuition distributional models meaning comes early linguistic work example zellig harris said oculist eye doctor occur almost environments b almost identical environments say theyre synonyms firth back said shall know word company keeps heres example bunch sentences tesgüino nida says bottle tesgüinos table everybody likes tesgüino tesgüino makes drunk make tesgüino corn context words human easy guess tesgüino means kind alcoholic beverage kind beer made corn intuition algorithm two words similar surrounded similar words simple idea lets see make work remember termdocument matrix saw information retrieval cell termdocument matrix count term document called term frequency thought meant document count vector column term document matrix document like shakespeare play count vector lots words im showing four words battle soldier fool clown like count vector two documents similar vectors similar julius caesar high counts battle soldier low counts fool clown similarly henry fifth high counts battle soldier low counts fool clown saw julius caesar henry fifth query document two documents similar vector similarity method gonna use exact intuition deciding two words similar look words term document matrix word count vector two words similar vectors similar fool high counts document like document twelfth night low counts julius caesar henry fifth clown high counts like twelfth night low counts julius caesar henry fifth say fool clown similar probably battle fool similar battle high counts low counts fool opposite high counts low counts intuition word similarity distributional word similarity instead using entire documents like used information retrieval lets use smaller context could use paragraph could use window ten words define word vector context counts whatever context lets suppose use context ten words left ten words right heres sample example ive grabbed brown corpus heres words word apricot word pineapple word digital word information ive shown one set context ten words apricot ten words apricot one uses apricot one one document corpus heres ten words pineapple various documents examples grabbed brown corpus examples words looking like examples compute little counts build termcontext matrix heres termcontext matrix four words apricot pineapple digital information dont ever appear word aardvark words digital information word computer within ten words twice digital information word pinch occurs apricot pineapple word sugar occurs whereas word data word result tend occur word digital word information say two words similar meaning context vectors similar apricot one pinch one sugar pineapple also one pinch one sugar zero computer data tells us probably words similar whereas digital information counts occur words like computer data result theyre probably similar well simple intuition like saw information retrieval comparing documents comparing words using reduced context termdocument matrix information retrieval used tfidf weighting didnt use raw counts used various kinds weighting sometimes tf sometimes idf sometimes termcontext matrix common use version pointwise mutual information called positive pointwise mutual information lets look pointwise mutual information information theoretic method says events x occur often independent pointwise mutual information two things x probability two occurring together divided probability two product probability two independently take log see two things occur often would expect chance often expect independence numerator much higher denominator two words pointwise mutual information log probability two words occurring together times product two words occurring independently positive pmi simply replaces negative values zero imagine matrix f termcontext matrix well call f frequency weve got words labeled rows labeled words columns labeled contexts could context word saw example context word aardvark computer data pinch gonna take heres counts counts f sub ij frequency row column j going turn probabilities first well say probability word j occurring together joint probability word context j frequency appear normalized sum frequency words contexts sum entire matrix thats denominator n probability word thats row ii count contextsi word occur sum possible contexts sum counts word contexts normalized n probability context sum words context occurs c counts normalized take probabilities probability word context occurring together times probability wor probability word times probability context take log thats pmi positive pmi less zero replace zero lets see works practice weve got little made simpler little matrix counts working example word digital occurs context computer twice context data context result never pinch sugar okay saw probability joint probability word context frequency word occurs context normalized total n sum counts words contexts lets first compute n n sum things two three four ten eleven twelve thirteen fourteen n nineteen weve got nineteen denominators gonna nineteen various probabilities probability joint probability word information context data word information context data weve got f sub ij six n nineteen ths thats joint probability information data need compute probabilities words probabilities contexts probability word sum row contexts word occur word information occurs eleven times context plus six plus four total eleven times n thats probability word thing contexts sum words context occurs counts normalize context data one plus six seven nineteen computation examples well get little matrix joint probabilities joint probabilities marginals probability words probability contexts heres thats probability data heres thats probability word information theres good ready compute pmi recall pmi log base two joint two marginals thats going divided times well take log thats using full precision see take log get positive numbers computer linked digital information data pineapple sugar weve replaced time got negative numbers since positive pmi replaced negative numbers zeroes problem pmi weighted toward infrequent events theres lovely survey paper turney pentel walks ways alleviate turns simple methods like laplace smoothing actually help lets look works addtwo smoothing ive added counts simple addtwo smoothing recomputed probability table marginals context word ive shown positive pmi tables heres original table smoothing addtwo smoothing youll notice original table without smoothing link apricot pineapple sugar high get high pmi despite fact dont know remember counts one whereas data lot evidence counts six four link information data information result much lower pmi values adding two affects lower counts higher counts see larger counts havent changed much weve discounted excessively high pmi values much reasonable related numbers pmi thats first half introduction distributional similarity well turn second half use cosine metrics actually compute similarity,Course2,W8-S1-L4,W8,S1,L4,Word,8,1,4,alright let turn second import method comput word similar distribut similar thesauri problem comput similar dont alway thesauru particular languag even thesauri problem recal word miss phrase miss connect sens may miss gener thesauri dont work well verb adject less structur hyponymi relat reason often use distribut model mean often call vectorspac model mean tend give higher recal handbuilt thesauri although might lower precis intuit distribut model mean come earli linguist work exampl zellig harri said oculist eye doctor occur almost environ b almost ident environ say theyr synonym firth back said shall know word compani keep here exampl bunch sentenc tesgüino nida say bottl tesgüino tabl everybodi like tesgüino tesgüino make drunk make tesgüino corn context word human easi guess tesgüino mean kind alcohol beverag kind beer made corn intuit algorithm two word similar surround similar word simpl idea let see make work rememb termdocu matrix saw inform retriev cell termdocu matrix count term document call term frequenc thought meant document count vector column term document matrix document like shakespear play count vector lot word im show four word battl soldier fool clown like count vector two document similar vector similar juliu caesar high count battl soldier low count fool clown similarli henri fifth high count battl soldier low count fool clown saw juliu caesar henri fifth queri document two document similar vector similar method gonna use exact intuit decid two word similar look word term document matrix word count vector two word similar vector similar fool high count document like document twelfth night low count juliu caesar henri fifth clown high count like twelfth night low count juliu caesar henri fifth say fool clown similar probabl battl fool similar battl high count low count fool opposit high count low count intuit word similar distribut word similar instead use entir document like use inform retriev let use smaller context could use paragraph could use window ten word defin word vector context count whatev context let suppos use context ten word left ten word right here sampl exampl ive grab brown corpu here word word apricot word pineappl word digit word inform ive shown one set context ten word apricot ten word apricot one use apricot one one document corpu here ten word pineappl variou document exampl grab brown corpu exampl word look like exampl comput littl count build termcontext matrix here termcontext matrix four word apricot pineappl digit inform dont ever appear word aardvark word digit inform word comput within ten word twice digit inform word pinch occur apricot pineappl word sugar occur wherea word data word result tend occur word digit word inform say two word similar mean context vector similar apricot one pinch one sugar pineappl also one pinch one sugar zero comput data tell us probabl word similar wherea digit inform count occur word like comput data result theyr probabl similar well simpl intuit like saw inform retriev compar document compar word use reduc context termdocu matrix inform retriev use tfidf weight didnt use raw count use variou kind weight sometim tf sometim idf sometim termcontext matrix common use version pointwis mutual inform call posit pointwis mutual inform let look pointwis mutual inform inform theoret method say event x occur often independ pointwis mutual inform two thing x probabl two occur togeth divid probabl two product probabl two independ take log see two thing occur often would expect chanc often expect independ numer much higher denomin two word pointwis mutual inform log probabl two word occur togeth time product two word occur independ posit pmi simpli replac neg valu zero imagin matrix f termcontext matrix well call f frequenc weve got word label row label word column label context could context word saw exampl context word aardvark comput data pinch gonna take here count count f sub ij frequenc row column j go turn probabl first well say probabl word j occur togeth joint probabl word context j frequenc appear normal sum frequenc word context sum entir matrix that denomin n probabl word that row ii count contextsi word occur sum possibl context sum count word context normal n probabl context sum word context occur c count normal take probabl probabl word context occur togeth time probabl wor probabl word time probabl context take log that pmi posit pmi less zero replac zero let see work practic weve got littl made simpler littl matrix count work exampl word digit occur context comput twice context data context result never pinch sugar okay saw probabl joint probabl word context frequenc word occur context normal total n sum count word context let first comput n n sum thing two three four ten eleven twelv thirteen fourteen n nineteen weve got nineteen denomin gonna nineteen variou probabl probabl joint probabl word inform context data word inform context data weve got f sub ij six n nineteen th that joint probabl inform data need comput probabl word probabl context probabl word sum row context word occur word inform occur eleven time context plu six plu four total eleven time n that probabl word thing context sum word context occur count normal context data one plu six seven nineteen comput exampl well get littl matrix joint probabl joint probabl margin probabl word probabl context here that probabl data here that probabl word inform there good readi comput pmi recal pmi log base two joint two margin that go divid time well take log that use full precis see take log get posit number comput link digit inform data pineappl sugar weve replac time got neg number sinc posit pmi replac neg number zero problem pmi weight toward infrequ event there love survey paper turney pentel walk way allevi turn simpl method like laplac smooth actual help let look work addtwo smooth ive ad count simpl addtwo smooth recomput probabl tabl margin context word ive shown posit pmi tabl here origin tabl smooth addtwo smooth youll notic origin tabl without smooth link apricot pineappl sugar high get high pmi despit fact dont know rememb count one wherea data lot evid count six four link inform data inform result much lower pmi valu ad two affect lower count higher count see larger count havent chang much weve discount excess high pmi valu much reason relat number pmi that first half introduct distribut similar well turn second half use cosin metric actual comput similar,[ 4  2 14 13 12]
211,Course2_W8-S1-L5_Word_Similarity-_Distributional_Similarity_II_8-15,right previous segment saw compute ppmi positive pointwise mutual information segment well see take values compute similarity words first lets talk different kind context word context common way define context use syntax relates back early linguistic arguments meaning restriction combination entities grammatically words two words similar similar parse contexts havent talked lot parsing give intuition words duty word responsibility modified similar adjectives additional duty administrative duty assumed duty additional responsibility administrative responsibility assumed responsibility also objects similar verbs assert duty assign duty assume duty assign responsibility assume responsibility similar words around grammatical context similar similar parse contexts capture using cooccurrence vectors based syntactic dependencies say context instead counts words previous ten words following ten words context instead many times particular word subject many times particular word adjectival modifier heres example dekang lin word sell say often subject verb absorb well often subject verb adapt subject verb behave prepositional object verb inside get counts contexts vector determined counts words occur within ten words counts times occur particular grammatical relation context saw word counts use pmi ppmi dependency relations intuition comes early work hindle count corpus parse corpus see verb drink object three times drink anything three times drink wine twice drink liquid twice well drink drink anything fact common drink wine wed like say wine drinkable thing found wine occurring lot verb two different verbs would think verbs probably similar found occurring object two verbs compute pmis pmi object verb verb drink see wine tea liquid higher pmi anything sort pmi see tea liquid wine associated words object verb drink pmi used noun associations words context excuse word associations words context also associations dependency relations right weve seen compute term context word context matrix weight pmi talked computing two ways based words neighborhood ten words whether im particular syntactic grammatical relationship words ready use compute actual similarity cosine metric going use cosine saw information retrieval remember dot product said cosine similarity two vectors two vectors indicating counts words dot product similarity dot product two words normalized length two vectors dot product v·w length v times length w could compute think computing separate unit vectors normalizing v length normalizing w length get unit vectors multiplying together compute whole thing heres dot product dimension v dimension w multiply values together normalize square root sum squared values get length vectors lets say ppmi v sub ppmi value word v context w sub ppmi value word w context remember cosine metric two vectors orthogonal theyre gonna cosine zero point opposite directions theyll cosine minus one point direction theyll cosine plus one turns raw frequency ppmi nonnegative theyre always zero greater means cosine range always zero one always part slope cosine similarity metric use ppmi weighted counts gonna get raw frequency matter gonna get number zero one lets compute use cosine compute similarity ive taken little subset example saw earlier apricot digital information context vector large data computer im going use counts instead ppmi values making example simple see real life course wed use ppmi cosine apricot information dot product apricot information one times one plus zero times six plus zero times one one plus zero plus zero square root length apricot one squared plus zero squared plus zero squared length information one squared plus six squared plus one squared thats gonna one square root similarly cosine digital information digital information zero times one plus one times six plus two times one thats gonna zero plus six plus two square root length digital thats length digital im sorry thats square root zero squared plus one squared plus two squared root zero plus one plus four length information saw get similarly apricot digital dot product apricot digital one time zero zero times one zero times two zero gonna get similarity zero number possible similarity metrics besides cosine use jaccard method saw earlier information retrieval dice metric theres family information theoretic methods like jensenshen divergence also used similarity cosine probably popular similarity distributional methods evaluated thesaurus based methods either intrinsic evaluation compute correlation human number human word similarity number extrinsically pick task like taking toefl exam detecting spelling errors compute error rate see similarity metric results better error rate summary distributional similarity metrics use method association like ppmi metric similarity like cosine give similarity two words distributional thesaurus based methods useful deciding two words similar nlp application,Course2,W8-S1-L5,W8,S1,L5,Word,8,1,5,right previou segment saw comput ppmi posit pointwis mutual inform segment well see take valu comput similar word first let talk differ kind context word context common way defin context use syntax relat back earli linguist argument mean restrict combin entiti grammat word two word similar similar pars context havent talk lot pars give intuit word duti word respons modifi similar adject addit duti administr duti assum duti addit respons administr respons assum respons also object similar verb assert duti assign duti assum duti assign respons assum respons similar word around grammat context similar similar pars context captur use cooccurr vector base syntact depend say context instead count word previou ten word follow ten word context instead mani time particular word subject mani time particular word adjectiv modifi here exampl dekang lin word sell say often subject verb absorb well often subject verb adapt subject verb behav preposit object verb insid get count context vector determin count word occur within ten word count time occur particular grammat relat context saw word count use pmi ppmi depend relat intuit come earli work hindl count corpu pars corpu see verb drink object three time drink anyth three time drink wine twice drink liquid twice well drink drink anyth fact common drink wine wed like say wine drinkabl thing found wine occur lot verb two differ verb would think verb probabl similar found occur object two verb comput pmi pmi object verb verb drink see wine tea liquid higher pmi anyth sort pmi see tea liquid wine associ word object verb drink pmi use noun associ word context excus word associ word context also associ depend relat right weve seen comput term context word context matrix weight pmi talk comput two way base word neighborhood ten word whether im particular syntact grammat relationship word readi use comput actual similar cosin metric go use cosin saw inform retriev rememb dot product said cosin similar two vector two vector indic count word dot product similar dot product two word normal length two vector dot product v·w length v time length w could comput think comput separ unit vector normal v length normal w length get unit vector multipli togeth comput whole thing here dot product dimens v dimens w multipli valu togeth normal squar root sum squar valu get length vector let say ppmi v sub ppmi valu word v context w sub ppmi valu word w context rememb cosin metric two vector orthogon theyr gonna cosin zero point opposit direct theyll cosin minu one point direct theyll cosin plu one turn raw frequenc ppmi nonneg theyr alway zero greater mean cosin rang alway zero one alway part slope cosin similar metric use ppmi weight count gonna get raw frequenc matter gonna get number zero one let comput use cosin comput similar ive taken littl subset exampl saw earlier apricot digit inform context vector larg data comput im go use count instead ppmi valu make exampl simpl see real life cours wed use ppmi cosin apricot inform dot product apricot inform one time one plu zero time six plu zero time one one plu zero plu zero squar root length apricot one squar plu zero squar plu zero squar length inform one squar plu six squar plu one squar that gonna one squar root similarli cosin digit inform digit inform zero time one plu one time six plu two time one that gonna zero plu six plu two squar root length digit that length digit im sorri that squar root zero squar plu one squar plu two squar root zero plu one plu four length inform saw get similarli apricot digit dot product apricot digit one time zero zero time one zero time two zero gonna get similar zero number possibl similar metric besid cosin use jaccard method saw earlier inform retriev dice metric there famili inform theoret method like jensenshen diverg also use similar cosin probabl popular similar distribut method evalu thesauru base method either intrins evalu comput correl human number human word similar number extrins pick task like take toefl exam detect spell error comput error rate see similar metric result better error rate summari distribut similar metric use method associ like ppmi metric similar like cosin give similar two word distribut thesauru base method use decid two word similar nlp applic,[ 4  2  0 14 13]
212,Course2_W8-S2-L1_What_is_Question_Answering_7-28,welcome back today gonna look question answering one oldest applications natural language processing first nlp tasks built punch card systems early include simmons et al system lets look answered questions like worms eat taking question parsing dependency format relationship eat worms eat looking matching dependency sentence somewhere large database answers sentences like worms eat grass birds eat worms horses worms eat grass idea dependency treelet worms eat matched worms eat grass even grass eaten worms horses eat grass birds eat worms idea answering question finding sentence looks like question answers old intuition gonna see underlies many modern systems well may know last year ibms watson jeopardy challenge answering questions example bram stoker novel inspired william wilkinsons book novel course dracula author bram stoker may also know apples siri another question answering application ask questions like need umbrella tomorrow san francisco check weather tell days gonna rain another question answering application wolfram alpha ask questions like many calories two slices banana cream pie give answer itll give little variables lets itll tell semantics answer looking amount pie thats two slices banana cream type asking total calories theres answer calories kind questions factoid questions questions like many calories two slices apple pie wrote universal declaration human rights average age onset autism kind questions pretty simple questions answered simple fact often named entity modern systems also deal complex narrative questions children acute febrile illness efficacy acetaminophen reducing fever scholars think jeffersons position dealing pirates complex questions generally answered research systems whereas factoid question answering widely used commercial application kind factoid questions questions locations louvre located paris france abbreviation means names ravens odin names currency ingredient instrument phone number theyre simple answers often single phrase named entity two main paradigms question answering irbased approach well spend lot time today thats pioneered annual trec evaluations used commercial systems like ibms googles ir based approach go find answer string web knowledge based approach build answer understanding parse question hybrid approaches take combination approaches might use online databases might use information retrieval approaches find sentences build answers modern systems would say kind hybrid knowledge information retrieval like perhaps wolfram alpha purely knowledge based systems use little bit simple web search actually answers lot questions already thats really intuition underlying ir based approach question answering well talk asked google names odins ravens first page get answer snippet google returns answer title page talks names ravens kinds simple questions like louvre museum located also answerable methods like google fact google applies modern question answering techniques give best guess location pulling semistructured resources like wikipedia answerscom lets see factoid questionanswering algorithm works general algorithm irbased factoid question answering starts question begins extracting information question two important common things extract question query thats going sent ir engine type answer tells us kind named entity looking advance take whole lot documents index query return whole lot documents documents extract passages parts documents passages processed answer processing looking type answer know looking returning actual answer thats general process well walk pieces today another way looking set processes three bigs parts algorithm question processing detecting question type formulating queries passage retrieval given queries retrieve documents break pieces answer processing extracting answers text snippets rank candidates using evidence text kinds sources contrast knowledge based approach well talk less today builds pure semantic representation query true siri wolfram alpha theyre gonna come semantic representation language questions understand might pick subdomain youre able build perfect semantic representation times dates locations scientific questions mathematical questions semantics map structured databases structured resources geospatial databases ontologies restaurant review systems things build pure semantics approach hybrid approaches ibm watson good example lots others build shallow semantic representation query processing query use ir methods generate many candidate answers use knowledgebased approaches use spatial databases temporal reasoning score candidates ir methods find possible candidates knowledge based methods score question answering one oldest topics natural language processing also one newest exciting research areas commercial potential,Course2,W8-S2-L1,W8,S2,L1,What,8,2,1,welcom back today gonna look question answer one oldest applic natur languag process first nlp task built punch card system earli includ simmon et al system let look answer question like worm eat take question pars depend format relationship eat worm eat look match depend sentenc somewher larg databas answer sentenc like worm eat grass bird eat worm hors worm eat grass idea depend treelet worm eat match worm eat grass even grass eaten worm hors eat grass bird eat worm idea answer question find sentenc look like question answer old intuit gonna see underli mani modern system well may know last year ibm watson jeopardi challeng answer question exampl bram stoker novel inspir william wilkinson book novel cours dracula author bram stoker may also know appl siri anoth question answer applic ask question like need umbrella tomorrow san francisco check weather tell day gonna rain anoth question answer applic wolfram alpha ask question like mani calori two slice banana cream pie give answer itll give littl variabl let itll tell semant answer look amount pie that two slice banana cream type ask total calori there answer calori kind question factoid question question like mani calori two slice appl pie wrote univers declar human right averag age onset autism kind question pretti simpl question answer simpl fact often name entiti modern system also deal complex narr question children acut febril ill efficaci acetaminophen reduc fever scholar think jefferson posit deal pirat complex question gener answer research system wherea factoid question answer wide use commerci applic kind factoid question question locat louvr locat pari franc abbrevi mean name raven odin name currenc ingredi instrument phone number theyr simpl answer often singl phrase name entiti two main paradigm question answer irbas approach well spend lot time today that pioneer annual trec evalu use commerci system like ibm googl ir base approach go find answer string web knowledg base approach build answer understand pars question hybrid approach take combin approach might use onlin databas might use inform retriev approach find sentenc build answer modern system would say kind hybrid knowledg inform retriev like perhap wolfram alpha pure knowledg base system use littl bit simpl web search actual answer lot question alreadi that realli intuit underli ir base approach question answer well talk ask googl name odin raven first page get answer snippet googl return answer titl page talk name raven kind simpl question like louvr museum locat also answer method like googl fact googl appli modern question answer techniqu give best guess locat pull semistructur resourc like wikipedia answerscom let see factoid questionansw algorithm work gener algorithm irbas factoid question answer start question begin extract inform question two import common thing extract question queri that go sent ir engin type answer tell us kind name entiti look advanc take whole lot document index queri return whole lot document document extract passag part document passag process answer process look type answer know look return actual answer that gener process well walk piec today anoth way look set process three big part algorithm question process detect question type formul queri passag retriev given queri retriev document break piec answer process extract answer text snippet rank candid use evid text kind sourc contrast knowledg base approach well talk less today build pure semant represent queri true siri wolfram alpha theyr gonna come semant represent languag question understand might pick subdomain your abl build perfect semant represent time date locat scientif question mathemat question semant map structur databas structur resourc geospati databas ontolog restaur review system thing build pure semant approach hybrid approach ibm watson good exampl lot other build shallow semant represent queri process queri use ir method gener mani candid answer use knowledgebas approach use spatial databas tempor reason score candid ir method find possibl candid knowledg base method score question answer one oldest topic natur languag process also one newest excit research area commerci potenti,[ 9  4  2 13 14]
213,Course2_W8-S2-L2_Answer_Types_and_Query_Formulation_8-47,first step kind question answering answer type detection query formulation going back box diagram flow chart question first thing going question processing understand whats asked many five things normally extract question perhaps important answer type named entity person place date tells us looking factoid question query formulation equally important thats set words gonna send ir engine tells us look passages likely answer others kinds things depending system may also extracted question type classification wanna know example definition question might find definition dictionary build one math question might wanna answer mathematics directly rather going find text snippets list question looking lists things might wanna look list sources kinds things like focus detection find question words replaced answer relation extraction find relations entities question talked relation extraction earlier used systems others heres example kind things might extract question jeopardy kind question theyre two states could reentering youre crossing floridas northern border looking state us state answer type state might want send ir engine two states border florida north words gonna good words find passages might answer focus thing trying answer two states relations might extract relations express answer answer unknown thing borders relation florida north something thats north florida databases express geographical information like state borders state might look relation databases first step answer type detection need named entities see question like founded virgin airlines answer type person see question like canadian city largest population answer city relatively simple need answer type taxonomy type named entities might useful question answering example heres taxonomy li roth talked six coarse classes might ask abbreviations entities descriptions entities people places numbers know kind things might inside might specific classes might asking city country mountain type location could asking person group people could asking sorts different kinds entities heres answer type taxonomy showing individual subtypes major six entity types numeric question could date question distance question percent question kind types might wanna find showing small font come back look later kind answer types li roth taxonomy might currencies diseases foods instruments examples locations might mountains states sorts things numbers speeds sizes temperatures jeopardy system also used lot answer types nice analysis looking answer types inside questions found frequent answer types covers less half data heres frequent jeopardy answer types see jeopardy tend asking people movies countries cities heres people authors still theres pretty broad distribution possible answer types gonna need large number answer type detectors really get large set questions something like jeopardy even easier factoid questions answer type detection like almost everything weve seen far hand written rules machine learning hybrids two going see three modern systems example kinds answers might well regular expression rules see form named entity person guess asking person question see answer person year dash year know looking person kind rules gonna using question head word head word generally head first noun phrase wh word wh word head word city wh word heres noun phrase state flower california head flower head word often tell us lot answer type tells us looking city tells us looking flower often rather simply writing handwritten rules treat problem machine learning define taxonomy question types weve seen already annotate training data question type well train classifiers using rich features might include handwritten rules weve talked decide question type question whats answer type particular question features going use answer types words phrases answer parts speech tag might use head words talked named entities semantically related words kind regular expressions hand write particular question patterns next step factoid qa weve done weve done answer type detection next step query formulation decide words send ir engine return documents passages one well known keyword selection algorithm moldovan et al uses number heuristics tell keywords question might important keywords put query see words quotations thats likely word people looking another important thing look proper names kinds nominals maybe less important verbs even less important adverbs maybe words maybe even less important ranking words important put query example heres slide mihai surdeanu question like coined term cyberspace novel neuromancer well might throw stop words might say well cyperspace neuromancer quotes probably things definitely putting query term novel nominals saw nominals important thing ranked rank four verb might rank seven might say extract two terms extremely likely want query something thats little less likely couple less likely things even less likely form query various ways could try sending things rank one could send go enough query words make long query send things rank one see many queries get back theres enough add query terms ranking queries depends exactly database querying whether web smaller database weve seen first step factoidbased irbased factoid questionanswering extracting answer type question extracting query terms gonna send ir engine,Course2,W8-S2-L2,W8,S2,L2,Answer,8,2,2,first step kind question answer answer type detect queri formul go back box diagram flow chart question first thing go question process understand what ask mani five thing normal extract question perhap import answer type name entiti person place date tell us look factoid question queri formul equal import that set word gonna send ir engin tell us look passag like answer other kind thing depend system may also extract question type classif wanna know exampl definit question might find definit dictionari build one math question might wanna answer mathemat directli rather go find text snippet list question look list thing might wanna look list sourc kind thing like focu detect find question word replac answer relat extract find relat entiti question talk relat extract earlier use system other here exampl kind thing might extract question jeopardi kind question theyr two state could reenter your cross florida northern border look state us state answer type state might want send ir engin two state border florida north word gonna good word find passag might answer focu thing tri answer two state relat might extract relat express answer answer unknown thing border relat florida north someth that north florida databas express geograph inform like state border state might look relat databas first step answer type detect need name entiti see question like found virgin airlin answer type person see question like canadian citi largest popul answer citi rel simpl need answer type taxonomi type name entiti might use question answer exampl here taxonomi li roth talk six coars class might ask abbrevi entiti descript entiti peopl place number know kind thing might insid might specif class might ask citi countri mountain type locat could ask person group peopl could ask sort differ kind entiti here answer type taxonomi show individu subtyp major six entiti type numer question could date question distanc question percent question kind type might wanna find show small font come back look later kind answer type li roth taxonomi might currenc diseas food instrument exampl locat might mountain state sort thing number speed size temperatur jeopardi system also use lot answer type nice analysi look answer type insid question found frequent answer type cover less half data here frequent jeopardi answer type see jeopardi tend ask peopl movi countri citi here peopl author still there pretti broad distribut possibl answer type gonna need larg number answer type detector realli get larg set question someth like jeopardi even easier factoid question answer type detect like almost everyth weve seen far hand written rule machin learn hybrid two go see three modern system exampl kind answer might well regular express rule see form name entiti person guess ask person question see answer person year dash year know look person kind rule gonna use question head word head word gener head first noun phrase wh word wh word head word citi wh word here noun phrase state flower california head flower head word often tell us lot answer type tell us look citi tell us look flower often rather simpli write handwritten rule treat problem machin learn defin taxonomi question type weve seen alreadi annot train data question type well train classifi use rich featur might includ handwritten rule weve talk decid question type question what answer type particular question featur go use answer type word phrase answer part speech tag might use head word talk name entiti semant relat word kind regular express hand write particular question pattern next step factoid qa weve done weve done answer type detect next step queri formul decid word send ir engin return document passag one well known keyword select algorithm moldovan et al use number heurist tell keyword question might import keyword put queri see word quotat that like word peopl look anoth import thing look proper name kind nomin mayb less import verb even less import adverb mayb word mayb even less import rank word import put queri exampl here slide mihai surdeanu question like coin term cyberspac novel neuromanc well might throw stop word might say well cyperspac neuromanc quot probabl thing definit put queri term novel nomin saw nomin import thing rank rank four verb might rank seven might say extract two term extrem like want queri someth that littl less like coupl less like thing even less like form queri variou way could tri send thing rank one could send go enough queri word make long queri send thing rank one see mani queri get back there enough add queri term rank queri depend exactli databas queri whether web smaller databas weve seen first step factoidbas irbas factoid questionansw extract answer type question extract queri term gonna send ir engin,[ 9 13  4 10  2]
214,Course2_W8-S2-L3_Passage_Retrieval_and_Answer_Extraction_6-38,next step question answering passage retrieval answer extraction looking flowchart weve processed question weve decided queries send ir engine job document retrieval find documents query words extract documents pull passages might relevant answer fragments well three steps retrieve documents ir engine segment document shorter units often use paragraph breaks passage ranking reranking passages depending well likely contain answer question third step passage ranking want talk weve seen ir already segmentation relatively simple might use paragraphs similar kinds things hard part really ive got whole lot passages came back breaking documents pieces ones contain answer kind features get used passage ranking might use rulebased classifier might use supervised machine learning might ask many named entities answer type occur passage im asking person date many people dates passage none ive got bad passage many query words occur passage know occurred document wanna know many also occurred particular passage instead words might look entire ngrams might look close query keywords occur passage ive got two three keywords right next im probably something related take longest sequence question words ask long thats another feature use course document passage already ranked rank document might useful feature throw things together ive done step weve weve got facts question weve retrieved ranked bunch answer passages last step pulling answer passages answer processing first thing answer extraction run named entity tagger passages answer type weve gotta named entity tagger detects answer type know looking city thats help dont tagger find cities raw text could full named entity tagger simple regular expression hybrid job return string right type example person question prime minister india going want able detect answers passages passage mannohan singh prime minister india want know thats person likely answer similarly length question passage contains length likely answer im gonna wanna able pull length person problem happens passage contains multiple candidate answers correct named entity type heres question queen victorias second son know looking person heres lovely passage marie biscuit named marie passage whole lot named entities whole lot people czar alexander ii alfred queen victoria prince albert alfred fact answer question alfred second son queen victoria deciding named entities correct answer machine learning problem need lots features tell us named entities correct likely correct answer extract use machine learning lots rich features ranking candidate answers candidate good candidate phrase correct answer type im im ive got good name thats right kind named entity thats good sign write regular expressions measure number question keywords look distance factors use passages use answers well good answer candidate apposition question terms might theres appositive clause might followed kind punctuation might look longest sequence question terms features used ranking candidate answers ibm watson answer actually scored whole lot rich knowledge sources components use unstructured text use semi structured text might use knowledge triple stores relation extractions talked earlier might give score possible answer knowledge might use geospatial knowledge know lets say geospatial database california southwest montana might help us deciding california good answer question involving southwest montana extract temporal relationships look reliable source passage full parses get logical forms well talk little bit later knowledge sources might used weve picked answer need decide good returning one answer use accuracy answer match gold labeled correct answer question often return multiple answers general information retrieval case use metric called mean reciprocal rank query gonna return ranked list candidate answers score query one rank first right answer theres five give five answers third one first one thats correct score query rd take mean ranks n queries thats mean reciprocal rank thats final step standard irbased algorithm factoid question answering,Course2,W8-S2-L3,W8,S2,L3,Passage,8,2,3,next step question answer passag retriev answer extract look flowchart weve process question weve decid queri send ir engin job document retriev find document queri word extract document pull passag might relev answer fragment well three step retriev document ir engin segment document shorter unit often use paragraph break passag rank rerank passag depend well like contain answer question third step passag rank want talk weve seen ir alreadi segment rel simpl might use paragraph similar kind thing hard part realli ive got whole lot passag came back break document piec one contain answer kind featur get use passag rank might use rulebas classifi might use supervis machin learn might ask mani name entiti answer type occur passag im ask person date mani peopl date passag none ive got bad passag mani queri word occur passag know occur document wanna know mani also occur particular passag instead word might look entir ngram might look close queri keyword occur passag ive got two three keyword right next im probabl someth relat take longest sequenc question word ask long that anoth featur use cours document passag alreadi rank rank document might use featur throw thing togeth ive done step weve weve got fact question weve retriev rank bunch answer passag last step pull answer passag answer process first thing answer extract run name entiti tagger passag answer type weve gotta name entiti tagger detect answer type know look citi that help dont tagger find citi raw text could full name entiti tagger simpl regular express hybrid job return string right type exampl person question prime minist india go want abl detect answer passag passag mannohan singh prime minist india want know that person like answer similarli length question passag contain length like answer im gonna wanna abl pull length person problem happen passag contain multipl candid answer correct name entiti type here question queen victoria second son know look person here love passag mari biscuit name mari passag whole lot name entiti whole lot peopl czar alexand ii alfr queen victoria princ albert alfr fact answer question alfr second son queen victoria decid name entiti correct answer machin learn problem need lot featur tell us name entiti correct like correct answer extract use machin learn lot rich featur rank candid answer candid good candid phrase correct answer type im im ive got good name that right kind name entiti that good sign write regular express measur number question keyword look distanc factor use passag use answer well good answer candid apposit question term might there apposit claus might follow kind punctuat might look longest sequenc question term featur use rank candid answer ibm watson answer actual score whole lot rich knowledg sourc compon use unstructur text use semi structur text might use knowledg tripl store relat extract talk earlier might give score possibl answer knowledg might use geospati knowledg know let say geospati databas california southwest montana might help us decid california good answer question involv southwest montana extract tempor relationship look reliabl sourc passag full pars get logic form well talk littl bit later knowledg sourc might use weve pick answer need decid good return one answer use accuraci answer match gold label correct answer question often return multipl answer gener inform retriev case use metric call mean reciproc rank queri gonna return rank list candid answer score queri one rank first right answer there five give five answer third one first one that correct score queri rd take mean rank n queri that mean reciproc rank that final step standard irbas algorithm factoid question answer,[ 9 13  2  4  5]
215,Course2_W8-S2-L4_Using_Knowledge_in_QA_4-25,rich knowledge also used modern question answering systems commonly used form knowledge relations relation extraction talked previous lecture important question answering relations occur pulling answers analyzing question example might databases relations triple stores might draw wikipedia infoboxes might stored places like dbpedia freebase various databases learn facts like born emma goldman june learn birth date somebody cao xueqin wrote dream red chamber favorite novel authorof relation bornin relation kinds things might able extract triple stores answer questions emma goldmans birth date name author dream red chamber requires know relation asked question gonna need also extract relations questions ask question like whose granddaughter starred et need know question relations like someone acted et someone else someone granddaughter kind relations could extract question answer matching triple stores help find answer thats gonna used modern question answering systems another kind rich use knowledge temporal reasoning relation databases also obituaries biographical dictionaries question ibms watson faced took job tax collector andalusia candidate answers thoreau cervantes wed like know thoreau bad answer thoreau born couldnt alive take job cervantes least possible answer alive kind temporal databases would let us reasoning people born died therefore answer possible question geospatial knowledge questions containment directionality borders help answer lots questions help us know beijing good answer say asian city new york good answer asian city california southwest montana example one databases geonamesorg lets type ive typed palo alto pull sorts facts kind databases population elevation latitude longitude fact part united states part california santa clara county many question answering systems simply one shot questions ask question get answer systems like siri one carry carry full conversation user context conversation matters example siri conversation might say book table il fornaio seven oclock mom might sentence one sentence two might also send email reminder knowing means mom kind thing matters youre going multiple sentence questions coreference resolving dialogue coreference knowing refers mom kind thing need question answering system full conversation multiple sentences row similarly might able ask clarification questions user says chicago pizza system like siri might ask mean pizza restaurants chicago chicago style pizza might ambiguity system resolves ambiguity asking follow question user kind complications occur questionanswering system able deal follow questions dialogue general knowledge like relation extraction important question answering depending question answering system going see kinds knowledge like geospatial temporal dialogue conversational knowledge playing role well,Course2,W8-S2-L4,W8,S2,L4,Using,8,2,4,rich knowledg also use modern question answer system commonli use form knowledg relat relat extract talk previou lectur import question answer relat occur pull answer analyz question exampl might databas relat tripl store might draw wikipedia infobox might store place like dbpedia freebas variou databas learn fact like born emma goldman june learn birth date somebodi cao xueqin wrote dream red chamber favorit novel authorof relat bornin relat kind thing might abl extract tripl store answer question emma goldman birth date name author dream red chamber requir know relat ask question gonna need also extract relat question ask question like whose granddaught star et need know question relat like someon act et someon els someon granddaught kind relat could extract question answer match tripl store help find answer that gonna use modern question answer system anoth kind rich use knowledg tempor reason relat databas also obituari biograph dictionari question ibm watson face took job tax collector andalusia candid answer thoreau cervant wed like know thoreau bad answer thoreau born couldnt aliv take job cervant least possibl answer aliv kind tempor databas would let us reason peopl born die therefor answer possibl question geospati knowledg question contain direction border help answer lot question help us know beij good answer say asian citi new york good answer asian citi california southwest montana exampl one databas geonamesorg let type ive type palo alto pull sort fact kind databas popul elev latitud longitud fact part unit state part california santa clara counti mani question answer system simpli one shot question ask question get answer system like siri one carri carri full convers user context convers matter exampl siri convers might say book tabl il fornaio seven oclock mom might sentenc one sentenc two might also send email remind know mean mom kind thing matter your go multipl sentenc question corefer resolv dialogu corefer know refer mom kind thing need question answer system full convers multipl sentenc row similarli might abl ask clarif question user say chicago pizza system like siri might ask mean pizza restaur chicago chicago style pizza might ambigu system resolv ambigu ask follow question user kind complic occur questionansw system abl deal follow question dialogu gener knowledg like relat extract import question answer depend question answer system go see kind knowledg like geospati tempor dialogu convers knowledg play role well,[ 9 13  4 14 12]
216,Course2_W8-S2-L5_Advanced-_Answering_Complex_Questions_4-52,kinds advanced questions dont tend answered modern commercial systems part modern research systems lets turn imagine following harder question water spinach answer wed like give really full paragraph talks water spinach history names different languages wed like able build kind question looking web merging together possible snippets might get different sources might get hard medical questions like children acute illness whats efficacy particular medicine answer might summary things might see particular pubmed paper might wanna read sentence decide read document extract sentences say came much believe kind harder things important research literature havent yet made commercial modern commercial systems answering harder questions often use approach thats called query focused summarization query focused summarization means applying natural language summarization algorithm going summarize multiple documents pull information multiple documents query focused summary everything thats documents parts relevant query two kinds algorithms query focused summarization one might call bottom snippet method find set relevant documents extract information bottom using tfidf deciding many sentences relevant tfidf score kinds scores combine modify sentences answer im gonna talk today second method might think information extraction method build specific answers work different question types might definition questions biography questions certain medical questions might decide constitutes good definition question good biography question good medical question gonna build specific answers example know good biography contains persons birth death dates contains famous theyre famous education nationality kind things whereas good definition contains whats often called genus hypernym know hajj type ritual good medical answer contains problem medicine designed solve intervention drug intervention gonna use outcome result study describing problem three kinds answers might know need extract definition question weve got build genus detector species extractor subtype extractor whereas biography question need build data extractor nationality extractor drug efficacy question extract population study run problem intervention ive given examples kinds sentences need extract kind answers heres sample architecture complex questionanswering system blairgoldensohn et al might question like hajj might specify wed like search twenty documents extract answer eight sentences well pull document retrieval pull lots relevant documents going build classifiers whose job pull genus species sentences hajj pilgrimage hajj milestone hajj pillar classifier pulls non non kinds definition sentences gonna cluster order produce response answer eight sentences summarizes good response question advanced questionanswering algorithms used laboratory either use information extraction methods use bottomup clustering methods combine information lots documents create set sentences answer question commonly commercial systems based factoid answers either using knowledge bases often using information retrieval techniques find sentences contain answer question extracting ranking answer presenting user,Course2,W8-S2-L5,W8,S2,L5,Advanced-,8,2,5,kind advanc question dont tend answer modern commerci system part modern research system let turn imagin follow harder question water spinach answer wed like give realli full paragraph talk water spinach histori name differ languag wed like abl build kind question look web merg togeth possibl snippet might get differ sourc might get hard medic question like children acut ill what efficaci particular medicin answer might summari thing might see particular pubm paper might wanna read sentenc decid read document extract sentenc say came much believ kind harder thing import research literatur havent yet made commerci modern commerci system answer harder question often use approach that call queri focus summar queri focus summar mean appli natur languag summar algorithm go summar multipl document pull inform multipl document queri focus summari everyth that document part relev queri two kind algorithm queri focus summar one might call bottom snippet method find set relev document extract inform bottom use tfidf decid mani sentenc relev tfidf score kind score combin modifi sentenc answer im gonna talk today second method might think inform extract method build specif answer work differ question type might definit question biographi question certain medic question might decid constitut good definit question good biographi question good medic question gonna build specif answer exampl know good biographi contain person birth death date contain famou theyr famou educ nation kind thing wherea good definit contain what often call genu hypernym know hajj type ritual good medic answer contain problem medicin design solv intervent drug intervent gonna use outcom result studi describ problem three kind answer might know need extract definit question weve got build genu detector speci extractor subtyp extractor wherea biographi question need build data extractor nation extractor drug efficaci question extract popul studi run problem intervent ive given exampl kind sentenc need extract kind answer here sampl architectur complex questionansw system blairgoldensohn et al might question like hajj might specifi wed like search twenti document extract answer eight sentenc well pull document retriev pull lot relev document go build classifi whose job pull genu speci sentenc hajj pilgrimag hajj mileston hajj pillar classifi pull non non kind definit sentenc gonna cluster order produc respons answer eight sentenc summar good respons question advanc questionansw algorithm use laboratori either use inform extract method use bottomup cluster method combin inform lot document creat set sentenc answer question commonli commerci system base factoid answer either use knowledg base often use inform retriev techniqu find sentenc contain answer question extract rank answer present user,[ 9  2  4 13 11]
217,Course2_W8-S3-L1_Introduction_to_Summarization,lets turn use summarization question answering goal summarization produce abridged version text contains information thats important relevant user particular user need might mean abstracting kind document article summarizing email threads meetings simplifying text well talk compressing sentences talk summarization two ways single document summarization given single document gonna produce abstract outline perhaps headline short summary document multiple document summarization given group documents presumably related documents job produce gist documents news stories event might summarize event picking information different documents could true also kind set web pages focused topic question distinguish generic summarization queryfocused summarization generic summarization ive talked far document set documents job build summary queryfocused summarization summarizing document respect information need user expresses query think queryfocused summarization another kind complex question answering answering question summarizing document information construct answer user needs weve seen query focused summaries thats search engines use show information page example google give characters words page summary page ive asked question google die brücke get lets say three urls back title url url text text snippet kind summary page helps user understand whats page think summarization general single documents generating things like snippets going talk general task specifically snippets web search think snippets characteristic example snippet answer question taking information single document often want answer multiple documents creating answers complex questions involves summarizing multiple documents gonna create cohesive answer combines information documents thats multiple document question answering think two ways summarization extractive summarization create summary taking particular words phrases document building summary words phrases snippet search engine like google extractive summary way contrast abstractive summary one create words different words text summarize content text going talking completely extractive summarization today abstractive summarization important research goal difficult one thing think whenever talking summarizing document whether web page anything else baselines summarization good writers often put ideas right beginning title first sentence simple baseline whenever measuring kind summarization algorithm taking first sentence example google query die brücke take output first hit look sentence die brücke group german expressionist artists formed dresden lets look page snippet comes see snippet really first characters page sometimes best snippet really beginning going use baseline later well see thats feature could use different kinds summarization algorithms weve introduced task natural language generation producing shorter abstracts summaries even headlines document weve talked query focused summarization use summarization answer particular question build summary document specifically designed answer question posed user weve talked single multiple document summarization well see details tasks lectures,Course2,W8-S3-L1,W8,S3,L1,Introduction,8,3,1,let turn use summar question answer goal summar produc abridg version text contain inform that import relev user particular user need might mean abstract kind document articl summar email thread meet simplifi text well talk compress sentenc talk summar two way singl document summar given singl document gonna produc abstract outlin perhap headlin short summari document multipl document summar given group document presum relat document job produc gist document news stori event might summar event pick inform differ document could true also kind set web page focus topic question distinguish gener summar queryfocus summar gener summar ive talk far document set document job build summari queryfocus summar summar document respect inform need user express queri think queryfocus summar anoth kind complex question answer answer question summar document inform construct answer user need weve seen queri focus summari that search engin use show inform page exampl googl give charact word page summari page ive ask question googl die brücke get let say three url back titl url url text text snippet kind summari page help user understand what page think summar gener singl document gener thing like snippet go talk gener task specif snippet web search think snippet characterist exampl snippet answer question take inform singl document often want answer multipl document creat answer complex question involv summar multipl document gonna creat cohes answer combin inform document that multipl document question answer think two way summar extract summar creat summari take particular word phrase document build summari word phrase snippet search engin like googl extract summari way contrast abstract summari one creat word differ word text summar content text go talk complet extract summar today abstract summar import research goal difficult one thing think whenev talk summar document whether web page anyth els baselin summar good writer often put idea right begin titl first sentenc simpl baselin whenev measur kind summar algorithm take first sentenc exampl googl queri die brück take output first hit look sentenc die brück group german expressionist artist form dresden let look page snippet come see snippet realli first charact page sometim best snippet realli begin go use baselin later well see that featur could use differ kind summar algorithm weve introduc task natur languag gener produc shorter abstract summari even headlin document weve talk queri focus summar use summar answer particular question build summari document specif design answer question pose user weve talk singl multipl document summar well see detail task lectur,[ 2  9  4 14 13]
218,Course2_W8-S3-L2_Generating_Snippets,lets see generate snippets summaries single documents heres example snippet coming google ive asked question cast metal movable type invented korea googles giving three little snippets answer see bold faced cases words query occurred snippet see use dot dot dots snippets telling combining pieces different pages different excuse different places page lets see kind snippets kind summaries based single documents built think summarization algorithm three stages first stage content selection extracting sentences need document document input gonna need extract sentences might segment sentences maybe use sentences periods full sentences maybe use kind moving window weve extracted kind little pieces little sentences segmented set sentences want pick ones important marked little little black dots picked set extracted sentences next task information ordering gonna decide order sentences go ordered set important sentences finally might modifications sentences perhaps gonna simplify something else thats sentence realization result four steps summary basic summarization algorithm one comes lot really uses one three steps content selection step simplest possible algorithm dont worry ordering sentences come dont modify sentences simply segment document sentences maybe windows pick important ones leave order came gonna use call document order original sentences simple baseline summarization one web based snippet generation algorithms certainly use commonly used algorithm content selection dates back earliest paper field pretty exciting ideas came early intuitions really simple choose sentences salient informative words well whats mean well youve seen tfidf thats way picking words particularly frequent dont contain words occur documents thats one way might define saliency informativity turns summarization tend use another approach log likelihood ratio sometimes called topic signature approach differs tfidf two ways one use slightly different statistic picking weighting words second instead picking words well choose words whose weight threshold salient words log likelihood ratio gives us statistic called lambda im gonna go details theyre lovely papers gonna choose words value two log lambda greater cutoff ten gives us threshold pick words particularly salient statistic gonna weight every word weight word gonna one word especially associated document meaning occurs times document background corpus threshold otherwise gonna gonna give weight zero details compute log likelihood intuition statistics see lovely ted dunning paper lin hovy paper proposed using summarization want modify algorithm dealing query focused summarization interested much pure summarization todays lecture use summarization techniques question answering topic signature based topic signature meaning pick words particularly associated document content selection choosing sentences weve got queries alright gonna modify algorithm slightly gonna choose words informative either log likelihood ratio words happen appear query going weight every word document going give weight one meets log likelihood threshold passes threshold ten going give weight one also word happens appear query question otherwise going give word weight zero weights simple one one zero could imagine learning complex weights research gone coming powerful ways learn detailed weights one one zero works pretty well turns going weigh sentence perhaps window dont actual sentences going weigh weight words going sum words sentence weight words going take average content selection algorithm described unsupervised didnt labeled training set summaries learn weights things like thats alternative approach supervised content selection labeled training set document good summary alignment every sentence summary knew sentence came document matching sentences could extract sorts features could extract position sentence document first sentences likely good summary sentences long features word informativeness things like kinds features based discourse information might might associate every sentence vector features train binary classifier shall put sentence summary yes might learn weights features features come algorithm sounds good practice turns hard get labeled training data type people actually write abstracts sentences theyre always authors dont always use exact words phrases certainly dont use entire sentences come document finding perfectly labeled abstracts extracts document hard hard alignment dont pick entire sentences may picking words phrases chunks hard figure words came even pick document turns surprisingly perhaps performance simply much better unsupervised algorithms practice unsupervised content selection using log likelihood ratio simple measures salient informative word hence sentence common method content selection weve seen generate summaries single document baseline algorithm picked simply come simple statistical way find sentence informative looking informative words talked loglikelihood ratio important way finding sentences,Course2,W8-S3-L2,W8,S3,L2,Generating,8,3,2,let see gener snippet summari singl document here exampl snippet come googl ive ask question cast metal movabl type invent korea googl give three littl snippet answer see bold face case word queri occur snippet see use dot dot dot snippet tell combin piec differ page differ excus differ place page let see kind snippet kind summari base singl document built think summar algorithm three stage first stage content select extract sentenc need document document input gonna need extract sentenc might segment sentenc mayb use sentenc period full sentenc mayb use kind move window weve extract kind littl piec littl sentenc segment set sentenc want pick one import mark littl littl black dot pick set extract sentenc next task inform order gonna decid order sentenc go order set import sentenc final might modif sentenc perhap gonna simplifi someth els that sentenc realiz result four step summari basic summar algorithm one come lot realli use one three step content select step simplest possibl algorithm dont worri order sentenc come dont modifi sentenc simpli segment document sentenc mayb window pick import one leav order came gonna use call document order origin sentenc simpl baselin summar one web base snippet gener algorithm certainli use commonli use algorithm content select date back earliest paper field pretti excit idea came earli intuit realli simpl choos sentenc salient inform word well what mean well youv seen tfidf that way pick word particularli frequent dont contain word occur document that one way might defin salienc inform turn summar tend use anoth approach log likelihood ratio sometim call topic signatur approach differ tfidf two way one use slightli differ statist pick weight word second instead pick word well choos word whose weight threshold salient word log likelihood ratio give us statist call lambda im gonna go detail theyr love paper gonna choos word valu two log lambda greater cutoff ten give us threshold pick word particularli salient statist gonna weight everi word weight word gonna one word especi associ document mean occur time document background corpu threshold otherwis gonna gonna give weight zero detail comput log likelihood intuit statist see love ted dun paper lin hovi paper propos use summar want modifi algorithm deal queri focus summar interest much pure summar today lectur use summar techniqu question answer topic signatur base topic signatur mean pick word particularli associ document content select choos sentenc weve got queri alright gonna modifi algorithm slightli gonna choos word inform either log likelihood ratio word happen appear queri go weight everi word document go give weight one meet log likelihood threshold pass threshold ten go give weight one also word happen appear queri question otherwis go give word weight zero weight simpl one one zero could imagin learn complex weight research gone come power way learn detail weight one one zero work pretti well turn go weigh sentenc perhap window dont actual sentenc go weigh weight word go sum word sentenc weight word go take averag content select algorithm describ unsupervis didnt label train set summari learn weight thing like that altern approach supervis content select label train set document good summari align everi sentenc summari knew sentenc came document match sentenc could extract sort featur could extract posit sentenc document first sentenc like good summari sentenc long featur word inform thing like kind featur base discours inform might might associ everi sentenc vector featur train binari classifi shall put sentenc summari ye might learn weight featur featur come algorithm sound good practic turn hard get label train data type peopl actual write abstract sentenc theyr alway author dont alway use exact word phrase certainli dont use entir sentenc come document find perfectli label abstract extract document hard hard align dont pick entir sentenc may pick word phrase chunk hard figur word came even pick document turn surprisingli perhap perform simpli much better unsupervis algorithm practic unsupervis content select use log likelihood ratio simpl measur salient inform word henc sentenc common method content select weve seen gener summari singl document baselin algorithm pick simpli come simpl statist way find sentenc inform look inform word talk loglikelihood ratio import way find sentenc,[2 4 5 3 9]
219,Course2_W8-S3-L3_Evaluating_Summaries-_ROUGE,talked earlier evaluating question answering answer factoid question simply evaluate seeing factoid system returns correct factoid summaries cant evaluated way cant single perfect summary document well introduce different algorithm called rouge rouge stands recall oriented understudy gisting evaluation proposed lin hovy heres idea intrinsic metric evaluating summaries gonna ask summary good summary extrinsic application summary based metric called bleu blue defined originally machine translation good rouge good using using humans say summary answer users question afford well certainly hire users test see answer answers users question rouge convenient testing building system works follows given document lets say weve got summarizer produces automatic summary query focused summary maybe know query user asked even generic summarization n humans produce set reference summaries document query focused summarization look query write summaries generic summarization write summaries set summaries one two three four human summaries document system produces another summary call x automatic summary four human summaries ask percentage bigrams humans summaries occur x obviously wont occur x good summary contain lot bigrams occur human summaries counting percentage get intuition whats good summary various versions rouge unigram rouge bigram rouge theres also versions talk length different ways well introduce one rouge two bigram rouge works pretty well asking bigrams sentences reference summaries take count bigrams notice bigrams look sentence summary bigram ask whats minimum count summary produced system human summary asking many bigrams occurred system human summary give us bigrams many occurred summary human references lets look example made three human summaries system summary heres human three summaries system answer question lets compute rouge numerator want know many bigrams occurred human summaries many bigrams also occurred system answer walk well water spinach thats answer spinach summary water spinach spinach third summary water spinach spinach also commonly eaten leaf vegetable asia add together three first summary three second summary six third many total bigrams human summary well count ten first example nine nine rouge score weve introduced rouge algorithm evaluating summaries whether theyre generic query focused looking many bigrams n grams general human summary occur machine generated summary better summary one overlaps human summary rouge doesnt work well humans actually answer question answer provide information user asked expensive rouge provide fasttorun convenient intrinsic metric use test systems end use humans see well really,Course2,W8-S3-L3,W8,S3,L3,Evaluating,8,3,3,talk earlier evalu question answer answer factoid question simpli evalu see factoid system return correct factoid summari cant evalu way cant singl perfect summari document well introduc differ algorithm call roug roug stand recal orient understudi gist evalu propos lin hovi here idea intrins metric evalu summari gonna ask summari good summari extrins applic summari base metric call bleu blue defin origin machin translat good roug good use use human say summari answer user question afford well certainli hire user test see answer answer user question roug conveni test build system work follow given document let say weve got summar produc automat summari queri focus summari mayb know queri user ask even gener summar n human produc set refer summari document queri focus summar look queri write summari gener summar write summari set summari one two three four human summari document system produc anoth summari call x automat summari four human summari ask percentag bigram human summari occur x obvious wont occur x good summari contain lot bigram occur human summari count percentag get intuit what good summari variou version roug unigram roug bigram roug there also version talk length differ way well introduc one roug two bigram roug work pretti well ask bigram sentenc refer summari take count bigram notic bigram look sentenc summari bigram ask what minimum count summari produc system human summari ask mani bigram occur system human summari give us bigram mani occur summari human refer let look exampl made three human summari system summari here human three summari system answer question let comput roug numer want know mani bigram occur human summari mani bigram also occur system answer walk well water spinach that answer spinach summari water spinach spinach third summari water spinach spinach also commonli eaten leaf veget asia add togeth three first summari three second summari six third mani total bigram human summari well count ten first exampl nine nine roug score weve introduc roug algorithm evalu summari whether theyr gener queri focus look mani bigram n gram gener human summari occur machin gener summari better summari one overlap human summari roug doesnt work well human actual answer question answer provid inform user ask expens roug provid fasttorun conveni intrins metric use test system end use human see well realli,[ 9  2  4 14 13]
220,Course2_W8-S3-L4_Summarizing_Multiple_Documents,lets conclude section looking advanced research systems try answer much complex questions consider following definition question water spinach ive given potential long answer question water spinach could generate answers definition questions like lets say didnt occur good place web wanted generate take medical question efficacy particular therapy particular disease might wanna give answer ones particular paper pubmed database answers question difficult hard answer questions might involve summarizing one documents theres fact competition answering complex questions ive given simplified versions questions compost made used gardening causes train wrecks done prevent whats human toll death injury tropical storms recent years kind questions answer youd want read lot documents automatically pull information summarize information difficult task answering harder questions task queryfocused multiple document summarization two standard algorithms might call bottom snippet style method like saw single document summarization going find set relevant documents first going extract informative sentences documents maybe ordering modification going grab sentences documents mix together topdown information extraction method going build specific answers different question types build answer definition questions one biography questions extracting particular information needed questions see little lecture heres snippet based method bottom method query focused multi document summarization gonna start grabbing sentences multiple documents one document set sentences gonna modify common sentence simplification well take sentence ill show second well simplify various ways well lots different simplified versions sentences weve got lots families clouds sentences gonna apply log likelihood ratio test wa methods pulling good sentences set sentences set extracted sentences little black dots ive marked gonna order extracted sentences talked single document summarization gonna modify various ways produce realized sentences often simplify sentences get rid unimportant details would make summary long one common ways involves parsing parse sentences hand written rules based parse tree suggest modifiers better prune example appositives kind thing might prune rajam artist living time philadelphia eliminate attribution clause international observer said tuesday blablabla well care said rebels agreed talks thats important fact delete maybe attribution clause summary preposition phrases especially without named entities turn kind thing delete keep summary concise increased sustainable number maybe shorter summary would say increased adverbials beginning like example hand matter fact things mattered lo nger document bur arent going appropriate small abstract small summary simplest method taking writing various rules kind papers gives another interesting sets rules use simplify sentences weve done large set sentences including original sentence one appositive deleted ones pps deleted whole bunch different sentences original various shortened versions next well wed like select redundant set sentences ones put summary theyre redundant two reasons come multiple documents might talk event weve added simplified sentences sorts redundancies one iterative method content selection given redundant sentences called mmr maximal marginal relevance idea follows going iteratively greedily choose best sentence set sentences insert summary far want best sentence satisfy satisfy two properties want maximally relevant users query might something simple like insure high cosine similarity query want novel meaning want minimally redundant summary far weve put sentence dont want put variant sentence thats mostly want measure measuring cosine similarity summary choosing sentences low cosine similarity one version mmr might well lambda weighting two factors want sentence high similarity query low well subtract similarity sentence similar sentence thats summary far well pick sentences dont look like sentences summary far look like query well add sentences iteratively well stop criterion perhaps weve achieved desired length gonna wanna combine intuitions log likelihood ratio picking informative sentences mmr choosing non redundant sentences one many ways might combine intuitions start scoring every sentence based log likelihood ratio using words either occurred expect chance words occurred query start including sentence highest log likelihood ratio score summary start iteratively adding summary sentences redundant sentences summary far single document summaries could pick document order ordering sentences summary thats little harder sentences come multiple documents theres various things summarizing news chronological ordering look date article came order sentences order actual time choose coherence based ordering put sentences near similar meaning could look cosine two sentences sentences high cosine could put next perhaps could look entities discussed sentences two sentences talk entity could put near could even kind fancy method look source documents look actual semantic topics happen documents could order tho pick ordering apply output sentences various ways information ordering thats bottomup snippetbased approach query focused multi document summarization grab whole bunch sentences rank log likelihood ratio mmr form summary way order method alternative method information extraction method used particular kind question wanna know kind things expect put answer know example good biography contains birth death maybe theyre famous education nationality whereas good definition contains whats called genus hypernym sentence hajj type ritual medical answer drugs use want talk problem medical condition want talk intervention drug procedure whats outcome knowing types questions know types things might want see answer could build little detector lets say finds genus sentences documents find genus sentence find species sentence find dates person born died educated find went school talking drug study run actual intervention could build classifiers extract kind information lets look example system blairgoldensohn kind complex question answering definition questions question hajj specify lets say going look twenty documents pull summary want summary length eight sentences long going extract documents going pull say definition question know need genus species sentence run classifier find genus species sentences documents find sentences gonna pieces information need build little clusters sentences use log likelihood ratio mmr approaches decide sentence pick gonna paste definition weve talked two methods answering complex difficult questions require looking multiple documents talked bottom snippet based method uses log likelihood ratio mmr choose right nonredundant informative sentences lots documents weve talked information extraction style method build separate templates question type pull particular attributes important things put answer question course research harder questions require looking multiple documents really beginning gonna important exciting area us follow future see research goes,Course2,W8-S3-L4,W8,S3,L4,Summarizing,8,3,4,let conclud section look advanc research system tri answer much complex question consid follow definit question water spinach ive given potenti long answer question water spinach could gener answer definit question like let say didnt occur good place web want gener take medic question efficaci particular therapi particular diseas might wanna give answer one particular paper pubm databas answer question difficult hard answer question might involv summar one document there fact competit answer complex question ive given simplifi version question compost made use garden caus train wreck done prevent what human toll death injuri tropic storm recent year kind question answer youd want read lot document automat pull inform summar inform difficult task answer harder question task queryfocus multipl document summar two standard algorithm might call bottom snippet style method like saw singl document summar go find set relev document first go extract inform sentenc document mayb order modif go grab sentenc document mix togeth topdown inform extract method go build specif answer differ question type build answer definit question one biographi question extract particular inform need question see littl lectur here snippet base method bottom method queri focus multi document summar gonna start grab sentenc multipl document one document set sentenc gonna modifi common sentenc simplif well take sentenc ill show second well simplifi variou way well lot differ simplifi version sentenc weve got lot famili cloud sentenc gonna appli log likelihood ratio test wa method pull good sentenc set sentenc set extract sentenc littl black dot ive mark gonna order extract sentenc talk singl document summar gonna modifi variou way produc realiz sentenc often simplifi sentenc get rid unimport detail would make summari long one common way involv pars pars sentenc hand written rule base pars tree suggest modifi better prune exampl apposit kind thing might prune rajam artist live time philadelphia elimin attribut claus intern observ said tuesday blablabla well care said rebel agre talk that import fact delet mayb attribut claus summari preposit phrase especi without name entiti turn kind thing delet keep summari concis increas sustain number mayb shorter summari would say increas adverbi begin like exampl hand matter fact thing matter lo nger document bur arent go appropri small abstract small summari simplest method take write variou rule kind paper give anoth interest set rule use simplifi sentenc weve done larg set sentenc includ origin sentenc one apposit delet one pp delet whole bunch differ sentenc origin variou shorten version next well wed like select redund set sentenc one put summari theyr redund two reason come multipl document might talk event weve ad simplifi sentenc sort redund one iter method content select given redund sentenc call mmr maxim margin relev idea follow go iter greedili choos best sentenc set sentenc insert summari far want best sentenc satisfi satisfi two properti want maxim relev user queri might someth simpl like insur high cosin similar queri want novel mean want minim redund summari far weve put sentenc dont want put variant sentenc that mostli want measur measur cosin similar summari choos sentenc low cosin similar one version mmr might well lambda weight two factor want sentenc high similar queri low well subtract similar sentenc similar sentenc that summari far well pick sentenc dont look like sentenc summari far look like queri well add sentenc iter well stop criterion perhap weve achiev desir length gonna wanna combin intuit log likelihood ratio pick inform sentenc mmr choos non redund sentenc one mani way might combin intuit start score everi sentenc base log likelihood ratio use word either occur expect chanc word occur queri start includ sentenc highest log likelihood ratio score summari start iter ad summari sentenc redund sentenc summari far singl document summari could pick document order order sentenc summari that littl harder sentenc come multipl document there variou thing summar news chronolog order look date articl came order sentenc order actual time choos coher base order put sentenc near similar mean could look cosin two sentenc sentenc high cosin could put next perhap could look entiti discuss sentenc two sentenc talk entiti could put near could even kind fanci method look sourc document look actual semant topic happen document could order tho pick order appli output sentenc variou way inform order that bottomup snippetbas approach queri focus multi document summar grab whole bunch sentenc rank log likelihood ratio mmr form summari way order method altern method inform extract method use particular kind question wanna know kind thing expect put answer know exampl good biographi contain birth death mayb theyr famou educ nation wherea good definit contain what call genu hypernym sentenc hajj type ritual medic answer drug use want talk problem medic condit want talk intervent drug procedur what outcom know type question know type thing might want see answer could build littl detector let say find genu sentenc document find genu sentenc find speci sentenc find date person born die educ find went school talk drug studi run actual intervent could build classifi extract kind inform let look exampl system blairgoldensohn kind complex question answer definit question question hajj specifi let say go look twenti document pull summari want summari length eight sentenc long go extract document go pull say definit question know need genu speci sentenc run classifi find genu speci sentenc document find sentenc gonna piec inform need build littl cluster sentenc use log likelihood ratio mmr approach decid sentenc pick gonna past definit weve talk two method answer complex difficult question requir look multipl document talk bottom snippet base method use log likelihood ratio mmr choos right nonredund inform sentenc lot document weve talk inform extract style method build separ templat question type pull particular attribut import thing put answer question cours research harder question requir look multipl document realli begin gonna import excit area us follow futur see research goe,[ 9  2  4  0 13]
221,Course3_W1-S1-L1_-_Introduction_-_11_slides_08-39,welcome natural language processing class introductory lecture name dragomir radev phd columbia university course going approximately hours long weeks hours intended audience primarily students computer science linguistics informatics fields mathematics statistics management engineering online courses available natural language processing course however introductory course michael collins taught coursera focused linguistics computational resources jurafsky manning version coursera first questions want ask natural language processing want study definition nlp natural language processing simple study computational treatment natural language people say natural language usually mean human language doesnt mean example language animal words course teaching computers understand also generate human language let start asking quote came know movie time kid probably havent seen movie person talking computer person says open pod bay doors hal computer says im sorry dave im afraid cannot heres answer quiz quote space odyssey science fiction movie stanley kubrick written arthur c clarke one first major science fiction movies talks computers interact humans using natural language nowadays however actual applications natural language processing real world science fiction im going show examples obvious ones search engines major search engines google yahoo bing search engines languages like baidu chinese use natural language processing technology understand queries find matching documents another application question answering example years ago ibms watson system famously played television best human contestants jeopardy nowadays natural language assistants apples siri translation systems youre probably familiar google translate applications companies example news digest yahoo applications text generation example la times applies computer software generate reports earthquakes automatically techniques use natural language processing many person years gone building systems course hours going try figure sort technologies sort insights necessary build systems like let make notes first computers inherently designed understand human language fact theyre confused human language specific techniques needed teach computers use human language natural language processing field teaches computers understand language want warn natural language processing multidisciplinary field draws research linguistics study language theoretical computer science mathematics statistics artificial intelligence even fields like psychology databases user interfaces whatnot class multiple goals first goal understand language processing hard cannot natural language processing dont intuitive understanding difficulties human language need know language difficult also need understand first theme class second theme provide students overview key problems natural language processing example machine translation parsing third topic learn methods used address problems techniques could involve specific statistical techniques specific language resources finally importantly need understand limitations methods dont expect techniques work one type text work another techniques work one language carry languages lets start first little bit background linguistics want warn course going spend lot time linguistic intuition behind natural language processing often see slides address specific linguistic issues relate field communication people focused interaction speaker listener theory applies interaction computers humans also interactions different people speaker speaker first intention something wants say include certain goal include shared knowledge beliefs topic theyre going discuss next thing goal formulated generate representation sentence discourse said third step actually synthesize say produce sentence done text speech depending system listener side three steps first one perception thats listener hears sees text uttered speaker second step interpretation speaker said interpretation done syntactic level grammatical structure sentence semantic level meaning sentence pragmatic level purpose sentence speaker intend communicate third step incorporation speaker said also know internalization understanding thats listener actually take action learn something based speaker said speaker listener common ground example restaurant point certain dish menu say talk certain person say part shared context speaker listener known linguistic theory part grounding basic natural language processing system following structure two components u stands understanding thats part takes language computer representation often natural language processing may understanding component example want ask question may want computer perform action based answer anything back cases may entire dialog system includes understanding generation case computer hear using technique called natural language generation produce sentence perhaps longer piece text would go back next segment going talk specific examples text understand present challenges computers,Course3,W1-S1-L1,W1,S1,L1,-,1,1,1,welcom natur languag process class introductori lectur name dragomir radev phd columbia univers cours go approxim hour long week hour intend audienc primarili student comput scienc linguist informat field mathemat statist manag engin onlin cours avail natur languag process cours howev introductori cours michael collin taught coursera focus linguist comput resourc jurafski man version coursera first question want ask natur languag process want studi definit nlp natur languag process simpl studi comput treatment natur languag peopl say natur languag usual mean human languag doesnt mean exampl languag anim word cours teach comput understand also gener human languag let start ask quot came know movi time kid probabl havent seen movi person talk comput person say open pod bay door hal comput say im sorri dave im afraid cannot here answer quiz quot space odyssey scienc fiction movi stanley kubrick written arthur c clark one first major scienc fiction movi talk comput interact human use natur languag nowaday howev actual applic natur languag process real world scienc fiction im go show exampl obviou one search engin major search engin googl yahoo bing search engin languag like baidu chines use natur languag process technolog understand queri find match document anoth applic question answer exampl year ago ibm watson system famous play televis best human contest jeopardi nowaday natur languag assist appl siri translat system your probabl familiar googl translat applic compani exampl news digest yahoo applic text gener exampl la time appli comput softwar gener report earthquak automat techniqu use natur languag process mani person year gone build system cours hour go tri figur sort technolog sort insight necessari build system like let make note first comput inher design understand human languag fact theyr confus human languag specif techniqu need teach comput use human languag natur languag process field teach comput understand languag want warn natur languag process multidisciplinari field draw research linguist studi languag theoret comput scienc mathemat statist artifici intellig even field like psycholog databas user interfac whatnot class multipl goal first goal understand languag process hard cannot natur languag process dont intuit understand difficulti human languag need know languag difficult also need understand first theme class second theme provid student overview key problem natur languag process exampl machin translat pars third topic learn method use address problem techniqu could involv specif statist techniqu specif languag resourc final importantli need understand limit method dont expect techniqu work one type text work anoth techniqu work one languag carri languag let start first littl bit background linguist want warn cours go spend lot time linguist intuit behind natur languag process often see slide address specif linguist issu relat field commun peopl focus interact speaker listen theori appli interact comput human also interact differ peopl speaker speaker first intent someth want say includ certain goal includ share knowledg belief topic theyr go discuss next thing goal formul gener represent sentenc discours said third step actual synthes say produc sentenc done text speech depend system listen side three step first one percept that listen hear see text utter speaker second step interpret speaker said interpret done syntact level grammat structur sentenc semant level mean sentenc pragmat level purpos sentenc speaker intend commun third step incorpor speaker said also know intern understand that listen actual take action learn someth base speaker said speaker listen common ground exampl restaur point certain dish menu say talk certain person say part share context speaker listen known linguist theori part ground basic natur languag process system follow structur two compon u stand understand that part take languag comput represent often natur languag process may understand compon exampl want ask question may want comput perform action base answer anyth back case may entir dialog system includ understand gener case comput hear use techniqu call natur languag gener produc sentenc perhap longer piec text would go back next segment go talk specif exampl text understand present challeng comput,[ 4  8  9 14 13]
222,Course3_W1-S1-L2_-_Examples_of_Text_-_14_slides_07-52,next course segment going discuss natural language processing challenges come text specifically going look example news story talks event happened recently brazil story candidate president died plane crash want build system would understand story able answer questions example happened im going highlight pieces document example word phrase expected entire sentence phrases specific reason highlight figure highlighted presents different sort challenge computer oh answer next slide well first sentence brazil crowds attend funeral late candidate campos reason chose sentence tells us main event story computers able understand entire sentences figure relate happened second sentence mr campos jet crashed bad weather santos background event current event funeral background event context funeral happens jet crashed third sentence also interesting says mr campos socialist party expected appoint event speculation may happen future president dilma rousseff well sentence gives property person tells dilma rousseff president brazil attended funeral mass world makes sense context previous sentence particular case whats important word refers hundreds thousands people attended funeral known natural language processing pronominal reference entity previous sentence many different genres text presents different challenges computers lets see common kinds text computers likely encounter well include obvious genres texts blogs emails press releases chats internet debates left hand side small snap shot youre expected read go website create debate see thats site im going describe really quickly anybody go website post controversial topic example manchester united best football team ever particular example think schools provide tests developmentally challenged people decide whether youre yes side side everybody participate discussion enter arguments sides kind text presents interesting challenges want understand persons opinion whether theyre yes side side second example wikipedia includes entry specific politician tony blair used understand basic events facts biography lets look challenging piece text scientific paper title already tells difficult topic induction influenza specific mucosal immunity attenuated recombinant sendai virus ive extracted text sentence document sorry want show specific challenges computers first thing phrase sendai virus named entity specifically virus recognized system examples named entities example human parainfluenza virus type one hpiv sendai virus mentioned times luciferase green fluorescent protein gfp examples named entities also examples speculation example reported suggesting purple instances species example words human mice ferrets cell types example mesoepithelial cells also facts references things square brackets references throughout papers computer need understand purpose words phrases references order understand content document another interesting type documents challenging computers medical records medical records two interesting properties first theyre often result transcriptions dont contain grammatical sentences second problem medical records theyre used research purposes anonymized example medical record name person changed something personally identifiable information interesting challenge comes literary texts fiction texts difficult computers process want show examples project gutenberg online digital library contains old books examples classic books like ulysses see proper names difficult understand dont know context story theres rare words waggoner another example jane eyre short relatively short sentences also sentences parentheses indicates additional information another example wizard oz little bit longer luckily computer program relatively short sentences easy understand end spectrum really long literary sentences example try parse sentence one sentence occupies entire screen talks obscure names people uses words even words english language guess text well previous slide showed excerpt gravitys rainbow book known use arcane words complicated sentences also fairly complicated plot structure another work may heard finnergans wake james joyce probably one difficult books read translating understanding poetry even difficult addition constraints imposed language also constraints imposed metric structure line structure individual line next item going talk funny sentences present specific challenges natural language processing systems,Course3,W1-S1-L2,W1,S1,L2,-,1,1,2,next cours segment go discuss natur languag process challeng come text specif go look exampl news stori talk event happen recent brazil stori candid presid die plane crash want build system would understand stori abl answer question exampl happen im go highlight piec document exampl word phrase expect entir sentenc phrase specif reason highlight figur highlight present differ sort challeng comput oh answer next slide well first sentenc brazil crowd attend funer late candid campo reason chose sentenc tell us main event stori comput abl understand entir sentenc figur relat happen second sentenc mr campo jet crash bad weather santo background event current event funer background event context funer happen jet crash third sentenc also interest say mr campo socialist parti expect appoint event specul may happen futur presid dilma rousseff well sentenc give properti person tell dilma rousseff presid brazil attend funer mass world make sens context previou sentenc particular case what import word refer hundr thousand peopl attend funer known natur languag process pronomin refer entiti previou sentenc mani differ genr text present differ challeng comput let see common kind text comput like encount well includ obviou genr text blog email press releas chat internet debat left hand side small snap shot your expect read go websit creat debat see that site im go describ realli quickli anybodi go websit post controversi topic exampl manchest unit best footbal team ever particular exampl think school provid test development challeng peopl decid whether your ye side side everybodi particip discuss enter argument side kind text present interest challeng want understand person opinion whether theyr ye side side second exampl wikipedia includ entri specif politician toni blair use understand basic event fact biographi let look challeng piec text scientif paper titl alreadi tell difficult topic induct influenza specif mucos immun attenu recombin sendai viru ive extract text sentenc document sorri want show specif challeng comput first thing phrase sendai viru name entiti specif viru recogn system exampl name entiti exampl human parainfluenza viru type one hpiv sendai viru mention time luciferas green fluoresc protein gfp exampl name entiti also exampl specul exampl report suggest purpl instanc speci exampl word human mice ferret cell type exampl mesoepitheli cell also fact refer thing squar bracket refer throughout paper comput need understand purpos word phrase refer order understand content document anoth interest type document challeng comput medic record medic record two interest properti first theyr often result transcript dont contain grammat sentenc second problem medic record theyr use research purpos anonym exampl medic record name person chang someth person identifi inform interest challeng come literari text fiction text difficult comput process want show exampl project gutenberg onlin digit librari contain old book exampl classic book like ulyss see proper name difficult understand dont know context stori there rare word waggon anoth exampl jane eyr short rel short sentenc also sentenc parenthes indic addit inform anoth exampl wizard oz littl bit longer luckili comput program rel short sentenc easi understand end spectrum realli long literari sentenc exampl tri pars sentenc one sentenc occupi entir screen talk obscur name peopl use word even word english languag guess text well previou slide show excerpt graviti rainbow book known use arcan word complic sentenc also fairli complic plot structur anoth work may heard finnergan wake jame joyc probabl one difficult book read translat understand poetri even difficult addit constraint impos languag also constraint impos metric structur line structur individu line next item go talk funni sentenc present specif challeng natur languag process system,[ 4 13  0  2  9]
223,Course3_W1-S1-L3_-_Funny_Sentences_-_10_slides_06-35,next segment going fun im going show sentences multiple possible interpretations one literal funny whereas second one intended one sometimes funny first example children make delicious snacks imagine headline newspaper story point author essentially report kids preparing snacks tasty however way sentence constructed syntactically also may interpreted children tasty eat let show sentences kind unintentional meaning funny stolen painting found tree tree found painting saw rockies flying san francisco funny interpretation one rockies flying san francisco court try shooting defendant instead trying putting trial defendant shooting actually want shoot defendant ban nude dancing governors desk red tape holds new bridges government head seeks arms blair wins budget lies ahead local high school dropouts cut half hospitals sued seven foot doctors dead expected rise miners refuse work death patient deaths door doctors pull finally america woman baby every minutes previous slide showed called classic examples ambiguous headlines slide going show examples actually collected internet recently first one vancouver police shoot man holding box cutter two possible interpretations man shot holding box cutter probably intended interpretation box cutter used police shoot man another example man armed box cutters shot officers story police shoot man box cutters astoria police shoot man yielding box cutters police shoot man box cutters third fourth fifth example instance known prepositional phrase attachment prepositional phrase box cutters refer either nearest noun man verb shoot case refers nearest noun modifies man carrying box cutters modifies verb box cutter modifies way person shot real headlines fun bulgaria doubles money parties motorola hire android developers massachusetts exhales bill passes heads canada us eyes returns moon finally flesheating bug survivor goes home real headlines see urls let show examples ambiguous recommendations illustrate different issues natural language funny example man like hard find written reference chronically absent employee intended interpretation sentence person difficult find absent time sentence made look like real recommendation sentence specifically somebody whos valuable people like really difficult find instance known lexico ambiguity word hard find im sorry phrase hard find multiple interpretations one case means somebody valuable case means somebody whos hiding heres examples lexico ambiguities dishonest employee say hes unbelievable worker hes unbelievable ambiguous word mean somebody good believe hard believe person exists also mean person never believed lazy employee say would indeed fortunate get person work ambiguity comes word fortunate one case means happy work person second case means miracle person got work categories linguistic construction cause ambiguity example structural ambiguity scope ambiguity others structure ambiguity show one example chronically absent employee say seemed career career taking ambiguity comes fact taking refer fact career starting meaning person even good things future means career business taking work scope ambiguity look example one employee whos worth considering job candidate say cannot say enough good things candidate recommend highly ambiguity comes fact say enough good things mean either good things say person way many good things say person want find examples kind funny recommendations look beatrice santorinis collection internet okay next segment going talk administration class,Course3,W1-S1-L3,W1,S1,L3,-,1,1,3,next segment go fun im go show sentenc multipl possibl interpret one liter funni wherea second one intend one sometim funni first exampl children make delici snack imagin headlin newspap stori point author essenti report kid prepar snack tasti howev way sentenc construct syntact also may interpret children tasti eat let show sentenc kind unintent mean funni stolen paint found tree tree found paint saw rocki fli san francisco funni interpret one rocki fli san francisco court tri shoot defend instead tri put trial defend shoot actual want shoot defend ban nude danc governor desk red tape hold new bridg govern head seek arm blair win budget lie ahead local high school dropout cut half hospit su seven foot doctor dead expect rise miner refus work death patient death door doctor pull final america woman babi everi minut previou slide show call classic exampl ambigu headlin slide go show exampl actual collect internet recent first one vancouv polic shoot man hold box cutter two possibl interpret man shot hold box cutter probabl intend interpret box cutter use polic shoot man anoth exampl man arm box cutter shot offic stori polic shoot man box cutter astoria polic shoot man yield box cutter polic shoot man box cutter third fourth fifth exampl instanc known preposit phrase attach preposit phrase box cutter refer either nearest noun man verb shoot case refer nearest noun modifi man carri box cutter modifi verb box cutter modifi way person shot real headlin fun bulgaria doubl money parti motorola hire android develop massachusett exhal bill pass head canada us eye return moon final flesheat bug survivor goe home real headlin see url let show exampl ambigu recommend illustr differ issu natur languag funni exampl man like hard find written refer chronic absent employe intend interpret sentenc person difficult find absent time sentenc made look like real recommend sentenc specif somebodi who valuabl peopl like realli difficult find instanc known lexico ambigu word hard find im sorri phrase hard find multipl interpret one case mean somebodi valuabl case mean somebodi who hide here exampl lexico ambigu dishonest employe say he unbeliev worker he unbeliev ambigu word mean somebodi good believ hard believ person exist also mean person never believ lazi employe say would inde fortun get person work ambigu come word fortun one case mean happi work person second case mean miracl person got work categori linguist construct caus ambigu exampl structur ambigu scope ambigu other structur ambigu show one exampl chronic absent employe say seem career career take ambigu come fact take refer fact career start mean person even good thing futur mean career busi take work scope ambigu look exampl one employe who worth consid job candid say cannot say enough good thing candid recommend highli ambigu come fact say enough good thing mean either good thing say person way mani good thing say person want find exampl kind funni recommend look beatric santorini collect internet okay next segment go talk administr class,[ 4  0 14 13 12]
224,Course3_W1-S1-L4_-_Administrative_-__9_slides_08-07,okay going talk administration class want tell expect see class going interact course four major parts first part covers basic linguistic mathematical computation background material noise second part covers computational models morphology syntax semantics discourse pragmatics main components linguistics third part include core natural language processing technology example things like parsing part speech tagging text generation others noise finally going cover applications text classification machine translation information extraction noise three major goals class one learn basic principles theoretical issues underlying natural language processing two learn techniques tools used develop practical robust systems understand text communicate users one languages finally gain insight open research problems natural language noise three books available cover kind material different levels detail first one speech language processing daniel jurafsky james martin noise second one foundations statistical natural language processing chris manning hinrich schutze third one natural language understanding james allen noise courses places cover similar material would list liked personally one criteria including slides available reading materials online always go back websites learn material would personally recommend courses johns hopkins university taught jason eisner cornell university taught lillian lee stanford university chris manning university maryland hal daume university california berkley dan klein university texas austin ray mooney also two courses coursera past one manning jurafsky one michael collins first one survey second one advanced noise main association research field called association computational linguistics acl noise theres difference computational linguistics natural language processing usually considered relatively minor difference noise want explain acronyms well see course mean first one natural language processing noise said study computational treatment human language computational linguistics comes slightly different perspective comes linguistics traditionally included noise mathematical formal approaches less applied practical aspects recent years noise two fields become less interchangeable noise acronyms well hear often information retrieval study finding information documents whether theyre text spoken form videos noise speech processing deals understanding generation spoken signals noise human language technologies used applied component natural language processing noise natural language engineering mostly synonymous human language technology noise finally one acronym see often class related classes ml stands machine learning computation statistical study learning noise lot research natural language processing would like point major conferences field acl annual conference association computational linguistics alternates north american european asian conferences computational linguistics also conference called emnlp stands empirical methods natural language processing also one top tier conferences focuses statistical empirical techniques noise sigir acm conference information retrieval aaai ijcai main international conferences artificial intelligence noise calling biannual conference natural language processing hlt human language technologist conference noise amta summit conferences machine translation finally icslp euro speech conferences speech processing many journals nlp important ones listed probably start looking computational linguistics journal tacl transactions acl published association computational linguistics would also recommend journal natural language engineering information retrieval information processing management well multiple acm transactions specifically acm transactions information systems acm transactions asian language information processing acm transactions spoken language processing many university centers deal research nlp im going list based recent papers acl particular order youre interested finding state art nlps basis go would like list berkeley columbia stanford carnegie mellon johns hopkins brown university massachusetts amherst mit university pennsylvania noise usc university southern california specifically information science institutes universities illinois michigan washington maryland many others outside united states commonly seen names papers university toronto canada edinburgh scotland cambridge sheffield england saarland germany cento italy prague czech republic qatar qatar computing research institute qatar national university singapore many others noise many universities also quite industrial research sites mention largest number papers around noise google microsoft research yahoo ibm sri bbm mitre att labs noise best place go find papers nlp acl anthology available acl website created ten years ago steven bird nowadays managed inaudible national university singapore research papers subfields computational linguistics nlp finally theres experimental website called acl anthology network developed university michigan includes data acl anthology far citations citing sentences concerned find example papers cited text used cite noise want find published year computational linguistics two places want go next segment im going talk specific challenges make natural language processing hard noise,Course3,W1-S1-L4,W1,S1,L4,-,1,1,4,okay go talk administr class want tell expect see class go interact cours four major part first part cover basic linguist mathemat comput background materi nois second part cover comput model morpholog syntax semant discours pragmat main compon linguist third part includ core natur languag process technolog exampl thing like pars part speech tag text gener other nois final go cover applic text classif machin translat inform extract nois three major goal class one learn basic principl theoret issu underli natur languag process two learn techniqu tool use develop practic robust system understand text commun user one languag final gain insight open research problem natur languag nois three book avail cover kind materi differ level detail first one speech languag process daniel jurafski jame martin nois second one foundat statist natur languag process chri man hinrich schutz third one natur languag understand jame allen nois cours place cover similar materi would list like person one criteria includ slide avail read materi onlin alway go back websit learn materi would person recommend cours john hopkin univers taught jason eisner cornel univers taught lillian lee stanford univers chri man univers maryland hal daum univers california berkley dan klein univers texa austin ray mooney also two cours coursera past one man jurafski one michael collin first one survey second one advanc nois main associ research field call associ comput linguist acl nois there differ comput linguist natur languag process usual consid rel minor differ nois want explain acronym well see cours mean first one natur languag process nois said studi comput treatment human languag comput linguist come slightli differ perspect come linguist tradit includ nois mathemat formal approach less appli practic aspect recent year nois two field becom less interchang nois acronym well hear often inform retriev studi find inform document whether theyr text spoken form video nois speech process deal understand gener spoken signal nois human languag technolog use appli compon natur languag process nois natur languag engin mostli synonym human languag technolog nois final one acronym see often class relat class ml stand machin learn comput statist studi learn nois lot research natur languag process would like point major confer field acl annual confer associ comput linguist altern north american european asian confer comput linguist also confer call emnlp stand empir method natur languag process also one top tier confer focus statist empir techniqu nois sigir acm confer inform retriev aaai ijcai main intern confer artifici intellig nois call biannual confer natur languag process hlt human languag technologist confer nois amta summit confer machin translat final icslp euro speech confer speech process mani journal nlp import one list probabl start look comput linguist journal tacl transact acl publish associ comput linguist would also recommend journal natur languag engin inform retriev inform process manag well multipl acm transact specif acm transact inform system acm transact asian languag inform process acm transact spoken languag process mani univers center deal research nlp im go list base recent paper acl particular order your interest find state art nlp basi go would like list berkeley columbia stanford carnegi mellon john hopkin brown univers massachusett amherst mit univers pennsylvania nois usc univers southern california specif inform scienc institut univers illinoi michigan washington maryland mani other outsid unit state commonli seen name paper univers toronto canada edinburgh scotland cambridg sheffield england saarland germani cento itali pragu czech republ qatar qatar comput research institut qatar nation univers singapor mani other nois mani univers also quit industri research site mention largest number paper around nois googl microsoft research yahoo ibm sri bbm mitr att lab nois best place go find paper nlp acl antholog avail acl websit creat ten year ago steven bird nowaday manag inaud nation univers singapor research paper subfield comput linguist nlp final there experiment websit call acl antholog network develop univers michigan includ data acl antholog far citat cite sentenc concern find exampl paper cite text use cite nois want find publish year comput linguist two place want go next segment im go talk specif challeng make natur languag process hard nois,[ 4  8 14 13 12]
225,Course3_W1-S1-L5_-_Why_is_NLP_hard_-_18_slides_25-56,next segment going give us examples difficulties human language make difficult adobe systems understand example box time flies like arrow think probably immediately come reasonable interpretation sentence however sentence one interpretation interpretations may reasonable grammatical one first thought different interpretations think well obvious meaning sentence time flies fast fast arrow therefore say time flies like arrow however metaphorical interpretation imagine computers really good metaphors may well come interpretations sentence much literal let run two possible interpretations first one start sentence like flies like honey insects called flies like eat honey modify sentence slightly say flies like arrow mean something else like arrow honey still valid object verb like finally change sentence fruit flies like arrow specific kind flies like arrow another valid interpretation original sentence yet another one think word time verb like take stopwatch use time race case say well lets time flies go back original sentence time flies like arrow let run classic examples ambiguous sentences confusing sentences natural language first one beverly hills everybody knows beverly hills city california teach computer recognize lets say compound like name city probably interpret next one also city whereas many probably heard name beverly sills know actually name famous person one example box pen example yehoshua barhillel years ago sentence obvious interpretation box could contained inside pen however boxes usually larger pens would completely different interpretation case going assume pen writing instrument instead going assume pen must something larger box could interpretation pen place animals live different pen box clearly talks writing instrument fit inside box lesson learn word order matter lot next examples mary sue mothers lets think sentence second many mothers well mary mother sue mother makes two mothers say mary sue sisters sentence looks similar previous one yet contexts would mean mary sue sisters therefore theyre semantic relationship ones previous sentence every american mother many mothers well assume means theyre roughly many mothers americans least order magnitude change simple word every american president going see semantics sentence changes lot case theres one president americans opposed previous example many mothers final example gave monkeys bananas hungry well subject hungry must refer monkeys bananas cannot hungry whereas next example say gave monkeys bananas overripe well overripe well common sense tells us bananas monkeys lesson learn two sentences may look similar super superficially want translate semantic representation may able straightforward manner many subtle interactions come play lets talk difference syntax semantics often linguistics literature people use stars question marks indicate certain sentence incorrect grammatical way difference two first sentence little mary lamb sentence syntactically correct would expect see sentence like mary little lamb correct syntactic structure sentences syntactically inaccurate usually marked stars question mark second example chomsky says colorless green ideas sleep furiously well think sentence actually syntactically well formed subject like mary little lamb case subject colorless green ideas verb phrase sleep furiously whole sentence sounds syntactically correct however semantics meaning problematic one way problems well first one ideas usually dont sleep order sleep entity capacity ideas doesnt fall category things sleep second problem sleep usually sleep peacefully furiously combination sleeping adverb furiously congruence finally colorless green ideas problematic two ways colorless green two adjectives contradict cannot green colorless time finally ideas cannot described colorless green ideas dont property color colorlessness summarize slide want make distinction syntactically poorly formed sentences like first example sentences semantically incorrect poorly formed like second one lets look different kinds word ambiguity lets start three simple words see every day word ball word board word plant meanings words well one meaning fact many different senses words fly rent tape well also ambiguous different way dont different senses also different parts speech example fly noun fly verb rent noun rent verb difference part speech third category words im showing try pronounce words see common give try next slide explain common lets look answers quiz address address pronounced different whether noun verb many words like example transport versus transport effect outline versus outline resent versus resent example verb infinity means hate somebody resent means send letter second time pronounced differently entrance noun verb means put somebody trance pronounced differently entrance versus entrance finally number either number means comparative adjective numb word number noun computer languages dont kind problem ambiguity designed unambiguous turns also human languages unambiguous design one example lojban picture see next name official logo language designed specifically unambiguous examples ambiguity also known nounnoun phrases sequence three words parsed either group first two modified third one group last two modified first one instance science fiction writer first kind science fiction type literature science fiction writer somebody writes science fiction customer service representative customer service unit customer service representative representative provides customer service first possible interpretation last example state chess tournament try pass phrase way previous two would assume state chess connected case particular example opposite kind grouping chess tournament unit state chess tournament kind chess tournament second possible parse im going jump couple problems naclo annual computation linguistic competition problems first problem called one two tree three tree noah smith kevin gimbel jason eisner deals counting number parse trees sentences second example called fakepapershelfmaker related problem grouping nouns written willie costello uses japanese noun compounds material problem solve problems go back find whether solutions match official solution lets look types ambiguity appear natural language first item morphological ambiguity joe quite impossible means joe possible word impossible morphologically analyzed possible im means negation whereas look similar word joe quite important word important looks superficially similar impossible means something different doesnt mean portant fact portant word important misleadingly considered negation phonetic examples already showed one joes finger got number number pronounced number dont know full sentence means similarly word finger different word singer comes sing different pronunciation finger doesnt come word fing doesnt even exist also ambiguities parts speech example joe first round first round could either verb noun adjective case noun preceded article adjective context could different part speech also syntactic ambiguities call joe taxi sentence either want hail cab joe want name joe name taxi propositional phrase attachment classic example joe ate pizza fork fork relate pizza relate ate refer pizza would mean somehow pizza connected fork definitely case second interpretation fork modifies verb ate tells way joe ate pizza meatballs use sentence would say joe ate pizza meatballs case meatballs modifies pizza joe ate pizza samantha definitely samantha part pizza modifies ate finally joe ate pizza pleasure pleasure modifies ate pizza say ambiguity joe took bar exam bar exam relate many different senses word bar case probably legal sense bar association qualifies lawyers want really funny also interpret taking exam place drinks theres also modality ambiguity joe may win lottery joe may win lottery gives essentially two possible worlds one win lottery one win lottery may way hedge two alternative futures lets look examples ambiguity subjectivity ambiguity joe believes stock rise hear sentence like lead believe stocks rise indicates somebody believes stocks rise subjective opinion another instance modality next example cc attachment cc stands coordinating conjunction words like example joe likes ripe apples pears ambiguity comes fact conjunction link apples pears also link ripe apples pears one examples pears ripe example pears specified either ripe ripe negation ambiguity joe likes pizza cheese tomatoes negation starts word whether applies cheese applies cheese tomatoes ambiguous also referential ambiguity joe yelled mike broken bike word interested refer joe refer mike refer another person well infer contacts probably refers mike otherwise joe wouldnt reason yell look similar example different interpretation joe yelled mike angry well refer mike joe well looks like fairly straightforward interpret sequence sentences referring joe people angry yell case joe angry mike therefore yells refers joe unlike previous example reflexive ambiguity john bought present refer well could refer john probably case would say something like john bought present reflexive pronoun explicitly refers back john whereas john bought present must refer another person one example ambiguity known linguistics ellipsis parallelism joe gave mike beer jeremy glass wine sentence consists two phrases one joe gives beer mike one joe gives jeremy glass wine yet joe gave missing second sentence inferred parallelism two components sentence somebody receiving drink person mike first example jeremy second example whereas person whos giving drink shown first half sentence joe missing second sentence therefore parallelism infer joe full sentence would read something like joe gave mike beer period joe gave jeremy glass wine one final example ambiguity known metonymy lets look sentence boston called left message joe well word boston used literal sense city boston instead sentence means office boston called specifically person office boston called left message joe word expression used refer expression instead instance metonymy addition ambiguities many sources difficulties natural language processing example nonstandard slang novel words usages come time lets look examples model airplane next thing number infinite number different numbers see text phone numbers formatted many different ways plus beginning parentheses without hyphens middle word spam used kind food used verb indicating youre sending somebody unsolicited email verb friend word friend used noun used verb friended facebook words see list recently recognized dictionary words exist years ago example yolo live selfie picture take chillax combination chill relax words recently added major dictionaries fact go urban dictionary find lot normal words unconventional sense still able process natural language system warning website much rrated looked parental warning also inconsistencies text example noncompound junior college type college compound like college junior type person even though two phrases words mean different things theres also major difference pet spray spray pets pet llama small llama see even though two sentences structure different interpretations explicitly spell wrong typos grammatical errors things like receipt people tend spell incorrectly frequently john hopkins instead johns hopkins instead theres also parsing problem deal example cup holder well cup holder could person cup example cup golf tool inside car used hold cup wh youre driving federal reserve board chairman parsed many different ways later class going look many different ways fact parse sequence nouns intended interpretation federal reserve united states national organization federal reserve board specific person chairman federal reserve board sources difficulties complex sentences looked examples counterfactual sentences example worked would happen humor sarcasm computers really hard time dealing humor sarcasm well metaphor nonliteral usages words implicature inference word knowledge refers cases sentence make sense enough shared knowledge people communicate example say late car broke normal person would understand sentence without difficulty however computer would hard time theres lot implied knowledge explicitly said previous sentence example car broke implies car use car get places also means car wheels car break car breaks causes late meeting explicitly mentions important computer also word knowledge often cannot come reasonable interpretation sentence dont access implied knowledge humans take granted theres also significant difference semantics pragmatics sentence semantically correct may specific purpose called pragmatics study words sentences used given context achieve certain goals example somebody asks know time going answer mean possible answers yes know time obviously correct answers user interlocutor wants hear want ask specific time language hard even humans shouldnt surprised even com computers problems example humans theres distinction learning l native language child learning l learning foreign language known difficult learn another instance confusion natural language processing use synonyms paraphrases synonyms refers words similar senses paraphrases refers phrases similar senses lets look three sentences come dow jones first sentence says sp climbed best close since certain date second sentence says nasdaq gained best showing since given date finally dow jones industrial average rose show number points highest level since march see three words left climbed gained rose pretty much synonyms context similarly three phrases right phrases best close best showing highest level indicate exact meaning three examples reason news wire uses different words natural dont want hear exact sentences exact words used next segment going look background needed build natural energy processing systems specifically linguistic mathematical background,Course3,W1-S1-L5,W1,S1,L5,-,1,1,5,next segment go give us exampl difficulti human languag make difficult adob system understand exampl box time fli like arrow think probabl immedi come reason interpret sentenc howev sentenc one interpret interpret may reason grammat one first thought differ interpret think well obviou mean sentenc time fli fast fast arrow therefor say time fli like arrow howev metaphor interpret imagin comput realli good metaphor may well come interpret sentenc much liter let run two possibl interpret first one start sentenc like fli like honey insect call fli like eat honey modifi sentenc slightli say fli like arrow mean someth els like arrow honey still valid object verb like final chang sentenc fruit fli like arrow specif kind fli like arrow anoth valid interpret origin sentenc yet anoth one think word time verb like take stopwatch use time race case say well let time fli go back origin sentenc time fli like arrow let run classic exampl ambigu sentenc confus sentenc natur languag first one beverli hill everybodi know beverli hill citi california teach comput recogn let say compound like name citi probabl interpret next one also citi wherea mani probabl heard name beverli sill know actual name famou person one exampl box pen exampl yehoshua barhillel year ago sentenc obviou interpret box could contain insid pen howev box usual larger pen would complet differ interpret case go assum pen write instrument instead go assum pen must someth larger box could interpret pen place anim live differ pen box clearli talk write instrument fit insid box lesson learn word order matter lot next exampl mari sue mother let think sentenc second mani mother well mari mother sue mother make two mother say mari sue sister sentenc look similar previou one yet context would mean mari sue sister therefor theyr semant relationship one previou sentenc everi american mother mani mother well assum mean theyr roughli mani mother american least order magnitud chang simpl word everi american presid go see semant sentenc chang lot case there one presid american oppos previou exampl mani mother final exampl gave monkey banana hungri well subject hungri must refer monkey banana cannot hungri wherea next exampl say gave monkey banana overrip well overrip well common sens tell us banana monkey lesson learn two sentenc may look similar super superfici want translat semant represent may abl straightforward manner mani subtl interact come play let talk differ syntax semant often linguist literatur peopl use star question mark indic certain sentenc incorrect grammat way differ two first sentenc littl mari lamb sentenc syntact correct would expect see sentenc like mari littl lamb correct syntact structur sentenc syntact inaccur usual mark star question mark second exampl chomski say colorless green idea sleep furious well think sentenc actual syntact well form subject like mari littl lamb case subject colorless green idea verb phrase sleep furious whole sentenc sound syntact correct howev semant mean problemat one way problem well first one idea usual dont sleep order sleep entiti capac idea doesnt fall categori thing sleep second problem sleep usual sleep peac furious combin sleep adverb furious congruenc final colorless green idea problemat two way colorless green two adject contradict cannot green colorless time final idea cannot describ colorless green idea dont properti color colorless summar slide want make distinct syntact poorli form sentenc like first exampl sentenc semant incorrect poorli form like second one let look differ kind word ambigu let start three simpl word see everi day word ball word board word plant mean word well one mean fact mani differ sens word fli rent tape well also ambigu differ way dont differ sens also differ part speech exampl fli noun fli verb rent noun rent verb differ part speech third categori word im show tri pronounc word see common give tri next slide explain common let look answer quiz address address pronounc differ whether noun verb mani word like exampl transport versu transport effect outlin versu outlin resent versu resent exampl verb infin mean hate somebodi resent mean send letter second time pronounc differ entranc noun verb mean put somebodi tranc pronounc differ entranc versu entranc final number either number mean compar adject numb word number noun comput languag dont kind problem ambigu design unambigu turn also human languag unambigu design one exampl lojban pictur see next name offici logo languag design specif unambigu exampl ambigu also known nounnoun phrase sequenc three word pars either group first two modifi third one group last two modifi first one instanc scienc fiction writer first kind scienc fiction type literatur scienc fiction writer somebodi write scienc fiction custom servic repres custom servic unit custom servic repres repres provid custom servic first possibl interpret last exampl state chess tournament tri pass phrase way previou two would assum state chess connect case particular exampl opposit kind group chess tournament unit state chess tournament kind chess tournament second possibl pars im go jump coupl problem naclo annual comput linguist competit problem first problem call one two tree three tree noah smith kevin gimbel jason eisner deal count number pars tree sentenc second exampl call fakepapershelfmak relat problem group noun written willi costello use japanes noun compound materi problem solv problem go back find whether solut match offici solut let look type ambigu appear natur languag first item morpholog ambigu joe quit imposs mean joe possibl word imposs morpholog analyz possibl im mean negat wherea look similar word joe quit import word import look superfici similar imposs mean someth differ doesnt mean portant fact portant word import misleadingli consid negat phonet exampl alreadi show one joe finger got number number pronounc number dont know full sentenc mean similarli word finger differ word singer come sing differ pronunci finger doesnt come word fing doesnt even exist also ambigu part speech exampl joe first round first round could either verb noun adject case noun preced articl adject context could differ part speech also syntact ambigu call joe taxi sentenc either want hail cab joe want name joe name taxi proposit phrase attach classic exampl joe ate pizza fork fork relat pizza relat ate refer pizza would mean somehow pizza connect fork definit case second interpret fork modifi verb ate tell way joe ate pizza meatbal use sentenc would say joe ate pizza meatbal case meatbal modifi pizza joe ate pizza samantha definit samantha part pizza modifi ate final joe ate pizza pleasur pleasur modifi ate pizza say ambigu joe took bar exam bar exam relat mani differ sens word bar case probabl legal sens bar associ qualifi lawyer want realli funni also interpret take exam place drink there also modal ambigu joe may win lotteri joe may win lotteri give essenti two possibl world one win lotteri one win lotteri may way hedg two altern futur let look exampl ambigu subject ambigu joe believ stock rise hear sentenc like lead believ stock rise indic somebodi believ stock rise subject opinion anoth instanc modal next exampl cc attach cc stand coordin conjunct word like exampl joe like ripe appl pear ambigu come fact conjunct link appl pear also link ripe appl pear one exampl pear ripe exampl pear specifi either ripe ripe negat ambigu joe like pizza chees tomato negat start word whether appli chees appli chees tomato ambigu also referenti ambigu joe yell mike broken bike word interest refer joe refer mike refer anoth person well infer contact probabl refer mike otherwis joe wouldnt reason yell look similar exampl differ interpret joe yell mike angri well refer mike joe well look like fairli straightforward interpret sequenc sentenc refer joe peopl angri yell case joe angri mike therefor yell refer joe unlik previou exampl reflex ambigu john bought present refer well could refer john probabl case would say someth like john bought present reflex pronoun explicitli refer back john wherea john bought present must refer anoth person one exampl ambigu known linguist ellipsi parallel joe gave mike beer jeremi glass wine sentenc consist two phrase one joe give beer mike one joe give jeremi glass wine yet joe gave miss second sentenc infer parallel two compon sentenc somebodi receiv drink person mike first exampl jeremi second exampl wherea person who give drink shown first half sentenc joe miss second sentenc therefor parallel infer joe full sentenc would read someth like joe gave mike beer period joe gave jeremi glass wine one final exampl ambigu known metonymi let look sentenc boston call left messag joe well word boston use liter sens citi boston instead sentenc mean offic boston call specif person offic boston call left messag joe word express use refer express instead instanc metonymi addit ambigu mani sourc difficulti natur languag process exampl nonstandard slang novel word usag come time let look exampl model airplan next thing number infinit number differ number see text phone number format mani differ way plu begin parenthes without hyphen middl word spam use kind food use verb indic your send somebodi unsolicit email verb friend word friend use noun use verb friend facebook word see list recent recogn dictionari word exist year ago exampl yolo live selfi pictur take chillax combin chill relax word recent ad major dictionari fact go urban dictionari find lot normal word unconvent sens still abl process natur languag system warn websit much rrate look parent warn also inconsist text exampl noncompound junior colleg type colleg compound like colleg junior type person even though two phrase word mean differ thing there also major differ pet spray spray pet pet llama small llama see even though two sentenc structur differ interpret explicitli spell wrong typo grammat error thing like receipt peopl tend spell incorrectli frequent john hopkin instead john hopkin instead there also pars problem deal exampl cup holder well cup holder could person cup exampl cup golf tool insid car use hold cup wh your drive feder reserv board chairman pars mani differ way later class go look mani differ way fact pars sequenc noun intend interpret feder reserv unit state nation organ feder reserv board specif person chairman feder reserv board sourc difficulti complex sentenc look exampl counterfactu sentenc exampl work would happen humor sarcasm comput realli hard time deal humor sarcasm well metaphor nonliter usag word implicatur infer word knowledg refer case sentenc make sens enough share knowledg peopl commun exampl say late car broke normal person would understand sentenc without difficulti howev comput would hard time there lot impli knowledg explicitli said previou sentenc exampl car broke impli car use car get place also mean car wheel car break car break caus late meet explicitli mention import comput also word knowledg often cannot come reason interpret sentenc dont access impli knowledg human take grant there also signific differ semant pragmat sentenc semant correct may specif purpos call pragmat studi word sentenc use given context achiev certain goal exampl somebodi ask know time go answer mean possibl answer ye know time obvious correct answer user interlocutor want hear want ask specif time languag hard even human shouldnt surpris even com comput problem exampl human there distinct learn l nativ languag child learn l learn foreign languag known difficult learn anoth instanc confus natur languag process use synonym paraphras synonym refer word similar sens paraphras refer phrase similar sens let look three sentenc come dow jone first sentenc say sp climb best close sinc certain date second sentenc say nasdaq gain best show sinc given date final dow jone industri averag rose show number point highest level sinc march see three word left climb gain rose pretti much synonym context similarli three phrase right phrase best close best show highest level indic exact mean three exampl reason news wire use differ word natur dont want hear exact sentenc exact word use next segment go look background need build natur energi process system specif linguist mathemat background,[ 4  0 14 13 12]
226,Course3_W1-S1-L6_-_Background_-_11_slides_16-55,okay next segment going introduce linguistic background well mathematical background needed class told beginning class going look nlp slightly linguistic perspective courses topic want first introduce basicals basics linguistics little bit going even useful class want get coherent picture main topics linguistics lets look issues arise nlp origins linguistics community example idea constituent constituent linguistics unit syntax sentence specific role example sentence children eat pizza sentence eat pizza cases underlined fragment indicating subject sentence replaced sentence would still grammatical say cousins neighbors children eat pizza see three underlined expressions subject sentence therefore something common turns theyre nounphrases subjects sentence see length actually doesnt really matter fact second word even noun pronoun also doesnt matter whats important three items subjects sentence even instances subject sentence explicitly mentioned example tell eat pizza mean eating pizza implicit subject sentence way formulated sentence imperative order subject explicitly marked also need know things collocations collocations groups words appear frequently together would expect chance typically specific meaning example even though word strong synonym powerful contexts say beer strong cannot say beer powerful two synonyms cannot always interchangeable even funny example say big sister cannot say large sister even though big large often synonymous example looking news word rise versus ascend turns past first experiment maybe ten years ago people would never say stocks ascend instead would say stocks rise time hits google former latter one experiment class turns stocks ascend much acceptable collocation fact instances google still major difference frequency two collocations ten one ratio get kind linguistic knowledge nlp system well essentially two approaches one sort manual rules tell well big sister means large sister means something completely different encode part knowledge system hand automatically acquire kinds rules large text collections also known corpora kind knowledge language needed well older areas linguistics phonetics phonology study sounds example consonants vowels consonants stops versus fricatives morphology study word components example word impossible im prefix means negation impossible means something possible syntax study sentence phrase structure subject sentence object verb lexical semantics study meanings individual words compositional semantics combine words segments sentences understand meaning combined sentence pragmatics accomplish goals discourse conventions dealing units text larger single utterances single sentences example multisentential paragraph additional sentence somehow refers first sentence using pronouns forms reference separate lecture different levels language later class computer science look techniques like finitestate automata finitestate automaton machine consists states transitions one states start state also one final accept states example state zero start state indicated solid circle solid line circle state number accept state denoted double circle transitions go follows state zero state one letter c state one state two letter state two state three letter space three four whole automaton used encode sequence three words cat space cats space dogs another automaton known transducer combined previous automaton perform sort phonological morphological analysis sense second automaton starts state zero multiple paths lets look detail top path three consecutive edges first edge converts c zero zero indicates empty string example next edge one two takes input letter produces empty string output finally third transition goes state two state three reads letter produces capital n goes state number three acceptable final state perform specific operation two automata see screen specific operation called composition going happen going read c input top convert sequence sequence empty string followed empty string followed capital n sense simple morphological analyzer slash part speech tagger labeled word cat noun input string cat stopped right would accept state number three would produced noun label stopped however input contains three words middle automaton capability processing three words producing correct labels lets see trace already part look cat label noun n next symbol input space well space takes us state three state zero doesnt produce output essentially go back beginning cats second automaton takes us path produce end going state zero state one state two state three takes us state six going produce p output tape tell us word cats noun plural could stop since theres input process go back state six beginning state zero input empty string like space case output empty string go back state zero word dogs processed going state zero state four state five state three case output letter n one letter letter gets translated capital p point input string called transducer r accept state therefore stop produce output see bottom screen lets read carefully output two labels edge first label comes input second label comes transducer left right focusing first label edge going get original string cat space cats space dogs read second label transition going produce output toggle speech tagger lets try empty string empty string capital n label cat followed empty string empty string empty string capital n capital p label word cats noun plural followed empty strings followed capital n capital p label plural noun dogs point time transducer finished work produce output consists second labels every transition n np np small illustration techniques come theoretical computer science finite state automata used natural language processing areas theoretical computer science find useful natural language processing addition automata kind showed theres also pushdown automata use process sophisticated grammars weeks see work also need import techniques computer science deal grammar specifically regular grammars contextfree grammars contextsensitive grammars kinds grammars covered course later semester also issues related complexity different algorithms example long take process input certain size finally theres aspects programming dynamic programming dynamic programming specific algorithm used produce output depends sequence input much efficient trying possible combinations input talk dynamic programming later class addition linguistics computer science areas nlp originate mathematics statistics whole use probabilities indicate different sentences likely different extent use statistical models hypothesis testing linear algebra well optimization numerical methods going see techniques action later semester well additional mathematical computational tools popular natural language processing include language models used determine probability certain sequence symbols words estimation methods contextfree grammars trees hidden markov models conditional random fields sequences also different statistical models different generative discriminative models maximum entropy models techniques statistics use natural language processing briefly mentioned one socalled vector space representation word sense disambiguation going talk couple weeks noisy channel model machine translation im going illustration two bottom slide lefthand side shows different documents represented vectors vector space two documents query q oh three represented vectors determine two documents similar query q looking angle document query angle q alpha angle q theta case pretty obvious theta larger angle alpha therefore document one actually better match query q example right socalled noisy channel model machine translation based idea originated speech processing idea simple assume two languages want translate assume one languages encoded version language want translate text french english going try identify string english likely given french string converted using bayesian theorem second expression right also simplified third expression right knowing probability french sentence change change english sentence assume ranking purposes sentence english maximizes conditional probability e given f computed way even didnt know value probability french sentence finally third technique used natural language processing originates statistics socalled random walk method random walk method takes graph uses known monte carlo simulation label nodes graph using different values useful tasks natural language processing sentiment analysis input sentence either positive negative want label additional sentences either negative positive looking examples seen far techniques natural language processing originate artificial intelligence im going list include logic specifically first order logic predicate calculus ways present meaning sentences idea agents essentially entities communicate using speech acts idea planning plan sentence decide goes sentence decide want render certain way also techniques going focus little bit later semester constraint satisfaction machine learning next topic going introduction linguistics needed class,Course3,W1-S1-L6,W1,S1,L6,-,1,1,6,okay next segment go introduc linguist background well mathemat background need class told begin class go look nlp slightli linguist perspect cours topic want first introduc basic basic linguist littl bit go even use class want get coher pictur main topic linguist let look issu aris nlp origin linguist commun exampl idea constitu constitu linguist unit syntax sentenc specif role exampl sentenc children eat pizza sentenc eat pizza case underlin fragment indic subject sentenc replac sentenc would still grammat say cousin neighbor children eat pizza see three underlin express subject sentenc therefor someth common turn theyr nounphras subject sentenc see length actual doesnt realli matter fact second word even noun pronoun also doesnt matter what import three item subject sentenc even instanc subject sentenc explicitli mention exampl tell eat pizza mean eat pizza implicit subject sentenc way formul sentenc imper order subject explicitli mark also need know thing colloc colloc group word appear frequent togeth would expect chanc typic specif mean exampl even though word strong synonym power context say beer strong cannot say beer power two synonym cannot alway interchang even funni exampl say big sister cannot say larg sister even though big larg often synonym exampl look news word rise versu ascend turn past first experi mayb ten year ago peopl would never say stock ascend instead would say stock rise time hit googl former latter one experi class turn stock ascend much accept colloc fact instanc googl still major differ frequenc two colloc ten one ratio get kind linguist knowledg nlp system well essenti two approach one sort manual rule tell well big sister mean larg sister mean someth complet differ encod part knowledg system hand automat acquir kind rule larg text collect also known corpora kind knowledg languag need well older area linguist phonet phonolog studi sound exampl conson vowel conson stop versu fric morpholog studi word compon exampl word imposs im prefix mean negat imposs mean someth possibl syntax studi sentenc phrase structur subject sentenc object verb lexic semant studi mean individu word composit semant combin word segment sentenc understand mean combin sentenc pragmat accomplish goal discours convent deal unit text larger singl utter singl sentenc exampl multisententi paragraph addit sentenc somehow refer first sentenc use pronoun form refer separ lectur differ level languag later class comput scienc look techniqu like finitest automata finitest automaton machin consist state transit one state start state also one final accept state exampl state zero start state indic solid circl solid line circl state number accept state denot doubl circl transit go follow state zero state one letter c state one state two letter state two state three letter space three four whole automaton use encod sequenc three word cat space cat space dog anoth automaton known transduc combin previou automaton perform sort phonolog morpholog analysi sens second automaton start state zero multipl path let look detail top path three consecut edg first edg convert c zero zero indic empti string exampl next edg one two take input letter produc empti string output final third transit goe state two state three read letter produc capit n goe state number three accept final state perform specif oper two automata see screen specif oper call composit go happen go read c input top convert sequenc sequenc empti string follow empti string follow capit n sens simpl morpholog analyz slash part speech tagger label word cat noun input string cat stop right would accept state number three would produc noun label stop howev input contain three word middl automaton capabl process three word produc correct label let see trace alreadi part look cat label noun n next symbol input space well space take us state three state zero doesnt produc output essenti go back begin cat second automaton take us path produc end go state zero state one state two state three take us state six go produc p output tape tell us word cat noun plural could stop sinc there input process go back state six begin state zero input empti string like space case output empti string go back state zero word dog process go state zero state four state five state three case output letter n one letter letter get translat capit p point input string call transduc r accept state therefor stop produc output see bottom screen let read care output two label edg first label come input second label come transduc left right focus first label edg go get origin string cat space cat space dog read second label transit go produc output toggl speech tagger let tri empti string empti string capit n label cat follow empti string empti string empti string capit n capit p label word cat noun plural follow empti string follow capit n capit p label plural noun dog point time transduc finish work produc output consist second label everi transit n np np small illustr techniqu come theoret comput scienc finit state automata use natur languag process area theoret comput scienc find use natur languag process addit automata kind show there also pushdown automata use process sophist grammar week see work also need import techniqu comput scienc deal grammar specif regular grammar contextfre grammar contextsensit grammar kind grammar cover cours later semest also issu relat complex differ algorithm exampl long take process input certain size final there aspect program dynam program dynam program specif algorithm use produc output depend sequenc input much effici tri possibl combin input talk dynam program later class addit linguist comput scienc area nlp origin mathemat statist whole use probabl indic differ sentenc like differ extent use statist model hypothesi test linear algebra well optim numer method go see techniqu action later semest well addit mathemat comput tool popular natur languag process includ languag model use determin probabl certain sequenc symbol word estim method contextfre grammar tree hidden markov model condit random field sequenc also differ statist model differ gener discrimin model maximum entropi model techniqu statist use natur languag process briefli mention one socal vector space represent word sens disambigu go talk coupl week noisi channel model machin translat im go illustr two bottom slide lefthand side show differ document repres vector vector space two document queri q oh three repres vector determin two document similar queri q look angl document queri angl q alpha angl q theta case pretti obviou theta larger angl alpha therefor document one actual better match queri q exampl right socal noisi channel model machin translat base idea origin speech process idea simpl assum two languag want translat assum one languag encod version languag want translat text french english go tri identifi string english like given french string convert use bayesian theorem second express right also simplifi third express right know probabl french sentenc chang chang english sentenc assum rank purpos sentenc english maxim condit probabl e given f comput way even didnt know valu probabl french sentenc final third techniqu use natur languag process origin statist socal random walk method random walk method take graph use known mont carlo simul label node graph use differ valu use task natur languag process sentiment analysi input sentenc either posit neg want label addit sentenc either neg posit look exampl seen far techniqu natur languag process origin artifici intellig im go list includ logic specif first order logic predic calculu way present mean sentenc idea agent essenti entiti commun use speech act idea plan plan sentenc decid goe sentenc decid want render certain way also techniqu go focu littl bit later semest constraint satisfact machin learn next topic go introduct linguist need class,[10  4 12  0  2]
227,Course3_W1-S1-L7_-_Linguistics_-_23_slides_21-27,one important aspects natural language processing related study linguistics language next segment im going go basics language language analysis linguistics relate class linguistics study language covers many aspects language example sound system syntactic system use change time let start showing slides relate sounds language first chart ipa international phonetics association chart consonants consonant sound characterized two properties one socalled place articulation organs portions organs use produce one manner articulation way air goes mouth nose pronounce sound lets look examples first column includes sounds like puh buh muh known collectively bilabials bilabial stands two lips use lips pronounce puh buh muh also sounds pronounced example soft palate like velar sounds like kuh guh ung also velar sounds palatal sounds uvular sounds uvular sounds sounds pronounced using uvula little portion palate way back manner articulation pulsive likely flow air suddenly stops say sounds like pu bu tu du ku gu also nasal goes nose like sound sounds also fricative friction pronounced example covers sounds like sound sound third property consonants whether theyre voiced voiceless example pair box many two different consonants example pu bu theyre pronounced exactly way far pace articulation manner articulation concerned differ whether voiced put finger throat try pronounce pu bu realize say bu vocal chords vibrate say puh dont things applies pairs sounds like tuh duh kuh guh shuh juh addition constants vowels vowels characterized two properties one formed mouth front back middle also whether mouth open closed somewhere middle example e sound front vowel pronounced relatively closed mouth whereas much open e still pronounced front mouth sounds like oh oo pronounced back mouth chart like previous one includes vowel sounds english languages well thing want hear linguistics languages related languages related languages many languages related look slide see word night english similar words languages french german way albanian latin sanskrit words dont look exactly origin come language called popioeuropean pie currently existing new york pie languages originate popioeuropean doesnt exist anymore yet words language still survive days like example lets look different night english nuit french nacht german noch russian nosht bulgarian noite portuguese galician notte italian noapte romanian nakts latvian look relatively different yet something common form starts letter n followed vowel specific consonant pair consonants somewhat related words category known cognates cognate term linguistics means words common ancestry language european language family survive day grouped smaller categories example bengali urdu descend sanskrit farsi descends old persian old persian sanskrit part socalled indoiranian group languages greek descended hellenic group languages romanian french catalan descend latin form italic language lithuanian russian polish part baltoslavic group languages modern english descends old english germanic language german descends old high german another germanic language nonindoeuropean languages exist world even spoken europe nearby countries example turkish nonindoeuropean language part socalled altaic family finnish hungarian part finnougric family arabic hebrew part semitic family cover rest world also language families africa americas utoaztecan spoken portions north central america chart shows relative distribution language families world little bit hard see screen get original version wikipedia see many different families theyre distributed world give idea language diversity exists days would like refer website called ethnologue covers languages world grouped different families see screen see families large example austronesian family languages tunisian family languages languages spoken australia islands pacific large groups include sinotibetan languages include chinese related languages languages papua new guinea common fact papua new guinea one places world along indonesia largest number different languages another subject linguists study change language time ill give examples one specific example grimms law grimms law tells words wide range languages changed years systematic way used voiceless stops turned voiceless fricatives used voiced stops became voiceless stops finally used voiced aspirated stops aspirated ones pronounce aspiration like sound changed either voiced stops fricatives two examples example shows one changes occurred variety languages example pu sound appeared ancient greek latin sanskrit changed f sound english german swedish word foot listed examples second example word dog used start letter k ancient greek latin even welsh nowadays changed english dutch german sound huh good time show naclo problem related languages language cognates name problem family used contest time try solve problem next slide see link solution solution problem family deal related families languages cognates going back idea language change im going show ancient piece text im going ask guess source text language period see solution next slide well answer beowulf epic poem written th th century old english little hard read manuscript instead would show text way easier read lefthand side slide shows text appeared old english righthand side shows approximate translation modern english see even though modern english descended old english different difficult recognize words old english also say modern english descend old english also borrowed many words related languages french german years word easy recognize still fairly arcane word aerest erstwhile english means first highlighted boldface middle two texts youre interested english changed old english modern english go list links bottom page slide shows examples words used exist old english commonly used days modern english changed meanings completely example settle settle used mean seat bench nowadays use verb settle dont use settle noun certainly mean seat bench languages different families even languages within family diverse example languages articles dont native speakers russian often hard time speak english concept article words like russian russian speakers sometimes difficult say whether say apartment say apartment different languages also cases case indication word used different syntactic roles sentence example latin sentence like one shown puer puellam vexat puer means boy puellam means girl vexat form verb vex annoy order words sentence really important important first word puer appears socalled nominative case indicates subject sentence whereas middle word puellam indicates puella accusative case object verb finally vexat verb combining information together know boy annoying girl way around english way express use word order latin replace order words different word order still get semantics meaning semantic roles carried endings cases languages also differ sound systems example languages known glottal stop middle sound uhoh something appears languages others sounds dont exist english example socalled velar fricatives articulated back tongue soft palate examples sounds include voiceless velar fricative sound used languages like russian voiced sound used languages modern greek dont exist english examples english would include two th sounds voiceless voiced dont appear many languages problem native speakers languages thats sometimes people make fun way french people speak say zis zat used pronouncing th sound english proper languages also differ way refer people structure family example otousan japanese means somebody elses father chichi means somebodys father problematic english speaker goes japan wants refer somebody elses father chichi would considered rude least sign person native speaker kinship systems general complicated english uncles aunts daughtersinlaw languages different ways refer relatives fact one complicated kinship systems comes language called warlpiri brings us next naclo problem alan chang listed website go try solve problem realize language related specific community people different castes intermarry descendants people different caste different names depending many generations happened since intermarriage solution problem next topic linguistics would like talk concept language universals properties valid across languages world two types language universals unconditional conditional im going go examples language universals example example unconditional language universal languages verbs nouns spoken languages consonants vowels languages consonants languages vowels two examples conditional language universals come greenberg whos one founders theory language universals rule number list following declarative sentences sentences end periods opposed question marks exclamation points declarative sentences nominal noun subject object dominant order always one subject precedes object subject object nouns much common subject precede object rule number greenbergs list language inflection must also derivation words mean inflection deals fact change word changing morphology keeping part speech example take world sleep sleep change sleeps inflecting third person deal changing part speech word changing morphology example take word drink noun verb change drinkable adjective adding suffix able summarize rule number greenbergs list language inflection always derivation directional conditional universe doesnt apply opposite direction one website would recommend wals world atlas language structures available online walsinfo relatively recent project covers several hundred languages almost features features include things like order object verb turns object receives verb languages database verb receives objects cases languages dont particularly predominant order features covered database features absence common consonants turns database languages enumerated bilabial sounds fricatives nasals rule number inflectional future tense let explain means english want express certain verb future tense need use external word like shall want say sleep versus sleep future add additional word languages example french latin actually use inflection verb like inflection english plural third person verbs french example add specific ending verb indicate happen future based feature turns split languages evenly yes categories websites recommend world languages included slide first one ethnologue large website covers seven thousand languages world includes lot statistics examples usage next site long list common number words like many different languages would like use opportunity raise issue endangered languages turns many languages world every year dozen tend disappear disappear mean last speakers language either die stop using language altogether switch different language language extinction continues rate years majority worlds languages died theres effort save dying languages preserving records oral written form also producing technology help people speak understand languages continue use adult introduction linguistics next section going talk pattern speech,Course3,W1-S1-L7,W1,S1,L7,-,1,1,7,one import aspect natur languag process relat studi linguist languag next segment im go go basic languag languag analysi linguist relat class linguist studi languag cover mani aspect languag exampl sound system syntact system use chang time let start show slide relat sound languag first chart ipa intern phonet associ chart conson conson sound character two properti one socal place articul organ portion organ use produc one manner articul way air goe mouth nose pronounc sound let look exampl first column includ sound like puh buh muh known collect bilabi bilabi stand two lip use lip pronounc puh buh muh also sound pronounc exampl soft palat like velar sound like kuh guh ung also velar sound palat sound uvular sound uvular sound sound pronounc use uvula littl portion palat way back manner articul pulsiv like flow air suddenli stop say sound like pu bu tu du ku gu also nasal goe nose like sound sound also fric friction pronounc exampl cover sound like sound sound third properti conson whether theyr voic voiceless exampl pair box mani two differ conson exampl pu bu theyr pronounc exactli way far pace articul manner articul concern differ whether voic put finger throat tri pronounc pu bu realiz say bu vocal chord vibrat say puh dont thing appli pair sound like tuh duh kuh guh shuh juh addit constant vowel vowel character two properti one form mouth front back middl also whether mouth open close somewher middl exampl e sound front vowel pronounc rel close mouth wherea much open e still pronounc front mouth sound like oh oo pronounc back mouth chart like previou one includ vowel sound english languag well thing want hear linguist languag relat languag relat languag mani languag relat look slide see word night english similar word languag french german way albanian latin sanskrit word dont look exactli origin come languag call popioeuropean pie current exist new york pie languag origin popioeuropean doesnt exist anymor yet word languag still surviv day like exampl let look differ night english nuit french nacht german noch russian nosht bulgarian noit portugues galician nott italian noapt romanian nakt latvian look rel differ yet someth common form start letter n follow vowel specif conson pair conson somewhat relat word categori known cognat cognat term linguist mean word common ancestri languag european languag famili surviv day group smaller categori exampl bengali urdu descend sanskrit farsi descend old persian old persian sanskrit part socal indoiranian group languag greek descend hellen group languag romanian french catalan descend latin form ital languag lithuanian russian polish part baltoslav group languag modern english descend old english german languag german descend old high german anoth german languag nonindoeuropean languag exist world even spoken europ nearbi countri exampl turkish nonindoeuropean languag part socal altaic famili finnish hungarian part finnougr famili arab hebrew part semit famili cover rest world also languag famili africa america utoaztecan spoken portion north central america chart show rel distribut languag famili world littl bit hard see screen get origin version wikipedia see mani differ famili theyr distribut world give idea languag divers exist day would like refer websit call ethnologu cover languag world group differ famili see screen see famili larg exampl austronesian famili languag tunisian famili languag languag spoken australia island pacif larg group includ sinotibetan languag includ chines relat languag languag papua new guinea common fact papua new guinea one place world along indonesia largest number differ languag anoth subject linguist studi chang languag time ill give exampl one specif exampl grimm law grimm law tell word wide rang languag chang year systemat way use voiceless stop turn voiceless fric use voic stop becam voiceless stop final use voic aspir stop aspir one pronounc aspir like sound chang either voic stop fric two exampl exampl show one chang occur varieti languag exampl pu sound appear ancient greek latin sanskrit chang f sound english german swedish word foot list exampl second exampl word dog use start letter k ancient greek latin even welsh nowaday chang english dutch german sound huh good time show naclo problem relat languag languag cognat name problem famili use contest time tri solv problem next slide see link solut solut problem famili deal relat famili languag cognat go back idea languag chang im go show ancient piec text im go ask guess sourc text languag period see solut next slide well answer beowulf epic poem written th th centuri old english littl hard read manuscript instead would show text way easier read lefthand side slide show text appear old english righthand side show approxim translat modern english see even though modern english descend old english differ difficult recogn word old english also say modern english descend old english also borrow mani word relat languag french german year word easi recogn still fairli arcan word aerest erstwhil english mean first highlight boldfac middl two text your interest english chang old english modern english go list link bottom page slide show exampl word use exist old english commonli use day modern english chang mean complet exampl settl settl use mean seat bench nowaday use verb settl dont use settl noun certainli mean seat bench languag differ famili even languag within famili divers exampl languag articl dont nativ speaker russian often hard time speak english concept articl word like russian russian speaker sometim difficult say whether say apart say apart differ languag also case case indic word use differ syntact role sentenc exampl latin sentenc like one shown puer puellam vexat puer mean boy puellam mean girl vexat form verb vex annoy order word sentenc realli import import first word puer appear socal nomin case indic subject sentenc wherea middl word puellam indic puella accus case object verb final vexat verb combin inform togeth know boy annoy girl way around english way express use word order latin replac order word differ word order still get semant mean semant role carri end case languag also differ sound system exampl languag known glottal stop middl sound uhoh someth appear languag other sound dont exist english exampl socal velar fric articul back tongu soft palat exampl sound includ voiceless velar fric sound use languag like russian voic sound use languag modern greek dont exist english exampl english would includ two th sound voiceless voic dont appear mani languag problem nativ speaker languag that sometim peopl make fun way french peopl speak say zi zat use pronounc th sound english proper languag also differ way refer peopl structur famili exampl otousan japanes mean somebodi els father chichi mean somebodi father problemat english speaker goe japan want refer somebodi els father chichi would consid rude least sign person nativ speaker kinship system gener complic english uncl aunt daughtersinlaw languag differ way refer rel fact one complic kinship system come languag call warlpiri bring us next naclo problem alan chang list websit go tri solv problem realiz languag relat specif commun peopl differ cast intermarri descend peopl differ cast differ name depend mani gener happen sinc intermarriag solut problem next topic linguist would like talk concept languag univers properti valid across languag world two type languag univers uncondit condit im go go exampl languag univers exampl exampl uncondit languag univers languag verb noun spoken languag conson vowel languag conson languag vowel two exampl condit languag univers come greenberg who one founder theori languag univers rule number list follow declar sentenc sentenc end period oppos question mark exclam point declar sentenc nomin noun subject object domin order alway one subject preced object subject object noun much common subject preced object rule number greenberg list languag inflect must also deriv word mean inflect deal fact chang word chang morpholog keep part speech exampl take world sleep sleep chang sleep inflect third person deal chang part speech word chang morpholog exampl take word drink noun verb chang drinkabl adject ad suffix abl summar rule number greenberg list languag inflect alway deriv direct condit univers doesnt appli opposit direct one websit would recommend wal world atla languag structur avail onlin walsinfo rel recent project cover sever hundr languag almost featur featur includ thing like order object verb turn object receiv verb languag databas verb receiv object case languag dont particularli predomin order featur cover databas featur absenc common conson turn databas languag enumer bilabi sound fric nasal rule number inflect futur tens let explain mean english want express certain verb futur tens need use extern word like shall want say sleep versu sleep futur add addit word languag exampl french latin actual use inflect verb like inflect english plural third person verb french exampl add specif end verb indic happen futur base featur turn split languag evenli ye categori websit recommend world languag includ slide first one ethnologu larg websit cover seven thousand languag world includ lot statist exampl usag next site long list common number word like mani differ languag would like use opportun rais issu endang languag turn mani languag world everi year dozen tend disappear disappear mean last speaker languag either die stop use languag altogeth switch differ languag languag extinct continu rate year major world languag die there effort save die languag preserv record oral written form also produc technolog help peopl speak understand languag continu use adult introduct linguist next section go talk pattern speech,[ 4  8  0 14 13]
228,Course3_W10-S1-L1_-_Collocations_-_20_slides_11-54,welcome back natural language processing next segment going extracting collocations text collocations phrases dictionary definitions definitions different meaning individual words example say somebody kicked bucket doesnt mean person actually literally kicked bucket means person died theres famous saying firth says know word company keeps means words meanings depend words surround general phrases many different uses many different categories heres examples dead end street doesnt exit somebody died strong tea example indicate even though word strong word powerful synonyms dont say powerful tea always say strong tea two words form collocation different meaning individual words names people names diseases examples phrases collocations different properties commonly used one time occurrences general syntactic semantic rules theyre important nonnative speakers learn language fluently collocation acquisition learning collocation text important natural language processing applications things like question answering machine translation theres common agreement exactly forms collocation im going try give examples different multiword sequences makes different one categories called idioms second one called freeword combination third one called collocations heres example idiomatic expressions include things like kick bucket dead end catch collocations include things consist case common words combined meaning slightly different meaning individual words example trade actively table contents terminology like orthogonal projection third category free word combinations common words combining together compositional doesnt convey special meaning phrase example take bus means theres bus take go somewhere end road buy house interested freeword combinations mostly interested idioms collocations particular collocations properties arbitrary substitutions usually allowed example say make effort cannot say make exertion even though effort exertions similar say running commentary cannot say running discussion say commit treason cannot say commit treachery collocations language dialect specific go one language another notice collocations dont necessarily get translated translating individual word example french say direct traffic well say regler la circulation something like regulate traffic languages example russian german serbocroatian actually word regulate used directing traffic another example american english people say set table make decision whereas british english people say mostly lay table take decision heres one example french semer le dessaroi means wreak havoc actually translation sow disarray semer means sow collocations common technical language recurrent context many uses collocations used example disambiguation ambiguous word like word bank another word near example loan river part collocation able disambiguate word bank easily theyre also useful translation also type generation grammatically speaking multiple types collocations grammatical include things like phrasal verbs example come put afraid fond preposition phrases like accident also semantic certain synonyms allowed others flexible terms physical location words form example say find something chance words dont necessarily near could intermediate set words find followed entire clause chance one important distinction collocation analysis socalled base collocator base defined bears meaning collocation writers think base first foreign language speakers search expression dictionary based base decoding understanding purposes actually appropriate store collocation collocator examples bases collocators nouns verbs set table base noun table collocator verb noun adjective base would noun collocator adjective cases adverbs prepositions adverb preposition always collocator extract collocations automatically text take common bigrams perhaps trigrams fourgrams turns right approach often correspond freeword combinations drop function words turns give us additional mileage one possibility look part speech sequences said collocations fairly arbitrary theres reliable way extract based part speech sequences okay lets look techniques extracting collocations work one common techniques based mutual information remind mutual information turn random variables log ratio joint probability divided product individual probabilities example larger values means collocation stronger mutual information equal means correlation two words dont form collocation value mutual information negative means words actually less likely appear together would expect chance okay one techniques used extract collocation based socalled yule coefficient simple compute suppose two words w x id like use following notation capital w capital x means particular words appear given background little w little x means words dont appear frequency pairs involve words b frequency pairs bigrams involve one words c one neither yule coefficient computed taking diagonal contingency matrix ad bc divided sum numbers ad bc gives us numbers range indicates strong collocation lets look example topleft corner spreadsheet cases words appear b c finally cases none words appear compute formula get score mildly strong collocation heres example hansard corpus canadian parliamentary proceedings include french english text technique like used extract collocations translate one languages word prime prime minister translated following words french based mutual information sentences aligned two languages one thing think whether collocation flexible rigid heres example paper frank smarger p means second word collocation appears exactly first one p means appears two words later p means appears three words later p means appears right see case pair words appear together times cases appear right next still cases appear small number words would example fairly rigid collocation examples doesnt work cases even rigid collocations numbers would p p pair free trade xtract system developed frank smadja able extract collocations like dow jones industrial average flexible collocations like nyses composite index listed common stocks fell number number lets see translate collocations mentioned translating collocations trivial dont want translate one word time heres example brush lesson french gets translated repasser une lecon repasser means go bring expression english written phrasal verb russian translated single word foreign examples hansards corpus late spring translated fin du printemps means end spring atlantic canada opportunities agency gets translated agence de promotion economique du canada atlantique slightly different translation economic promotion atlantic canada heres examples websites contain phrasal collocations english language idioms theres large website includes many called idiomsitecom concludes section collocation extraction,Course3,W10-S1-L1,W10,S1,L1,-,10,1,1,welcom back natur languag process next segment go extract colloc text colloc phrase dictionari definit definit differ mean individu word exampl say somebodi kick bucket doesnt mean person actual liter kick bucket mean person die there famou say firth say know word compani keep mean word mean depend word surround gener phrase mani differ use mani differ categori here exampl dead end street doesnt exit somebodi die strong tea exampl indic even though word strong word power synonym dont say power tea alway say strong tea two word form colloc differ mean individu word name peopl name diseas exampl phrase colloc differ properti commonli use one time occurr gener syntact semant rule theyr import nonn speaker learn languag fluentli colloc acquisit learn colloc text import natur languag process applic thing like question answer machin translat there common agreement exactli form colloc im go tri give exampl differ multiword sequenc make differ one categori call idiom second one call freeword combin third one call colloc here exampl idiomat express includ thing like kick bucket dead end catch colloc includ thing consist case common word combin mean slightli differ mean individu word exampl trade activ tabl content terminolog like orthogon project third categori free word combin common word combin togeth composit doesnt convey special mean phrase exampl take bu mean there bu take go somewher end road buy hous interest freeword combin mostli interest idiom colloc particular colloc properti arbitrari substitut usual allow exampl say make effort cannot say make exert even though effort exert similar say run commentari cannot say run discuss say commit treason cannot say commit treacheri colloc languag dialect specif go one languag anoth notic colloc dont necessarili get translat translat individu word exampl french say direct traffic well say regler la circul someth like regul traffic languag exampl russian german serbocroatian actual word regul use direct traffic anoth exampl american english peopl say set tabl make decis wherea british english peopl say mostli lay tabl take decis here one exampl french semer le dessaroi mean wreak havoc actual translat sow disarray semer mean sow colloc common technic languag recurr context mani use colloc use exampl disambigu ambigu word like word bank anoth word near exampl loan river part colloc abl disambigu word bank easili theyr also use translat also type gener grammat speak multipl type colloc grammat includ thing like phrasal verb exampl come put afraid fond preposit phrase like accid also semant certain synonym allow other flexibl term physic locat word form exampl say find someth chanc word dont necessarili near could intermedi set word find follow entir claus chanc one import distinct colloc analysi socal base colloc base defin bear mean colloc writer think base first foreign languag speaker search express dictionari base base decod understand purpos actual appropri store colloc colloc exampl base colloc noun verb set tabl base noun tabl colloc verb noun adject base would noun colloc adject case adverb preposit adverb preposit alway colloc extract colloc automat text take common bigram perhap trigram fourgram turn right approach often correspond freeword combin drop function word turn give us addit mileag one possibl look part speech sequenc said colloc fairli arbitrari there reliabl way extract base part speech sequenc okay let look techniqu extract colloc work one common techniqu base mutual inform remind mutual inform turn random variabl log ratio joint probabl divid product individu probabl exampl larger valu mean colloc stronger mutual inform equal mean correl two word dont form colloc valu mutual inform neg mean word actual less like appear togeth would expect chanc okay one techniqu use extract colloc base socal yule coeffici simpl comput suppos two word w x id like use follow notat capit w capit x mean particular word appear given background littl w littl x mean word dont appear frequenc pair involv word b frequenc pair bigram involv one word c one neither yule coeffici comput take diagon conting matrix ad bc divid sum number ad bc give us number rang indic strong colloc let look exampl topleft corner spreadsheet case word appear b c final case none word appear comput formula get score mildli strong colloc here exampl hansard corpu canadian parliamentari proceed includ french english text techniqu like use extract colloc translat one languag word prime prime minist translat follow word french base mutual inform sentenc align two languag one thing think whether colloc flexibl rigid here exampl paper frank smarger p mean second word colloc appear exactli first one p mean appear two word later p mean appear three word later p mean appear right see case pair word appear togeth time case appear right next still case appear small number word would exampl fairli rigid colloc exampl doesnt work case even rigid colloc number would p p pair free trade xtract system develop frank smadja abl extract colloc like dow jone industri averag flexibl colloc like nyse composit index list common stock fell number number let see translat colloc mention translat colloc trivial dont want translat one word time here exampl brush lesson french get translat repass une lecon repass mean go bring express english written phrasal verb russian translat singl word foreign exampl hansard corpu late spring translat fin du printemp mean end spring atlant canada opportun agenc get translat agenc de promot economiqu du canada atlantiqu slightli differ translat econom promot atlant canada here exampl websit contain phrasal colloc english languag idiom there larg websit includ mani call idiomsitecom conclud section colloc extract,[ 4  8 14 13 12]
229,Course3_W10-S1-L2_-_Information_Retrieval_-_32_slides_21-38,next segment going first one sequence segments information retrieval information retrieval one important tasks web people search web daily far class concerned natural language processing useful variety information retrieval tasks search engines things like google bing baidu yandex dominate online communities people send billions queries every day information retrieval building search engines improving analyzing results heres examples yahoo search common search engine query word ebola list documents returned system amazon different type search engine involves products put product name samsung galaxy output list products match description properties types search engines example library congress website search books based topics authors publication years many search criteria addition conventional search engine library catalogs search keywords titles authors also search engines based text example lexisnexis google yahoo youre typically allowed search keywords theres limited support queries natural language also imagebased search engines search images based shapes colors also keywords appear captions descriptions also question answering systems example askcom specialize answering natural language questions discuss different section class also experimental clustering systems old examples include vivisimo clusty results shown form clusters related documents word multiple senses senses get separate cluster results example word jaguar system doesnt know youre looking car animal football team figure multiple senses word give results separate last type search engines want mention research systems lemur nutch mostly used research projects lets look kind queries people ask internet example get rid stretch marks single words like dodge names people questions like many calories pumpkin pie typo example examples queries angelina jolie brad pitt vote derek jeter interstellar trailer last example shows theyre looking specific object namely movie trailer specific movie interstellar theyre also asking definitional questions example ebola extracted website thats part google shows commonly asked queries given day large web well according world wide websites billion pages indexed google billion indexed bing fairly large number heres statistics web reported twitter gets million tweets per day billion photos uploaded facebook every month googles clusters process petabytes data per day challenges information retrieval example dealing things like dynamically generate content pages static rather get output based user query indexing new pages get added web point time decentralized fashion increasing number blog posts said size blogosphere doubles every six months kind queries users post internet well result theyre usually form sessions users ask one query come back revisit later theyre still interested topic may replace words add words get better results possibly start completely different query still within session often asked short queries median number words query internet two words long theres large number typos small number popular queries theres inaudible division frequency queries long tale infrequent queries asked twice given time period even though search engines provide advanced query operators word one near word two word one immediately adjacent word two operations users dont use except phrases users learned included double quotes information retrieval important process involves collection documents users query goal process find relevant documents match users query heres key terms used information retrieval query like presentation user looking list words phrase document information entity user wants retrieve document doesnt single webpage passage entire website collection set documents index representation information makes automatic process quarrying easier term word concept appears document query lets look documents documents printed paper said sort records pages entire sites images people movies books theyre often encoded many different formats process converts standard representation example unicode documents represented internally computer preprocessed example metadata removed javascript additional notations removed documents segmented words terms also types tokens remind type set occurrences word token every particular instance particular word search engine look like first decide want index collect information index efficiently make sure index kept date built index make sure provide user friendly query facilities heres architecture typical search engine information sources document collector creates document collection document processor converts documents index side query gets processed query processor form query representation mapping documents queries process called document ranking processes documents finds ones similar users query returns search results shown user search interface user looks documents decides ones relevant ones possibly revises information need starting completely new query go back search engine go next page hits example essentially saying first page hits good enough many search engines possible update query produce completely different ranking documents represented well theyre represented form termdocument matrices terms n documents also represented document document matrices matrix n documents n documents represents similar two documents medium sized collection would something like n equals three million documents equals terms multiply two numbers together get size typical term document matrix medium sized collection case web however get something much bigger said may many billion documents web size vocabulary size number different terms want index may one million see enormously large matrix cannot stored computers figure ways represent information one thing store inside matrix either boolean values integer values boolean values say specific term appears particular document whereas integer value represents number times stream appears document store data information retrieval purposes lets go back example three million documents words vocabulary large think termdocument matrix going example way better instead large number cells heuristic use simplify amount storage necessary turns technique called inverted index instead incidence vector tell us documents contain particular word something called posting table posting table tells us given work restore list documents word appears lets say instead lets say million values zero keep ones zeros vermont may appear document one document two document six massachusetts may appear documents one five six seven also use linked lists insert new document postings order keep list sorted also use link list remove existing posting example example document five longer contains words massachusetts immediately delete sorted link list keep kind inverted index easily used compute document frequency one hint keep everything sorted youre going logarithmic improvement access find certain element list logarithmic time basic operations perform inverted indexes first one conjunction mean two words want find documents include words turns theres simple algorithm iteratively merge two posting lists going left right increasing order complexity order xy sum lengths two postings algorithm disjunction similar reminder junction would finding documents contain one words negation little bit difficult deal documents dont contain word lot ones contain example querys vermont massachusetts massachusetts vermont possible build recursive operation deal parenthesis also optimize inverted indexes starting smallest set example three words b c may want start looking combining b one query result forms smaller set documents combine set documents set corresponds third one documents typically presented called vector model many dimensional space different terms one two three example documents presented vector vector space remember another lecture find similarity two documents vector space taking angle two vectors correspond taking cosine angle queries often represented way documents several important advantages mathematically easier manage example formulas computing similarity applied documents queries pairs documents pairs queries exactly way problems queries said shorter documents additional processing necessary example query doesnt contain words document may want additional query expansion actually get matching documents also syntactic differences documents typically syntactically wellformed whereas queries theres also problem repetitions words lack thereof queries typically dont repeat words except limited cases phrasal combinations whereas documents many words repeated lets see represent query vector like documents represent vector form efficient representation looks like ten words example document labeled w w also counts words numbered c c whats matching process document space documents want find whether document similar query whether documents similar done using several different metrics based distances based similarity one important thing realize case distances larger distances means less similar whereas large similarity means similar euclidean distance euclidean distance straight line distance two points vector space manhattan distance also commonly used tells many steps take dimension get one point another example go two steps direction first term three steps direction second term gives manhattan distance five whereas equivalent euclidean distance would square root two squared plus five squared word overlap number words two documents common jaccard coefficient normalized version word overlap okay lets look specific examples similarity measures commonly used one cosine measure normalized dot product let see computed document query could also two documents example document q query formula cosine similarity sigma document query q given middle numerator sides intersection two words appear documents divided square root two vectors length times length q square root terms actual word counts sub number times word appears document q sub number times word appears query use formula righthand side compute cosine jaccard coefficient similar size intersection two divided size union two heres exercise please try compute cosine scores following examples two documents pair documents given documents represented vector finally represented interested two combinations compute corresponding euclidean distance manhattan distances jaccard coefficients figure match dont match okay next thing need figure deal phrase based queries things like new york new mexico new york city ann arbor barack obama commonly used query dont want able match york city new hampshire even though contains exact words appear new york city want find document contains words query also want words appear right order done using technique called positional indexing going keep track words documents appear also positions words documents case find multi word phrase need look matching words appearing next example position new may appear position york city okay lets see document ranking general straightforward process able compute similarity users query documents collection typically done using cosine similarity one thing discuss little bit later tfidf weighting suffice say formula gives weight content words names important words whereas low content words socalled stop words low tfidf corresponds prepositions articles found documents similarities query documents return top k matching documents user also possible document reranking document return similar documents already returned may want skip document whats formula idf well idea idf used find words appear fewer documents word appears many documents means probably useful corresponds prepositions articles said idf stands inverse document frequency want metric higher word appears many documents n number documents collection sub k number documents contain term k f sub ik absolute frequency term k document w sub ik weight term k document many times appears idf defined term k log base two n divided sub k plus example word appears documents n divided n equal log base equal lowest possible idf get equal hand certain word appears small fraction documents example one means nd sub k going logarithm going equal base base two idf example going going continue new segment information,Course3,W10-S1-L2,W10,S1,L2,-,10,1,2,next segment go first one sequenc segment inform retriev inform retriev one import task web peopl search web daili far class concern natur languag process use varieti inform retriev task search engin thing like googl bing baidu yandex domin onlin commun peopl send billion queri everi day inform retriev build search engin improv analyz result here exampl yahoo search common search engin queri word ebola list document return system amazon differ type search engin involv product put product name samsung galaxi output list product match descript properti type search engin exampl librari congress websit search book base topic author public year mani search criteria addit convent search engin librari catalog search keyword titl author also search engin base text exampl lexisnexi googl yahoo your typic allow search keyword there limit support queri natur languag also imagebas search engin search imag base shape color also keyword appear caption descript also question answer system exampl askcom special answer natur languag question discuss differ section class also experiment cluster system old exampl includ vivisimo clusti result shown form cluster relat document word multipl sens sens get separ cluster result exampl word jaguar system doesnt know your look car anim footbal team figur multipl sens word give result separ last type search engin want mention research system lemur nutch mostli use research project let look kind queri peopl ask internet exampl get rid stretch mark singl word like dodg name peopl question like mani calori pumpkin pie typo exampl exampl queri angelina joli brad pitt vote derek jeter interstellar trailer last exampl show theyr look specif object name movi trailer specif movi interstellar theyr also ask definit question exampl ebola extract websit that part googl show commonli ask queri given day larg web well accord world wide websit billion page index googl billion index bing fairli larg number here statist web report twitter get million tweet per day billion photo upload facebook everi month googl cluster process petabyt data per day challeng inform retriev exampl deal thing like dynam gener content page static rather get output base user queri index new page get ad web point time decentr fashion increas number blog post said size blogospher doubl everi six month kind queri user post internet well result theyr usual form session user ask one queri come back revisit later theyr still interest topic may replac word add word get better result possibl start complet differ queri still within session often ask short queri median number word queri internet two word long there larg number typo small number popular queri there inaud divis frequenc queri long tale infrequ queri ask twice given time period even though search engin provid advanc queri oper word one near word two word one immedi adjac word two oper user dont use except phrase user learn includ doubl quot inform retriev import process involv collect document user queri goal process find relev document match user queri here key term use inform retriev queri like present user look list word phrase document inform entiti user want retriev document doesnt singl webpag passag entir websit collect set document index represent inform make automat process quarri easier term word concept appear document queri let look document document print paper said sort record page entir site imag peopl movi book theyr often encod mani differ format process convert standard represent exampl unicod document repres intern comput preprocess exampl metadata remov javascript addit notat remov document segment word term also type token remind type set occurr word token everi particular instanc particular word search engin look like first decid want index collect inform index effici make sure index kept date built index make sure provid user friendli queri facil here architectur typic search engin inform sourc document collector creat document collect document processor convert document index side queri get process queri processor form queri represent map document queri process call document rank process document find one similar user queri return search result shown user search interfac user look document decid one relev one possibl revis inform need start complet new queri go back search engin go next page hit exampl essenti say first page hit good enough mani search engin possibl updat queri produc complet differ rank document repres well theyr repres form termdocu matric term n document also repres document document matric matrix n document n document repres similar two document medium size collect would someth like n equal three million document equal term multipli two number togeth get size typic term document matrix medium size collect case web howev get someth much bigger said may mani billion document web size vocabulari size number differ term want index may one million see enorm larg matrix cannot store comput figur way repres inform one thing store insid matrix either boolean valu integ valu boolean valu say specif term appear particular document wherea integ valu repres number time stream appear document store data inform retriev purpos let go back exampl three million document word vocabulari larg think termdocu matrix go exampl way better instead larg number cell heurist use simplifi amount storag necessari turn techniqu call invert index instead incid vector tell us document contain particular word someth call post tabl post tabl tell us given work restor list document word appear let say instead let say million valu zero keep one zero vermont may appear document one document two document six massachusett may appear document one five six seven also use link list insert new document post order keep list sort also use link list remov exist post exampl exampl document five longer contain word massachusett immedi delet sort link list keep kind invert index easili use comput document frequenc one hint keep everyth sort your go logarithm improv access find certain element list logarithm time basic oper perform invert index first one conjunct mean two word want find document includ word turn there simpl algorithm iter merg two post list go left right increas order complex order xy sum length two post algorithm disjunct similar remind junction would find document contain one word negat littl bit difficult deal document dont contain word lot one contain exampl queri vermont massachusett massachusett vermont possibl build recurs oper deal parenthesi also optim invert index start smallest set exampl three word b c may want start look combin b one queri result form smaller set document combin set document set correspond third one document typic present call vector model mani dimension space differ term one two three exampl document present vector vector space rememb anoth lectur find similar two document vector space take angl two vector correspond take cosin angl queri often repres way document sever import advantag mathemat easier manag exampl formula comput similar appli document queri pair document pair queri exactli way problem queri said shorter document addit process necessari exampl queri doesnt contain word document may want addit queri expans actual get match document also syntact differ document typic syntact wellform wherea queri there also problem repetit word lack thereof queri typic dont repeat word except limit case phrasal combin wherea document mani word repeat let see repres queri vector like document repres vector form effici represent look like ten word exampl document label w w also count word number c c what match process document space document want find whether document similar queri whether document similar done use sever differ metric base distanc base similar one import thing realiz case distanc larger distanc mean less similar wherea larg similar mean similar euclidean distanc euclidean distanc straight line distanc two point vector space manhattan distanc also commonli use tell mani step take dimens get one point anoth exampl go two step direct first term three step direct second term give manhattan distanc five wherea equival euclidean distanc would squar root two squar plu five squar word overlap number word two document common jaccard coeffici normal version word overlap okay let look specif exampl similar measur commonli use one cosin measur normal dot product let see comput document queri could also two document exampl document q queri formula cosin similar sigma document queri q given middl numer side intersect two word appear document divid squar root two vector length time length q squar root term actual word count sub number time word appear document q sub number time word appear queri use formula righthand side comput cosin jaccard coeffici similar size intersect two divid size union two here exercis pleas tri comput cosin score follow exampl two document pair document given document repres vector final repres interest two combin comput correspond euclidean distanc manhattan distanc jaccard coeffici figur match dont match okay next thing need figur deal phrase base queri thing like new york new mexico new york citi ann arbor barack obama commonli use queri dont want abl match york citi new hampshir even though contain exact word appear new york citi want find document contain word queri also want word appear right order done use techniqu call posit index go keep track word document appear also posit word document case find multi word phrase need look match word appear next exampl posit new may appear posit york citi okay let see document rank gener straightforward process abl comput similar user queri document collect typic done use cosin similar one thing discuss littl bit later tfidf weight suffic say formula give weight content word name import word wherea low content word socal stop word low tfidf correspond preposit articl found document similar queri document return top k match document user also possibl document rerank document return similar document alreadi return may want skip document what formula idf well idea idf use find word appear fewer document word appear mani document mean probabl use correspond preposit articl said idf stand invers document frequenc want metric higher word appear mani document n number document collect sub k number document contain term k f sub ik absolut frequenc term k document w sub ik weight term k document mani time appear idf defin term k log base two n divid sub k plu exampl word appear document n divid n equal log base equal lowest possibl idf get equal hand certain word appear small fraction document exampl one mean nd sub k go logarithm go equal base base two idf exampl go go continu new segment inform,[ 2  4 14 13 12]
230,Course3_W10-S1-L3_-_Evaluation_of_IR_-_14_slides_11-15,next segment information retrieval deal evaluation information retrieval systems important evaluate search engines difficult compare purely based anecdotal evidence metrics use validate information retrieval systems include size index large engines like bing google get high score criteria speed indexing also important want whenever change document indexed added index quickly possible speed retrieval also important users typically dont want wait fraction second get hits accuracy important provide correct answers users queries timeliness index date versions documents whenever use wants find happened dont want show older versions documents ease use course also important easy formulate query advise query another important criterion expressiveness language allow natural language questions allow searching phrases words appear together sentence words different languages alternatives let show something called contingency table used often information retrieval evaluation also many tasks including question answering summarization seen sections class contingency table four cells first column going present number documents retrieved information retrieval system given query second column show documents retrieved system first row includes documents actually relevant query second row ones relevant query sum numbers four cells equal number documents entire collection w number two positives documents repeated system also relevant users query z number two negatives number documents retrieved system also supposed retrieved theyre relevant user query w z obviously good thing want system good values high values w z diagonal x equals false negative documents retrieved system supposed retrieved theyre misses also similarly equal number false positives documents supposed retrieved system returned incorrectly n sum w total retrieved documents n w x number relevant documents n number documents collection w x z like text summarization define precision recall recall w w x precision w w refresh memories recall means recall going high system returns relevant actual documents relevant precision going high almost documents returned system indeed relevant okay lets look issues involved evaluating information table systems one thing easily cross ones mind use accuracy evaluation metric accuracy would sum two good values w number two positives z number two negatives obviously normalized n one problem typical case value z much larger value w completely possible get accuracy even higher even w zero close zero obviously something want dont want emphasize two negatives two positives one issue represent report rather performance system large number queries typically averaging precision possible queries given evaluation another thing typically done report value precision equal recall done system doesnt artificially inflate p expense r vice versa finally one metric used report single number evaluation f measure represents weighted combination precisions recall practice commonly used f one measure sum reciprocals recall precision words harmonic mean precision recall often research papers see f measures reported single metrics next slide going show us sample trec query youre going ask trec trec text retrieval conference organized annually national institutes standards technology united states provides assistance collection documents also collection queries three different sections heres example three sections title description narrative title example dangerous vehicles description crashworthy least crashworthy passenger vehicles narrative systems participate evaluation supposed use one two three fields determine settings relevant documents collection example ids right represent documents considered relevant query human annotators heres document typically looks like trec marked gml xml recently contains markup metadata headline length date paragraphs marked appropriately youre interested trec evaluation nist website find different trecs used different years theres set presentations shows different systems compare one another addition tech evaluation corpus people reference first collections information retrieval generic retrieval things like ohsumed collection medical documents cranfield cacm include things like manuals also types scientific papers text classification one topics going look little bit later people use reuters collection newsgroups collection questioning answering use trec question answering collection web retrieval use dotgov wtg first block call entire dotgov domain second one gigabyte collection web corps blog purposes people use many different datasets including buzzmetric datasets finally ad hoc information retrieval document retrieval people use trec collections relatively small size gb web use trec web collections lot data lets see compare performance two systems suppose one system gives f measure another gives f measure claim second system better clearly cannot one query given single query completely possible one system happens really good job one fail instead need find average performance many different query one thing even better make sure every query one systems performs one okay one common methods used find system better another socalled sign test technique comes statistics community based principle hypothesis testing two systems systems run number queries collected performance queries want compare many times system outperforms system b many times got performance many times b outperformed statistical hypothesis testing called null hypothesis says two systems equally good based data observed want find probability null hypothesis rejected means system actually better system b plug numbers sign test calculator going see probability case means unlikely two systems equally good reject null hypothesis assume better b another example better b times b better times case ratio much smaller two categories plug numbers calculator well see public means dont enough evidence reject model hypothesis therefore cannot claim system better system b important write research paper information retrieval include statistical significant stats like actually validate claim system superior heres external link actual scientist calculator use experiments theres tests used dont look sign difference two systems one better actually look difference dividers two systems one tests socalled student ttest heres another calculator corresponds test another test used lot information retrieval called wilcoxon matchedpairs signedranks test im including link calculator concludes section evaluation information retrieval system,Course3,W10-S1-L3,W10,S1,L3,-,10,1,3,next segment inform retriev deal evalu inform retriev system import evalu search engin difficult compar pure base anecdot evid metric use valid inform retriev system includ size index larg engin like bing googl get high score criteria speed index also import want whenev chang document index ad index quickli possibl speed retriev also import user typic dont want wait fraction second get hit accuraci import provid correct answer user queri timeli index date version document whenev use want find happen dont want show older version document eas use cours also import easi formul queri advis queri anoth import criterion express languag allow natur languag question allow search phrase word appear togeth sentenc word differ languag altern let show someth call conting tabl use often inform retriev evalu also mani task includ question answer summar seen section class conting tabl four cell first column go present number document retriev inform retriev system given queri second column show document retriev system first row includ document actual relev queri second row one relev queri sum number four cell equal number document entir collect w number two posit document repeat system also relev user queri z number two neg number document retriev system also suppos retriev theyr relev user queri w z obvious good thing want system good valu high valu w z diagon x equal fals neg document retriev system suppos retriev theyr miss also similarli equal number fals posit document suppos retriev system return incorrectli n sum w total retriev document n w x number relev document n number document collect w x z like text summar defin precis recal recal w w x precis w w refresh memori recal mean recal go high system return relev actual document relev precis go high almost document return system inde relev okay let look issu involv evalu inform tabl system one thing easili cross one mind use accuraci evalu metric accuraci would sum two good valu w number two posit z number two neg obvious normal n one problem typic case valu z much larger valu w complet possibl get accuraci even higher even w zero close zero obvious someth want dont want emphas two neg two posit one issu repres report rather perform system larg number queri typic averag precis possibl queri given evalu anoth thing typic done report valu precis equal recal done system doesnt artifici inflat p expens r vice versa final one metric use report singl number evalu f measur repres weight combin precis recal practic commonli use f one measur sum reciproc recal precis word harmon mean precis recal often research paper see f measur report singl metric next slide go show us sampl trec queri your go ask trec trec text retriev confer organ annual nation institut standard technolog unit state provid assist collect document also collect queri three differ section here exampl three section titl descript narr titl exampl danger vehicl descript crashworthi least crashworthi passeng vehicl narr system particip evalu suppos use one two three field determin set relev document collect exampl id right repres document consid relev queri human annot here document typic look like trec mark gml xml recent contain markup metadata headlin length date paragraph mark appropri your interest trec evalu nist websit find differ trec use differ year there set present show differ system compar one anoth addit tech evalu corpu peopl refer first collect inform retriev gener retriev thing like ohsum collect medic document cranfield cacm includ thing like manual also type scientif paper text classif one topic go look littl bit later peopl use reuter collect newsgroup collect question answer use trec question answer collect web retriev use dotgov wtg first block call entir dotgov domain second one gigabyt collect web corp blog purpos peopl use mani differ dataset includ buzzmetr dataset final ad hoc inform retriev document retriev peopl use trec collect rel small size gb web use trec web collect lot data let see compar perform two system suppos one system give f measur anoth give f measur claim second system better clearli cannot one queri given singl queri complet possibl one system happen realli good job one fail instead need find averag perform mani differ queri one thing even better make sure everi queri one system perform one okay one common method use find system better anoth socal sign test techniqu come statist commun base principl hypothesi test two system system run number queri collect perform queri want compar mani time system outperform system b mani time got perform mani time b outperform statist hypothesi test call null hypothesi say two system equal good base data observ want find probabl null hypothesi reject mean system actual better system b plug number sign test calcul go see probabl case mean unlik two system equal good reject null hypothesi assum better b anoth exampl better b time b better time case ratio much smaller two categori plug number calcul well see public mean dont enough evid reject model hypothesi therefor cannot claim system better system b import write research paper inform retriev includ statist signific stat like actual valid claim system superior here extern link actual scientist calcul use experi there test use dont look sign differ two system one better actual look differ divid two system one test socal student ttest here anoth calcul correspond test anoth test use lot inform retriev call wilcoxon matchedpair signedrank test im includ link calcul conclud section evalu inform retriev system,[ 2  9  4  8 14]
231,Course3_W10-S1-L4_-_Text_Classification_-_26_slides_26-36,okay welcome natural language processing next topic text classification text classification want assign documents predefined categories example based topics lets say science technology versus music based languages based different users given set classes capital c given document x determine class c belongs many different ways classification first important distinction whether want hierarchical versus flat classification hierarchical classification general topic example lets say business within business additional subcategories example finance management flatcut classification one set categories hierarchical could also distinguish overlapping nonoverlapping classification overlapping classification also known soft classification document classified multiple classes example document business sports may classified business sports whereas nonoverlapping classification make hard decision classify document exactly one class many different techniques classifications one use manual classification using set rules example list countries world states united states document contains mention countries states classify geography example heres example rule says word columbia word university classify education columbia university university columbia south carolina want classify document geography columbia capital south carolina popular techniques classification fall two general categories one called generative model include k nearest neighbors naive bayes another one discriminative includes svm support vector machines regression generative classification model joint probability x x document class use bayesian prediction compute probability class given document discriminative classification model probability give x directly without using joint probability document classification also clustering one topics well cover later documents represented exactly way document retrieval typically use vector based representation words like cat dog corresponds single dimension vector space also use dimensions things like document length author name method data features addition words document represented vector ndimensional space idea similar documents going appear nearby vector space use distance measures ones used document retrieval cosine similarity jaccard coefficient lets look naive bayes classification naive bayes classification simple idea looked example tech summarization skip slide remember going describe slightly different way directly relevant document classification setup given document want find probability document belongs certain class c given includes features f fk according bayesian formula rewrite formula probability feature set given document class normalize probability document class first place divided probability seeing particular combination features given joint probability distribution features typically run problems lot different features want use naive bayesian assumption assume features statistically independent case formula changes one probability document class given features describe going equal product conditional probabilities individual feature given doctor class times probability doctor class whole product divided product probabilities individual features features typically words phrases sometimes metadata issues naive bayes solve get values well example find probability document particular class purpose use maximum likelihood estimation dividing number time appears class training data total number documents collection also compute conditional probabilities similar way assume theyre generated using multinomial generator maximum likelihood estimator going number times appear divided total number occurrences need smoothing like statistical parsing section class frequencies low one possible type smoothing used laplacian smoothing add one count occurrences divide normalized sum important naive bayes implement everything correctly since youre going multiplying lot small numbers may run floating point underflow case much better take logarithms values instead something like minus th power would number minus want multiply multiple values numbers space logarithmic space would add logarithms lets look specific example text classification problem spam recognition heres message many may received years back get email sent randomly says im sick want help save investments im going give commission order first send deposit use withdraw money bank typical examples spam messages recognize spam automatically using text classification well techniques based basic classification spam assassin open source project thats part apache foundation go website corresponds spamassassin look different tests performs every message gets sent emailed tests essentially ways compute features combined together determine score message let give examples body incorporate tracking id number yes certain number points added stamp square document looking body email message html text parts different yes indicates larger probability document spam message header date three six hours received date means large mailing many different recipients messages got delayed another feature indicates message may spam examples body html font size huge attempt obfuscate words subject example take common word appears spam emails replace letter exclamation point something like spam filter looking particular word going catch also different sorts regular expressions correspond typical spam messages example anything includes urgent transfer money warning reply proposal notification dollar amounts going labeled spam determine features important include name based classifier every feature important features may fact completely irrelevant classification process one techniques used lot socalled chisquared test chisquared capital greek letter chi computed following way youre given specific term feature want compute many times feature appears relation number currencies individual classes let explain table represents c means particular document spam c equals means document spam sub equals means particular term feature present document equals means particular feature present document inaudible contingency table check whether feature class independent probability example getting class c feature equals equal probability times probability means two independent different two likely feature informative particular class probability joint probability larger product marginal probabilities means feature positively correlated class negative means feature negatively correlated class positively correlated negative class compute probabilities contingency table example probability class sum counts k k normalized n n total number documents probability class equals sum values second k k n probability equal k k normalized n compute chisquare value well theres simple formula given plug values numbers two diagonals k k k k also plug n sum different ks value get chisquare score particular feature high values chisquare indicate lower belief independence also means feature important indicative positive class typically chisquare value five six ten means feature good included classification process practice want compute chisquare value words features pick top k among another important criterion since youre computing numbers training set always run risk words may even appear test set thats important pick words features high chisquare values also want pick relatively small yet known zero high square values likely appear test set lets quickly look wellknown datasets used text classification evaluation include newsgroups collection articles usenet groups sports politics technology reuters collection documents reuters collected different categories mostly business news articles future exchanges webkb extracted cmu website webpages departments people courses rcv reuters collection many documents original reuters evaluate text classification one possibility microaveraging performances classes macroaveraging means use pooled table performances lets look little bit vector space classification techniques based inaudible example x one dimensions corresponds one terms documents x another dimension documents representing vector spaces one two classes example topic represented red circles topic represented stars vector space classification want find decision boundary separates circles stars one possibility take marker circle elements belong one classes decide decision boundary problem build kind decision boundary trending data working correctly test data fitting specific training data important come decision surface relatively small complexity example straight line hyper plane rather something cannot described parameters one possibility use decision trees vector space set horizontal vertical lines example classifier says looking vertical dotted line document right dotted line classify star else starting look two horizontal lines upper horizontal line classify star lower horizontal line classify star else classify circle case going decision tree four nodes nodes correspond one classes ambiguous either star circle obviously much better come linear boundary dont overfit data example straight line correspond decision boundary probably notice easily makes mistake classifies correctly seven eight documents incorrectly labels one documents circle even though actually star lets look examples different vector space classifiers one techniques used build centroid cluster imagine centroid vector corresponds way vectors belong class want build line straight line equidistant two centroids hyperplane thats equidistant two centroids decision boundary cough decision boundary given equation wx wx b let explain values w w weights x x coordinates vector two dimensions x xb b called bias b equal zero essentially saying two classes dont prior distinction b different favor one two classes default wx wx b equation line separates two classes new document value wx wx b means classified positive class similarly less b classify negative class general case ndimensional spaces extension formula weight vector transposed multiply dot product x vector document want check result b case document falls decision boundary greater less b case classify lets look example two dimensional space decision boundary corresponds two classes even though error dotted line corresponds normal vector w orthogonal decision boundary new document example star appears lowest page dot product weighed vector going positive angle less degrees therefore labeled class star one circles going angle greater degrees weight vector therefore labeled circle class lets see practice specific numbers setting setting already weight vectors correspond decision boundary receive new document want decide classify hear weights two columns correspond words f positive weights theyre associated positive class set words g l negative weights correspond negative class lets assume simplicity bias equal new document comes one instance word one instance word one e one h classify document well essentially compute doc product vector weighed vector heres done times means word appears document weight decision boundary vector times also appears left column times e finally h column minus times add numbers together get score greater therefore going classify document positive class mean pretty obvious looking table instead e g would lot evidence favor negative class would therefore classify document negative class lets look quickly important algorithm called perceptron algorithm used machine learning classification obviously right class discuss detail take class machine learning find im going sketch use perceptron input following data set training vectors classes x vector document labeled corresponds positive class equals corresponds negative class n training examples also parameter eta used determine fast algorithms going converge algorithm seven eight lines code heres works goal remember learn set weights w going iterative fashion first going assign value zero weights sot zeroth iteration w going assign zero k k number steps taken going repeat n times following code going take ith element data going check class assigned document sub matches dot product weight document two means product going greater zero two different means one positive one negative dot product going less zero mean product less equal zero means different sign product w times x means mismatch classification process case going going readjust weight vector k plus first iteration starting version kth iteration adding eta times sub times x sub going increase number k number iterations algorithm stops eight iterations going produce w sub k thats set weights lets see practice example chris bishop heres works decision boundary black line separates two classes red blue classes obviously doesnt good job point blue red dots sides line want somehow update line blue dots one side red dots side going pick one dots lets say one circled green realize dot decision boundary makes classification mistake labels blue cluster whereas point red whats going happen going change decision boundary moving vector adding red vector black vector get new black vector corresponds normal decision boundary new normal vector shown right hand side corresponding decision boundary data original one still mistakes red blue dots sides curve take next misclassified dot one shown green thing add vector corresponds dot current normal vector red arrow points green circle add black vector get new red vector means rotate decision boundary one time case stop decision boundary correctly classifies red dots blue dots whatever normal vector sets weights want return part algorithm okay concludes section perceptron lets quickly look one technique text classification namely generated model known knearest neighbor knearest neighbors want take vectors training data figure vectors closest want classify vector based majority vectors closest called knearest neighbors k different numbers theres equality heres compute score cluster document c using sum bias class sort prior belief random document belongs class plus sum documents knearest neighbors document trying classify similarity document neighbors rank process based score easy program knearest neighbor algorithm however issue need figure values k b k odd number shouldnt one typically values often work reasonably well value b bias corresponds prior belief particular class frequent others theres nice online demo knearest neighbors url im going stop segment going continue soon next one,Course3,W10-S1-L4,W10,S1,L4,-,10,1,4,okay welcom natur languag process next topic text classif text classif want assign document predefin categori exampl base topic let say scienc technolog versu music base languag base differ user given set class capit c given document x determin class c belong mani differ way classif first import distinct whether want hierarch versu flat classif hierarch classif gener topic exampl let say busi within busi addit subcategori exampl financ manag flatcut classif one set categori hierarch could also distinguish overlap nonoverlap classif overlap classif also known soft classif document classifi multipl class exampl document busi sport may classifi busi sport wherea nonoverlap classif make hard decis classifi document exactli one class mani differ techniqu classif one use manual classif use set rule exampl list countri world state unit state document contain mention countri state classifi geographi exampl here exampl rule say word columbia word univers classifi educ columbia univers univers columbia south carolina want classifi document geographi columbia capit south carolina popular techniqu classif fall two gener categori one call gener model includ k nearest neighbor naiv bay anoth one discrimin includ svm support vector machin regress gener classif model joint probabl x x document class use bayesian predict comput probabl class given document discrimin classif model probabl give x directli without use joint probabl document classif also cluster one topic well cover later document repres exactli way document retriev typic use vector base represent word like cat dog correspond singl dimens vector space also use dimens thing like document length author name method data featur addit word document repres vector ndimension space idea similar document go appear nearbi vector space use distanc measur one use document retriev cosin similar jaccard coeffici let look naiv bay classif naiv bay classif simpl idea look exampl tech summar skip slide rememb go describ slightli differ way directli relev document classif setup given document want find probabl document belong certain class c given includ featur f fk accord bayesian formula rewrit formula probabl featur set given document class normal probabl document class first place divid probabl see particular combin featur given joint probabl distribut featur typic run problem lot differ featur want use naiv bayesian assumpt assum featur statist independ case formula chang one probabl document class given featur describ go equal product condit probabl individu featur given doctor class time probabl doctor class whole product divid product probabl individu featur featur typic word phrase sometim metadata issu naiv bay solv get valu well exampl find probabl document particular class purpos use maximum likelihood estim divid number time appear class train data total number document collect also comput condit probabl similar way assum theyr gener use multinomi gener maximum likelihood estim go number time appear divid total number occurr need smooth like statist pars section class frequenc low one possibl type smooth use laplacian smooth add one count occurr divid normal sum import naiv bay implement everyth correctli sinc your go multipli lot small number may run float point underflow case much better take logarithm valu instead someth like minu th power would number minu want multipli multipl valu number space logarithm space would add logarithm let look specif exampl text classif problem spam recognit here messag mani may receiv year back get email sent randomli say im sick want help save invest im go give commiss order first send deposit use withdraw money bank typic exampl spam messag recogn spam automat use text classif well techniqu base basic classif spam assassin open sourc project that part apach foundat go websit correspond spamassassin look differ test perform everi messag get sent email test essenti way comput featur combin togeth determin score messag let give exampl bodi incorpor track id number ye certain number point ad stamp squar document look bodi email messag html text part differ ye indic larger probabl document spam messag header date three six hour receiv date mean larg mail mani differ recipi messag got delay anoth featur indic messag may spam exampl bodi html font size huge attempt obfusc word subject exampl take common word appear spam email replac letter exclam point someth like spam filter look particular word go catch also differ sort regular express correspond typic spam messag exampl anyth includ urgent transfer money warn repli propos notif dollar amount go label spam determin featur import includ name base classifi everi featur import featur may fact complet irrelev classif process one techniqu use lot socal chisquar test chisquar capit greek letter chi comput follow way your given specif term featur want comput mani time featur appear relat number currenc individu class let explain tabl repres c mean particular document spam c equal mean document spam sub equal mean particular term featur present document equal mean particular featur present document inaud conting tabl check whether featur class independ probabl exampl get class c featur equal equal probabl time probabl mean two independ differ two like featur inform particular class probabl joint probabl larger product margin probabl mean featur posit correl class neg mean featur neg correl class posit correl neg class comput probabl conting tabl exampl probabl class sum count k k normal n n total number document probabl class equal sum valu second k k n probabl equal k k normal n comput chisquar valu well there simpl formula given plug valu number two diagon k k k k also plug n sum differ ks valu get chisquar score particular featur high valu chisquar indic lower belief independ also mean featur import indic posit class typic chisquar valu five six ten mean featur good includ classif process practic want comput chisquar valu word featur pick top k among anoth import criterion sinc your comput number train set alway run risk word may even appear test set that import pick word featur high chisquar valu also want pick rel small yet known zero high squar valu like appear test set let quickli look wellknown dataset use text classif evalu includ newsgroup collect articl usenet group sport polit technolog reuter collect document reuter collect differ categori mostli busi news articl futur exchang webkb extract cmu websit webpag depart peopl cours rcv reuter collect mani document origin reuter evalu text classif one possibl microaverag perform class macroaverag mean use pool tabl perform let look littl bit vector space classif techniqu base inaud exampl x one dimens correspond one term document x anoth dimens document repres vector space one two class exampl topic repres red circl topic repres star vector space classif want find decis boundari separ circl star one possibl take marker circl element belong one class decid decis boundari problem build kind decis boundari trend data work correctli test data fit specif train data import come decis surfac rel small complex exampl straight line hyper plane rather someth cannot describ paramet one possibl use decis tree vector space set horizont vertic line exampl classifi say look vertic dot line document right dot line classifi star els start look two horizont line upper horizont line classifi star lower horizont line classifi star els classifi circl case go decis tree four node node correspond one class ambigu either star circl obvious much better come linear boundari dont overfit data exampl straight line correspond decis boundari probabl notic easili make mistak classifi correctli seven eight document incorrectli label one document circl even though actual star let look exampl differ vector space classifi one techniqu use build centroid cluster imagin centroid vector correspond way vector belong class want build line straight line equidist two centroid hyperplan that equidist two centroid decis boundari cough decis boundari given equat wx wx b let explain valu w w weight x x coordin vector two dimens x xb b call bia b equal zero essenti say two class dont prior distinct b differ favor one two class default wx wx b equat line separ two class new document valu wx wx b mean classifi posit class similarli less b classifi neg class gener case ndimension space extens formula weight vector transpos multipli dot product x vector document want check result b case document fall decis boundari greater less b case classifi let look exampl two dimension space decis boundari correspond two class even though error dot line correspond normal vector w orthogon decis boundari new document exampl star appear lowest page dot product weigh vector go posit angl less degre therefor label class star one circl go angl greater degre weight vector therefor label circl class let see practic specif number set set alreadi weight vector correspond decis boundari receiv new document want decid classifi hear weight two column correspond word f posit weight theyr associ posit class set word g l neg weight correspond neg class let assum simplic bia equal new document come one instanc word one instanc word one e one h classifi document well essenti comput doc product vector weigh vector here done time mean word appear document weight decis boundari vector time also appear left column time e final h column minu time add number togeth get score greater therefor go classifi document posit class mean pretti obviou look tabl instead e g would lot evid favor neg class would therefor classifi document neg class let look quickli import algorithm call perceptron algorithm use machin learn classif obvious right class discuss detail take class machin learn find im go sketch use perceptron input follow data set train vector class x vector document label correspond posit class equal correspond neg class n train exampl also paramet eta use determin fast algorithm go converg algorithm seven eight line code here work goal rememb learn set weight w go iter fashion first go assign valu zero weight sot zeroth iter w go assign zero k k number step taken go repeat n time follow code go take ith element data go check class assign document sub match dot product weight document two mean product go greater zero two differ mean one posit one neg dot product go less zero mean product less equal zero mean differ sign product w time x mean mismatch classif process case go go readjust weight vector k plu first iter start version kth iter ad eta time sub time x sub go increas number k number iter algorithm stop eight iter go produc w sub k that set weight let see practic exampl chri bishop here work decis boundari black line separ two class red blue class obvious doesnt good job point blue red dot side line want somehow updat line blue dot one side red dot side go pick one dot let say one circl green realiz dot decis boundari make classif mistak label blue cluster wherea point red what go happen go chang decis boundari move vector ad red vector black vector get new black vector correspond normal decis boundari new normal vector shown right hand side correspond decis boundari data origin one still mistak red blue dot side curv take next misclassifi dot one shown green thing add vector correspond dot current normal vector red arrow point green circl add black vector get new red vector mean rotat decis boundari one time case stop decis boundari correctli classifi red dot blue dot whatev normal vector set weight want return part algorithm okay conclud section perceptron let quickli look one techniqu text classif name gener model known knearest neighbor knearest neighbor want take vector train data figur vector closest want classifi vector base major vector closest call knearest neighbor k differ number there equal here comput score cluster document c use sum bia class sort prior belief random document belong class plu sum document knearest neighbor document tri classifi similar document neighbor rank process base score easi program knearest neighbor algorithm howev issu need figur valu k b k odd number shouldnt one typic valu often work reason well valu b bia correspond prior belief particular class frequent other there nice onlin demo knearest neighbor url im go stop segment go continu soon next one,[ 2  4  5 14 13]
232,Course3_W10-S1-L5_-_Text_Clustering_-_17_slides_15-39,recently discussed text classification going switch slightly related topic information retrieval specifically text clustering difference text classification text clustering text classification know advance sort classes looking whether theyre overlapping whether theyre hierarchical however text clustering dont know advance figure first many clusters data decide documents go cluster clustering also issue exclusive versus overlapping clusters document first case may assigned one cluster second case may assigned multiple clusters also distinction classification hierarchical flat clustering information retrieval theres important principle called cluster hypothesis says documents cluster usually relevant query somehow several documents similar one relevant particular query means rest documents cluster also expected relevant practice lets look example clusty old information retrieval system clusters documents based word senses word jaguar jaguar obviously ambiguous word english correspond sports team car animal several things idea search engine retrieves documents match jaguar looks similar tries identify clusters correspond different word senses jaguar without knowing advance jaguar ambiguous without knowing certain number senses gives one cluster cars one cluster animals one simplest techniques document clustering called kmeans method done iteratively determining cluster point belongs adjusting cluster centroid cluster repeat needed however know number clusters advance k kmeans based hard decisions document assigned certain cluster cannot change clusters cannot assigned another cluster simultaneously heres algorithm initialize cluster centroids k cluster arbitrary vector improvement possible following document find cluster closest document assign document cluster c cluster c recompute centroid cluster c based documents thats okay lets look example kmeans clustering k equals six vectors want classify two clusters documents said assign arbitrary values cluster centroids beginning lets make simple going class labeled centroid class class centroid much easier compute distances vectors centroids heads lets look document closer well pretty obvious based euclidian manhattan distance belongs second cluster document b value going closer centroid c also going closer interesting example right middle two centroids cannot really use round ignore e closer finally f closer happens first half first iteration six documents labeled five one way specifically documents b c f belong cluster centered documents e belong second cluster second part first iteration namely recomputing centroids lets cluster used includes new centroid going average vectors plus plus equal divide eightthirds first dimension second dimension plus plus divided second dimension essentially twothirds comma new centroid first cluster second cluster one used going computed follows take average first dimension going second dimension going two new centroids first full iteration repeat procedure reassigning documents b c e f one two clusters recomputing centroids repeat sequence iterations many times necessary centroids converge means moment two centroids dont change one iteration next stop theyre never going change point websites interesting demos kmeans one second one third one fourth one would like encourage look tutorials understand kmeans little bit better evaluate clustering cases far assume know advance number clusters one important thing realize dont know number clusters essentially following try different values k figure clusterings gives us better performance one way evaluate performance clustering first technique going introduce called purity based majority class clusters second one called rand index going shown next slide lets look purity first three clusters first one three xs two circles second one three circles one x one last one four two xs pure clusters well first cluster majority class x three five elements cluster xs therefore purity second cluster value three circles five majority class purity finally last cluster four six majority class take average entire set compute overall purity total number elements clustered gives total purity obviously first cluster consisting xs second cluster consisting circles third one consisting get purity next metric used evaluating clustering socalled rand index rand index based following principle going score points every time get two objects really belong class labeled together cluster going lose points every time two objects cluster label different clusters rand index value equal number true positives plus number true negatives divided total number pairs objects heres example using example previous slide number true positives number false positives together first example five total objects theres five choose two pairs five objects gives us ten also five choose two pairs second cluster another ten finally six choose two pairs third cluster total number positives things supposed get cluster similarly compute number true positives separately first case three xs choose equal thing next cluster choose final cluster two different kinds objects appear together first one appears four times four choose two last one two pick two choose two sum four terms equal number false positives equal first number minus second number minus equal gives us two thousand contingency tables specifically true positive false positives use math compute false negatives true negatives compute rand index two positives plus two negatives divided total number plus divided equal moderately good agreement clustering algorithm everything correctly clustered exact way gold standard rand index going equal one going true positives true negatives none two categories methods clustering specifically designated hierarchical clustering everything looked far mostly flat clustering one techniques used socalled singlelinkage method singlelinkage take two objects figure similar think similar enough put cluster collapse two clusters figure theres even one pair documents close enough going merge two clusters disadvantages get long chains documents even related theyre pairwise related first one different last one still cluster another method called completelinkage method case order merge two clusters consider pairs documents one one cluster one cluster theyre similar enough merge two clusters one disadvantage method conservative finally socalled averagelinkage method based average similarity objects two clusters lets look example suppose documents going start collapsing together documents similar example cluster cluster cluster cluster point going four clusters two documents case depending algorithm would either merge would merge last step hierarchical clustering process going merge two remaining clusters one end going one big cluster includes documents two subclusters include four documents finally four going grouped groups two lets look example hierarchy called agglomerative clustering using dendrogram dendrogram technique builds hierarchical structure based similarity heres example language similarity six germanic languages icelandic norwegian swedish danish dutch german want measure similar based use different words case turns norwegian swedish similar around going cluster together level new cluster norwegian swedish going merge cluster danish dutch german similar theyre going clustered together group consists norwegian swedish danish going merged icelandic around finally two remaining clusters going clustered whole collection gives us nice representation allows us produce number clusters start righthand side move vertical bar left bar lets say say one cluster lower going cross two horizontal lines going nordic group four languages another group two languages slide vertical bar lets say going get four clusters slide way wee going get six clusters example shown website lets look example clustering using dendrograms suppose following sentences following documents first one abcba next one adccade want build hierarchical clustering diagram using dendrograms need compute pairwise similarities pairs documents one two one three one four identify closest pair case could example document one document two merge single node based frequencies words appear two documents combined repeat documents clustered also represent venn diagram depending similarity metric may get something like first document second document one cluster fourth fifth one another cluster fourth fifth combined third one another cluster five another cluster concludes section text clustering going continue different topic,Course3,W10-S1-L5,W10,S1,L5,-,10,1,5,recent discuss text classif go switch slightli relat topic inform retriev specif text cluster differ text classif text cluster text classif know advanc sort class look whether theyr overlap whether theyr hierarch howev text cluster dont know advanc figur first mani cluster data decid document go cluster cluster also issu exclus versu overlap cluster document first case may assign one cluster second case may assign multipl cluster also distinct classif hierarch flat cluster inform retriev there import principl call cluster hypothesi say document cluster usual relev queri somehow sever document similar one relev particular queri mean rest document cluster also expect relev practic let look exampl clusti old inform retriev system cluster document base word sens word jaguar jaguar obvious ambigu word english correspond sport team car anim sever thing idea search engin retriev document match jaguar look similar tri identifi cluster correspond differ word sens jaguar without know advanc jaguar ambigu without know certain number sens give one cluster car one cluster anim one simplest techniqu document cluster call kmean method done iter determin cluster point belong adjust cluster centroid cluster repeat need howev know number cluster advanc k kmean base hard decis document assign certain cluster cannot chang cluster cannot assign anoth cluster simultan here algorithm initi cluster centroid k cluster arbitrari vector improv possibl follow document find cluster closest document assign document cluster c cluster c recomput centroid cluster c base document that okay let look exampl kmean cluster k equal six vector want classifi two cluster document said assign arbitrari valu cluster centroid begin let make simpl go class label centroid class class centroid much easier comput distanc vector centroid head let look document closer well pretti obviou base euclidian manhattan distanc belong second cluster document b valu go closer centroid c also go closer interest exampl right middl two centroid cannot realli use round ignor e closer final f closer happen first half first iter six document label five one way specif document b c f belong cluster center document e belong second cluster second part first iter name recomput centroid let cluster use includ new centroid go averag vector plu plu equal divid eightthird first dimens second dimens plu plu divid second dimens essenti twothird comma new centroid first cluster second cluster one use go comput follow take averag first dimens go second dimens go two new centroid first full iter repeat procedur reassign document b c e f one two cluster recomput centroid repeat sequenc iter mani time necessari centroid converg mean moment two centroid dont chang one iter next stop theyr never go chang point websit interest demo kmean one second one third one fourth one would like encourag look tutori understand kmean littl bit better evalu cluster case far assum know advanc number cluster one import thing realiz dont know number cluster essenti follow tri differ valu k figur cluster give us better perform one way evalu perform cluster first techniqu go introduc call puriti base major class cluster second one call rand index go shown next slide let look puriti first three cluster first one three xs two circl second one three circl one x one last one four two xs pure cluster well first cluster major class x three five element cluster xs therefor puriti second cluster valu three circl five major class puriti final last cluster four six major class take averag entir set comput overal puriti total number element cluster give total puriti obvious first cluster consist xs second cluster consist circl third one consist get puriti next metric use evalu cluster socal rand index rand index base follow principl go score point everi time get two object realli belong class label togeth cluster go lose point everi time two object cluster label differ cluster rand index valu equal number true posit plu number true neg divid total number pair object here exampl use exampl previou slide number true posit number fals posit togeth first exampl five total object there five choos two pair five object give us ten also five choos two pair second cluster anoth ten final six choos two pair third cluster total number posit thing suppos get cluster similarli comput number true posit separ first case three xs choos equal thing next cluster choos final cluster two differ kind object appear togeth first one appear four time four choos two last one two pick two choos two sum four term equal number fals posit equal first number minu second number minu equal give us two thousand conting tabl specif true posit fals posit use math comput fals neg true neg comput rand index two posit plu two neg divid total number plu divid equal moder good agreement cluster algorithm everyth correctli cluster exact way gold standard rand index go equal one go true posit true neg none two categori method cluster specif design hierarch cluster everyth look far mostli flat cluster one techniqu use socal singlelinkag method singlelinkag take two object figur similar think similar enough put cluster collaps two cluster figur there even one pair document close enough go merg two cluster disadvantag get long chain document even relat theyr pairwis relat first one differ last one still cluster anoth method call completelinkag method case order merg two cluster consid pair document one one cluster one cluster theyr similar enough merg two cluster one disadvantag method conserv final socal averagelinkag method base averag similar object two cluster let look exampl suppos document go start collaps togeth document similar exampl cluster cluster cluster cluster point go four cluster two document case depend algorithm would either merg would merg last step hierarch cluster process go merg two remain cluster one end go one big cluster includ document two subclust includ four document final four go group group two let look exampl hierarchi call agglom cluster use dendrogram dendrogram techniqu build hierarch structur base similar here exampl languag similar six german languag iceland norwegian swedish danish dutch german want measur similar base use differ word case turn norwegian swedish similar around go cluster togeth level new cluster norwegian swedish go merg cluster danish dutch german similar theyr go cluster togeth group consist norwegian swedish danish go merg iceland around final two remain cluster go cluster whole collect give us nice represent allow us produc number cluster start righthand side move vertic bar left bar let say say one cluster lower go cross two horizont line go nordic group four languag anoth group two languag slide vertic bar let say go get four cluster slide way wee go get six cluster exampl shown websit let look exampl cluster use dendrogram suppos follow sentenc follow document first one abcba next one adccad want build hierarch cluster diagram use dendrogram need comput pairwis similar pair document one two one three one four identifi closest pair case could exampl document one document two merg singl node base frequenc word appear two document combin repeat document cluster also repres venn diagram depend similar metric may get someth like first document second document one cluster fourth fifth one anoth cluster fourth fifth combin third one anoth cluster five anoth cluster conclud section text cluster go continu differ topic,[11  2  4 14 13]
233,Course3_W10-S1-L6_-_Information_Retrieval_Toolkits_-_8_slides_02-38,okay final segment information retrieval going quick summary existing information retrieval toolkits data sets theres number open source ir toolkits historical reasons example smart system around plus years theres mg australia new zealand theres lemur carnegie mellon university university massachusetts theres terrier university glasgow clairlib university michigan also large open source project called lucene apache foundation lets look smart influential information retrieval system developed gerard sartin many years ago since based vector space model lot different weighting options written c language improved years point tech evaluations actually first place still available download used lot information retreival classes teaching tool mg highly efficient toolkit retrieval text images developed people universities waikato melbourne rmit australia also written c based vector spaced model uses lot tricks efficient compression data representation also achieved good trec performance recently systems called lemur indri based language model based informational retreival developed theyre written c highly extensible theyre based vector space probabilistic models also achieved good trec performance finally lucene open source information retrieval toolkit written java distributed apache foundation ported multiple languages good building practical ir web applications scratch many applications built top lucene example nutch solr concludes section information retrieval,Course3,W10-S1-L6,W10,S1,L6,-,10,1,6,okay final segment inform retriev go quick summari exist inform retriev toolkit data set there number open sourc ir toolkit histor reason exampl smart system around plu year there mg australia new zealand there lemur carnegi mellon univers univers massachusett there terrier univers glasgow clairlib univers michigan also larg open sourc project call lucen apach foundat let look smart influenti inform retriev system develop gerard sartin mani year ago sinc base vector space model lot differ weight option written c languag improv year point tech evalu actual first place still avail download use lot inform retreiv class teach tool mg highli effici toolkit retriev text imag develop peopl univers waikato melbourn rmit australia also written c base vector space model use lot trick effici compress data represent also achiev good trec perform recent system call lemur indri base languag model base inform retreiv develop theyr written c highli extens theyr base vector space probabilist model also achiev good trec perform final lucen open sourc inform retriev toolkit written java distribut apach foundat port multipl languag good build practic ir web applic scratch mani applic built top lucen exampl nutch solr conclud section inform retriev,[ 4  2 14 13 12]
234,Course3_W11-S1-L1_-_Sentiment_Analysis_-_17_slides_08-49,okay welcome back natural language processing next topic going sentiment analysis sentiment analysis popular subarea natural language processing popularized last years fact first papers published late important contributions made last years sentiment analysis let start example going pick book particularly like japanese author haruki murakami suppose want find sort reviews posted book way books title q reviews posted internet examples baltimore examiner q tremendous feat triumph mustread anyone wants come terms contemporary japanese culture another example perhaps one important works science fiction year q disappoint envelops reader shifting world strange cults peculiar characters surreal entrancing finally another example review ambitious sprawling thoroughly stunning orwellian dystopia scifi modern world terrorism drugs apathy pop novels blend dreamlike strange wholly unforgettable epic well point obvious reviews actually fairly positive like kind books based reviews decide read lets look things see reviews make us believe positive first presence specific words phrases typically correlated positive review triumph feat tremendous feat mustread entrancing ambitious stunning unforgettable epic examples positive phrases fact negative phrases example heres another example companies theres website shows real time sentiment towards different companies stocks people use kind information trading pick sort stock ticker observe sentiment company changes time within days hours examples sentiment analysis things like movie reviews product reviews debates example createdebatecom website many posts blogs express sentiment express personal opinions different research questions interested one something called subjectivity analysis decide whether something positive negative decide whether subjective clearly factual information different subjective information performed subjectivity analysis polarity analysis essentially labeling things either positive negative perhaps giving number stars lets say read movie review wed say based text review well give four stars five perhaps five stars also viewpoint analysis different topics debate person chelsea fan manchester united fan person republican democrat another research question figure sentiment target giving opinion entire product example apple iphone specific part example screen wireless connection level granularity important sentiment entire document example movie review level individual sentences perhaps individual attributes example music certain movie specific actor opinion words important theres different types base words example pretty difficult also comparative movies better one processor slower one well count positive negative words going get good classifier example looking negative words enough negated say something difficult means easy one important component sentiment analysis negation analysis take occurrences negation phrases didnt try figure things negate possibly change polarity well go back reviews q second review includes negative words example word disappoint fact read review carefully realize says disappoint negation negative word turns entire thing positive phrase okay lets look product reviews heres website google people written reviews specific smart phone samsung galaxy see total reviews given one two three stars product quite given four stars rest given five stars whats interesting also sentiment individual aspects product example battery size camera see battery gets highest positive rating whereas design screen speakers headset get relatively fewer positive reviews done automatically reading reviews identifying polarity words associated product individual aspects also possible use social media sites twitter compute sentiment theres website called sentiment see sort things people say product twitter using samsung galaxy example see majority reviews twitter positive theres important problems resolved im going propose solutions point wanted make sure know exist problems subtlety sometimes people use subtle formulations explain think certain target concession example say maybe product problem average pretty good product also manipulation attempts example trying convince product good people also use sarcasm irony difficult detect lets look sentiment analysis classification problem want classify documents sentences positive negative kind features use use individual words example whether appear interested frequency time theyre going used punctuation example emoticons phrases syntax sentence could valid features classification one good thing sentiment analysis lot training data available take entire websites product reviews also look number stars given product use training dataset nice dataset paper bo pang lillian lee cornell university movie reviews annotated polarity positive negative also number stars techniques used standard techniques used classification example maximum entropy support vector machines naive bayes nice resources available example cmu twitter parser available noah smiths website end introduction sentiment analysis next section going look specifically sentiment lexicons,Course3,W11-S1-L1,W11,S1,L1,-,11,1,1,okay welcom back natur languag process next topic go sentiment analysi sentiment analysi popular subarea natur languag process popular last year fact first paper publish late import contribut made last year sentiment analysi let start exampl go pick book particularli like japanes author haruki murakami suppos want find sort review post book way book titl q review post internet exampl baltimor examin q tremend feat triumph mustread anyon want come term contemporari japanes cultur anoth exampl perhap one import work scienc fiction year q disappoint envelop reader shift world strang cult peculiar charact surreal entranc final anoth exampl review ambiti sprawl thoroughli stun orwellian dystopia scifi modern world terror drug apathi pop novel blend dreamlik strang wholli unforgett epic well point obviou review actual fairli posit like kind book base review decid read let look thing see review make us believ posit first presenc specif word phrase typic correl posit review triumph feat tremend feat mustread entranc ambiti stun unforgett epic exampl posit phrase fact neg phrase exampl here anoth exampl compani there websit show real time sentiment toward differ compani stock peopl use kind inform trade pick sort stock ticker observ sentiment compani chang time within day hour exampl sentiment analysi thing like movi review product review debat exampl createdebatecom websit mani post blog express sentiment express person opinion differ research question interest one someth call subject analysi decid whether someth posit neg decid whether subject clearli factual inform differ subject inform perform subject analysi polar analysi essenti label thing either posit neg perhap give number star let say read movi review wed say base text review well give four star five perhap five star also viewpoint analysi differ topic debat person chelsea fan manchest unit fan person republican democrat anoth research question figur sentiment target give opinion entir product exampl appl iphon specif part exampl screen wireless connect level granular import sentiment entir document exampl movi review level individu sentenc perhap individu attribut exampl music certain movi specif actor opinion word import there differ type base word exampl pretti difficult also compar movi better one processor slower one well count posit neg word go get good classifi exampl look neg word enough negat say someth difficult mean easi one import compon sentiment analysi negat analysi take occurr negat phrase didnt tri figur thing negat possibl chang polar well go back review q second review includ neg word exampl word disappoint fact read review care realiz say disappoint negat neg word turn entir thing posit phrase okay let look product review here websit googl peopl written review specif smart phone samsung galaxi see total review given one two three star product quit given four star rest given five star what interest also sentiment individu aspect product exampl batteri size camera see batteri get highest posit rate wherea design screen speaker headset get rel fewer posit review done automat read review identifi polar word associ product individu aspect also possibl use social media site twitter comput sentiment there websit call sentiment see sort thing peopl say product twitter use samsung galaxi exampl see major review twitter posit there import problem resolv im go propos solut point want make sure know exist problem subtleti sometim peopl use subtl formul explain think certain target concess exampl say mayb product problem averag pretti good product also manipul attempt exampl tri convinc product good peopl also use sarcasm ironi difficult detect let look sentiment analysi classif problem want classifi document sentenc posit neg kind featur use use individu word exampl whether appear interest frequenc time theyr go use punctuat exampl emoticon phrase syntax sentenc could valid featur classif one good thing sentiment analysi lot train data avail take entir websit product review also look number star given product use train dataset nice dataset paper bo pang lillian lee cornel univers movi review annot polar posit neg also number star techniqu use standard techniqu use classif exampl maximum entropi support vector machin naiv bay nice resourc avail exampl cmu twitter parser avail noah smith websit end introduct sentiment analysi next section go look specif sentiment lexicon,[ 4 14 13 12 11]
235,Course3_W11-S1-L2_-_Sentiment_Lexicons_-_12_slides_07-53,okay next segment going automatically building sentiment lexicons manually built sentiment lexicons sentiwordnet general inquirer liwc multiperspective question answer subjectivity lexicon used lot research sentiment analysis also important learn sentiment lexicons automatically whether english specific domain perhaps foreign language lets look general inquirer large data set includes annotations polarity also many different effects information example powerful versus weak active versus passive pleasure versus pain available website university maryland small section specifically positive negative words includes words like able accolade accuracy adept adequate positive words addiction adversity adultery affliction aggressive negative lets look automatic methods well start first dictionary based methods goal identify additional concept words either positive negative way methods work starting set known seeds example happy positive work angry negative word expand sets positive negative set using wordnet add synonyms happy positive set synonyms angry negative set leave center things like hypernyms hyponyms many techniques based randomwalk methods take new words example word sleepy lets say perform random walk wordnet reach one known seed words look distance current word known labeled words based distance label either way addition dictionary methods also semisupervised methods first one developed hatzivassiloglou mckeown acl paper also going give us opportunity look another naclo problem incorporates ideas method heres works following naclo problem called molistic way point adjective polarity adjectives real english words easy figure polarity english words looking unknown words problem becomes much infrastructional given input set sentences include adjective molistic cluvious blitty real english sentences talks person people people described sequence adjectives connected different conjunctions figure adjectives positive ones negative ideas following first realize conjunctions relate words variety example relates words positive negative relates words different varieties blitty cloovy means one positive one negative possible build graph nodes corresponds adjective two types agents either positive agents means two words belong cluster negative agents means two words belongs different clusters even correctly inconsistencies still figure two clusters positive ones negative luckily example one sentence sentence number eight addition things says says also diane pleasure watch idea adjectives appear sentence also positive lets see solve problem youre given sentences adjectives two questions asked one two one asking whether sentences abc makes sense sentence make sense adjectives correctly connected together example sentence considered correct blitty brastic polarity based training data false otherwise part determine whether adjectives listed positive negative try graph helps figure solution number corresponds one sentences lefthand side top sentence connects molistic slatty positive edge means cluster number three connects molistic donty dashed line means different clusters four connects donty cloovy members cluster finally number eight connects stuffy strungy pleasure watch cluster numbers appear dashed lines across graph like indicate two end points different clusters create decision boundary goes dashed lines therefore clustering graphic one cluster five words one cluster eight words last thing need figure positive one negative one said pleasure watch grounds two adjectives connected therefore makes right hand side cluster positive extension ones left negative okay next thing want look socalled pmi method introduced peter inaudible identifying polarity words based core currents words known positive negative words large web based corpus pmi stands pointwise mutual information something looked class given word want see many times appears near known positive word example excellent many times appears near known negative word poor pointwise mutual information return two words general lets say word trying classify word excellent logarithm number hits search engine words divided individual counts one words one theres another interesting data set download johns hopkins website created mark dredze colleagues contains large number training data sentiment analysis concludes section sentiment lexicons,Course3,W11-S1-L2,W11,S1,L2,-,11,1,2,okay next segment go automat build sentiment lexicon manual built sentiment lexicon sentiwordnet gener inquir liwc multiperspect question answer subject lexicon use lot research sentiment analysi also import learn sentiment lexicon automat whether english specif domain perhap foreign languag let look gener inquir larg data set includ annot polar also mani differ effect inform exampl power versu weak activ versu passiv pleasur versu pain avail websit univers maryland small section specif posit neg word includ word like abl accolad accuraci adept adequ posit word addict advers adulteri afflict aggress neg let look automat method well start first dictionari base method goal identifi addit concept word either posit neg way method work start set known seed exampl happi posit work angri neg word expand set posit neg set use wordnet add synonym happi posit set synonym angri neg set leav center thing like hypernym hyponym mani techniqu base randomwalk method take new word exampl word sleepi let say perform random walk wordnet reach one known seed word look distanc current word known label word base distanc label either way addit dictionari method also semisupervis method first one develop hatzivassilogl mckeown acl paper also go give us opportun look anoth naclo problem incorpor idea method here work follow naclo problem call molist way point adject polar adject real english word easi figur polar english word look unknown word problem becom much infrastruct given input set sentenc includ adject molist cluviou blitti real english sentenc talk person peopl peopl describ sequenc adject connect differ conjunct figur adject posit one neg idea follow first realiz conjunct relat word varieti exampl relat word posit neg relat word differ varieti blitti cloovi mean one posit one neg possibl build graph node correspond adject two type agent either posit agent mean two word belong cluster neg agent mean two word belong differ cluster even correctli inconsist still figur two cluster posit one neg luckili exampl one sentenc sentenc number eight addit thing say say also dian pleasur watch idea adject appear sentenc also posit let see solv problem your given sentenc adject two question ask one two one ask whether sentenc abc make sens sentenc make sens adject correctli connect togeth exampl sentenc consid correct blitti brastic polar base train data fals otherwis part determin whether adject list posit neg tri graph help figur solut number correspond one sentenc lefthand side top sentenc connect molist slatti posit edg mean cluster number three connect molist donti dash line mean differ cluster four connect donti cloovi member cluster final number eight connect stuffi strungi pleasur watch cluster number appear dash line across graph like indic two end point differ cluster creat decis boundari goe dash line therefor cluster graphic one cluster five word one cluster eight word last thing need figur posit one neg one said pleasur watch ground two adject connect therefor make right hand side cluster posit extens one left neg okay next thing want look socal pmi method introduc peter inaud identifi polar word base core current word known posit neg word larg web base corpu pmi stand pointwis mutual inform someth look class given word want see mani time appear near known posit word exampl excel mani time appear near known neg word poor pointwis mutual inform return two word gener let say word tri classifi word excel logarithm number hit search engin word divid individu count one word one there anoth interest data set download john hopkin websit creat mark dredz colleagu contain larg number train data sentiment analysi conclud section sentiment lexicon,[11  4  6 14 13]
236,Course3_W11-S1-L3_-_Semantics_-_15_slides_07-24,okay welcome back natural language processing next segment going semantics one important areas natural language processing ties nicely logic philosophy knowledge representation semantics meaning defined language also defined many different things example arithmetic expressions give expression plus times plus ask meaning well straightforward say plus plus times therefore meaning expression build building parse tree sentence case arithmetic expression five two numbers converted expressions functors like plus combine nouns functors encompassing expressions expressions functors recursively get top level e value expression equal nice able compute applying recursive principles original expression however variable unknown value somewhere expression example plus times plus z well would answer well case start parse tree z instead three lower righthand corner case see value top level number sort procedure lets compute value multiplication two numbers sum two numbers first one five two second one adding four z even though dont know exact value top level say obtained using procedure know value z sense converted expression procedure compute semantics recursively lets look different example using natural language text specifically english language sentence something like every human mortal define meaning sentence representing meaning one important aspects semantics want able capture meaning linguistic utterances using sort formal notation mean meaning well linguistic meaning different known pragmatic meaning say pm mean oclock evening linguistic meaning sentence also different pragmatic meeting may mean something like pm means time leave point watch say pm really mean lets go semantic analysis want assign word sort meaning combine meanings words form meaning phrases extension entire sentences ultimately want able convert sentence like bought book something like exists objects x x instance buying event buying event two arguments buyer bought item buyer speaker bought item book would legitimate first order logic representation sentence going first order logic next two slides okay presentation sentence maybe something like record consists predicate buying active two value pairs argument buyer whos speaker sentence bought item book lets introduce two important concepts semantics semantic analysis well use later one called entailment second one called presupposition entailment entailment one fact follows one another say cats whiskers martin cat means entail statement martin whiskers fact martin whiskers entailed two another example know martin whiskers tail entails fact martin whiskers drop tail still valid statement lets look presupposition presupposition better explained example said queen utopia dead means presuppose utopia queen otherwise sentence doesnt make sense good point introduce one naclo problems describes entailment presupposition detail problem written aleka blackwell available naclo website let show problem first part problem introduces concepts entailment presupposition bottom part gives examples entailment next slide examples presupposition example presupposition say regret seeing shaun whites gold medal run suppose shaun white gold medal run otherwise doesnt make sense okay questions given pair sentences possible entailment presupposition none one questions asking give examples pairs sentences neither entails presupposes sentence b second question come example sentences entails presupposes b third example fourth example ask cases either proposition entailment way around lets think examples sentences get see answers next slide pulled naclo website examples four combinations entailment presupposition go understand selected first page none second slide one one okay concludes introductory segment semantics,Course3,W11-S1-L3,W11,S1,L3,-,11,1,3,okay welcom back natur languag process next segment go semant one import area natur languag process tie nice logic philosophi knowledg represent semant mean defin languag also defin mani differ thing exampl arithmet express give express plu time plu ask mean well straightforward say plu plu time therefor mean express build build pars tree sentenc case arithmet express five two number convert express functor like plu combin noun functor encompass express express functor recurs get top level e valu express equal nice abl comput appli recurs principl origin express howev variabl unknown valu somewher express exampl plu time plu z well would answer well case start pars tree z instead three lower righthand corner case see valu top level number sort procedur let comput valu multipl two number sum two number first one five two second one ad four z even though dont know exact valu top level say obtain use procedur know valu z sens convert express procedur comput semant recurs let look differ exampl use natur languag text specif english languag sentenc someth like everi human mortal defin mean sentenc repres mean one import aspect semant want abl captur mean linguist utter use sort formal notat mean mean well linguist mean differ known pragmat mean say pm mean oclock even linguist mean sentenc also differ pragmat meet may mean someth like pm mean time leav point watch say pm realli mean let go semant analysi want assign word sort mean combin mean word form mean phrase extens entir sentenc ultim want abl convert sentenc like bought book someth like exist object x x instanc buy event buy event two argument buyer bought item buyer speaker bought item book would legitim first order logic represent sentenc go first order logic next two slide okay present sentenc mayb someth like record consist predic buy activ two valu pair argument buyer who speaker sentenc bought item book let introduc two import concept semant semant analysi well use later one call entail second one call presupposit entail entail one fact follow one anoth say cat whisker martin cat mean entail statement martin whisker fact martin whisker entail two anoth exampl know martin whisker tail entail fact martin whisker drop tail still valid statement let look presupposit presupposit better explain exampl said queen utopia dead mean presuppos utopia queen otherwis sentenc doesnt make sens good point introduc one naclo problem describ entail presupposit detail problem written aleka blackwel avail naclo websit let show problem first part problem introduc concept entail presupposit bottom part give exampl entail next slide exampl presupposit exampl presupposit say regret see shaun white gold medal run suppos shaun white gold medal run otherwis doesnt make sens okay question given pair sentenc possibl entail presupposit none one question ask give exampl pair sentenc neither entail presuppos sentenc b second question come exampl sentenc entail presuppos b third exampl fourth exampl ask case either proposit entail way around let think exampl sentenc get see answer next slide pull naclo websit exampl four combin entail presupposit go understand select first page none second slide one one okay conclud introductori segment semant,[ 4  0 14 13 12]
237,Course3_W11-S1-L4_-_Representing_and_Understanding_Meaning_-_14_slides_09-22,moving additional topics semantics going talk representing understanding meaning mean understand meaning sentence well one possible definitions agent hears sentence act accordingly say agent understood heres example say leave book table able understand book table im talking know able perform act understand sentence understanding may involve inference example maybe book wrapped paper dont know see paper infer book inside also involves pragmatics book table perhaps agent need ask additional questions clarify answers questions getting right answers able perform act understanding necessarily able directly perform action may involve procedure getting missing information performing action properties semantic representation listed first one verifiability possible determine certain statement true knowledge base example cat martin whiskers database statement says maybe says cats whiskers martin cat cases verify truth value statement second property unambiguousness want able determine sentence one possible semantic interpretation say give book many different contexts would ambiguous sentence may multiple books im pointing specific book book already introduced discourse sentence may unambiguous also consider something canonical form sentence may multiple ways express exact semantics also consider expressiveness presentation example use specific logical formalism represent meaning used express temporal relations beliefs events domain independent use presenting arbitrary knowledge okay last property need take account whether formalism use semantic representation allows us sound inference lets consider specific methods use representing meaning first one us logical representation example first order logic im going explain first order logic means next slides also want able use sort theorem proving inference determine whether one statement entails another going start simplest logical representation something called propositional logic simplest type logic involves proposition symbols p p correspond known sentences rules build complicated sentences simple ones sentence also sentence sentences also sentence known conjunction sentences also sentence known disjunction sentences implies also sentence finally sentences equivalent also sentence example biconditional relation propositional logic expressed dnf backus naur form following way sentence either atomic sentence without operators complex sentence atomic sentence either true false individual sentence presents proposition u complex sentence sentence parentheses negation sentence conjunction disjunction implication biconditional theres different precedence operators highest precedence given negation followed implies equivalent lets see translate propositions english suppose following two propositions today holiday b going park implies b mean b implies b b implies finally b implies translate english sentences think ill show answer answers next slide okay lets look answers questions translating propositions english remind proposition today holiday b proposition going park implies b mean well english translation today holiday going park b today holiday going park implies b today holiday going park b implies going park today holiday finally b implies going park today holiday see oppositional logic makes lot sense used restrictive types sentences things present single facts logical connections negations implication okay lets look semantics propositional logic truth values different operators true false true true true either true implies true true false true means false true false finally equivalent true implies true implies also true recursively compute truth value much longer formulas including parentheses multiple operators heres full truth value different operators p q given statement propositional logic compute values p p q p q p implies q p equivalent q using table p q false p equal true p q false p q false p implies q true finally p equivalent q also true one thing remember p q p implies q p implies q false p true q false people get confused example thing p false q true p implies q also false case premise p false anything put right hand side q also true heres full table logical equivalents different theorems used simplify propositional logic statements example first one commutativity conjunction b b next one commutativity junction b b two theorems associativity junction conjunction one double negation negation note contraposition implies b note b implies note implication elimination implies b b additional ones im going skip biconditional elimination two forms de morgan one disjunction one conjunction two forms distributivity concludes section propositional logic going switch firstorder logic,Course3,W11-S1-L4,W11,S1,L4,-,11,1,4,move addit topic semant go talk repres understand mean mean understand mean sentenc well one possibl definit agent hear sentenc act accordingli say agent understood here exampl say leav book tabl abl understand book tabl im talk know abl perform act understand sentenc understand may involv infer exampl mayb book wrap paper dont know see paper infer book insid also involv pragmat book tabl perhap agent need ask addit question clarifi answer question get right answer abl perform act understand necessarili abl directli perform action may involv procedur get miss inform perform action properti semant represent list first one verifi possibl determin certain statement true knowledg base exampl cat martin whisker databas statement say mayb say cat whisker martin cat case verifi truth valu statement second properti unambigu want abl determin sentenc one possibl semant interpret say give book mani differ context would ambigu sentenc may multipl book im point specif book book alreadi introduc discours sentenc may unambigu also consid someth canon form sentenc may multipl way express exact semant also consid express present exampl use specif logic formal repres mean use express tempor relat belief event domain independ use present arbitrari knowledg okay last properti need take account whether formal use semant represent allow us sound infer let consid specif method use repres mean first one us logic represent exampl first order logic im go explain first order logic mean next slide also want abl use sort theorem prove infer determin whether one statement entail anoth go start simplest logic represent someth call proposit logic simplest type logic involv proposit symbol p p correspond known sentenc rule build complic sentenc simpl one sentenc also sentenc sentenc also sentenc known conjunct sentenc also sentenc known disjunct sentenc impli also sentenc final sentenc equival also sentenc exampl bicondit relat proposit logic express dnf backu naur form follow way sentenc either atom sentenc without oper complex sentenc atom sentenc either true fals individu sentenc present proposit u complex sentenc sentenc parenthes negat sentenc conjunct disjunct implic bicondit there differ preced oper highest preced given negat follow impli equival let see translat proposit english suppos follow two proposit today holiday b go park impli b mean b impli b b impli final b impli translat english sentenc think ill show answer answer next slide okay let look answer question translat proposit english remind proposit today holiday b proposit go park impli b mean well english translat today holiday go park b today holiday go park impli b today holiday go park b impli go park today holiday final b impli go park today holiday see opposit logic make lot sens use restrict type sentenc thing present singl fact logic connect negat implic okay let look semant proposit logic truth valu differ oper true fals true true true either true impli true true fals true mean fals true fals final equival true impli true impli also true recurs comput truth valu much longer formula includ parenthes multipl oper here full truth valu differ oper p q given statement proposit logic comput valu p p q p q p impli q p equival q use tabl p q fals p equal true p q fals p q fals p impli q true final p equival q also true one thing rememb p q p impli q p impli q fals p true q fals peopl get confus exampl thing p fals q true p impli q also fals case premis p fals anyth put right hand side q also true here full tabl logic equival differ theorem use simplifi proposit logic statement exampl first one commut conjunct b b next one commut junction b b two theorem associ junction conjunct one doubl negat negat note contraposit impli b note b impli note implic elimin impli b b addit one im go skip bicondit elimin two form de morgan one disjunct one conjunct two form distribut conclud section proposit logic go switch firstord logic,[ 4  6  9  8 14]
238,Course3_W11-S1-L5_-_First_Order_Logic_-_16_slides_07-37,welcome back natural language processing going continue sequence segments semantics focusing time firstorder logic far looked propositional logic said essentially simplest form logic properties propositional logic include pros compositional define tools values long sentences based tools values constituents declarative also cons limited expressive power used represent facts want go beyond go something called first order logic lot additional properties propositional logic doesnt first order logic used represent everything propositional logic represent also things like objects object example martin cat relation example martin moses brothers functions martins age okay lets talk little bit first order logic bmf format basic concept formula formula one following things either atomic formula two formulas connected connective sequence quantifier variable formula negation formula formula parenthesis atomic formula predicate consists one multiple terms parenthesis predicate something like likes case likes two arguments person liking person liked term either function terms constant variable connectives either implies quantifiers either universal quantifier existential quantifier constant either something like name person name object variables denoted variable names like x could also predicates example likes eats functions example ageof colorof two common mistakes people make use quantifier want explain quantifiers work well first mistake people dont realize implies main connective universal quantifier mistake people make instead use main connective let give example say x x cat x eatsfish well actually right representation want say cats eat fish really says everyone cat everyone eats fish wanted instead need say x cat x implies eats fish x second mistake existential quantifier rule main connective existential quantifier mistake people make use implies instead heres example suppose want say exists cat eats fish representation actually incorrect said implication operator lefthand side false anything go righthand side means statement exists x catx implies eatsfishx statement true exists anyone cat person object righthand side doesnt matter correct representation would exists x x cat x eats fish little bit first order logic im going introduce naclo problem written ben king illustrates interesting aspects first order logic problem downloaded naclo website want show real quick problem called bertrand russell pun name famous philosopher logician bertrand russell page describes first order logic something already class explains different connectives negation conjunction disjunction implication two quantifiers universal existential quantifier asks questions nine ten formulas j matched one five sentences first sentence everyone either passed failed test figure letters j go five empty boxes part h answer shown next slides second part problem translate logical expressions firstorder logic english language sentences goal find letters h go five boxes top page finally solutions parts make sure get right didnt get right go back try understand better first order logic works next thing want introduce section concept lambda expression im going example suppose want define function called increment x takes argument x returns value one x way represent lambda notation say lambda function one argument x returns x apply function lambda x argument essentially write following way increment applying lambda function lambda x x argument result applying function argument number lambda functions even multiple arguments example sums add lambda function represented lambda x lambda returning sum x want compute sum number applying lambda function two arguments lambda x lambda x two arguments expanded form two different ways either first expand lambda x lambda case chose expand lambda x first removing x replace expression two arguments new one lambda lambda function applied argument gives us lambda expressions going come handy next sections,Course3,W11-S1-L5,W11,S1,L5,-,11,1,5,welcom back natur languag process go continu sequenc segment semant focus time firstord logic far look proposit logic said essenti simplest form logic properti proposit logic includ pro composit defin tool valu long sentenc base tool valu constitu declar also con limit express power use repres fact want go beyond go someth call first order logic lot addit properti proposit logic doesnt first order logic use repres everyth proposit logic repres also thing like object object exampl martin cat relat exampl martin mose brother function martin age okay let talk littl bit first order logic bmf format basic concept formula formula one follow thing either atom formula two formula connect connect sequenc quantifi variabl formula negat formula formula parenthesi atom formula predic consist one multipl term parenthesi predic someth like like case like two argument person like person like term either function term constant variabl connect either impli quantifi either univers quantifi existenti quantifi constant either someth like name person name object variabl denot variabl name like x could also predic exampl like eat function exampl ageof colorof two common mistak peopl make use quantifi want explain quantifi work well first mistak peopl dont realiz impli main connect univers quantifi mistak peopl make instead use main connect let give exampl say x x cat x eatsfish well actual right represent want say cat eat fish realli say everyon cat everyon eat fish want instead need say x cat x impli eat fish x second mistak existenti quantifi rule main connect existenti quantifi mistak peopl make use impli instead here exampl suppos want say exist cat eat fish represent actual incorrect said implic oper lefthand side fals anyth go righthand side mean statement exist x catx impli eatsfishx statement true exist anyon cat person object righthand side doesnt matter correct represent would exist x x cat x eat fish littl bit first order logic im go introduc naclo problem written ben king illustr interest aspect first order logic problem download naclo websit want show real quick problem call bertrand russel pun name famou philosoph logician bertrand russel page describ first order logic someth alreadi class explain differ connect negat conjunct disjunct implic two quantifi univers existenti quantifi ask question nine ten formula j match one five sentenc first sentenc everyon either pass fail test figur letter j go five empti box part h answer shown next slide second part problem translat logic express firstord logic english languag sentenc goal find letter h go five box top page final solut part make sure get right didnt get right go back tri understand better first order logic work next thing want introduc section concept lambda express im go exampl suppos want defin function call increment x take argument x return valu one x way repres lambda notat say lambda function one argument x return x appli function lambda x argument essenti write follow way increment appli lambda function lambda x x argument result appli function argument number lambda function even multipl argument exampl sum add lambda function repres lambda x lambda return sum x want comput sum number appli lambda function two argument lambda x lambda x two argument expand form two differ way either first expand lambda x lambda case chose expand lambda x first remov x replac express two argument new one lambda lambda function appli argument give us lambda express go come handi next section,[ 4 14 13 12 11]
239,Course3_W11-S1-L6_-_Knowledge_Representation_-_15_slides_12-18,moving next topic knowledge representation knowledge representation well essentially anything storing semantic information able reason one important concepts knowledge representation idea ontology ontology may example relation different objects object type another object example table type furniture furniture type object knowledge representation deal categories objects different things category set objects categories nested one another whereas objects cannot nested one another objects part categories thing want represent events times beliefs lets look examples objects example object martin cat category may cat ontology may following levels mammal category includes categories cats dogs whales cat include subcategories example persian cat manx cat define two relations isa relation object category say martin cat martin object cat category also define relation ako stands kind defined two different categories say persiancat category kind cat relations defined example hasa also known inaudible relation example cat tail means objects category cat tails lets look semantics first order logic first order logic sentences assigned value true false say milo cat sentence true milo younger martin say following way age milo age martin connect two last symbol entire predicate value true whenever milo younger martin also say age milo equal age martin case lets look examples quantifiers suppose want represent statement cats eat fish one way say x x cat implies x eats fish also want represent events suppose want represent statement martin ate figure may also want represent statement martin ate morning martin ate fish martin ate fish morning one possible representation represent sentences completely independently ones first order logic define predicate eating first species eating events eating one argument namely eater every time know somebody ate represent fact eating predicate persons name objects name first argument suppose would want represent fact martin ate morning cannot use predicate eating one additive one means one argument define new predicate eating two actually number argument second version eating predicate particular version takes two arguments first argument eater second argument time eating want present third sentence martin ate fish final third predicate also two arguments theyre different previous two first one still person object eating second one object eaten want represent fourth sentence define fourth predicate eating three arguments eater eaten time difficult reason kind predicates first theres infinite number predicates even type event second want somehow able represent compactly one way using socalled meaning postulates meaning postulates something like statement knowledge base says whenever instance eating arguments x z implies also statement eating arguments x skipping argument z eating arguments x z implies eating two arguments x z also eating x z implies eating argument x partial solution problem still going nice one going large number meaning postulates interpretations get others way second possible interpretation way works suppose eating predicate arguments x z say always represent eating events instances eating leave arguments unspecified eating going say z unspecified seems work problems first many commitments make cannot know values different arguments difficult combine eating argument z missing eating argument missing cannot combine two eating instance fish morning specified going go third possible representation thats one represent eating event special object process called reification example define event e e instance categorical eating event eater event e eaters name martin eaten argument event e fish lets look ways represent time heres example want able say martin went kitchen yard one possible way say event e e instance going event going takes argument goer origin target goer case martin origin e kitchen target yard however representation doesnt take account anything time cannot say example went kitchen yard particular order representation doesnt give us information whether sentence present past future let introduce important term semantics called fluents fluent predicate true given time example predicate time heres slide shows represent different relations tempo events two events set meet end point event coincides starting point another event defined endpoint event equal starting point another event start point event start point another event endpoint endpoint event similarly define overlap thats starting point first event starting point second event end point first event end point event see overlap defined asymmetrically j possible overlaps j definition j doesnt overlap starts two events start time finishes end time equals start finish example russell norvig one issue related knowledge representation called theory time takes account three different points time point event point entrance point reference want represent sentence eaten want say utterance done thats u point reference point past theres event even past eaten event time happens reference point happens tense simple past ate reference point event time occurred tense present perfect eaten thats event past reference point present coincides utterance time present time three utterance reference point event two instances future tenses simple future reference point equal utterance time event happens future finally future perfect thats utterance reference point future event happens somewhere reference point example jurafsky martin final thing want represent beliefs lets look example want say milo believes martin ate fish one possible representation say two events one event named e instance eating another event called b instance believing entire first order representation isa eating eater e martin eaten e fish information believing event b instance believing event believer b milo believed takes second argument object type eating second argument believed e event eating one problem representation following first order logic conjunction multiple statements truth value doesnt change drop drop terms infer martin ate fish correct dont know sure martin ate fish know milo believes martin ate fish representing beliefs kind first order logic format going work problem people instead look detail take course philosophy logic use something called modal logic modal logic allows represent higher order operators possibility time beliefs concludes section knowledge representation,Course3,W11-S1-L6,W11,S1,L6,-,11,1,6,move next topic knowledg represent knowledg represent well essenti anyth store semant inform abl reason one import concept knowledg represent idea ontolog ontolog may exampl relat differ object object type anoth object exampl tabl type furnitur furnitur type object knowledg represent deal categori object differ thing categori set object categori nest one anoth wherea object cannot nest one anoth object part categori thing want repres event time belief let look exampl object exampl object martin cat categori may cat ontolog may follow level mammal categori includ categori cat dog whale cat includ subcategori exampl persian cat manx cat defin two relat isa relat object categori say martin cat martin object cat categori also defin relat ako stand kind defin two differ categori say persiancat categori kind cat relat defin exampl hasa also known inaud relat exampl cat tail mean object categori cat tail let look semant first order logic first order logic sentenc assign valu true fals say milo cat sentenc true milo younger martin say follow way age milo age martin connect two last symbol entir predic valu true whenev milo younger martin also say age milo equal age martin case let look exampl quantifi suppos want repres statement cat eat fish one way say x x cat impli x eat fish also want repres event suppos want repres statement martin ate figur may also want repres statement martin ate morn martin ate fish martin ate fish morn one possibl represent repres sentenc complet independ one first order logic defin predic eat first speci eat event eat one argument name eater everi time know somebodi ate repres fact eat predic person name object name first argument suppos would want repres fact martin ate morn cannot use predic eat one addit one mean one argument defin new predic eat two actual number argument second version eat predic particular version take two argument first argument eater second argument time eat want present third sentenc martin ate fish final third predic also two argument theyr differ previou two first one still person object eat second one object eaten want repres fourth sentenc defin fourth predic eat three argument eater eaten time difficult reason kind predic first there infinit number predic even type event second want somehow abl repres compactli one way use socal mean postul mean postul someth like statement knowledg base say whenev instanc eat argument x z impli also statement eat argument x skip argument z eat argument x z impli eat two argument x z also eat x z impli eat argument x partial solut problem still go nice one go larg number mean postul interpret get other way second possibl interpret way work suppos eat predic argument x z say alway repres eat event instanc eat leav argument unspecifi eat go say z unspecifi seem work problem first mani commit make cannot know valu differ argument difficult combin eat argument z miss eat argument miss cannot combin two eat instanc fish morn specifi go go third possibl represent that one repres eat event special object process call reific exampl defin event e e instanc categor eat event eater event e eater name martin eaten argument event e fish let look way repres time here exampl want abl say martin went kitchen yard one possibl way say event e e instanc go event go take argument goer origin target goer case martin origin e kitchen target yard howev represent doesnt take account anyth time cannot say exampl went kitchen yard particular order represent doesnt give us inform whether sentenc present past futur let introduc import term semant call fluent fluent predic true given time exampl predic time here slide show repres differ relat tempo event two event set meet end point event coincid start point anoth event defin endpoint event equal start point anoth event start point event start point anoth event endpoint endpoint event similarli defin overlap that start point first event start point second event end point first event end point event see overlap defin asymmetr j possibl overlap j definit j doesnt overlap start two event start time finish end time equal start finish exampl russel norvig one issu relat knowledg represent call theori time take account three differ point time point event point entranc point refer want repres sentenc eaten want say utter done that u point refer point past there event even past eaten event time happen refer point happen tens simpl past ate refer point event time occur tens present perfect eaten that event past refer point present coincid utter time present time three utter refer point event two instanc futur tens simpl futur refer point equal utter time event happen futur final futur perfect that utter refer point futur event happen somewher refer point exampl jurafski martin final thing want repres belief let look exampl want say milo believ martin ate fish one possibl represent say two event one event name e instanc eat anoth event call b instanc believ entir first order represent isa eat eater e martin eaten e fish inform believ event b instanc believ event believ b milo believ take second argument object type eat second argument believ e event eat one problem represent follow first order logic conjunct multipl statement truth valu doesnt chang drop drop term infer martin ate fish correct dont know sure martin ate fish know milo believ martin ate fish repres belief kind first order logic format go work problem peopl instead look detail take cours philosophi logic use someth call modal logic modal logic allow repres higher order oper possibl time belief conclud section knowledg represent,[ 4 14 13 12 11]
240,Course3_W11-S1-L7_-_Inference_-_9_slides_06-45,okay going switch next topic inference simplest form inference using method called modus ponens way works statement lets say alpha also know alpha implies beta infer beta true heres example know martin cat know x cat x implies eats fish x infer martin eats fish inference done many different ways use forward chaining thats individual facts added knowledge base derive possible inferences follow individual facts also use backward chaining thats start query example want find certain statement true figure go back database find statements true order query true recursively reach statements truth values unknown back jennings implemented example natively prolog programming language ill show brief example prolog prolog define following inference statement knowledge base x father means true x parent x male following facts john parent bill jane parent bill jane female john male new knowledge base get query says find person whos father bill using backward chaining need go knowledge base find order father bill statements true values say parent bill male going back find statements parent bill two possible instances john jane time want male theres one statement male john therefore combination statements knowledge base satisfies query john equal also equal x bill equal therefore going return value equal john lets see use first order logic inference im going show three examples kinship domain first one represent fact brothers siblings well say x x brother implies x sibling another statement ones mother ones female parent define saying c like mother child mother mother c female parent c finally want able represent fact sibling relation symmetric represented following way x sibling xy equivalent sibling yx lets see inference little bit detail next thing want introduce universal instantiation say every instantiation universally quantified sentence entailed know v alpha true infer substitution v replaced specific instance g also true true variable v ground term example constant g heres example x x cat fish implies x eats represented following substitution martin cat blub fish implies martin eats blub example existential instantiation sentence alpha variable v constant symbol k doesnt appear somewhere else knowledge base know exists v alpha true infer replace v specific constant symbol k statement alpha also going true example exists cat called x x eatsfish represent using c specific cat one exists claim specific cat c c cat c eatsfish way special constant symbol known logic field skolem constant lets talk little bit unification unification done possible substitution example statement p x eats statement q x eats blub possible unify two substitution replaces variable specific object blub another example p statement martin eats q statement x eats blub possible unify two using substitution x martin blub another example p statement martin eats q statement eats blub impossible unify two want simultaneously satisfy two substitutions want equal martin equal blub martin blub different unification process going fail okay many cases unification one want unify variable object another example want unify two objects object also possible unify two things one subsumes one heres example know cats eat fish know martin cat blub fish unify statements together,Course3,W11-S1-L7,W11,S1,L7,-,11,1,7,okay go switch next topic infer simplest form infer use method call modu ponen way work statement let say alpha also know alpha impli beta infer beta true here exampl know martin cat know x cat x impli eat fish x infer martin eat fish infer done mani differ way use forward chain that individu fact ad knowledg base deriv possibl infer follow individu fact also use backward chain that start queri exampl want find certain statement true figur go back databas find statement true order queri true recurs reach statement truth valu unknown back jen implement exampl nativ prolog program languag ill show brief exampl prolog prolog defin follow infer statement knowledg base x father mean true x parent x male follow fact john parent bill jane parent bill jane femal john male new knowledg base get queri say find person who father bill use backward chain need go knowledg base find order father bill statement true valu say parent bill male go back find statement parent bill two possibl instanc john jane time want male there one statement male john therefor combin statement knowledg base satisfi queri john equal also equal x bill equal therefor go return valu equal john let see use first order logic infer im go show three exampl kinship domain first one repres fact brother sibl well say x x brother impli x sibl anoth statement one mother one femal parent defin say c like mother child mother mother c femal parent c final want abl repres fact sibl relat symmetr repres follow way x sibl xy equival sibl yx let see infer littl bit detail next thing want introduc univers instanti say everi instanti univers quantifi sentenc entail know v alpha true infer substitut v replac specif instanc g also true true variabl v ground term exampl constant g here exampl x x cat fish impli x eat repres follow substitut martin cat blub fish impli martin eat blub exampl existenti instanti sentenc alpha variabl v constant symbol k doesnt appear somewher els knowledg base know exist v alpha true infer replac v specif constant symbol k statement alpha also go true exampl exist cat call x x eatsfish repres use c specif cat one exist claim specif cat c c cat c eatsfish way special constant symbol known logic field skolem constant let talk littl bit unif unif done possibl substitut exampl statement p x eat statement q x eat blub possibl unifi two substitut replac variabl specif object blub anoth exampl p statement martin eat q statement x eat blub possibl unifi two use substitut x martin blub anoth exampl p statement martin eat q statement eat blub imposs unifi two want simultan satisfi two substitut want equal martin equal blub martin blub differ unif process go fail okay mani case unif one want unifi variabl object anoth exampl want unifi two object object also possibl unifi two thing one subsum one here exampl know cat eat fish know martin cat blub fish unifi statement togeth,[ 4 14 13 12 11]
241,Course3_W11-S1-L8_-_Semantic_Parsing_-_24_slides_09-30,okay next segment semantic parsing semantic parsing converting natural language logical form example build executable code specific applications airline reservation geographical query system semantic parsing two stages first take input example sentence convert syntactic structure using syntactic analysis perform semantic analysis come semantic representation compositional semantics well done adding semantic attachments context free grammar rules first parse sentence syntactically associate semantics individual words use context free grammar semantic attachment build semantic representations non terminal nodes end day get semantics full sentence associated root prior string heres example sentence javier likes pizza want produce output predicate firstorder logic like likes javier first argument pizza second argument heres associate semantic expression nodes javier likes pizza leaf nodes represented case noun another noun lambda expression heres lambda expressions come handy want represent verb likes get represented lambda expression two arguments x turns predicate likes two arguments next thing combine node node pizza going turn two argument lambda expression single argument lambda expression going one variables bound pizza one unbound variable lambda expression x still missing one argument combine two remaining uncombined nodes javier lambda expression verb phrase going get semantics entire sentence applying remaining unbound lambda expression javier going get predicate sentence likes javier pizza practical purposes lot recent work semantic parsing using communitorial categorical grammar introduced mark steidmon let give examples ccg used represent semantics example adjectives represented along expressions one variable lambda x tall x expression represents adjective tall transformation rule says snp adjective essentially one constituents ccgs corresponds lambda expression function f argument x np noun phrase example im going use name basketball player yao ming noun phrases get represented form represent sentence yao ming tall ccg heres start words yao ming labeled noun phrase tall adjective snp adjective gets translated ccg lambda expression one function one attribute combine two together get expression requires noun phrase left form sentence combine together noun phrase get expression yao ming tall relation exercise problem naclo jonathan kummerfeld alexa blackwell patrick littell available naclo web site two parts one generic ccg second one specific language first part introduces ccg expanding meaning forward backward slashes gives little grammar using four words shows combine different expressions form grammatical parses also gives example ungrammatical parses first part problem asks following three questions one explain ccg works parse sentence number two take sentence enjoy long books able figure parse sentence successfully grammar finally part three asks given specific grammar come sentences cannot parsed using grammar dont think answers three questions look answer next slide answers first three parts first second part language called tok psin new guinea sentences language scrambled english translations figure first match english sentences tok psin sentences second part translate one sentence one languages vice versa final part figure map different words language different ccg categories right hand side think look answer next slide okay fun nice naclo problem going look examples recent work semantic parsing first one called geoquery one earliest semantic parsing systems used parse semantically represent questions geographical data example took questions like capital state largest population major cities kansas able represent first order logic format statements able translate c capital city x major p place capital c b okay lets switch older paper recent one luke zettlemoyer mike collins paper use ccg semantic parsing domain involves geographical questions define utah noun phrase idaho noun phrase borders something takes one noun phrase left one right form sentence heres representations end states border texas lambda expression single variable x x state x border texas largest state lambda expression x x state computing size x want find value statement maximizes size final example states borders state borders states coming much convulated way using two lambda functions one state one another type state heres derivations utah borders idaho states border texas take look derivations understand output lambda format produced using ccg derivations snapshots paper items learned system entire grammar conclude section semantic parse,Course3,W11-S1-L8,W11,S1,L8,-,11,1,8,okay next segment semant pars semant pars convert natur languag logic form exampl build execut code specif applic airlin reserv geograph queri system semant pars two stage first take input exampl sentenc convert syntact structur use syntact analysi perform semant analysi come semant represent composit semant well done ad semant attach context free grammar rule first pars sentenc syntact associ semant individu word use context free grammar semant attach build semant represent non termin node end day get semant full sentenc associ root prior string here exampl sentenc javier like pizza want produc output predic firstord logic like like javier first argument pizza second argument here associ semant express node javier like pizza leaf node repres case noun anoth noun lambda express here lambda express come handi want repres verb like get repres lambda express two argument x turn predic like two argument next thing combin node node pizza go turn two argument lambda express singl argument lambda express go one variabl bound pizza one unbound variabl lambda express x still miss one argument combin two remain uncombin node javier lambda express verb phrase go get semant entir sentenc appli remain unbound lambda express javier go get predic sentenc like javier pizza practic purpos lot recent work semant pars use communitori categor grammar introduc mark steidmon let give exampl ccg use repres semant exampl adject repres along express one variabl lambda x tall x express repres adject tall transform rule say snp adject essenti one constitu ccg correspond lambda express function f argument x np noun phrase exampl im go use name basketbal player yao ming noun phrase get repres form repres sentenc yao ming tall ccg here start word yao ming label noun phrase tall adject snp adject get translat ccg lambda express one function one attribut combin two togeth get express requir noun phrase left form sentenc combin togeth noun phrase get express yao ming tall relat exercis problem naclo jonathan kummerfeld alexa blackwel patrick littel avail naclo web site two part one gener ccg second one specif languag first part introduc ccg expand mean forward backward slash give littl grammar use four word show combin differ express form grammat pars also give exampl ungrammat pars first part problem ask follow three question one explain ccg work pars sentenc number two take sentenc enjoy long book abl figur pars sentenc success grammar final part three ask given specif grammar come sentenc cannot pars use grammar dont think answer three question look answer next slide answer first three part first second part languag call tok psin new guinea sentenc languag scrambl english translat figur first match english sentenc tok psin sentenc second part translat one sentenc one languag vice versa final part figur map differ word languag differ ccg categori right hand side think look answer next slide okay fun nice naclo problem go look exampl recent work semant pars first one call geoqueri one earliest semant pars system use pars semant repres question geograph data exampl took question like capit state largest popul major citi kansa abl repres first order logic format statement abl translat c capit citi x major p place capit c b okay let switch older paper recent one luke zettlemoy mike collin paper use ccg semant pars domain involv geograph question defin utah noun phrase idaho noun phrase border someth take one noun phrase left one right form sentenc here represent end state border texa lambda express singl variabl x x state x border texa largest state lambda express x x state comput size x want find valu statement maxim size final exampl state border state border state come much convul way use two lambda function one state one anoth type state here deriv utah border idaho state border texa take look deriv understand output lambda format produc use ccg deriv snapshot paper item learn system entir grammar conclud section semant pars,[ 0  4 10  9  8]
242,Course3_W12-S1-L1_-_Discourse_Analysis_-_19_slides_14-56,welcome back natural language processing going continue unit discourse analysis discourse analysis different see natural language processing far deals information goes across sentences often documents going process analyze multiple sentences news article story fiction work many sentences many issues discourse need focus computationally listed first one called anaphora anaphora greek word used refer expressions correlate earlier occurrence document example say went see grandfather hospital period old man weeks phrase old man refers grandfather used sentence anaphoric last sentence surgery days ago instance anaphora pronoun used refer back grandfather one goals computational discourse analysis able group three expressions grandfather old man one set know refer person two concepts want introduce time referring expressions antecedent referring expression examples something like old men antecedent grandfather anaphora problem multisentential text also single sentences completely possible frequent anaphoric expression referring expression may refer antecedent back sentence need address issue computationally well first thing need model discourse tells us people actually go generating text includes anaphora moment understand easier time computing intent lets look phenomena related discourse one coreference example say joe saw mary park period every morning walking dog refer able figure words two sentences possibly refer first know refer named entity sorry refer noun phrase noun phrases john mary park every morning dog know one correct antecedent well lets think every morning clearly person whereas person discrepancy another discrepancy similar nature park park something refer inanimate john man refer either gender agreement dog possible reference structure sentence makes impossible right antecedent thing left mary check fact makes sense right gender right animacy animate person appears previously discourse features used computational analysis coreference annual competition least annual competition called muc specific task coreference extraction evaluation participants provided nist large collection documents annotated coreference manually heres examples difficult read probably pause read carefully idea sentence like russian airline aeroflot hit writ loss damages found hong kong every entity marked number example russian airline aeroflot id whereas hong kong id goal figure entities refer one another lets look examples second paragraph slide see entities references point back entities people board aeroflot airbus aeroflot marked named entity airbus also marked entity see aeroflot id ten took first back entity numbered six later pronoun new id also identified coref named entity using kind data could build automatic systems use classification take account every single occurrence pronoun anaphoric expression look possible antecedents use features determine correct one lets look properties discourse looked definition screwdriver tool wikipedia heres roughly looks like fairly long paragraph dont need read whole thing screwdriver tool manual powered turning screws forth need pay attention however presence discourse structure example word shaft tip word handles introduced beginning sentences refer back objects introduced directly indirectly previous sentences ascending applies word final sentence coreference resolution mentioned need look agreement constraints also positional constraints agreement constraints make sense gender example male female number singular plural animacy know animate inanimate also look syntactic constraints example parallelism two sentences roughly structure use information determine anaphoric expression refers order sentences also important anaphora specifically phenomenon refers back earlier occurrences document similar term called cataphora refers referring expressions point antecedents appear example would new role movie brad pitt going become even famous expression appears antecedent brad pitt example cataphora sentence ordering recency important likely antecendent referring expression within current sentence earlier previous sentence rarely earlier sentence within paragraph rarely would see instances anaphora refer back entities introduced previous paragraph unless intervening anaphoric expression refers back current paragraph go example based paper lappin leass early first time looked computational treatment coreference manually came list rules tell given list candidates anaphoric resolution properties make likely correct antecedents seven properties lappin leass looking want explain next slide detail first want say sentence recency important feature entity introduced sentence anaphoric expression likely correct antecedent one introduced earlier features count largest number points subject emphasis entity subject sentence im going explain next slide detail deal recency entity candidate antecedent current referring expression every time cross sentence boundary weight going cut half effectively gets rid expressions sentences fact lappin leass also rule says four sentences candidates get removed heres examples different features use example subject acura integra parked lot acura integra car subject sentence acura integra parked lot example second feature thats existential predicate third example john parked acura lot case acura object sentence john gave susan acura indirect object finally acura integra john showed susan new cd player example adverbial prepositional phrase includes candidate referent see order show even decreasing likelihood particular word car antecedent pronoun appears later lets right example described also jurafsky martin book example procedure described lappin leass name resolution anaphora procedure called also rap recent years open source implementation algorithm inaudible group national university singapore lets go algorithm detail going take existing expression want disintegrate purpose going collect possible reference four sentences back going remove potential referents agree number gender pronoun one earlier examples john john would removed example remove potential constraints pass intrasentential syntactic coreference constraints means sentence doesnt make syntactic sense particular referent going ignore going compute total salience value referent adding applicable values things like role parallelism gives us extra points also cataphora actually removes points added feature scores going select referent highest salience value tie tie breaker going closest currently disintegrated expression also take account recency move new sentence going halve scores existing entities list okay lets look example raskin markin three sentences shows lappin leass algorithm works pronoun resolution first sentence talks following john saw beautiful acura integra dealership last week showed bill bought four pronouns need resolve first one second sentence point candidate reference john acura integra dealership lets see kind scores get get points recent sentence addition john gets points subject integra gets points object none get points existential phrase indirect object present sentence three expressions also get bonus points adverbial phrase also get bonus points head total number points john total number points integra total number points dealership point algorithm going tell us john likely antecedent word largest number points point move second sentence disambiguate pronoun since crossed sentence boundary halve values antecedents john still available candidate however score gets dropped points score integra gets dropped half dealership well since added group involves john phrase cluster john means first occurrence since current sentence cluster going get sum points john plus thats point john still salient entity discourse going move next example going disintegrate going get high score matches right features case going refer integra new score going added old score acura integra therefore raising point john still lead integra close behind still choose time would still go john default instead integra processed rest pronouns going get sort structure bill gets also additional number points whereas dealership doesnt reference going keep lowest score points move end second sentence halve score going keep relative order absolute values going smaller continue reach end discourse okay next topic want discuss going coherence next slide,Course3,W12-S1-L1,W12,S1,L1,-,12,1,1,welcom back natur languag process go continu unit discours analysi discours analysi differ see natur languag process far deal inform goe across sentenc often document go process analyz multipl sentenc news articl stori fiction work mani sentenc mani issu discours need focu comput list first one call anaphora anaphora greek word use refer express correl earlier occurr document exampl say went see grandfath hospit period old man week phrase old man refer grandfath use sentenc anaphor last sentenc surgeri day ago instanc anaphora pronoun use refer back grandfath one goal comput discours analysi abl group three express grandfath old man one set know refer person two concept want introduc time refer express anteced refer express exampl someth like old men anteced grandfath anaphora problem multisententi text also singl sentenc complet possibl frequent anaphor express refer express may refer anteced back sentenc need address issu comput well first thing need model discours tell us peopl actual go gener text includ anaphora moment understand easier time comput intent let look phenomena relat discours one corefer exampl say joe saw mari park period everi morn walk dog refer abl figur word two sentenc possibl refer first know refer name entiti sorri refer noun phrase noun phrase john mari park everi morn dog know one correct anteced well let think everi morn clearli person wherea person discrep anoth discrep similar natur park park someth refer inanim john man refer either gender agreement dog possibl refer structur sentenc make imposs right anteced thing left mari check fact make sens right gender right animaci anim person appear previous discours featur use comput analysi corefer annual competit least annual competit call muc specif task corefer extract evalu particip provid nist larg collect document annot corefer manual here exampl difficult read probabl paus read care idea sentenc like russian airlin aeroflot hit writ loss damag found hong kong everi entiti mark number exampl russian airlin aeroflot id wherea hong kong id goal figur entiti refer one anoth let look exampl second paragraph slide see entiti refer point back entiti peopl board aeroflot airbu aeroflot mark name entiti airbu also mark entiti see aeroflot id ten took first back entiti number six later pronoun new id also identifi coref name entiti use kind data could build automat system use classif take account everi singl occurr pronoun anaphor express look possibl anteced use featur determin correct one let look properti discours look definit screwdriv tool wikipedia here roughli look like fairli long paragraph dont need read whole thing screwdriv tool manual power turn screw forth need pay attent howev presenc discours structur exampl word shaft tip word handl introduc begin sentenc refer back object introduc directli indirectli previou sentenc ascend appli word final sentenc corefer resolut mention need look agreement constraint also posit constraint agreement constraint make sens gender exampl male femal number singular plural animaci know anim inanim also look syntact constraint exampl parallel two sentenc roughli structur use inform determin anaphor express refer order sentenc also import anaphora specif phenomenon refer back earlier occurr document similar term call cataphora refer refer express point anteced appear exampl would new role movi brad pitt go becom even famou express appear anteced brad pitt exampl cataphora sentenc order recenc import like antecend refer express within current sentenc earlier previou sentenc rare earlier sentenc within paragraph rare would see instanc anaphora refer back entiti introduc previou paragraph unless interven anaphor express refer back current paragraph go exampl base paper lappin leass earli first time look comput treatment corefer manual came list rule tell given list candid anaphor resolut properti make like correct anteced seven properti lappin leass look want explain next slide detail first want say sentenc recenc import featur entiti introduc sentenc anaphor express like correct anteced one introduc earlier featur count largest number point subject emphasi entiti subject sentenc im go explain next slide detail deal recenc entiti candid anteced current refer express everi time cross sentenc boundari weight go cut half effect get rid express sentenc fact lappin leass also rule say four sentenc candid get remov here exampl differ featur use exampl subject acura integra park lot acura integra car subject sentenc acura integra park lot exampl second featur that existenti predic third exampl john park acura lot case acura object sentenc john gave susan acura indirect object final acura integra john show susan new cd player exampl adverbi preposit phrase includ candid refer see order show even decreas likelihood particular word car anteced pronoun appear later let right exampl describ also jurafski martin book exampl procedur describ lappin leass name resolut anaphora procedur call also rap recent year open sourc implement algorithm inaud group nation univers singapor let go algorithm detail go take exist express want disintegr purpos go collect possibl refer four sentenc back go remov potenti refer agre number gender pronoun one earlier exampl john john would remov exampl remov potenti constraint pass intrasententi syntact corefer constraint mean sentenc doesnt make syntact sens particular refer go ignor go comput total salienc valu refer ad applic valu thing like role parallel give us extra point also cataphora actual remov point ad featur score go select refer highest salienc valu tie tie breaker go closest current disintegr express also take account recenc move new sentenc go halv score exist entiti list okay let look exampl raskin markin three sentenc show lappin leass algorithm work pronoun resolut first sentenc talk follow john saw beauti acura integra dealership last week show bill bought four pronoun need resolv first one second sentenc point candid refer john acura integra dealership let see kind score get get point recent sentenc addit john get point subject integra get point object none get point existenti phrase indirect object present sentenc three express also get bonu point adverbi phrase also get bonu point head total number point john total number point integra total number point dealership point algorithm go tell us john like anteced word largest number point point move second sentenc disambigu pronoun sinc cross sentenc boundari halv valu anteced john still avail candid howev score get drop point score integra get drop half dealership well sinc ad group involv john phrase cluster john mean first occurr sinc current sentenc cluster go get sum point john plu that point john still salient entiti discours go move next exampl go disintegr go get high score match right featur case go refer integra new score go ad old score acura integra therefor rais point john still lead integra close behind still choos time would still go john default instead integra process rest pronoun go get sort structur bill get also addit number point wherea dealership doesnt refer go keep lowest score point move end second sentenc halv score go keep rel order absolut valu go smaller continu reach end discours okay next topic want discuss go coher next slide,[ 4 13  0  5 10]
243,Course3_W12-S1-L2_-_Coherence_-_19_slides_16-39,okay previous segment looked discourse analysis point view resolving anaphoric expressions going look properties discourse specifically concept coherence coherence important property course let show examples make obvious first example saw mary street looking bookstore pretty obvious two sentences fit well together second sentence explains little bit mary explains mentioned saw street theres problem second example becomes little bit problematic saw mary street cat well sentences perhaps true little awkward use paragraph theres really logical connection two unless perhaps theres course says cat cat likes go outside therefore mary took cat outside thats saw street overall looks little awkward next example saw mary street pistons well really bad example coherent discourse theres really connection two sentences hard imagine situation two would make sense together see variable degree coherence course better written text coherent one theories used computational linguistics understand coherence works introduced mann thompson theory called rst rhetorical structure theory used determine structure discourse identify relations hold sentences factions sentences stimulations determined two items one called nucleus relation one called satellite let give examples suppose two sentences carpenter tired working day see second sentence elaborates first one gives us explanation carpenter tired one thing important starts pronoun clearly sentence second sentence less important first one depends first one existence start rsd relation two sentences determined follows link two says working day satellite relation nucleus carpenter tired names probably clearly indicating nucleus important satellite wanted summarize paragraph would probably want pick nucleus pick satellite one definitions rst satellite increases belief relation described nucleus relations one nucleus others one others one nucleus one satellite examples rst simulations want show today listed first one result example carpenter worked day new cabinet ready evening see example second sentence showed result action first sentence explanation carpenter tired spent entire day building new cabinet example second sentence explains fact first sentence true third example parallel carpenter worked day period upholsterer took day two sentences parallel structure second one parallel first one elaboration carpenter built cabinet period cabinet four drawers oversized rear panel second sentence pretty obvious gives additional information expands detail elaborates information first sentence relations rst im going show list separately nucleus satellite include circumstance volitional cause purpose interpretation restatement summary others multinuclear words one nucleus sequence contrast joint heres larger table examples im going mention detail lets tackle first one antithesis nucleus antitheses relation ideas favored author satellite ideas disfavored author example author may say like dogs hate chihuahuas second example satellites something disfavored first example nucleus ideas favored author lets look example course analyzed using rhetorical structure theory document comes christian science monitor sentences title bouquets basket living flowers goes like gardening revolution going people planting flower baskets living plants mixing many types one container full summer floral beauty create victorian bouquet flowers choose varying shapes sizes forms besides variety complimentary colors new sentence plants grow tall surrounded smaller ones filled others tumble side hanging basket period leaf textures colors also important finally silver white foliage dusty miller feathery threads lotus vine floating deep greens chartreuse even widely varied foliage colors coleus document analyzed mann matthiessen thompson using rst representation came total nine call utterances utterances portions sentences separate semantics correspond directly sentences however sentence split multiple utterances lowest level eight nine sentences eight nine connected using elaboration rst stimulation remember slides away arrow points satellite nucleus okay two utterances eight nine combined together next thing merge six seven also six eight nine seven group eight nine satellites nucleus appears six relation elaboration chunk six nine connected five using purpose relationship case satellite left create victorian bouquet flowers nucleus six nine lefthand side diagram elaboration relation utterances three four four satellite combine three four together together group form satellite another elaboration relation nucleus two combine group group using background satellite finally another relation top called preparation links satellites think number one nuclear switch consists everything else sense example annotate texts rhetorical structure relations want tell right exists automatic parser take narrative texts nature automatically label rst relationships hierarchical way one nice resource simon fraser university canada website largest repository texts annotated rst relations used training also includes lot examples papers topic process identifying discourse structure called discourse parsing im going give one example lot work find acl anthology website paper wanted discuss briefly marcu echihabi example looked four rst relations contrast causeexplanationevidence sequence condition elaboration also null category corresponds nonrelation use million automatically labeled examples per relation use simple classifier based naive bayes used features word cooccurence features build discourse trees details work found paper rst website briefly im going discuss one influential paper history discourse analysis work gross sidner people centering centering theory tells possible candidates artful resolution one socalled center important concept centered concept likely used anaphoric expressions future sentences goal centering understand local coherence discourse understand texts considered coherent others one ideas centering inference load cognitive load person reading texts associated referring expressions chosen badly example prounoun refers word several sentences back also much focus shift makes text hard understand idea centering going keep sort theme document go along sentences theme going change relatively rarely centering example im going give based idea backwards looking centers forward looking centers every utterance un known called backwards looking center cb connects current utterance previous utterance un minus one also every utterance partially ordered set forward looking centers c sub f related next utterance order depends syntax example subject salience candidate higher one object finally going pin preferred center among forward looking centers based highest salient score details work read original papers gross sidner quickly wanted mention additional work cause document structure discourse analysis across multiple related documents csd based relations similar rst extent apply across multiple sentences multiple documents example things like identity thats text appears one document appears first sentence p refers paragraph refers document cst relationships include subsumption example one sentences may include facts b next sentence may include facts b c say first sentence subsumes second one theres two pages relations appear csd work automatically identifying sentence structure document sets contain related documents one thing makes cst different rst however rst deliberate relation whereas cst typically deliberate documents may different sources written different people cst surface structure relation whereas rst measure deliberate coherence discourse written humans one example course analysis something introduced simone teufel marc moens early known argumentative zoning discourse model analyzing scientific papers work spans several years looked following labels different zones scientific documents aim one example aim research goal paper textual statements sentence structure example next section going talk x going conclude section seven description authors work example methodology results discussion background generally accepted scientific background example moon rotates around earth contrast comparison work basis statements agreement work final categorys everything else example description researchers work one thing want mention discourse analysis idea local entity coherence work done recently barzilay got lapata idea look way entities introduced documents referred later document lets look example paper first sentence says justice department subject sentence conducting anti trust trial object microsoft corporation microsoft corporation neither subject object sentence evidence x mean subject object company company another subject increasingly attempting crush competitors competitors object idea want group together expressions refer entity example microsoft corporation microsoft company would form one cluster netscape would form another cluster justice department would form third cluster within clusters want see whether first time entity mentioned described subject maybe perhaps later described object finally mostly described something else x paper barzilay lapata came entity grid shows number sentences document case six sentences adjectives mentioned subject object something else come model similar hmm tells whats probability sum additive referred first subject subject first subject object combinations x nothing one example bars align lobota paper want tell theres lot active research course analysis acl built top previous work concludes section course analysis going switch next dialogue systems,Course3,W12-S1-L2,W12,S1,L2,-,12,1,2,okay previou segment look discours analysi point view resolv anaphor express go look properti discours specif concept coher coher import properti cours let show exampl make obviou first exampl saw mari street look bookstor pretti obviou two sentenc fit well togeth second sentenc explain littl bit mari explain mention saw street there problem second exampl becom littl bit problemat saw mari street cat well sentenc perhap true littl awkward use paragraph there realli logic connect two unless perhap there cours say cat cat like go outsid therefor mari took cat outsid that saw street overal look littl awkward next exampl saw mari street piston well realli bad exampl coher discours there realli connect two sentenc hard imagin situat two would make sens togeth see variabl degre coher cours better written text coher one theori use comput linguist understand coher work introduc mann thompson theori call rst rhetor structur theori use determin structur discours identifi relat hold sentenc faction sentenc stimul determin two item one call nucleu relat one call satellit let give exampl suppos two sentenc carpent tire work day see second sentenc elabor first one give us explan carpent tire one thing import start pronoun clearli sentenc second sentenc less import first one depend first one exist start rsd relat two sentenc determin follow link two say work day satellit relat nucleu carpent tire name probabl clearli indic nucleu import satellit want summar paragraph would probabl want pick nucleu pick satellit one definit rst satellit increas belief relat describ nucleu relat one nucleu other one other one nucleu one satellit exampl rst simul want show today list first one result exampl carpent work day new cabinet readi even see exampl second sentenc show result action first sentenc explan carpent tire spent entir day build new cabinet exampl second sentenc explain fact first sentenc true third exampl parallel carpent work day period upholster took day two sentenc parallel structur second one parallel first one elabor carpent built cabinet period cabinet four drawer overs rear panel second sentenc pretti obviou give addit inform expand detail elabor inform first sentenc relat rst im go show list separ nucleu satellit includ circumst volit caus purpos interpret restat summari other multinuclear word one nucleu sequenc contrast joint here larger tabl exampl im go mention detail let tackl first one antithesi nucleu antithes relat idea favor author satellit idea disfavor author exampl author may say like dog hate chihuahua second exampl satellit someth disfavor first exampl nucleu idea favor author let look exampl cours analyz use rhetor structur theori document come christian scienc monitor sentenc titl bouquet basket live flower goe like garden revolut go peopl plant flower basket live plant mix mani type one contain full summer floral beauti creat victorian bouquet flower choos vari shape size form besid varieti complimentari color new sentenc plant grow tall surround smaller one fill other tumbl side hang basket period leaf textur color also import final silver white foliag dusti miller featheri thread lotu vine float deep green chartreus even wide vari foliag color coleu document analyz mann matthiessen thompson use rst represent came total nine call utter utter portion sentenc separ semant correspond directli sentenc howev sentenc split multipl utter lowest level eight nine sentenc eight nine connect use elabor rst stimul rememb slide away arrow point satellit nucleu okay two utter eight nine combin togeth next thing merg six seven also six eight nine seven group eight nine satellit nucleu appear six relat elabor chunk six nine connect five use purpos relationship case satellit left creat victorian bouquet flower nucleu six nine lefthand side diagram elabor relat utter three four four satellit combin three four togeth togeth group form satellit anoth elabor relat nucleu two combin group group use background satellit final anoth relat top call prepar link satellit think number one nuclear switch consist everyth els sens exampl annot text rhetor structur relat want tell right exist automat parser take narr text natur automat label rst relationship hierarch way one nice resourc simon fraser univers canada websit largest repositori text annot rst relat use train also includ lot exampl paper topic process identifi discours structur call discours pars im go give one exampl lot work find acl antholog websit paper want discuss briefli marcu echihabi exampl look four rst relat contrast causeexplanationevid sequenc condit elabor also null categori correspond nonrel use million automat label exampl per relat use simpl classifi base naiv bay use featur word cooccur featur build discours tree detail work found paper rst websit briefli im go discuss one influenti paper histori discours analysi work gross sidner peopl center center theori tell possibl candid art resolut one socal center import concept center concept like use anaphor express futur sentenc goal center understand local coher discours understand text consid coher other one idea center infer load cognit load person read text associ refer express chosen badli exampl prounoun refer word sever sentenc back also much focu shift make text hard understand idea center go keep sort theme document go along sentenc theme go chang rel rare center exampl im go give base idea backward look center forward look center everi utter un known call backward look center cb connect current utter previou utter un minu one also everi utter partial order set forward look center c sub f relat next utter order depend syntax exampl subject salienc candid higher one object final go pin prefer center among forward look center base highest salient score detail work read origin paper gross sidner quickli want mention addit work caus document structur discours analysi across multipl relat document csd base relat similar rst extent appli across multipl sentenc multipl document exampl thing like ident that text appear one document appear first sentenc p refer paragraph refer document cst relationship includ subsumpt exampl one sentenc may includ fact b next sentenc may includ fact b c say first sentenc subsum second one there two page relat appear csd work automat identifi sentenc structur document set contain relat document one thing make cst differ rst howev rst deliber relat wherea cst typic deliber document may differ sourc written differ peopl cst surfac structur relat wherea rst measur deliber coher discours written human one exampl cours analysi someth introduc simon teufel marc moen earli known argument zone discours model analyz scientif paper work span sever year look follow label differ zone scientif document aim one exampl aim research goal paper textual statement sentenc structur exampl next section go talk x go conclud section seven descript author work exampl methodolog result discuss background gener accept scientif background exampl moon rotat around earth contrast comparison work basi statement agreement work final categori everyth els exampl descript research work one thing want mention discours analysi idea local entiti coher work done recent barzilay got lapata idea look way entiti introduc document refer later document let look exampl paper first sentenc say justic depart subject sentenc conduct anti trust trial object microsoft corpor microsoft corpor neither subject object sentenc evid x mean subject object compani compani anoth subject increasingli attempt crush competitor competitor object idea want group togeth express refer entiti exampl microsoft corpor microsoft compani would form one cluster netscap would form anoth cluster justic depart would form third cluster within cluster want see whether first time entiti mention describ subject mayb perhap later describ object final mostli describ someth els x paper barzilay lapata came entiti grid show number sentenc document case six sentenc adject mention subject object someth els come model similar hmm tell what probabl sum addit refer first subject subject first subject object combin x noth one exampl bar align lobota paper want tell there lot activ research cours analysi acl built top previou work conclud section cours analysi go switch next dialogu system,[ 4 13  2  0 14]
244,Course3_W12-S1-L3_-_Dialogue_Systems_-_18_slides_09-22,welcome back natural language processing going continue short segment dialogue systems dialogue systems important component computer science natural language processing popular days example person interacting computer asking questions getting answers im going show first example video watch youtube link shown famous skit years ago abbott costello serious misunderstanding discuss names baseball players name skit whos first funny thing skit name one players one participants conversation says something like whos first hes really asking question hes actually making statement saying name person first base second person doesnt understand asks follow questions turns name second baseman third baseman dont know imagine fun dialogue get encourage watch see dialogue interesting properties least involves one person therefore whenever somebody asks question theyre usually expecting answer one participants dialogue makes pause means may time person start talking idea called turntaking two people two people take turns specific rules turntaking example people expected interrupt others unless good reason unless lot time elapsed theres idea default turntaking role says less say want say give chance participant say whatever want say default many exceptions places possible change turns youre relevant place example pause question bargein possible analyzed separate another interesting property dialogue systems something called conversational impliciture implictature shared information participants dialogue heres example says help b says looking thai restaurant typical example dialogue clear first person willing help second person looking specific past run probably know neighborhood theyre located first person going send second person thai restaurant different location mexican restaurant part shared information implicature term refers meaningful inferences listener make going see examples later grices maxims refer properties dialogue make easier understand logical first one called maxim quantity idea want make contribution dialogue informative dont want say either much little second one maxim quality want say things believe true dont say things lack evidence third one maxim relevance fourth one maxim manner want avoid ambiguities discourse dialogue collaborative endeavor expected participants going make effort make possible party understand going actual problem grices maxims going show briefly going pause try solve problem robot teaches players play card game robots name ggg follow following maxims grices list relevance manner quantity quality gives hints cards play please read carefully try solve problem goal understand following things robot gives us particular piece advice determine whether sentence violates maxims ask one first part question theres two additional components next line im going show answer final thing want mention course dialogue analysis speech acts speech acts specific expressions factual statements example assertives things like suggesting putting forward swearing boasting concluding directives things like asking ordering requesting inviting advising begging commissives equals promising planning vowing betting opposing expressives example thanking apologizing welcoming deploring finally declarations things like resign youre fired say resign im saying resign im also taking action say youre fired result sentence second person lost job action taken place even though sentence conclude segment im going go typical architecture dialogue system involves typically following components understanding component makes sense sentences asked human dialogue manager based heard user continues carry dialogue asking followup questions also say exists socalled mixed initiative dialogue systems system may one interacts user theres task manager based dialogue manager system may decide example send query database lets say airplane reservations search engine output task manager sent back dialogue manager generates answer user go loop necessary example generation may involve followup questions may additional rounds understanding dialogue management generation task completed one interesting property course didnt mention idea prosody prosody property text deals issues like rhythm intonation stress lot work speech literature natural language literature identifying kind prosodic expressions features documents especially spoken documents important lot text especially usergenerated content social media used express emotions emphasis important good natural language understanding system fluent peoples use prosody heres example one specific instance prosody called emphasis im going show one sentence want propose seven different ways pronounce see ways pronounce different meaning heres sentence never said stole money tasks right front say sentence seven times time emphasize one words sentence order first time say emphasize never try see much meaning sentence changes emphasis clearly something computer understand case spoken text text would never able recognize emphasis although completely possible somebody wanted convey idea written text could use something like italics bold face stars around words indicate word theyre emphasizing theres lot interesting work done still done optic community prosody recognition also mention prosody generation want produce effective text would want use automatic prosody generation concludes segment dialogue im going see next segment,Course3,W12-S1-L3,W12,S1,L3,-,12,1,3,welcom back natur languag process go continu short segment dialogu system dialogu system import compon comput scienc natur languag process popular day exampl person interact comput ask question get answer im go show first exampl video watch youtub link shown famou skit year ago abbott costello seriou misunderstand discuss name basebal player name skit who first funni thing skit name one player one particip convers say someth like who first he realli ask question he actual make statement say name person first base second person doesnt understand ask follow question turn name second baseman third baseman dont know imagin fun dialogu get encourag watch see dialogu interest properti least involv one person therefor whenev somebodi ask question theyr usual expect answer one particip dialogu make paus mean may time person start talk idea call turntak two peopl two peopl take turn specif rule turntak exampl peopl expect interrupt other unless good reason unless lot time elaps there idea default turntak role say less say want say give chanc particip say whatev want say default mani except place possibl chang turn your relev place exampl paus question bargein possibl analyz separ anoth interest properti dialogu system someth call convers implicitur implictatur share inform particip dialogu here exampl say help b say look thai restaur typic exampl dialogu clear first person will help second person look specif past run probabl know neighborhood theyr locat first person go send second person thai restaur differ locat mexican restaur part share inform implicatur term refer meaning infer listen make go see exampl later grice maxim refer properti dialogu make easier understand logic first one call maxim quantiti idea want make contribut dialogu inform dont want say either much littl second one maxim qualiti want say thing believ true dont say thing lack evid third one maxim relev fourth one maxim manner want avoid ambigu discours dialogu collabor endeavor expect particip go make effort make possibl parti understand go actual problem grice maxim go show briefli go paus tri solv problem robot teach player play card game robot name ggg follow follow maxim grice list relev manner quantiti qualiti give hint card play pleas read care tri solv problem goal understand follow thing robot give us particular piec advic determin whether sentenc violat maxim ask one first part question there two addit compon next line im go show answer final thing want mention cours dialogu analysi speech act speech act specif express factual statement exampl assert thing like suggest put forward swear boast conclud direct thing like ask order request invit advis beg commiss equal promis plan vow bet oppos express exampl thank apolog welcom deplor final declar thing like resign your fire say resign im say resign im also take action say your fire result sentenc second person lost job action taken place even though sentenc conclud segment im go go typic architectur dialogu system involv typic follow compon understand compon make sens sentenc ask human dialogu manag base heard user continu carri dialogu ask followup question also say exist socal mix initi dialogu system system may one interact user there task manag base dialogu manag system may decid exampl send queri databas let say airplan reserv search engin output task manag sent back dialogu manag gener answer user go loop necessari exampl gener may involv followup question may addit round understand dialogu manag gener task complet one interest properti cours didnt mention idea prosodi prosodi properti text deal issu like rhythm inton stress lot work speech literatur natur languag literatur identifi kind prosod express featur document especi spoken document import lot text especi usergener content social media use express emot emphasi import good natur languag understand system fluent peopl use prosodi here exampl one specif instanc prosodi call emphasi im go show one sentenc want propos seven differ way pronounc see way pronounc differ mean here sentenc never said stole money task right front say sentenc seven time time emphas one word sentenc order first time say emphas never tri see much mean sentenc chang emphasi clearli someth comput understand case spoken text text would never abl recogn emphasi although complet possibl somebodi want convey idea written text could use someth like ital bold face star around word indic word theyr emphas there lot interest work done still done optic commun prosodi recognit also mention prosodi gener want produc effect text would want use automat prosodi gener conclud segment dialogu im go see next segment,[ 4  9 14 13 12]
245,Course3_W12-S1-L4_-_Machine_Translation_-_21_slides_10-55,okay next topic going talk machine translation machine translation one exciting areas natural language processing research important well languages world turns majority documents however web english see diagram english small segments languages like russian german spanish chinese french smaller percentages languages remaining close seven thousand languages stark contrast number internet users world second diagram see english chinese speakers form majority internet users followed spanish japanese percentage speakers languages languages top ten goes seems people speak one maximum two languages common would encounter document web language avoided need come ways translate text automatically exactly machine translation im going show little picture famous painting peter bruegel famous painter depicting tower babel theres biblical story says people speaking one language time built tower babel god created different languages people couldnt talk easily trying solve problem creating machine translation systems okay lets see much machine translation works modern software based statistics im going show example socalled rosetta stone illustrates statistics used translate languages heres rosetta stone looks like inscriptions three languages one shown egyptian top greek middle stone carved egypt bc deciphered contemporary humans champollion way deciphered figure texts even though different languages translations one another able figure using pattern recognition manual pattern recognition symbols one languages match symbols language youre interested rosetta stone detail go website look entire text alignments heres naclo problem similar rosetta stone fact category problems naclo called rosetta stone problems written simon zwarts based work kevin knight heres problem looks like real human language although little twist said going explain minutes instead want focus story describe response specifically arcturan intergalactic language come communication constellation language know sentence left centauri language sentence right arcturan language question figure word one languages matches word language would im going let think looking next couple slides im going show answer specific task clue find individual words example word farok centauri word appears two sentences need figure sentences arcturan corresponds see theres one single word two sentences appears sentences continue sentences words unique translations heres solution turns even though languages labeled centauri arcturan fact english spanish example using kevin knight introduce translation works every word english spanish translated madeup word artificial languages see makes sense example word garcia appears languages english spanish since persons name use principle person names typically left untranslated figure word voon two languages recursively figure rest words full solution okay idea machine translation statistical machine translation rather works need known parallel corpora parallel corpus text one language aligned text another language forms translation many instances parallel corpora first one rosetta stone obviously short theres data like handsards corpus handsards corpus proceedings canadian parliament law keep parliamentary proceedings french english equally official every time somebody speaks english translation done humans french viceversa since government data obtained easily hansards website early statistical machine translation systems train using kind data french english many parallel corpora available example news stories translated different languages user manuals discerning company publishes people different countries use products one common data sets used machine translation training available thousands languages bible lets look examples hansards heres english paragraph two sentences translations french ill give example words aligned government english appears gouvernement french thats straightforward example also forms cognate two words historical connection next thing postmaster general gets translated le ministre des postes words minister posts thats exactly translation postmaster general means person see even though documents refer person languages may use idiosyncratic vocabulary directly translatable heres one example bible paragraph english translation cebuano language philippines lets try figure translate three words god heaven earth english cebuano heres one pair aligned sentences another one third one use lot information first cebuano spoken philippines philippines spanish used commonly spoken language words cebuano got cognates example dios know spanish immediately figure means god thats basic idea statistical machine translation whats important look core current statistics looking words appear sentences language dont appear others example im going pause moment let figure figure translations heaven earth based three examples heres information use cooccurrence sentences word order especially among across languages relatively similar probably case also cognates useful languages related need corpora nature aligned sentence level aligned first thing need algorithm work align automatically exactly text alignment actually done exactly text may known comparable corpora site design would maybe much trickier noisier still possible use build translation systems purely based comparable heres naclo problem download language similarity see often identify cognates among languages historically related first sentence universal declaration human rights different languages figure ones language family similar words first shown slide two languages labeled english latin six second slide pause look last two slides see figure languages belong family answer shown next seven clusters look universal declaration human rights internet figure specific languages include languages like romanian italian basque latvian going stop moment continue techniques central relation,Course3,W12-S1-L4,W12,S1,L4,-,12,1,4,okay next topic go talk machin translat machin translat one excit area natur languag process research import well languag world turn major document howev web english see diagram english small segment languag like russian german spanish chines french smaller percentag languag remain close seven thousand languag stark contrast number internet user world second diagram see english chines speaker form major internet user follow spanish japanes percentag speaker languag languag top ten goe seem peopl speak one maximum two languag common would encount document web languag avoid need come way translat text automat exactli machin translat im go show littl pictur famou paint peter bruegel famou painter depict tower babel there biblic stori say peopl speak one languag time built tower babel god creat differ languag peopl couldnt talk easili tri solv problem creat machin translat system okay let see much machin translat work modern softwar base statist im go show exampl socal rosetta stone illustr statist use translat languag here rosetta stone look like inscript three languag one shown egyptian top greek middl stone carv egypt bc deciph contemporari human champollion way deciph figur text even though differ languag translat one anoth abl figur use pattern recognit manual pattern recognit symbol one languag match symbol languag your interest rosetta stone detail go websit look entir text align here naclo problem similar rosetta stone fact categori problem naclo call rosetta stone problem written simon zwart base work kevin knight here problem look like real human languag although littl twist said go explain minut instead want focu stori describ respons specif arcturan intergalact languag come commun constel languag know sentenc left centauri languag sentenc right arcturan languag question figur word one languag match word languag would im go let think look next coupl slide im go show answer specif task clue find individu word exampl word farok centauri word appear two sentenc need figur sentenc arcturan correspond see there one singl word two sentenc appear sentenc continu sentenc word uniqu translat here solut turn even though languag label centauri arcturan fact english spanish exampl use kevin knight introduc translat work everi word english spanish translat madeup word artifici languag see make sens exampl word garcia appear languag english spanish sinc person name use principl person name typic left untransl figur word voon two languag recurs figur rest word full solut okay idea machin translat statist machin translat rather work need known parallel corpora parallel corpu text one languag align text anoth languag form translat mani instanc parallel corpora first one rosetta stone obvious short there data like handsard corpu handsard corpu proceed canadian parliament law keep parliamentari proceed french english equal offici everi time somebodi speak english translat done human french viceversa sinc govern data obtain easili hansard websit earli statist machin translat system train use kind data french english mani parallel corpora avail exampl news stori translat differ languag user manual discern compani publish peopl differ countri use product one common data set use machin translat train avail thousand languag bibl let look exampl hansard here english paragraph two sentenc translat french ill give exampl word align govern english appear gouvern french that straightforward exampl also form cognat two word histor connect next thing postmast gener get translat le ministr de post word minist post that exactli translat postmast gener mean person see even though document refer person languag may use idiosyncrat vocabulari directli translat here one exampl bibl paragraph english translat cebuano languag philippin let tri figur translat three word god heaven earth english cebuano here one pair align sentenc anoth one third one use lot inform first cebuano spoken philippin philippin spanish use commonli spoken languag word cebuano got cognat exampl dio know spanish immedi figur mean god that basic idea statist machin translat what import look core current statist look word appear sentenc languag dont appear other exampl im go paus moment let figur figur translat heaven earth base three exampl here inform use cooccurr sentenc word order especi among across languag rel similar probabl case also cognat use languag relat need corpora natur align sentenc level align first thing need algorithm work align automat exactli text align actual done exactli text may known compar corpora site design would mayb much trickier noisier still possibl use build translat system pure base compar here naclo problem download languag similar see often identifi cognat among languag histor relat first sentenc univers declar human right differ languag figur one languag famili similar word first shown slide two languag label english latin six second slide paus look last two slide see figur languag belong famili answer shown next seven cluster look univers declar human right internet figur specif languag includ languag like romanian italian basqu latvian go stop moment continu techniqu central relat,[ 8  4  3 14 13]
246,Course3_W12-S1-L5_-_Machine_Translation_Basic_Techniques_-_13_slides_11-48,okay lets look basic techniques machine translation one fundamental ideas behind modern translation systems translation actually method decoding heres famous quote want read aloud one naturally wonders problem translation could conceivably treated problem cryptography look article russian say really written english coded strange symbols proceed decode quote warren weaver one founders information theory book called translation published almost years ago heres question audience know french least look next two slides translated automatically google translate see figure whether system good job sort mistakes made documents recipe english look next two slides pause much need go back forth figure identify translation issues heres original recipe ingredients directions read one sentence cook pasta large pot boiling salted water al dente heres automatic translation google french foreign lets spend little bit time see whats wrong translations also works im going show examples next slide heres answer want go back second see last ingredient reason pound sign probably way google translated one pound really funny incorrect lets look problems ill show three first one syntactically correct foreign means something like near boiling point grammatically incorrect next problem use sequence verbs form nice parallel structure english specifically cook reduce simmer stir stir drain serve english verbs appear imperative sentences whereas french translation somehow mixed first one foreign infinitive foreign infinitive one infinitive switches noun another infinitive last example want show problem agreements foreign plural listed foreign singular extra end indicate plural french problem adjectives nouns agree number gender cases example disagreement lets see causes machine translation systems problems translate one language another many reasons first one word order languages subjectverbobject example english mandarin others verbsubjectobject others subjectobjectverb obviously sentences like languages different word order categories use syntactic information figure move subject verb object another example prepositions japanese japanese prepositions actually postpositions actually put words therefore say something like mariko japanese would say foreign another example inflection example spanish spanish word translated many different ways depending person number verb also whether infinitive foreign example first person singular foreign second person singular foreign first personal plural foreign third person plural foreign infinitive appear english want produce system translates english spanish figure subject figure correct verb form examples lexical distinctions example spanish making distinction word use two languages english say bottle floated spanish correct translation would foreign means left place floating floating another example japanese word multiple translations word brother brother japanese either foreign younger brother foreign older brother one example french work english translated either foreign foreign depending whether group feminine masculine example next slide word order phrases example french adjective typically follows noun say foreign blue house noun verb french noun adjective french another example much complicated word order japanese subjectverbobject versus subjectobjectverb order want translate like drink coffee english japanese word order english pronoun subject followed verb followed phrase involves verb noun japanese would translate something like foreign means foreign stays right place foreign subject marker english foreign direct object coffee object appears verb foreign expression drink nominalization verb drink foreign marker topic foreign like verb sentence appears way end imagine translating english japanese much difficult lets say translating english similar language french another example vocabulary spanish english word wall multiple meanings whether internal wall external wall words spanish foreign foreign obviously need understand context document come correct translation wall final example french entire phrases substituted single word word play english noun indicating drama comedy theaters play translated french foreign probably guess something like piece theater whats important many cases one word one language gets translated multiple words another language vice versa maybe even multiword expressions translated multiword expressions different internal structure okay understand machine translation difficult lets figure build working machine translation systems many approaches many based triangle f stands french e stands english many instances use f indicate foreign language translated english stands interlingua interlingua sort semantic representation text dependent underlying language lets look different strategies first one going discuss called direct approach direct approach says start beginning foreign sentence look dictionary one word time translate english obviously doesnt take account problems weve discussed far ambiguous words dont know translation pick syntactic disagreements approach tried powerful computers language resources software parallel data dictionaries obviously naive approach way work im show two funny examples time period show bad first example people trying build system translates english russian figure good job manually tried translate back russian english see get thing sentence tried give flesh weak spirit strong sentence got translated russian back english people scared see translation went like meat rotten vodka good see bad translation see system would make mistake funny example time translate expression sight mind went russian back came back blind idiot see bad another approach developed later called indirect transfer method transfer method set grammatical rules apply different pairs languages example french adjective follows noun english noun follows adjective rules like identify adjectives nouns least translate right order third approach use something called interlingua interlingua translate lets say foreign language logical form example first order logic modal logic use generation translate back interlingual representation target language heres examples want translate blue house direct approach would translate word separately transfer would make sure least get blue house right order interlingua would sort logical representation h house h blue use generation produce english version next segment going look noisy channel methods machine translation form basis modern translation technology,Course3,W12-S1-L5,W12,S1,L5,-,12,1,5,okay let look basic techniqu machin translat one fundament idea behind modern translat system translat actual method decod here famou quot want read aloud one natur wonder problem translat could conceiv treat problem cryptographi look articl russian say realli written english code strang symbol proceed decod quot warren weaver one founder inform theori book call translat publish almost year ago here question audienc know french least look next two slide translat automat googl translat see figur whether system good job sort mistak made document recip english look next two slide paus much need go back forth figur identifi translat issu here origin recip ingredi direct read one sentenc cook pasta larg pot boil salt water al dent here automat translat googl french foreign let spend littl bit time see what wrong translat also work im go show exampl next slide here answer want go back second see last ingredi reason pound sign probabl way googl translat one pound realli funni incorrect let look problem ill show three first one syntact correct foreign mean someth like near boil point grammat incorrect next problem use sequenc verb form nice parallel structur english specif cook reduc simmer stir stir drain serv english verb appear imper sentenc wherea french translat somehow mix first one foreign infinit foreign infinit one infinit switch noun anoth infinit last exampl want show problem agreement foreign plural list foreign singular extra end indic plural french problem adject noun agre number gender case exampl disagr let see caus machin translat system problem translat one languag anoth mani reason first one word order languag subjectverbobject exampl english mandarin other verbsubjectobject other subjectobjectverb obvious sentenc like languag differ word order categori use syntact inform figur move subject verb object anoth exampl preposit japanes japanes preposit actual postposit actual put word therefor say someth like mariko japanes would say foreign anoth exampl inflect exampl spanish spanish word translat mani differ way depend person number verb also whether infinit foreign exampl first person singular foreign second person singular foreign first person plural foreign third person plural foreign infinit appear english want produc system translat english spanish figur subject figur correct verb form exampl lexic distinct exampl spanish make distinct word use two languag english say bottl float spanish correct translat would foreign mean left place float float anoth exampl japanes word multipl translat word brother brother japanes either foreign younger brother foreign older brother one exampl french work english translat either foreign foreign depend whether group feminin masculin exampl next slide word order phrase exampl french adject typic follow noun say foreign blue hous noun verb french noun adject french anoth exampl much complic word order japanes subjectverbobject versu subjectobjectverb order want translat like drink coffe english japanes word order english pronoun subject follow verb follow phrase involv verb noun japanes would translat someth like foreign mean foreign stay right place foreign subject marker english foreign direct object coffe object appear verb foreign express drink nomin verb drink foreign marker topic foreign like verb sentenc appear way end imagin translat english japanes much difficult let say translat english similar languag french anoth exampl vocabulari spanish english word wall multipl mean whether intern wall extern wall word spanish foreign foreign obvious need understand context document come correct translat wall final exampl french entir phrase substitut singl word word play english noun indic drama comedi theater play translat french foreign probabl guess someth like piec theater what import mani case one word one languag get translat multipl word anoth languag vice versa mayb even multiword express translat multiword express differ intern structur okay understand machin translat difficult let figur build work machin translat system mani approach mani base triangl f stand french e stand english mani instanc use f indic foreign languag translat english stand interlingua interlingua sort semant represent text depend underli languag let look differ strategi first one go discuss call direct approach direct approach say start begin foreign sentenc look dictionari one word time translat english obvious doesnt take account problem weve discuss far ambigu word dont know translat pick syntact disagr approach tri power comput languag resourc softwar parallel data dictionari obvious naiv approach way work im show two funni exampl time period show bad first exampl peopl tri build system translat english russian figur good job manual tri translat back russian english see get thing sentenc tri give flesh weak spirit strong sentenc got translat russian back english peopl scare see translat went like meat rotten vodka good see bad translat see system would make mistak funni exampl time translat express sight mind went russian back came back blind idiot see bad anoth approach develop later call indirect transfer method transfer method set grammat rule appli differ pair languag exampl french adject follow noun english noun follow adject rule like identifi adject noun least translat right order third approach use someth call interlingua interlingua translat let say foreign languag logic form exampl first order logic modal logic use gener translat back interlingu represent target languag here exampl want translat blue hous direct approach would translat word separ transfer would make sure least get blue hous right order interlingua would sort logic represent h hous h blue use gener produc english version next segment go look noisi channel method machin translat form basi modern translat technolog,[ 8  4  0  3 14]
247,Course3_W12-S1-L6_-__Machine_Translation_Noisy_Channel_Methods_-_22_slides_11-53,okay going see noisy channel methods used basis modern machine translation looked noisy channels idea source signal gets transferred channel thats mode communication noisy channel model essentially parametric probabilistic model language translation lets see works given foreign language sentence f guess english translation e going think process follows english sentence either one us tried guess converted foreign language center using encoder e f actually observe f want get back original e decoder gets f input produces e output actually doesnt produce e produces e prime want pick e prime close possible original e really english sentence translation model language model going pick e maximizes probability want probability e given f largest going model using bayesian theorem representing product translation model f given e language model e pe means given english string whats probability forms valid english sentence pf given e says particular english string e particular foreign sentence f whats probability f produced e lets look example suppose want translate french expression une fleur rouge lets look several different candidates infinite number candidates english going look see method works first one flower red second one red flower third one flower red lets see values p e p f given e product examples flower red low p e flower red trigram expect see english however high value p f given e exact words expect see translation multiply two youre probably going get relatively low score thing applies next two lines table okay flower r also good terms relationship foreign sentence legitimate english translation thing applies third one lets look red dog red dog going much higher pe something expect see english pf gives given e going low words ones saw framed sentence next one dog cat mouse going low values features finally red flower going high values therefore high product idea estimate p e p f given e multiply together rank candidate english phrases based product pick one highest score correct translation heres example want go example english chinese text chinese want find every english sentence probability chinese sentence given sentence multiply probability english sentence going give us estimate probability english given chinese okay noisy channel models used machine translation many tasks example text text generation summarization input long document output short document lecture summarization talk method also things like text signal speech recognition ocr spelling correction cases well us input one type text representation output another type text representation noisy channel converts one specifically ocr probability text given pixel map product probability text times probability pixels given text ibm model developed early probably one fundamental contributions natural language processing goes like going start sentence one language going perform sequence transformations get sentence target language transformations may look unintuitive however picked good reason relatively possible straightforward implement computationally engineering achievement linguistic achievement want system works even necessarily based way humans actually translation bear second youll see story discuss later makes sense heres want translate english french watched interesting play first step actually order steps example doesnt match order ibm system uses still give good idea works first step going produce words english many times need get actual number words french translation saw word play english translated three words french going something like watched watched interesting play play play reason three plays french translation play three words reason two words watched french translation watched also going two words okay next thing need figure right word order french nouns adjectives appear different order english going switch order going get watched watched play play play interesting final step going convert english words one french words corresponds becomes j j watch becomes ai uv play becomes piece de theatre interesting becomes interessante ideal model works roughly essentially three sets probabilities first one called fertility probability tells us whats probability words lengths expression length one languages going translated expression length j language second word ordering distortion tells us whats probability word position seven one language going switch position eight language vice versa finally translation probabilities tell us whats probability certain word english gets translated certain word french probabilities automatically obtained using method called em expectation maximization pairs aligned sentences two languages im going go lot detail models want list five em trained models translation alignment fertility ones already mentioned also models include something called classbased alignment nondeficient algorithm steps need undertake order machine translation first tokenize input document words perform sentence alignment time going alignments especially case good parallel corpora perhaps relatively rare thats sentence b one language get translated lets say sentences x language doesnt exactly match x b doesnt exactly match finally examples example mappings one sentence one language gets split two sentences method used first problem church gale based sentence length look sequences grams cognates languages especially especially works languages like french english kind alignments get look paragraph length english paragraph length german see paragraph lengths highly correlated scattered plot diagram shows easily output see pairs sentences mapping examples alignments extremely rare less finally examples little bit alignments performed sentence alignment apply ibm models necessary learn actual translations fertilities distortion probabilities lets look briefly ibm model ibm model based alignments sentences example pair la maison bleue french blue house look possible alignments things like means first french word becomes first english word second becomes second third one becomes third one second alignment means french word becomes first english word swap order second third one combinations example say second english word doesnt exist french third english word corresponds second third french words even go extreme say three words french get translated first word english sentence second third english words appear french theres certain number alignments consider beginning youre going assume equally likely er algorithm going tell one likely time give likely alignments individual words going go lot detail ibm models however youre interested go read excellent tutorial kevin knight available website also download implementation statistical machine translation system called moses statmtorg going continue next segment advanced methods machine translation,Course3,W12-S1-L6,W12,S1,L6,-,12,1,6,okay go see noisi channel method use basi modern machin translat look noisi channel idea sourc signal get transfer channel that mode commun noisi channel model essenti parametr probabilist model languag translat let see work given foreign languag sentenc f guess english translat e go think process follow english sentenc either one us tri guess convert foreign languag center use encod e f actual observ f want get back origin e decod get f input produc e output actual doesnt produc e produc e prime want pick e prime close possibl origin e realli english sentenc translat model languag model go pick e maxim probabl want probabl e given f largest go model use bayesian theorem repres product translat model f given e languag model e pe mean given english string what probabl form valid english sentenc pf given e say particular english string e particular foreign sentenc f what probabl f produc e let look exampl suppos want translat french express une fleur roug let look sever differ candid infinit number candid english go look see method work first one flower red second one red flower third one flower red let see valu p e p f given e product exampl flower red low p e flower red trigram expect see english howev high valu p f given e exact word expect see translat multipli two your probabl go get rel low score thing appli next two line tabl okay flower r also good term relationship foreign sentenc legitim english translat thing appli third one let look red dog red dog go much higher pe someth expect see english pf give given e go low word one saw frame sentenc next one dog cat mous go low valu featur final red flower go high valu therefor high product idea estim p e p f given e multipli togeth rank candid english phrase base product pick one highest score correct translat here exampl want go exampl english chines text chines want find everi english sentenc probabl chines sentenc given sentenc multipli probabl english sentenc go give us estim probabl english given chines okay noisi channel model use machin translat mani task exampl text text gener summar input long document output short document lectur summar talk method also thing like text signal speech recognit ocr spell correct case well us input one type text represent output anoth type text represent noisi channel convert one specif ocr probabl text given pixel map product probabl text time probabl pixel given text ibm model develop earli probabl one fundament contribut natur languag process goe like go start sentenc one languag go perform sequenc transform get sentenc target languag transform may look unintuit howev pick good reason rel possibl straightforward implement comput engin achiev linguist achiev want system work even necessarili base way human actual translat bear second youll see stori discuss later make sens here want translat english french watch interest play first step actual order step exampl doesnt match order ibm system use still give good idea work first step go produc word english mani time need get actual number word french translat saw word play english translat three word french go someth like watch watch interest play play play reason three play french translat play three word reason two word watch french translat watch also go two word okay next thing need figur right word order french noun adject appear differ order english go switch order go get watch watch play play play interest final step go convert english word one french word correspond becom j j watch becom ai uv play becom piec de theatr interest becom interessant ideal model work roughli essenti three set probabl first one call fertil probabl tell us what probabl word length express length one languag go translat express length j languag second word order distort tell us what probabl word posit seven one languag go switch posit eight languag vice versa final translat probabl tell us what probabl certain word english get translat certain word french probabl automat obtain use method call em expect maxim pair align sentenc two languag im go go lot detail model want list five em train model translat align fertil one alreadi mention also model includ someth call classbas align nondefici algorithm step need undertak order machin translat first token input document word perform sentenc align time go align especi case good parallel corpora perhap rel rare that sentenc b one languag get translat let say sentenc x languag doesnt exactli match x b doesnt exactli match final exampl exampl map one sentenc one languag get split two sentenc method use first problem church gale base sentenc length look sequenc gram cognat languag especi especi work languag like french english kind align get look paragraph length english paragraph length german see paragraph length highli correl scatter plot diagram show easili output see pair sentenc map exampl align extrem rare less final exampl littl bit align perform sentenc align appli ibm model necessari learn actual translat fertil distort probabl let look briefli ibm model ibm model base align sentenc exampl pair la maison bleue french blue hous look possibl align thing like mean first french word becom first english word second becom second third one becom third one second align mean french word becom first english word swap order second third one combin exampl say second english word doesnt exist french third english word correspond second third french word even go extrem say three word french get translat first word english sentenc second third english word appear french there certain number align consid begin your go assum equal like er algorithm go tell one like time give like align individu word go go lot detail ibm model howev your interest go read excel tutori kevin knight avail websit also download implement statist machin translat system call mose statmtorg go continu next segment advanc method machin translat,[ 8  4  3 14 13]
248,Course3_W12-S1-L7_-_Machine_Translation_Advanced_Methods_-_13_slides_09-36,im going continue brief introduction advanced methods used statistical machine translation beyond ibm models briefly include things like treetotree translations thats example work yamada knight idea parse sentences one languages transformations syntactic trees generate trees language another technique introduced och ney called phrased based mission translation idea recognize contiguous chunks text form phrases translate units instead one word time third method syntax based introduced och et al idea use usual ibm models put use top candidate translation perform technique called discriminative reranking compute set features translations combine using log linear model pick translation best part long linear model include features example whether sentence syntactically reasonable whether right length one technique want mention briefly close structure closing idea take document language general idea syntactic structure case german youre going perform sequence steps make text similar english following syntactic patterns target language instead saying perhaps adopt change adopt perhaps matches german better adopt becomes adopt peculiarity german syntax adopt becomes adopt moving subject right location embedded clause looking exact particle german common phrasal verbs one parts preposition like anrufen moves back main verb example say rufen sie bitte noch einmal means call right back please case verb anrufen gets split rufen see preposition prefix moved arbitrarily later sentence performing set transformations one languages render language much similar target language going avoid common problems ibm models statistical machine translation able get much better syntactic structure output one thing mention statistical matching translation idea synchronous grammar synchronous grammar introduced machine translation inaudible idea generate parse trees parallel two languages using different rules start example languages youre going apply different rules take account example whether languages subject verb object subject object verb whether adjectives follow nouns rule english says np goes adjective noun english time parallel rule np goes n adjective spanish wed like move evaluation machine translation want introduce basic techniques use appear research papers published machine translation machine translation particularly difficult evaluate theres one single answer ask different human translators come translations sentence theyre likely come similar yet widely divergent translations human judgements unique even judge one human another human youre going get perfect agreement metrics used past evaluating translation involve asking humans judge translations manually things like adequacy adequate translation original document grammaticality grammatical output kind technique evaluation expansive compare many different systems many different inaudible many different sentences would need thousands humans humans actually fairly unreliable expensive focus recent years automatic methods specifically common technique days technique called bleu introduced kishore papineni et al ibm simple technique based multiple human references unigram bigram overlap system produces output compare output human judgments unigram bigram trigram fullgram levels theres another technique thats also relatively news called edit cost example number edits human would need perform translation example moving words around characters around counting number minutes takes revise translation correct translation focus bleu heres works simple ngram precision multiple human references important includes brevity penalty otherwise could come translation thats short focus two three obvious words translation high precision bleu includes additional parameter makes impossible cheat way bleu ideal metric however shown correlate relatively well human assessments automatic systems good however used compare humans guess automatic translations bottom line bleu people dont like use anyway pretty much excepted standard evaluation machine translations systems days many datasets available example ldc multiple translation chinese multiple translation arabic corpora comes large number translations humans also automatic systems used training evaluation translation systems heres example look web detail wanted show first sentence chinese sentence actually headline human references get corpus see theyre exactly even though headline relatively large diversity lets move last topic matching translation briefly idea decoding built ibm models including positive phrase models figure translations likely decoding process actually find role candidate translations one maximizes probability f given e times p e vitals probabilities need find translation maximizes expression fortunately even simple model like ibm model npcomplete problem longer difficult get best translation people instead efficiency reasons use phrase transition table specifically famous pharaoh system phillip koehn use search like combining cost translation certain point estimate translation rest sentence done level phrases make things efficient combined beam search small set candidates evaluated step going conclude section machine translation giving pointers tools projects assignments first one language modeling tool kits many available command sri language modeling toolkit available research purposes internet research translations systems example giza moses available statmtorg decoders download pharoah also websites concludes section machine translation see next segment,Course3,W12-S1-L7,W12,S1,L7,-,12,1,7,im go continu brief introduct advanc method use statist machin translat beyond ibm model briefli includ thing like treetotre translat that exampl work yamada knight idea pars sentenc one languag transform syntact tree gener tree languag anoth techniqu introduc och ney call phrase base mission translat idea recogn contigu chunk text form phrase translat unit instead one word time third method syntax base introduc och et al idea use usual ibm model put use top candid translat perform techniqu call discrimin rerank comput set featur translat combin use log linear model pick translat best part long linear model includ featur exampl whether sentenc syntact reason whether right length one techniqu want mention briefli close structur close idea take document languag gener idea syntact structur case german your go perform sequenc step make text similar english follow syntact pattern target languag instead say perhap adopt chang adopt perhap match german better adopt becom adopt peculiar german syntax adopt becom adopt move subject right locat embed claus look exact particl german common phrasal verb one part preposit like anrufen move back main verb exampl say rufen sie bitt noch einmal mean call right back pleas case verb anrufen get split rufen see preposit prefix move arbitrarili later sentenc perform set transform one languag render languag much similar target languag go avoid common problem ibm model statist machin translat abl get much better syntact structur output one thing mention statist match translat idea synchron grammar synchron grammar introduc machin translat inaud idea gener pars tree parallel two languag use differ rule start exampl languag your go appli differ rule take account exampl whether languag subject verb object subject object verb whether adject follow noun rule english say np goe adject noun english time parallel rule np goe n adject spanish wed like move evalu machin translat want introduc basic techniqu use appear research paper publish machin translat machin translat particularli difficult evalu there one singl answer ask differ human translat come translat sentenc theyr like come similar yet wide diverg translat human judgement uniqu even judg one human anoth human your go get perfect agreement metric use past evalu translat involv ask human judg translat manual thing like adequaci adequ translat origin document grammat grammat output kind techniqu evalu expans compar mani differ system mani differ inaud mani differ sentenc would need thousand human human actual fairli unreli expens focu recent year automat method specif common techniqu day techniqu call bleu introduc kishor papineni et al ibm simpl techniqu base multipl human refer unigram bigram overlap system produc output compar output human judgment unigram bigram trigram fullgram level there anoth techniqu that also rel news call edit cost exampl number edit human would need perform translat exampl move word around charact around count number minut take revis translat correct translat focu bleu here work simpl ngram precis multipl human refer import includ breviti penalti otherwis could come translat that short focu two three obviou word translat high precis bleu includ addit paramet make imposs cheat way bleu ideal metric howev shown correl rel well human assess automat system good howev use compar human guess automat translat bottom line bleu peopl dont like use anyway pretti much except standard evalu machin translat system day mani dataset avail exampl ldc multipl translat chines multipl translat arab corpora come larg number translat human also automat system use train evalu translat system here exampl look web detail want show first sentenc chines sentenc actual headlin human refer get corpu see theyr exactli even though headlin rel larg divers let move last topic match translat briefli idea decod built ibm model includ posit phrase model figur translat like decod process actual find role candid translat one maxim probabl f given e time p e vital probabl need find translat maxim express fortun even simpl model like ibm model npcomplet problem longer difficult get best translat peopl instead effici reason use phrase transit tabl specif famou pharaoh system phillip koehn use search like combin cost translat certain point estim translat rest sentenc done level phrase make thing effici combin beam search small set candid evalu step go conclud section machin translat give pointer tool project assign first one languag model tool kit mani avail command sri languag model toolkit avail research purpos internet research translat system exampl giza mose avail statmtorg decod download pharoah also websit conclud section machin translat see next segment,[ 8  4  0 14 13]
249,Course3_W12-S1-L8_-_Text_Generation_-_11_slides_05-49,segment going briefly talk second portion natural language understanding pipeline specifically text generation let remind nlp pipeline looks like understanding generation well start language like sentence understanding takes us computer representation computer understand computer wants respond human would go generation component produce language back nlp systems parts completely reasonable expect system understand human language perform actions without generate anything back also possible system second part example computer access weather data use automatic text generation produce weather reports access stock market data produce financial reports definition natural language generation process deliberately constructing natural language text order meet specific communicative goal communicative goal may want give specific information user particular context using much space definition david mcdonald nlg one important components mapping meaning text meaning typically presented semantic form example ccg fug fug stages following first perform something called content selection determining content want share user perform lexical choice deciding words use example whether use nominalization verb express certain concept perform something called sentence structure generation performing actions like aggregation collecting multiple facts one generating referring expressions example pronouns also worry discourse structure know individual sentences may want generate discourse connectives example therefore consequently addition text flow small thing heres example nlg system fog system designed goldberg et al use years designed generate weather forecast reports canadian weather service french english lets input numerical simulation data annotated humans example france expected precipitation output little bit harder see probably stop zoom see detail actual weather report specific location one two target languages one example generation system developed columbia university belfort goal produce reports describe difference simulation options engineer lays cables telephone system already explored input simulation log file designed developed bellcore columbia university input something like theres specific runid specific type cable located particular place particular date certain way get collapsed one output something like saved fiber refinement includes dlc changes runid alldlc runid fiberall demanded plan activate fiber csas second quarter requested replacement fiber cable co section output system used fuf surge two generational systems developed columbia mid text generation theres important considerations mentioned already deal choices consider sort classify takes multiple inputs decide one take choices content want say coherence make text flow coherently style media media refers fact generate multimedia presentations involve lets say information visual form rest textual form balance amount information conveyed media also determine syntactic structure use example whether use nominalization verb aggregate facts get concise sentences instead saying example cable unit one cable unit two may say something like two cables units one two also figure generate differing expressions text doesnt look completely automatically generated worry lexical choice words appropriate given context end introduction natural language generation going continue next segment,Course3,W12-S1-L8,W12,S1,L8,-,12,1,8,segment go briefli talk second portion natur languag understand pipelin specif text gener let remind nlp pipelin look like understand gener well start languag like sentenc understand take us comput represent comput understand comput want respond human would go gener compon produc languag back nlp system part complet reason expect system understand human languag perform action without gener anyth back also possibl system second part exampl comput access weather data use automat text gener produc weather report access stock market data produc financi report definit natur languag gener process deliber construct natur languag text order meet specif commun goal commun goal may want give specif inform user particular context use much space definit david mcdonald nlg one import compon map mean text mean typic present semant form exampl ccg fug fug stage follow first perform someth call content select determin content want share user perform lexic choic decid word use exampl whether use nomin verb express certain concept perform someth call sentenc structur gener perform action like aggreg collect multipl fact one gener refer express exampl pronoun also worri discours structur know individu sentenc may want gener discours connect exampl therefor consequ addit text flow small thing here exampl nlg system fog system design goldberg et al use year design gener weather forecast report canadian weather servic french english let input numer simul data annot human exampl franc expect precipit output littl bit harder see probabl stop zoom see detail actual weather report specif locat one two target languag one exampl gener system develop columbia univers belfort goal produc report describ differ simul option engin lay cabl telephon system alreadi explor input simul log file design develop bellcor columbia univers input someth like there specif runid specif type cabl locat particular place particular date certain way get collaps one output someth like save fiber refin includ dlc chang runid alldlc runid fiberal demand plan activ fiber csa second quarter request replac fiber cabl co section output system use fuf surg two gener system develop columbia mid text gener there import consider mention alreadi deal choic consid sort classifi take multipl input decid one take choic content want say coher make text flow coher style media media refer fact gener multimedia present involv let say inform visual form rest textual form balanc amount inform convey media also determin syntact structur use exampl whether use nomin verb aggreg fact get concis sentenc instead say exampl cabl unit one cabl unit two may say someth like two cabl unit one two also figur gener differ express text doesnt look complet automat gener worri lexic choic word appropri given context end introduct natur languag gener go continu next segment,[ 4  8 14 13 12]
250,Course3_W2-S1-L1_-_Parts_of_speech_-_17_slides_15-50,next segment going different parts speech exist language lets look first example sentence like nathalie likes black cats replace word black persian tabby small still sentences make sense nathalie likes tabby cats nathalie likes persian cats well single word used position usually referred syntactic category case syntactic category part speech single word adjective many syntactic categories english languages include open closed categories open categories include categories new words added time example noflyzone noun twerk verb words added vocabulary english relative recent past also closed functional categories respectively determiners articles prepositions lets look example sentence dog chased yellow bird multiple parts speech article noun verb article adjective noun english eight general types parts speech looked already nouns verbs adjectives common nouns include things like dog tree computer idea either concrete like first three abstract vary number singular plural gender case english languages example latin word puer means boy spelled many different ways singular one boy puer socalled nominative case subject case puerum accusative object pueri genitive boy plural pueri nominative subject form pueros accusative finally puerorum genitive boys gender doesnt need match actual sex object described typical example people give word german madchen word madchen means girl diminutive chen part indicates small person word neuter gender german even though refers female person lets look one famous examples courses nature short poem extracted lewis carrolls alice wonderland specifically second part book may seen havent see realize gibberish words valid english words yet possible speaker english understand least parts speech words even though cases tricky lets try together want give minute figure parts speech words boldface lets look answer previous question parts speech words boldface first word wabe borogoves likely nouns know well follow word english typically word noun next one little trickier brillig kind part speech well first guess adjective could also noun lets look examples could something like adjective like early noun noon either case dont know exactly part speech know one two mimsy well mimsys likely adjective way appears sentence slithey towes well either adjective noun combination small people also noun verb combination similar bell tolls finally expression mome rahs ougrabe could combination adjective noun verb could also combination noun verb adverb birds fly outside point cases guess part speech unknown word based context cases still tricky reason important example mimics computers human language see text really dont understand see seq sequence words dont enough prior knowledge language words wouldnt able understand anything case either use prior knowledge reason probabilistically may rule system says next word noun probability adjective next word noun probability combine probabilities sentences observe come best estimate consistent rules know text seen also important use context example word ambiguous maybe context see related words help disambiguate example use word bar sentence whole text talks legal issues lawyers people finishing law school likely use word bar legal sense rather establishment restauranttype sense also computers wrong make mistake propagate whole system example ignore negation assume opposite said actually true next category parts speech pronouns english include things like mine pronouns vary person first second third gender masculine feminine number case english actually cases pronouns includes nominative accusative possessive second possessive specifically could mine pronouns also reflexive anaphoric forms example say samantha gave haircut must refer different person whereas say samantha gave haircut gave haircut samantha reflexive pronoun categories parts speech english include determiners adjectives determiners things like articles demonstratives things like adjectives include words describe properties used either attributively predicatively small house attributive house small second example small used predicatively adjectives agree gender number different languages positive form short also comparative superlative form shorter shortest comparative superlative forms either derivative periphrastic derivative form like small smaller smallest periphrastic difficult difficult difficult next category parts speech verbs includes words describe actions throw activities walk states four verb forms english languages many tenses present past future different variants inflection exists english includes number person verbs also include gerunds ing forms infinitives thats form verb follows word verbs also distinguished aspect either progressive perfective depending whether action continue finally distinguished based voice sentence bought house verb bought active voice whereas house bought word bought passive tense passive voice sorry things verbs include things like participles ed forms example auxiliaries words like may shall verbs different arguments words come verb indicate modifications verb example dog sleeps instance intransitive verb theres direct object thats intransitive means dog chased cat example word chased transitive verb direct object finally sentence mary gave dog bone example ditransitive verb thats verb takes two objects gave somebody something also things like irregular verbs slept caught said far applies mostly english languages much richer inflections examples may familiar include languages like french latin different forms certain verb languages like finnish many different forms sometimes noun heres example much sophisticated inflectional paradigm verbs example french slide alone different forms verb go promise least two slides show even forms verb depending whether present past continuous form whether different mood conditional subjunctive conclude list parts speech parts speech english include adverbs things like happily describes manner describes location never describes time prepositions particles particles sometimes confused prepositions theres important test tell apart particles usually used form socalled phrasal verbs phrasal verb could something like take take special way taking completely different verb wanted take matter principal take phrasal verb two cases showed two fs particles part phrasal verbs theres test mentioned used distinguish prepositions particles common example people use ran bill versus ran hill first example ran bill ran phrasal verb whereas second example ran hill hill prepositional phrase associated ran way tell two apart simple move rest sentence beginning say ran hill hill ran sounds like valid paraphrase therefore instance preposition thing ran bill say bill ran means example run parts word parts verb rather cannot split case particle conju parts speech havent talked include coordinating conjunctions used connect similar parts sentence example apples oranges subordinating conjunctions used connect different portions sentence equal example entire sentence inserted preceding word say go home unless give money example unless introduces entire relative clause give money cannot switch order two parts sentence obtain meaning finally interjections things like sounds like meow ouch conclude section would like show labels use part speech tagging natural speech processing part speech tagging process automatically assigning parts speech words existing part speech taggers natural language parsers use convention nn shorthand singular noun np shorthand proper noun plural noun first letter code tells part speech whether n j n c r v second third letters tell something little bit detailed word example vb uninflected verb vbn en passive perfective form verb taken looked used past participle vbd stands verb used past tense took looked looked versus previous sentence looked used passive voice concludes section part speech tagging next section going morphology lexicon,Course3,W2-S1-L1,W2,S1,L1,-,2,1,1,next segment go differ part speech exist languag let look first exampl sentenc like nathali like black cat replac word black persian tabbi small still sentenc make sens nathali like tabbi cat nathali like persian cat well singl word use posit usual refer syntact categori case syntact categori part speech singl word adject mani syntact categori english languag includ open close categori open categori includ categori new word ad time exampl noflyzon noun twerk verb word ad vocabulari english rel recent past also close function categori respect determin articl preposit let look exampl sentenc dog chase yellow bird multipl part speech articl noun verb articl adject noun english eight gener type part speech look alreadi noun verb adject common noun includ thing like dog tree comput idea either concret like first three abstract vari number singular plural gender case english languag exampl latin word puer mean boy spell mani differ way singular one boy puer socal nomin case subject case puerum accus object pueri genit boy plural pueri nomin subject form puero accus final puerorum genit boy gender doesnt need match actual sex object describ typic exampl peopl give word german madchen word madchen mean girl diminut chen part indic small person word neuter gender german even though refer femal person let look one famou exampl cours natur short poem extract lewi carrol alic wonderland specif second part book may seen havent see realiz gibberish word valid english word yet possibl speaker english understand least part speech word even though case tricki let tri togeth want give minut figur part speech word boldfac let look answer previou question part speech word boldfac first word wabe borogov like noun know well follow word english typic word noun next one littl trickier brillig kind part speech well first guess adject could also noun let look exampl could someth like adject like earli noun noon either case dont know exactli part speech know one two mimsi well mimsi like adject way appear sentenc slithey tow well either adject noun combin small peopl also noun verb combin similar bell toll final express mome rah ougrab could combin adject noun verb could also combin noun verb adverb bird fli outsid point case guess part speech unknown word base context case still tricki reason import exampl mimic comput human languag see text realli dont understand see seq sequenc word dont enough prior knowledg languag word wouldnt abl understand anyth case either use prior knowledg reason probabilist may rule system say next word noun probabl adject next word noun probabl combin probabl sentenc observ come best estim consist rule know text seen also import use context exampl word ambigu mayb context see relat word help disambigu exampl use word bar sentenc whole text talk legal issu lawyer peopl finish law school like use word bar legal sens rather establish restauranttyp sens also comput wrong make mistak propag whole system exampl ignor negat assum opposit said actual true next categori part speech pronoun english includ thing like mine pronoun vari person first second third gender masculin feminin number case english actual case pronoun includ nomin accus possess second possess specif could mine pronoun also reflex anaphor form exampl say samantha gave haircut must refer differ person wherea say samantha gave haircut gave haircut samantha reflex pronoun categori part speech english includ determin adject determin thing like articl demonstr thing like adject includ word describ properti use either attribut predic small hous attribut hous small second exampl small use predic adject agre gender number differ languag posit form short also compar superl form shorter shortest compar superl form either deriv periphrast deriv form like small smaller smallest periphrast difficult difficult difficult next categori part speech verb includ word describ action throw activ walk state four verb form english languag mani tens present past futur differ variant inflect exist english includ number person verb also includ gerund ing form infinit that form verb follow word verb also distinguish aspect either progress perfect depend whether action continu final distinguish base voic sentenc bought hous verb bought activ voic wherea hous bought word bought passiv tens passiv voic sorri thing verb includ thing like participl ed form exampl auxiliari word like may shall verb differ argument word come verb indic modif verb exampl dog sleep instanc intransit verb there direct object that intransit mean dog chase cat exampl word chase transit verb direct object final sentenc mari gave dog bone exampl ditransit verb that verb take two object gave somebodi someth also thing like irregular verb slept caught said far appli mostli english languag much richer inflect exampl may familiar includ languag like french latin differ form certain verb languag like finnish mani differ form sometim noun here exampl much sophist inflect paradigm verb exampl french slide alon differ form verb go promis least two slide show even form verb depend whether present past continu form whether differ mood condit subjunct conclud list part speech part speech english includ adverb thing like happili describ manner describ locat never describ time preposit particl particl sometim confus preposit there import test tell apart particl usual use form socal phrasal verb phrasal verb could someth like take take special way take complet differ verb want take matter princip take phrasal verb two case show two fs particl part phrasal verb there test mention use distinguish preposit particl common exampl peopl use ran bill versu ran hill first exampl ran bill ran phrasal verb wherea second exampl ran hill hill preposit phrase associ ran way tell two apart simpl move rest sentenc begin say ran hill hill ran sound like valid paraphras therefor instanc preposit thing ran bill say bill ran mean exampl run part word part verb rather cannot split case particl conju part speech havent talk includ coordin conjunct use connect similar part sentenc exampl appl orang subordin conjunct use connect differ portion sentenc equal exampl entir sentenc insert preced word say go home unless give money exampl unless introduc entir rel claus give money cannot switch order two part sentenc obtain mean final interject thing like sound like meow ouch conclud section would like show label use part speech tag natur speech process part speech tag process automat assign part speech word exist part speech tagger natur languag parser use convent nn shorthand singular noun np shorthand proper noun plural noun first letter code tell part speech whether n j n c r v second third letter tell someth littl bit detail word exampl vb uninflect verb vbn en passiv perfect form verb taken look use past participl vbd stand verb use past tens took look look versu previou sentenc look use passiv voic conclud section part speech tag next section go morpholog lexicon,[ 4  0 14 13 12]
251,Course3_W2-S1-L2_-_Morphology_and_the_Lexicon_-_20_slides_21-02,new segment morphology lexicon people learn new words store known linguistics psychology mental lexicon used store meaning pronunciation part speech among things tell store word like cat know little furry creature says meow know pronounced cat cuh tuh know noun sometimes may know information yet still able store information mental lexicon example see word wug first time may part speech could probably figure pronunciation probably dont know meaning told wug noun means small creature use sentence would know part speech meaning say example things like saw wug today wug walked similarly word like cluvious even dont know meaning may able infer information words english form adjectives even reason words havent seen compare traftful traftless figure theyre likely opposites many pairs words english particular endings autonomy relationship one things interested today interpret words morphologically take word like cluvious figure ending think adjective people intuition grammar born helps understand words seen also property called productivity allows create new words different things lets look words store mental lexicon word like runs clearly two possible meanings english one cases noun plural second case verb third person singular clearly part speech changes meaning changes pronunciation doesnt seem change particular case ambiguous way people store ambiguous words mental lexicon disambiguate fly pronounce read sentences also special cases words stored known allomorphs example cats derivative cat means plural singular cat special cases like oxen formed irregularly plural ox past tense play played one past tenses swing swung formed irregularly words stored lexicon somehow may obtainable directly following morphological derivation like cats words lot affixes include things like prefixes go front stem suffixes endings one subjects morphology called derivation morphology understanding different forms words created words adding different affixes look word drinkable say whole word drinkable adjective remember jj standard label natural language processing adjectives also know drinkable formed converting verb drink adjective adding suffix able represent whole word drinkable sort tree verb drink first suffix able two words combined form new word drinkable many cases english suffixes change part speech word er one example example sleep verb sleeper multiple meanings noun derived sleep adding suffix er many cases infer meaning morphemes example morpheme able means capable verb suffix er may person certain activity lets try morphemes ness example used form nouns example sleepy adjective sleepiness noun derived sleepy affixe ness turns adjective noun able already saw turn verb adjective also similarly consider meaning prefixes suffixes ing example usually used beginning word means repeat something un means negation example undo finally er adjectives means comparative form adjective example showed draw diagram tells us jj transformed sequence verb followed suffix able turns morphemic rules used recursively long word like unconcernednesses really formed concerned adding first suffix ed another suffix ness negating finally adding extra es end plural cases easy look complicated word figure derived cases may ambiguities example word undoable see next slide two different morphological interpretations whereas word like unbelievable may one think case lets look examples previous slides undoable formed unable done something undoable something able undone case would analyzed undoable doesnt apply unbelievable one interpretations undoable something unable believed second interpretation able unbelieved sounds unnatural case dont ambiguity lets look morphological examples different languages rules arent simple ones english adding plural example language pangasinan duplication morphing amigo friend amimigo plural friends case reduplication morphing middle word similarly samoan word savali means travels savavali plural travel circumfixes example german word spielen infinitive verb play gespielt past form form case circumfix part morphing goes beginning word ge part part goes end case form even look artificial phenomena example language kids familiar pig latin pig latin take first syllable word move end happy get something like appyhay fun languages like countries example verlan french slang language word verlan comes lenvers french means reverse reverse syllables lenvers get verlan includes words like cefran verlan form francais switching two syllables well ripou switched version pourri means crook let give example english may heard expression going im going massafreakinchusetts insert phrase like middle word insert randomly example massachusetts wouldnt able say mafreakinsachusetts massachufreakinsetts clearly freakin inserted random place would insert example word education could say educafreakintion doesnt sound natural okay probably say edufreakincation rule actually freakin infix inserted left syllable bears main stress said earlier say edufreakincation cannot say educafreakintion efreakinducation exceptions rule applies cases mentioned morphemes word individual units morphological meaning include stems affixes words affixes prefixes suffixes endings theres languages concatenative morphology like english example affixes added beginning end word also cases templatic morphology well example semitic languages heres example hebrew template learn lmd sequence three consonants want conjugate inflect get different forms learn example lamad studied limed taught lumad means taught inserting different sets vowels different places modify tense template form different words lets look inflectional morphology one main areas morphology words different inflection based tense present past future number first second third example person english thats singular plural languages persons example duo used two people moods things like indicative subjunctive conditional aspect example progressive perfective english words five verb forms example word however different forms french six cases russian look website listed examples could many forms word turkish example special form means cause x cause dot dot dot z recursive set rules create really long words morphological analysis area natural language processing takes word converts morphological representation related stemming stem work also gives part speech morphological information associated word example word sleeps translated infinitive sleep label verb label third person label singular word done translated infinitive verb followed label verb v pp case stands past participle let show interesting phenomena related morphology different languages first part example turkish vowel harmony mentioned vowels grouped front back high low english common distinction vowels turkish theres additional distinction rounded unrounded vowels every time vowel formed putting mouth unrounded position round lips like change sound different vowel example ee oo turkish eight vowels total form possible combinations front back high low rounded unrounded vowels back vowels formed back mouth front vowels formed front mouth vowel harmony principle says within word back vowels front vowels languages property addition turkish example hungarian lets look example suffix da de turkish used indicate location use da rest vowels word back vowels use de rest vowels word front vowels lets look examples room odada word oda means room back vowels therefore add da end another back vowel kapi door kapida door examples front vowels ev home home evde gol lake golde lake finally kopru bridge koprude bridge fact want point naclo problem talks turkish vowel harmony interesting phenomena access website youre done solving look solution available lets look another example turkish shows complicated morphology different languages slide courtesy kemal oflazer qatar shows one take english sentence entire sentence convert single english word following morphological rules turkish heres word turkish one listed bottom means able make something dot dot dot becomes strong first step start sentence label different colors different modifications strong example future tense possibility able property becoming something reorder labels get order turkish strong stem pieces form morphological inflections become first adds las make something tir able abil ecek se finally goes end final affix ku gives us long word bottom heres example japanese shows japanese imported lot words english use katakana one main alphabets japanese represent different sounds different morphological combinations come english example right middle slide foreign stands coffee see f sound changed sound macrons bars top two vowels indicate syllables long syllables lets look one example example resutoran stands restaurant reason second syllable vowel resutoran japanese grammar kinds syllables allowed ones end vowel one exception syllables end vowel followed letter n thats insert u middle restaurant going move levels linguistic analysis looked morphology lets see comes next one levels semantics semantics study meaning words sentences split lexical semantics compositional semantics lexical semantics meaning individual words relationships meanings pairs words includes specific lexical relationships synonyms also hypernyms hyponyms special terms general specific concepts antonyms going look detail one future lectures lexical semantics also deals senses words example words multiple senses known polysemous words also talks colocation colocation sequence words appear together frequently would expect combine probabilities individual words example stock market appears frequently stock market multiplied also idioms specific collocations meaning literally inferable components example kick bucket expression english means die obviously doesnt mean kick particular bucket addition lexical semantics compositional semantics deals understanding meaning sentence based meaning components talk lexical semantics compositional semantics later next level studying linguistics called pragmatics study knowledge world language conventions interact literal meaning covers things like speech acts resolution anaphoric relations example pronouns covers modelling speech acts dialogue later semester going look pragmatics detail many areas linguistics directly related natural language processing well list briefly social linguistics interaction social organization language historical linguistics studies change languages time linguistic typology looks languages related common properties language acquisition first second language finally psycholinguistics study people produce perceive language real time,Course3,W2-S1-L2,W2,S1,L2,-,2,1,2,new segment morpholog lexicon peopl learn new word store known linguist psycholog mental lexicon use store mean pronunci part speech among thing tell store word like cat know littl furri creatur say meow know pronounc cat cuh tuh know noun sometim may know inform yet still abl store inform mental lexicon exampl see word wug first time may part speech could probabl figur pronunci probabl dont know mean told wug noun mean small creatur use sentenc would know part speech mean say exampl thing like saw wug today wug walk similarli word like cluviou even dont know mean may abl infer inform word english form adject even reason word havent seen compar traft traftless figur theyr like opposit mani pair word english particular end autonomi relationship one thing interest today interpret word morpholog take word like cluviou figur end think adject peopl intuit grammar born help understand word seen also properti call product allow creat new word differ thing let look word store mental lexicon word like run clearli two possibl mean english one case noun plural second case verb third person singular clearli part speech chang mean chang pronunci doesnt seem chang particular case ambigu way peopl store ambigu word mental lexicon disambigu fli pronounc read sentenc also special case word store known allomorph exampl cat deriv cat mean plural singular cat special case like oxen form irregularli plural ox past tens play play one past tens swing swung form irregularli word store lexicon somehow may obtain directli follow morpholog deriv like cat word lot affix includ thing like prefix go front stem suffix end one subject morpholog call deriv morpholog understand differ form word creat word ad differ affix look word drinkabl say whole word drinkabl adject rememb jj standard label natur languag process adject also know drinkabl form convert verb drink adject ad suffix abl repres whole word drinkabl sort tree verb drink first suffix abl two word combin form new word drinkabl mani case english suffix chang part speech word er one exampl exampl sleep verb sleeper multipl mean noun deriv sleep ad suffix er mani case infer mean morphem exampl morphem abl mean capabl verb suffix er may person certain activ let tri morphem ness exampl use form noun exampl sleepi adject sleepi noun deriv sleepi affix ness turn adject noun abl alreadi saw turn verb adject also similarli consid mean prefix suffix ing exampl usual use begin word mean repeat someth un mean negat exampl undo final er adject mean compar form adject exampl show draw diagram tell us jj transform sequenc verb follow suffix abl turn morphem rule use recurs long word like unconcerned realli form concern ad first suffix ed anoth suffix ness negat final ad extra es end plural case easi look complic word figur deriv case may ambigu exampl word undoabl see next slide two differ morpholog interpret wherea word like unbeliev may one think case let look exampl previou slide undoabl form unabl done someth undoabl someth abl undon case would analyz undoabl doesnt appli unbeliev one interpret undoabl someth unabl believ second interpret abl unbeliev sound unnatur case dont ambigu let look morpholog exampl differ languag rule arent simpl one english ad plural exampl languag pangasinan duplic morph amigo friend amimigo plural friend case redupl morph middl word similarli samoan word savali mean travel savavali plural travel circumfix exampl german word spielen infinit verb play gespielt past form form case circumfix part morph goe begin word ge part part goe end case form even look artifici phenomena exampl languag kid familiar pig latin pig latin take first syllabl word move end happi get someth like appyhay fun languag like countri exampl verlan french slang languag word verlan come lenver french mean revers revers syllabl lenver get verlan includ word like cefran verlan form francai switch two syllabl well ripou switch version pourri mean crook let give exampl english may heard express go im go massafreakinchusett insert phrase like middl word insert randomli exampl massachusett wouldnt abl say mafreakinsachusett massachufreakinsett clearli freakin insert random place would insert exampl word educ could say educafreakint doesnt sound natur okay probabl say edufreakinc rule actual freakin infix insert left syllabl bear main stress said earlier say edufreakinc cannot say educafreakint efreakinduc except rule appli case mention morphem word individu unit morpholog mean includ stem affix word affix prefix suffix end there languag concaten morpholog like english exampl affix ad begin end word also case templat morpholog well exampl semit languag here exampl hebrew templat learn lmd sequenc three conson want conjug inflect get differ form learn exampl lamad studi lime taught lumad mean taught insert differ set vowel differ place modifi tens templat form differ word let look inflect morpholog one main area morpholog word differ inflect base tens present past futur number first second third exampl person english that singular plural languag person exampl duo use two peopl mood thing like indic subjunct condit aspect exampl progress perfect english word five verb form exampl word howev differ form french six case russian look websit list exampl could mani form word turkish exampl special form mean caus x caus dot dot dot z recurs set rule creat realli long word morpholog analysi area natur languag process take word convert morpholog represent relat stem stem work also give part speech morpholog inform associ word exampl word sleep translat infinit sleep label verb label third person label singular word done translat infinit verb follow label verb v pp case stand past participl let show interest phenomena relat morpholog differ languag first part exampl turkish vowel harmoni mention vowel group front back high low english common distinct vowel turkish there addit distinct round unround vowel everi time vowel form put mouth unround posit round lip like chang sound differ vowel exampl ee oo turkish eight vowel total form possibl combin front back high low round unround vowel back vowel form back mouth front vowel form front mouth vowel harmoni principl say within word back vowel front vowel languag properti addit turkish exampl hungarian let look exampl suffix da de turkish use indic locat use da rest vowel word back vowel use de rest vowel word front vowel let look exampl room odada word oda mean room back vowel therefor add da end anoth back vowel kapi door kapida door exampl front vowel ev home home evd gol lake gold lake final kopru bridg koprud bridg fact want point naclo problem talk turkish vowel harmoni interest phenomena access websit your done solv look solut avail let look anoth exampl turkish show complic morpholog differ languag slide courtesi kemal oflaz qatar show one take english sentenc entir sentenc convert singl english word follow morpholog rule turkish here word turkish one list bottom mean abl make someth dot dot dot becom strong first step start sentenc label differ color differ modif strong exampl futur tens possibl abl properti becom someth reorder label get order turkish strong stem piec form morpholog inflect becom first add la make someth tir abl abil ecek se final goe end final affix ku give us long word bottom here exampl japanes show japanes import lot word english use katakana one main alphabet japanes repres differ sound differ morpholog combin come english exampl right middl slide foreign stand coffe see f sound chang sound macron bar top two vowel indic syllabl long syllabl let look one exampl exampl resutoran stand restaur reason second syllabl vowel resutoran japanes grammar kind syllabl allow one end vowel one except syllabl end vowel follow letter n that insert u middl restaur go move level linguist analysi look morpholog let see come next one level semant semant studi mean word sentenc split lexic semant composit semant lexic semant mean individu word relationship mean pair word includ specif lexic relationship synonym also hypernym hyponym special term gener specif concept antonym go look detail one futur lectur lexic semant also deal sens word exampl word multipl sens known polysem word also talk coloc coloc sequenc word appear togeth frequent would expect combin probabl individu word exampl stock market appear frequent stock market multipli also idiom specif colloc mean liter infer compon exampl kick bucket express english mean die obvious doesnt mean kick particular bucket addit lexic semant composit semant deal understand mean sentenc base mean compon talk lexic semant composit semant later next level studi linguist call pragmat studi knowledg world languag convent interact liter mean cover thing like speech act resolut anaphor relat exampl pronoun cover model speech act dialogu later semest go look pragmat detail mani area linguist directli relat natur languag process well list briefli social linguist interact social organ languag histor linguist studi chang languag time linguist typolog look languag relat common properti languag acquisit first second languag final psycholinguist studi peopl produc perceiv languag real time,[ 4  0 14 13 12]
252,Course3_W2-S1-L3_-_Text_Similarity-_Introduction_-_8_slides_07-27,segment going text similarity text similarity one important applications linguistics statistics natural language processing helps many different applications many ways people express concept related concepts example could say plane leaves pm also say flight departs noon except words like rest words sentences different yet express exact meaning mentioned earlier text similarity one key components natural language processing example information table task user looking information cats may want system return documents mention word kittens word cat document may words common query still related cat kitten similar words another example user looking information fruit dessert may want nlp system return documents peach tarts apple cobblers examples specific fruit specific dessert items special cases fruit dessert another example speech recognition system example want fly dulles airport sometimes system may hear incorrectly figure want go dallas may book wrong flight however system knows advance dulles dallas sound similar may tweak algorithm picks one two may ask confirm didnt mean one doesnt need names cities similar example ask dulles never going ask really mean san francisco segment lecture im going teach text similarity modeled computationally lets start first human judgments similarity im showing example paper finkelstein et al ask people determine similar two words give words like tiger tiger obviously got maximum similarity score human judges case give tiger cat got similarity average book paper similarity range computer keyboard also examples plane car got similarity cucumber potato similarity one interesting thing variance scores actually pretty high clearly much user agreement whether certain two words similar less similar still agreed generally overall level similarity one example recent paper published felix hill et al much larger data set words kinds parts speech including adjectives verbs nouns adverbs example figure delightful wonderful similarly similarity whereas modest flexible similar similarity look examples slide talk one argue persuade moderately related similarity versus pursue persuade much lower similarity kind data set used train natural language systems also used evaluate systems automatically compute text similarity one recent example mikolov et al paper published uses wordvec approach im going talk later able compute automatically words similar france based context appear words shown table surprisingly words similar france countries near france geographically spain belgium netherlands italy switzerland let describe different kinds text similarity exists first kind morphological similarity two words respect respectful stem additional morphological change particular example word respectful adjective derived respect suffix ful tells us adjective two words morphologically similar share pretty much meaning next example spelling similarity useful example dealing different versions dialects english example british american english word theater spelled either er end want system understand pretty much word look similar follow specific pattern changes appears across languages synonymy two words similar meaning rare two words exactly meaning usually enough close enough considered synonyms talkative chatty synonyms another category similarity text homophony thats multiple words possibly different meanings pronunciation raise raise raze raze also rays rays pronounced way also different kinds semantic similarity example cat tabby semantically related word tabby usually used refer specific kind cat specific color cat also similarities among sentences example two sentences may paraphrase also similarity level documents example two news stories reported independently event often similar content would also like add additional example similarity namely crosslingual similarity example word japan japanese nihon sometimes name organization may translated nihon japan depending translation want able identify refer country next segment going talk specifically morphological similarity stemming,Course3,W2-S1-L3,W2,S1,L3,-,2,1,3,segment go text similar text similar one import applic linguist statist natur languag process help mani differ applic mani way peopl express concept relat concept exampl could say plane leav pm also say flight depart noon except word like rest word sentenc differ yet express exact mean mention earlier text similar one key compon natur languag process exampl inform tabl task user look inform cat may want system return document mention word kitten word cat document may word common queri still relat cat kitten similar word anoth exampl user look inform fruit dessert may want nlp system return document peach tart appl cobbler exampl specif fruit specif dessert item special case fruit dessert anoth exampl speech recognit system exampl want fli dull airport sometim system may hear incorrectli figur want go dalla may book wrong flight howev system know advanc dull dalla sound similar may tweak algorithm pick one two may ask confirm didnt mean one doesnt need name citi similar exampl ask dull never go ask realli mean san francisco segment lectur im go teach text similar model comput let start first human judgment similar im show exampl paper finkelstein et al ask peopl determin similar two word give word like tiger tiger obvious got maximum similar score human judg case give tiger cat got similar averag book paper similar rang comput keyboard also exampl plane car got similar cucumb potato similar one interest thing varianc score actual pretti high clearli much user agreement whether certain two word similar less similar still agre gener overal level similar one exampl recent paper publish felix hill et al much larger data set word kind part speech includ adject verb noun adverb exampl figur delight wonder similarli similar wherea modest flexibl similar similar look exampl slide talk one argu persuad moder relat similar versu pursu persuad much lower similar kind data set use train natur languag system also use evalu system automat comput text similar one recent exampl mikolov et al paper publish use wordvec approach im go talk later abl comput automat word similar franc base context appear word shown tabl surprisingli word similar franc countri near franc geograph spain belgium netherland itali switzerland let describ differ kind text similar exist first kind morpholog similar two word respect respect stem addit morpholog chang particular exampl word respect adject deriv respect suffix ful tell us adject two word morpholog similar share pretti much mean next exampl spell similar use exampl deal differ version dialect english exampl british american english word theater spell either er end want system understand pretti much word look similar follow specif pattern chang appear across languag synonymi two word similar mean rare two word exactli mean usual enough close enough consid synonym talk chatti synonym anoth categori similar text homophoni that multipl word possibl differ mean pronunci rais rais raze raze also ray ray pronounc way also differ kind semant similar exampl cat tabbi semant relat word tabbi usual use refer specif kind cat specif color cat also similar among sentenc exampl two sentenc may paraphras also similar level document exampl two news stori report independ event often similar content would also like add addit exampl similar name crosslingu similar exampl word japan japanes nihon sometim name organ may translat nihon japan depend translat want abl identifi refer countri next segment go talk specif morpholog similar stem,[ 4  2  8 13 14]
253,Course3_W2-S1-L4_-_Morphological_Similarity-_Stemming_-_19_slides_15-05,next segment going specific type text similarity logical similarity want able identify two words morphologically related typically done process named stemming words root usually similar meanings example word scan base form also converted scans scanned scanning different forms inflection first one could plural third person singular verb second one past tense third one gerund also add suffix ending like er forms derived word noun particular case also derived forms use prefixes example verb rescan means scan something second time combinations derived inflected forms rescanned prefix evasion suffix inflection process stemming take word convert base form known stem removing different suffixes endings sometimes performing additional transformation practice prefixes often preserved rescan stemmed scan lets look examples want convert past tense scan base form scan also want convert noun indication base form verb indicated way want find similarity two sentences would consider scanned scan similar two random words indicate indication similar words method used lot natural language communitys called porters method introduced martin porter paper called algorithm suffix stripping cited times according google scholar pretty fundamental method porters method rule based doesnt use machine learning training works english rules generated manually input algorithm individual word indicates output stem work obtained performing series transformations original word one caveat porter stemmer sometimes wrong wellknown cases always fixed post processing lets look examples porters algorithm first example starts computation produces output word compute may say compute spelled compute word correct stem word computational want able take word computer wed use stem comput comput end day computational computer stamped exactly way similarity stamped space perfect one important aspects porters algorithm concept measure word measure word string indication rough indication number syllables doesnt measure exactly number syllables approximates number lets take simple word like cat going think sequence vowels consonants first c consonant second vowel third consonant well c v c sequence instead word cats would still count cvc sequence porters algorithm number adjacent consonants doesnt matter number adjacent vowels doesnt matter either measure word tells many sequences vcs representation look last bullet see word represented optional sequence consonants c followed number vc sequences followed optional v words abbreviate middle portion vcvc vc suffix k tells vcs repeated k times lets look examples measures first line shows five examples k equal zero see example word wrong sequence consonant followed vowel vowel followed consonant therefore doesnt measure one measure zero thing applies glee sequence consonants followed sequence vowels follows pattern k equals zero next line k vowel followed consonant matches pattern previous slide east sequence vowels followed sequence consonants matches pattern well word street starts optional consonant sequence vowels sequence consonants also according definition previous slide match k finally long words k two three larger example word easternmost measure three vowel sequence ea followed consonant sequence st thats first part measure e followed rnm thats second part finally followed st third part porters algorithm starts taking word checking sequence transformation patterns order words sort decision list rules ordered start beginning first rule left hand side matches current representation work going applied certain transformation going made current work example role form measure word greater zero ation conflated ate would take account transformation medication medicate dedication dedicate however going convert nation nate measure nation greater zero know measure measured portion word listed pattern nation using n followed ation want part left hand side n measure greater zero case n measure zero pattern matches word transformed algorithm restarts beginning list patterns newly transformed word repeated rules match whole sequence words point algorithm stops outputs recently transformed version word let us look complicated example first four lines slide show rules belong step porters algorithm example sses gets transformed ss end word ies gets transformed ask ies get transformed example example word weekly plural weeklies ies want two forms weekly singular weeklies plural stand way conversion li takes account would convert ies separate rule shown also applies words would stand weekly weekli step includes rule form measure greater performs certain type transformation refereed changes referee measure refer greater zero rule doesnt apply bleed measure bl zero rules step ational become ate example inflational turns inflate ational doesnt match tional matches gets converted tion see order rules really matters rule tional first ational obviously second one would never apply especially taking care special case ational word doesnt match pattern go next pattern tional able look rest examples slide figure kind words apply ivness gets converted ive example forgiveness turn forgive attentiveness attentive step includes rules lets look examples icate turns ic replicate becomes replic ative becomes blank thats symbol means example informative gets turned inform step four endings example al end word typically indicates specific type noun adjective skipped appraisal becomes apprais ance conductance also skipped turns conductance conduct next rule er end words sufficient length measure greater one applies remove suffix er altogether container changed contain lets look long word well need go list rules multiple times gets stemmed completely word computational going start first row list well find first matching pattern part step two tells us replace ational ate going change computational computate even word dont care still done stemming process instead want go back beginning list rows look row matches computate well row measure longer certain number replace final ate blank get comput comput computate point go back beginning list rules realize rules apply stop compute stem second example simpler word computer matches specific role part step four porter stemmer allows us drop final er get comput comput computation really lesson learn wanted two words stand exact representation porters algorithm make sure youre interested porters algorithm detail suggest go website top online demo algorithm type text get output also read original paper martin porter download implementations porters algorithm many different programming languages python c java one additional link showing natural language toolkit mentioned elsewhere also contains implementation porter algorithm lets short quiz want give four words construction increasing unexplained differentiable want take minute look original porter paper possibly code stemmer figure output either running code tracing manually look output get think whether expected try think im going give answer next slide question find porter stemmer stems words construction increasing unexplained differentiable well heres converts construction construct good stem increasing increas without e good stem final e verb increase part stem unexplained becomes unexplain may different would expect may expect would get explain stem mentioned earlier porters algorithm explicitly take account prefixes removes characters end word finally differentiable turns differenti probably standard youd expected would classify example one places borders algorithm falls short problem naclo eric breck stemming minute please download problem try solve home solution thorny stems problem eric breck available url tried solve check solution next segment going talk another type lexical similarity specifically concept edit distance dynamic programming methods used compute edit distance,Course3,W2-S1-L4,W2,S1,L4,-,2,1,4,next segment go specif type text similar logic similar want abl identifi two word morpholog relat typic done process name stem word root usual similar mean exampl word scan base form also convert scan scan scan differ form inflect first one could plural third person singular verb second one past tens third one gerund also add suffix end like er form deriv word noun particular case also deriv form use prefix exampl verb rescan mean scan someth second time combin deriv inflect form rescan prefix evas suffix inflect process stem take word convert base form known stem remov differ suffix end sometim perform addit transform practic prefix often preserv rescan stem scan let look exampl want convert past tens scan base form scan also want convert noun indic base form verb indic way want find similar two sentenc would consid scan scan similar two random word indic indic similar word method use lot natur languag commun call porter method introduc martin porter paper call algorithm suffix strip cite time accord googl scholar pretti fundament method porter method rule base doesnt use machin learn train work english rule gener manual input algorithm individu word indic output stem work obtain perform seri transform origin word one caveat porter stemmer sometim wrong wellknown case alway fix post process let look exampl porter algorithm first exampl start comput produc output word comput may say comput spell comput word correct stem word comput want abl take word comput wed use stem comput comput end day comput comput stamp exactli way similar stamp space perfect one import aspect porter algorithm concept measur word measur word string indic rough indic number syllabl doesnt measur exactli number syllabl approxim number let take simpl word like cat go think sequenc vowel conson first c conson second vowel third conson well c v c sequenc instead word cat would still count cvc sequenc porter algorithm number adjac conson doesnt matter number adjac vowel doesnt matter either measur word tell mani sequenc vc represent look last bullet see word repres option sequenc conson c follow number vc sequenc follow option v word abbrevi middl portion vcvc vc suffix k tell vc repeat k time let look exampl measur first line show five exampl k equal zero see exampl word wrong sequenc conson follow vowel vowel follow conson therefor doesnt measur one measur zero thing appli glee sequenc conson follow sequenc vowel follow pattern k equal zero next line k vowel follow conson match pattern previou slide east sequenc vowel follow sequenc conson match pattern well word street start option conson sequenc vowel sequenc conson also accord definit previou slide match k final long word k two three larger exampl word easternmost measur three vowel sequenc ea follow conson sequenc st that first part measur e follow rnm that second part final follow st third part porter algorithm start take word check sequenc transform pattern order word sort decis list rule order start begin first rule left hand side match current represent work go appli certain transform go made current work exampl role form measur word greater zero ation conflat ate would take account transform medic medic dedic dedic howev go convert nation nate measur nation greater zero know measur measur portion word list pattern nation use n follow ation want part left hand side n measur greater zero case n measur zero pattern match word transform algorithm restart begin list pattern newli transform word repeat rule match whole sequenc word point algorithm stop output recent transform version word let us look complic exampl first four line slide show rule belong step porter algorithm exampl ss get transform ss end word i get transform ask i get transform exampl exampl word weekli plural weekli i want two form weekli singular weekli plural stand way convers li take account would convert i separ rule shown also appli word would stand weekli weekli step includ rule form measur greater perform certain type transform refere chang refere measur refer greater zero rule doesnt appli bleed measur bl zero rule step ation becom ate exampl inflat turn inflat ation doesnt match tional match get convert tion see order rule realli matter rule tional first ation obvious second one would never appli especi take care special case ation word doesnt match pattern go next pattern tional abl look rest exampl slide figur kind word appli iv get convert ive exampl forgiv turn forgiv attent attent step includ rule let look exampl icat turn ic replic becom replic ativ becom blank that symbol mean exampl inform get turn inform step four end exampl al end word typic indic specif type noun adject skip apprais becom apprai anc conduct also skip turn conduct conduct next rule er end word suffici length measur greater one appli remov suffix er altogeth contain chang contain let look long word well need go list rule multipl time get stem complet word comput go start first row list well find first match pattern part step two tell us replac ation ate go chang comput comput even word dont care still done stem process instead want go back begin list row look row match comput well row measur longer certain number replac final ate blank get comput comput comput point go back begin list rule realiz rule appli stop comput stem second exampl simpler word comput match specif role part step four porter stemmer allow us drop final er get comput comput comput realli lesson learn want two word stand exact represent porter algorithm make sure your interest porter algorithm detail suggest go websit top onlin demo algorithm type text get output also read origin paper martin porter download implement porter algorithm mani differ program languag python c java one addit link show natur languag toolkit mention elsewher also contain implement porter algorithm let short quiz want give four word construct increas unexplain differenti want take minut look origin porter paper possibl code stemmer figur output either run code trace manual look output get think whether expect tri think im go give answer next slide question find porter stemmer stem word construct increas unexplain differenti well here convert construct construct good stem increas increa without e good stem final e verb increas part stem unexplain becom unexplain may differ would expect may expect would get explain stem mention earlier porter algorithm explicitli take account prefix remov charact end word final differenti turn differenti probabl standard youd expect would classifi exampl one place border algorithm fall short problem naclo eric breck stem minut pleas download problem tri solv home solut thorni stem problem eric breck avail url tri solv check solut next segment go talk anoth type lexic similar specif concept edit distanc dynam program method use comput edit distanc,[ 4  0 14 13 12]
254,Course3_W2-S1-L5_-_Spelling_Similarity-_Edit_Distance_-_32_slides_21-23,recently talked one type text similarity specifically one computed stemming words going look different kind spelling similarity added distance often people make mistakes go search engines example may want looking pictures britney spears type name brittany spears often spell name like katherine hepburn incorrectly spell word receipt e even though well aware rule use one way also variances spelling mentioned theater spelled er want able capture similarity spellings user types brittany spears able know meant britney spears two spellings similar enough return documents let share little funny example different language want exposed examples many languages possible know arabic probably read without problem cannot want give hints want ask guess successive hints behalf person arabic spelled right left symbol marked pronounced sound one pronounced sound f combination symbols pronounced sound al theres reason im showing example figure person may well muammar gaddafi former leader libya deposed killed years ago interesting example common conventions transliterate certain words like name rendered media muammar muammar moamer one instead u word gaddafi last name rendered g kh one two ds middle al omitted without changing meaning name results variety different transliterations first name spelled following set ways followed either u followed followed one two ms followed e followed r middle box shows different options al could either lowercase el uppercase el al uppercase al blank third box shows spelling last name gives us total eight times five time total different combinations used media seen different ways spelling name want find documents contain references person take transliteration issues account examples showed far gaddafi theater britney spears may realize changes different versions word random arbitrary typically following form insertions deletions example word behaviour british english spelled iour end american english spelled ior start british version want convert american version would deletion would go way around go american english british english also applies word al al qaddafi type operation common editing substitution string spring similar words one letter difference want transform string spring would one edit operation specifically one substitution qaddafi example would swap k q want convert sleep slept would need multiple edits think minimum number edits convert sleep slept allowed insertions deletions substitutions one possible way convert sleep slept drop one es insert thats two operations even possibly replace second e sleep p replace p thats still two operations sense second one less likely make sense inserting p one place deleting p another place one specific type operation shown swap adjacent letters im going talk minutes mean time lets stick insertions deletions substitutions single characters method used compute added distance two words two strings often referred levenshtein method names well based dynamic programming starts computing added distance two empty strings recursively computes added distance longer longer substrings original words uses costs one insertions deletions substitutions length one lets look example picked two words trend strength want show levenshtein method going compute similarity two way works create two dimensional chart four empty boxes upper left corner start spelling one words across starting cell strength located going right spelling second word first column position three second row second column numbered zero length words first case zero eight second case zero five levenshteins method works like going first compute added distance leftmost substring strength specifically letter leftmost one character substring trend namely two letters going added distance dont match added distance zero going compute value cells table making sure previously computed cell immediately immediately left immediately left one want compute point compute either distance cell three labels already computed example arent really anything compute distance recurrence relation used levenshtein method uses following definitions si ith character string sj jth character string dij edit distance prefix length prefix length j tij cost aligning ith character string jth character string lets see computed right hand side slide youll see bases cases di essentially cost aligning string length string length zero equal need insertions turn empty string string length symmetry edit distance empty string string length j equal j need j definitions convert string length j string length zero finally recursive case compute edit distance substring si substring sj need compute values cell cell left cell left figure three expressions shown min gives us smallest value pick one new edit distance sense start cell left add one start cell add one would correspond insertion deletion start cell left add either one zero depending whether ith character string matches jth character string edit distance bottom part right hand side tells tij ith character si equal jth character equal otherwise would give us either match cost zero substitution agreed earlier would cost one lets look example compute edit distance said would either one plus one two one left plus one also two one diagonally left zero plus cost substituting three expressions two two one minimum one one therefore going produce new value one new cell continue filling table recursively edit distance well either add one two would give us three add one one left gives us two start diagonally left add edit distance character therefore edit distance zero summarize smallest possible value cell computing right one rest table point time keep track path cell used compute smallest distance lets look one example computed table far want compute edit distance r r minimum two plus one two plus one one plus zero rs strings particular locations choice numbers three three one clearly going pick one new value end day filled entire table number bottom righthand corner going give us edit difference two words case four strength converted trend minimum four edit operations one possible way dropping replacing g inserting h ways well edit transcript keeps track order cells computed use reconstitute sequence operations converts one strings people proposed modifications levenshtein method example damerau proposed swaps two adjacent characters also cost one reasonable fact damerau modification commonly used computing edit distance linguistically motivated also motivated user status people likely swap adjacent characters accident levenshtein edit distance cats cast would would either two substitutions would need delete one location insert different location gives us two operations whereas damerau modification edit distance score one swaps st ts counts one operation may distance operations specific different tasks let following question figure edit distances pairs shown indicated first example distance sit clown sit equal one clearly based either levenshtein damerau method would cost two would need drop c replace l well case according yet specified edit metric distance going equal one second example distance word qeather weather equal one understandable one substitution q w distance leather weather equal two think would need distance metric makes kind assignments value well ill give answer second well turns two edit distances actually motivated process people create text first example distance sit sit clown small optical character recognition ocr possible mistake character sequence c l look two look pretty similar often ocr software make kind mistake want recover mistakes ocr want assign relatively low edit distance sit sit clown instances ocr makes frequent mistakes example lower case english sometimes interpreted little followed little n many others examples second example distance qeather weather smaller distance leather weather want model way people introduce spelling errors using fat fingers essentially clicking adjacent key keyboard instead one wanted click q w adjacent keyboard whereas l far away want penalize people making typos want queather considered relatively close weather likely mistake user make whereas substituting weather leather unlikely caused fat finger process want case say really distance two words okay another quiz question levenshtein algorithms similar algorithms based dynamic programming used languages like english arabic french also kinds languages look sequence think recognize maybe high school source language see answer next slide answer genetic sequence human language sense english arabic still human natural sequence appear also animals obviously nucleotides encoded genetic sequence one four letters g c dna sequences particular sequence showed wellknown sequence cannot even pronounce name represents something look encyclopedia genetic sequences method described based dynamic programming used aligning text sequences similar methods also used aligning nontextual sequences example nucleotide sequences like previous example either dna rna sequences consisting symbols like acgt dna acgu rna also special wild cards dash stands gap length n stands either one nucleotides similarly one use dynamic programming techniques aligning amino acid sequences example fmelsedg letters represents specific amino acid aspartate glutamate glutamine particular case also wild cards x stands amino acid z stands either glutamate glutamine costs alignments determined empirically reflect evolutionary divergence different protein sequences example aligning v lower cost aligning v h biology background probably recognize molecules amino acids see valine isoleucine molecules look similar histidine looks different external demos levenshtein demo available url shown top provide bunch links external sites perform different forms biological sequence alignment naclo problem related segment called noknok spelled noknok naclo problem eugene fink look external website try solve come back solution noknok problem available external website compare solutions one officially accepted one problem naclo related sequence alignment lost tram problem part showed earlier today naclo problem boris iomdin solution shown next slide concludes segment edit distance using dynamic programming going continue soon next chapter semantic similarity specifically going look lexical similarity synonomy lexical semantic relationships,Course3,W2-S1-L5,W2,S1,L5,-,2,1,5,recent talk one type text similar specif one comput stem word go look differ kind spell similar ad distanc often peopl make mistak go search engin exampl may want look pictur britney spear type name brittani spear often spell name like katherin hepburn incorrectli spell word receipt e even though well awar rule use one way also varianc spell mention theater spell er want abl captur similar spell user type brittani spear abl know meant britney spear two spell similar enough return document let share littl funni exampl differ languag want expos exampl mani languag possibl know arab probabl read without problem cannot want give hint want ask guess success hint behalf person arab spell right left symbol mark pronounc sound one pronounc sound f combin symbol pronounc sound al there reason im show exampl figur person may well muammar gaddafi former leader libya depos kill year ago interest exampl common convent transliter certain word like name render media muammar muammar moamer one instead u word gaddafi last name render g kh one two ds middl al omit without chang mean name result varieti differ transliter first name spell follow set way follow either u follow follow one two ms follow e follow r middl box show differ option al could either lowercas el uppercas el al uppercas al blank third box show spell last name give us total eight time five time total differ combin use media seen differ way spell name want find document contain refer person take transliter issu account exampl show far gaddafi theater britney spear may realiz chang differ version word random arbitrari typic follow form insert delet exampl word behaviour british english spell iour end american english spell ior start british version want convert american version would delet would go way around go american english british english also appli word al al qaddafi type oper common edit substitut string spring similar word one letter differ want transform string spring would one edit oper specif one substitut qaddafi exampl would swap k q want convert sleep slept would need multipl edit think minimum number edit convert sleep slept allow insert delet substitut one possibl way convert sleep slept drop one es insert that two oper even possibl replac second e sleep p replac p that still two oper sens second one less like make sens insert p one place delet p anoth place one specif type oper shown swap adjac letter im go talk minut mean time let stick insert delet substitut singl charact method use comput ad distanc two word two string often refer levenshtein method name well base dynam program start comput ad distanc two empti string recurs comput ad distanc longer longer substr origin word use cost one insert delet substitut length one let look exampl pick two word trend strength want show levenshtein method go comput similar two way work creat two dimension chart four empti box upper left corner start spell one word across start cell strength locat go right spell second word first column posit three second row second column number zero length word first case zero eight second case zero five levenshtein method work like go first comput ad distanc leftmost substr strength specif letter leftmost one charact substr trend name two letter go ad distanc dont match ad distanc zero go comput valu cell tabl make sure previous comput cell immedi immedi left immedi left one want comput point comput either distanc cell three label alreadi comput exampl arent realli anyth comput distanc recurr relat use levenshtein method use follow definit si ith charact string sj jth charact string dij edit distanc prefix length prefix length j tij cost align ith charact string jth charact string let see comput right hand side slide youll see base case di essenti cost align string length string length zero equal need insert turn empti string string length symmetri edit distanc empti string string length j equal j need j definit convert string length j string length zero final recurs case comput edit distanc substr si substr sj need comput valu cell cell left cell left figur three express shown min give us smallest valu pick one new edit distanc sens start cell left add one start cell add one would correspond insert delet start cell left add either one zero depend whether ith charact string match jth charact string edit distanc bottom part right hand side tell tij ith charact si equal jth charact equal otherwis would give us either match cost zero substitut agre earlier would cost one let look exampl comput edit distanc said would either one plu one two one left plu one also two one diagon left zero plu cost substitut three express two two one minimum one one therefor go produc new valu one new cell continu fill tabl recurs edit distanc well either add one two would give us three add one one left give us two start diagon left add edit distanc charact therefor edit distanc zero summar smallest possibl valu cell comput right one rest tabl point time keep track path cell use comput smallest distanc let look one exampl comput tabl far want comput edit distanc r r minimum two plu one two plu one one plu zero rs string particular locat choic number three three one clearli go pick one new valu end day fill entir tabl number bottom righthand corner go give us edit differ two word case four strength convert trend minimum four edit oper one possibl way drop replac g insert h way well edit transcript keep track order cell comput use reconstitut sequenc oper convert one string peopl propos modif levenshtein method exampl damerau propos swap two adjac charact also cost one reason fact damerau modif commonli use comput edit distanc linguist motiv also motiv user statu peopl like swap adjac charact accid levenshtein edit distanc cat cast would would either two substitut would need delet one locat insert differ locat give us two oper wherea damerau modif edit distanc score one swap st ts count one oper may distanc oper specif differ task let follow question figur edit distanc pair shown indic first exampl distanc sit clown sit equal one clearli base either levenshtein damerau method would cost two would need drop c replac l well case accord yet specifi edit metric distanc go equal one second exampl distanc word qeather weather equal one understand one substitut q w distanc leather weather equal two think would need distanc metric make kind assign valu well ill give answer second well turn two edit distanc actual motiv process peopl creat text first exampl distanc sit sit clown small optic charact recognit ocr possibl mistak charact sequenc c l look two look pretti similar often ocr softwar make kind mistak want recov mistak ocr want assign rel low edit distanc sit sit clown instanc ocr make frequent mistak exampl lower case english sometim interpret littl follow littl n mani other exampl second exampl distanc qeather weather smaller distanc leather weather want model way peopl introduc spell error use fat finger essenti click adjac key keyboard instead one want click q w adjac keyboard wherea l far away want penal peopl make typo want queather consid rel close weather like mistak user make wherea substitut weather leather unlik caus fat finger process want case say realli distanc two word okay anoth quiz question levenshtein algorithm similar algorithm base dynam program use languag like english arab french also kind languag look sequenc think recogn mayb high school sourc languag see answer next slide answer genet sequenc human languag sens english arab still human natur sequenc appear also anim obvious nucleotid encod genet sequenc one four letter g c dna sequenc particular sequenc show wellknown sequenc cannot even pronounc name repres someth look encyclopedia genet sequenc method describ base dynam program use align text sequenc similar method also use align nontextu sequenc exampl nucleotid sequenc like previou exampl either dna rna sequenc consist symbol like acgt dna acgu rna also special wild card dash stand gap length n stand either one nucleotid similarli one use dynam program techniqu align amino acid sequenc exampl fmelsedg letter repres specif amino acid aspart glutam glutamin particular case also wild card x stand amino acid z stand either glutam glutamin cost align determin empir reflect evolutionari diverg differ protein sequenc exampl align v lower cost align v h biolog background probabl recogn molecul amino acid see valin isoleucin molecul look similar histidin look differ extern demo levenshtein demo avail url shown top provid bunch link extern site perform differ form biolog sequenc align naclo problem relat segment call noknok spell noknok naclo problem eugen fink look extern websit tri solv come back solut noknok problem avail extern websit compar solut one offici accept one problem naclo relat sequenc align lost tram problem part show earlier today naclo problem bori iomdin solut shown next slide conclud segment edit distanc use dynam program go continu soon next chapter semant similar specif go look lexic similar synonomi lexic semant relationship,[12  4  3 14 13]
255,Course3_W2-S1-L6_-_NACLO_-_11_slides_03-51,one activities im personally involved called naclo competition high school students interested linguistics computational linguistics many problems use class related naclo want give idea stands covers naclo competition linguistics covers also many computation linguistics problems existed since usually every year students united states canada participate top eight go international level listed ones well international level people like adam hestenburg rebecca jacobs alex wades many gold medals international level many countries really well international competition places like russia united kingdom netherlands poland bulgaria south korea recent years canada china international contest started recent ones manchester england last year beijing recently next one bulgaria website international competition shown see related high school competitions math physics chemistry biology competitions lets get points see naclo call traditional poems trying understand foreign language specifically linguistic phenomenon language seen im going go examples quickly ancient greek problem donkey every house heres one japanese noun compounds heres one aligning texts two different languages case swedish norwegian one writing system blind japan called tanji figure characters similar braille english used computational example using finite state automata understand words language called rotokas coast new guinea formed ones writing systems specifically people figure armenian script works looking map station names names given english use logical thinking figure appear map figured appear map figure different symbols mean use symbols figure names remaining stations years one hundred problems used naclo picked interesting computational problems use challenge part class brief introduction naclo later class going look specific problems used competition would like use opportunity encourage people submit problems future years,Course3,W2-S1-L6,W2,S1,L6,-,2,1,6,one activ im person involv call naclo competit high school student interest linguist comput linguist mani problem use class relat naclo want give idea stand cover naclo competit linguist cover also mani comput linguist problem exist sinc usual everi year student unit state canada particip top eight go intern level list one well intern level peopl like adam hestenburg rebecca jacob alex wade mani gold medal intern level mani countri realli well intern competit place like russia unit kingdom netherland poland bulgaria south korea recent year canada china intern contest start recent one manchest england last year beij recent next one bulgaria websit intern competit shown see relat high school competit math physic chemistri biolog competit let get point see naclo call tradit poem tri understand foreign languag specif linguist phenomenon languag seen im go go exampl quickli ancient greek problem donkey everi hous here one japanes noun compound here one align text two differ languag case swedish norwegian one write system blind japan call tanji figur charact similar braill english use comput exampl use finit state automata understand word languag call rotoka coast new guinea form one write system specif peopl figur armenian script work look map station name name given english use logic think figur appear map figur appear map figur differ symbol mean use symbol figur name remain station year one hundr problem use naclo pick interest comput problem use challeng part class brief introduct naclo later class go look specif problem use competit would like use opportun encourag peopl submit problem futur year,[ 4 13 14 12 11]
256,Course3_W2-S1-L7_-_Preprocessing_-_9_slides_11-48,segment preprocessing build natural language processing system going get text format system would able understand need convert format easier process stage inaudible called preprocessing lets look examples first need remove nontextual pieces example ads images javascript code need understand text encoding example whether unicode encoding convert one system understands need sentence segmentation understand boundaries sentences located need bunch things lets look detail first called normalization normalization means word multiple variants want convert form confused labeled labelled spelled differently british english american english one ls two ls want perhaps merge one word extraterrestrial spelled hyphen middle single word space middle want able collapse one specific normalized version next episode called stemming merging words computer computation category based stem compute well talk stemming detail separately morphological analysis deals relationship words inflection example cars labeled plural form car would labeled nouns one singular one plural capitalization matters lot texts entirely capital letters may difficult mixed case want able distinguish things like first one adverb time second one national organization women similarly led could past tense word lead light emitting diodes acronym capital x also want see interferes named entity extraction see usa capital letters easily picked named entity recognizer country name text foreign language example usa spanish means uses would confuse named entity recognizer theres distinction made types tokens type sequence characters represent specific word token occurrence type type may appear document token may appear multiple times sentence four types six tokens types appear twice need also something called tokenization tokenization periods punctuation symbols middle text need figure words start words end example want collapse together als without periods als periods assume second example tells us really three sentences periods mean sentence boundaries identifying boundaries words sentences actually tricky example pauls apostrophe split apostrophe make decision use consistently throughout entire natural language pipeline abbreviations end punctuations confusing sometimes example willow dr ends punctuation dont want split part sentence dr willow words dont want split either even though period cases phrases used chunks example new york ad hoc twoword compounds want store message memory computer single unit punctuation symbols also create problems example new yorklos angeles flight split spaces going assume yorklos would word clearly case want split phrase new yorklos angeles correct way interpret notice different minneapolisst paul name one place rather example left numbers challenging see often phone numbers dates different formats may useful recognize entire sequence single number also number one shown right hand side standing goals dates spelled many different ways january versus versus th january urls also problematic portions separated slashes periods portions may look like words dont recognize whole unit url may erroneously try parse individual component words languages additional challenges example text new store japanese japanese like chinese languages dont use word bondings theres spaces words difficult figure word starts word ends plus furthermore use example showing screen japanese three different alphabets katakana hiragana kanji separated parsed separate word segmentation difficult lets look examples first example arabic word katabu means book arabic spelled right left symbol righthand side cursand next one sound last thing separate rest book space two doesnt mean word boundary happens letter letter bu space inbetween try split going get two meaningless words japanese example kono hon ha omoi means book interesting four words first two symbols next one one symbol next one one symbol last one two dont know third symbol left represents book hon wouldnt understand put word boundaries case languages long words example german something like finanzdienstleistung means something like financial services case three different words merged together one long word tricky want find example documents financial issues want able retrieve document well even though word finance appears part another word segmenting tolkienizing german presents particular challenge lets look one example word segmentation make mistake chinese example world television consists characters electricity look split two words well get something like looking electricity definitely intended meaning television mentioned japanese three different alphabets heres example shows text blue kanji chinese characters two alphabets addition kanji hiragana used ending words prepositions service words katakana used foreign words like example new york word left red occasionally japanese text also include text romaji essentially latin character like english also numbers particular example phrase foreign means new york city located state new york three pieces text new york america new york katakana text romaji spelled english four characters hiragana six characters kanji imagine difficult preprocess kind text final task speech processing would like mention today sentence boundary recognition goal sentence boundary recognition figure punctuation symbols indicate end sentence looked example earlier willow dr dr willow often period may indicate abbreviation rather sentence boundaryso sentence boundary recognition typically dealt using decision trees look features actual punctuation exclamation point period question mark question marks unlikely appear acronyms example part abbreviations whereas periods appear acronyms frequently look formatting example space period capitalization look fonts theres font change indicate new sentence new paragraph also look specific list abbreviations dr doctor drive morning example rule decision tree would something like theres space period dont assume sentence boundary convention include one two spaces period text different language rule may apply idea preprocessing works important natural language processing assume rest course texts dealing already preprocessed properly segmented segments excuse properly segmented sentences words concludes section preprocessing,Course3,W2-S1-L7,W2,S1,L7,-,2,1,7,segment preprocess build natur languag process system go get text format system would abl understand need convert format easier process stage inaud call preprocess let look exampl first need remov nontextu piec exampl ad imag javascript code need understand text encod exampl whether unicod encod convert one system understand need sentenc segment understand boundari sentenc locat need bunch thing let look detail first call normal normal mean word multipl variant want convert form confus label label spell differ british english american english one ls two ls want perhap merg one word extraterrestri spell hyphen middl singl word space middl want abl collaps one specif normal version next episod call stem merg word comput comput categori base stem comput well talk stem detail separ morpholog analysi deal relationship word inflect exampl car label plural form car would label noun one singular one plural capit matter lot text entir capit letter may difficult mix case want abl distinguish thing like first one adverb time second one nation organ women similarli led could past tens word lead light emit diod acronym capit x also want see interfer name entiti extract see usa capit letter easili pick name entiti recogn countri name text foreign languag exampl usa spanish mean use would confus name entiti recogn there distinct made type token type sequenc charact repres specif word token occurr type type may appear document token may appear multipl time sentenc four type six token type appear twice need also someth call token token period punctuat symbol middl text need figur word start word end exampl want collaps togeth al without period al period assum second exampl tell us realli three sentenc period mean sentenc boundari identifi boundari word sentenc actual tricki exampl paul apostroph split apostroph make decis use consist throughout entir natur languag pipelin abbrevi end punctuat confus sometim exampl willow dr end punctuat dont want split part sentenc dr willow word dont want split either even though period case phrase use chunk exampl new york ad hoc twoword compound want store messag memori comput singl unit punctuat symbol also creat problem exampl new yorklo angel flight split space go assum yorklo would word clearli case want split phrase new yorklo angel correct way interpret notic differ minneapolisst paul name one place rather exampl left number challeng see often phone number date differ format may use recogn entir sequenc singl number also number one shown right hand side stand goal date spell mani differ way januari versu versu th januari url also problemat portion separ slash period portion may look like word dont recogn whole unit url may erron tri pars individu compon word languag addit challeng exampl text new store japanes japanes like chines languag dont use word bond there space word difficult figur word start word end plu furthermor use exampl show screen japanes three differ alphabet katakana hiragana kanji separ pars separ word segment difficult let look exampl first exampl arab word katabu mean book arab spell right left symbol righthand side cursand next one sound last thing separ rest book space two doesnt mean word boundari happen letter letter bu space inbetween tri split go get two meaningless word japanes exampl kono hon ha omoi mean book interest four word first two symbol next one one symbol next one one symbol last one two dont know third symbol left repres book hon wouldnt understand put word boundari case languag long word exampl german someth like finanzdienstleistung mean someth like financi servic case three differ word merg togeth one long word tricki want find exampl document financi issu want abl retriev document well even though word financ appear part anoth word segment tolkien german present particular challeng let look one exampl word segment make mistak chines exampl world televis consist charact electr look split two word well get someth like look electr definit intend mean televis mention japanes three differ alphabet here exampl show text blue kanji chines charact two alphabet addit kanji hiragana use end word preposit servic word katakana use foreign word like exampl new york word left red occasion japanes text also includ text romaji essenti latin charact like english also number particular exampl phrase foreign mean new york citi locat state new york three piec text new york america new york katakana text romaji spell english four charact hiragana six charact kanji imagin difficult preprocess kind text final task speech process would like mention today sentenc boundari recognit goal sentenc boundari recognit figur punctuat symbol indic end sentenc look exampl earlier willow dr dr willow often period may indic abbrevi rather sentenc boundaryso sentenc boundari recognit typic dealt use decis tree look featur actual punctuat exclam point period question mark question mark unlik appear acronym exampl part abbrevi wherea period appear acronym frequent look format exampl space period capit look font there font chang indic new sentenc new paragraph also look specif list abbrevi dr doctor drive morn exampl rule decis tree would someth like there space period dont assum sentenc boundari convent includ one two space period text differ languag rule may appli idea preprocess work import natur languag process assum rest cours text deal alreadi preprocess properli segment segment excus properli segment sentenc word conclud section preprocess,[ 4 13  0 14 12]
257,Course3_W3-S1-L1_-_Semantic_Similarity-_Synonymy_and_other_Semantic_Relations_-_23__slides_14-38,okay next section text similarity going semantic similarity going look synonymy others semantic relations showed example introduction three different announcements stock market used different words yet meant less climbed gained rose mean pretty much context stock index going right hand side also looked paraphrases best close best showing highest level difference left column right column left one synonyms second case paraphrases synonyms something may studied high school synonyms different words sometimes word compounds similar meanings example adjectives tepid lukewarm similar meanings substituted one another many different contexts example water tepid versus water lukewarm theyre exactly cases want use one practical purposes substituted contexts mentioned earlier example big large synonyms many contexts contexts example there’s major difference big leagues large leagues big leagues actual concept whereas large leagues used verbs sweat perspire also near synonyms differ different things example frequency use context appear another property words polysemy polysemy property words multiple senses typically see dictionary open dictionary look definitions word book see refer many different things literary work example anna karenina tolstoy thats literary work stack pages may may blank example notebook record business transactions think bookkeeper bookkeeper person keeps tracks accounts books record bets bookmaker person takes bets also list buy sell orders financial markets want buy sell certain stock theres list orders whole list called book senses book im going mention word also multiple parts speech set senses example word book verb mean make reservation occupy different senses word dont equally frequent sense example word see watch something observe something frequent sense see holy see holy spelled holy see spelled see used specific context specifically vatican holy see special name vatican senses word may overlap example first two senses book previous slide remember one literary work one stack pages talk book may say bring book shelf really talking stack pages also thinking work literature instead thats different dictionaries different sets word senses word cases may difference two sizes word like title dictionaries may conflict little joke somebody said favorite books anna karenina fathers checkbook see case two different kinds books lumped together words highly polysemous according wordnet going discuss new slides verb get least different meanings many words english many meanings example words like dull put also dozens meanings semantic relationships interesting natural language processing example antonymy antonyms means near opposites example word raise raise bar antonym lower lower bar another semantic relationship hypernymy hypernym general concept another example deer general concept elk hyponymy opposite hypernymy case word specific instance another word elk hyponym deer one concept meronymy two kinds meronymy membership meronymy part meronymy membership meronymy refers words like flock includes bunch sheep possibly bunch birds thats example membership meronymy part meronymy refers relationship table legs table includes legs semantic relations hold word senses words example antonym word hot either mild hot used sense spicy cold hot used sense warm even unattractive hot used sense attractive depending sense may different antonyms another example following immediate hypernym bar one many among others room musical notation obstruction profession depending sense bar result people community use term synset group together synonyms word word polysemous may associated multiple synsets takes us wordnet wordnet special database lexical relationships words english created princeton university years started george miller unfortunately passed away couple years ago project run christiane fellbaum still princeton university includes large database words english mainly nouns verbs also material number adjectives adverbs also includes semantic relationships main relation wordnet hypernymy overall structure database treelike next slide show example small subsection wordnet tree wordnet one valuable resources natural language processing would like encourage everybody look two references listed one collection papers edited christiane fellbaum one introductory article wordnet george miller communications acm years ago example im going show small subtree wordnet matches relationship different animals see word ungulate shown top ungulates divided eventoed ungulates oddtoed ungulates ungulates include equines others im showing equines equine mostly horses related animals also belong mules zebras also example object specific horse case pony left hand side okapis deer giraffes examples ruminants elk wapiti caribou examples deer remind deer hypernym caribou caribou hyponym deer lets look examples wordnet word bar wordnet senses sorted particular way often first frequent senses barroom bar saloon gin mill taproom words synset bar sense room alcoholic drinks served counter second sense bar actual counter purchase food drink third sense rigid piece metal example iron bar fourth sense measure music definition notation repeating pattern musical beats written followed vertical bar many senses bar verb bar opposed noun bar also four senses lets look one two first one bar exclude barred membership club second sense barricade block blockade block block means render something unsuitable passage barricade streets bar streets first sense bar closest hypernym room hypernym room area hypernym area structure construction artifact object entity entity one root categories wordnet hierarchy say wordnet really tree misspoke earlier like forest multiple roots entity something one show others later second sense bar bar hyponym counter hyponym table piece furniture goes back entity something goes back root note lets briefly look interpretations bar bar implement implement type instrumentation instrumentations type artifact go back entity fourth sense abstract concept bar sense musical notation hyponym notational system hyponym written communication way abstraction takes us another root note wordnet forest look examples sense five takes us entity sense six takes us act human action human activity seven obstruction eight goes entity nine goes group grouping ten goes back entity something properties words wordnet include familiarity polysemy slide going show examples left hand side slide shows familiarity certain words right hand side shows polysemy polysemy name indicates means number senses poly means many semy means senses polysemy count word board used noun nine bar earlier familiarity word board used noun familiar thats common category categories available common uncommon also rare rare word serendipity example rare one thing may notice based example also true general common words also familiar also senses like example word board familiar also high polysemy count lexical networks addition wordnet want bring heres sampling eurowordnet multiple european languages spanish french german would like emphasize exists many including many open source external thesauri language resources used natural language processing ways similar wordnet different databases incorporated natural language systems example open thesaurus freebase dbpedia babelnet various thesauri case youre wondering thesauri means plural thesaurus thesaurus special kind dictionary tells words similar related words heres example babelnet word song see tells different languages usages word contains links definitions word multiple languages mesh another interesting hierarchy concepts used medical literature mesh stand medical subject headings includes concepts diseases drugs body parts sera link bottom shows access mesh mesh freely downloaded used lot biomedical natural language processing community concludes section wordnet related lexical networks going continue moment next section thesaurusbased word similarity methods,Course3,W3-S1-L1,W3,S1,L1,-,3,1,1,okay next section text similar go semant similar go look synonymi other semant relat show exampl introduct three differ announc stock market use differ word yet meant less climb gain rose mean pretti much context stock index go right hand side also look paraphras best close best show highest level differ left column right column left one synonym second case paraphras synonym someth may studi high school synonym differ word sometim word compound similar mean exampl adject tepid lukewarm similar mean substitut one anoth mani differ context exampl water tepid versu water lukewarm theyr exactli case want use one practic purpos substitut context mention earlier exampl big larg synonym mani context context exampl there’ major differ big leagu larg leagu big leagu actual concept wherea larg leagu use verb sweat perspir also near synonym differ differ thing exampl frequenc use context appear anoth properti word polysemi polysemi properti word multipl sens typic see dictionari open dictionari look definit word book see refer mani differ thing literari work exampl anna karenina tolstoy that literari work stack page may may blank exampl notebook record busi transact think bookkeep bookkeep person keep track account book record bet bookmak person take bet also list buy sell order financi market want buy sell certain stock there list order whole list call book sens book im go mention word also multipl part speech set sens exampl word book verb mean make reserv occupi differ sens word dont equal frequent sens exampl word see watch someth observ someth frequent sens see holi see holi spell holi see spell see use specif context specif vatican holi see special name vatican sens word may overlap exampl first two sens book previou slide rememb one literari work one stack page talk book may say bring book shelf realli talk stack page also think work literatur instead that differ dictionari differ set word sens word case may differ two size word like titl dictionari may conflict littl joke somebodi said favorit book anna karenina father checkbook see case two differ kind book lump togeth word highli polysem accord wordnet go discuss new slide verb get least differ mean mani word english mani mean exampl word like dull put also dozen mean semant relationship interest natur languag process exampl antonymi antonym mean near opposit exampl word rais rais bar antonym lower lower bar anoth semant relationship hypernymi hypernym gener concept anoth exampl deer gener concept elk hyponymi opposit hypernymi case word specif instanc anoth word elk hyponym deer one concept meronymi two kind meronymi membership meronymi part meronymi membership meronymi refer word like flock includ bunch sheep possibl bunch bird that exampl membership meronymi part meronymi refer relationship tabl leg tabl includ leg semant relat hold word sens word exampl antonym word hot either mild hot use sens spici cold hot use sens warm even unattract hot use sens attract depend sens may differ antonym anoth exampl follow immedi hypernym bar one mani among other room music notat obstruct profess depend sens bar result peopl commun use term synset group togeth synonym word word polysem may associ multipl synset take us wordnet wordnet special databas lexic relationship word english creat princeton univers year start georg miller unfortun pass away coupl year ago project run christian fellbaum still princeton univers includ larg databas word english mainli noun verb also materi number adject adverb also includ semant relationship main relat wordnet hypernymi overal structur databas treelik next slide show exampl small subsect wordnet tree wordnet one valuabl resourc natur languag process would like encourag everybodi look two refer list one collect paper edit christian fellbaum one introductori articl wordnet georg miller commun acm year ago exampl im go show small subtre wordnet match relationship differ anim see word ungul shown top ungul divid evento ungul oddto ungul ungul includ equin other im show equin equin mostli hors relat anim also belong mule zebra also exampl object specif hors case poni left hand side okapi deer giraff exampl rumin elk wapiti carib exampl deer remind deer hypernym carib carib hyponym deer let look exampl wordnet word bar wordnet sens sort particular way often first frequent sens barroom bar saloon gin mill taproom word synset bar sens room alcohol drink serv counter second sens bar actual counter purchas food drink third sens rigid piec metal exampl iron bar fourth sens measur music definit notat repeat pattern music beat written follow vertic bar mani sens bar verb bar oppos noun bar also four sens let look one two first one bar exclud bar membership club second sens barricad block blockad block block mean render someth unsuit passag barricad street bar street first sens bar closest hypernym room hypernym room area hypernym area structur construct artifact object entiti entiti one root categori wordnet hierarchi say wordnet realli tree misspok earlier like forest multipl root entiti someth one show other later second sens bar bar hyponym counter hyponym tabl piec furnitur goe back entiti someth goe back root note let briefli look interpret bar bar implement implement type instrument instrument type artifact go back entiti fourth sens abstract concept bar sens music notat hyponym notat system hyponym written commun way abstract take us anoth root note wordnet forest look exampl sens five take us entiti sens six take us act human action human activ seven obstruct eight goe entiti nine goe group group ten goe back entiti someth properti word wordnet includ familiar polysemi slide go show exampl left hand side slide show familiar certain word right hand side show polysemi polysemi name indic mean number sens poli mean mani semi mean sens polysemi count word board use noun nine bar earlier familiar word board use noun familiar that common categori categori avail common uncommon also rare rare word serendip exampl rare one thing may notic base exampl also true gener common word also familiar also sens like exampl word board familiar also high polysemi count lexic network addit wordnet want bring here sampl eurowordnet multipl european languag spanish french german would like emphas exist mani includ mani open sourc extern thesauri languag resourc use natur languag process way similar wordnet differ databas incorpor natur languag system exampl open thesauru freebas dbpedia babelnet variou thesauri case your wonder thesauri mean plural thesauru thesauru special kind dictionari tell word similar relat word here exampl babelnet word song see tell differ languag usag word contain link definit word multipl languag mesh anoth interest hierarchi concept use medic literatur mesh stand medic subject head includ concept diseas drug bodi part sera link bottom show access mesh mesh freeli download use lot biomed natur languag process commun conclud section wordnet relat lexic network go continu moment next section thesaurusbas word similar method,[ 4 13  0 14 12]
258,Course3_W3-S1-L2_-_Thesaurus-based_Word_Similarity_Methods_-_11_slides_07-52,okay next section text similarity deal specific kind word similarity methods based thesauri wordnet let ask question look four pairs words shown tell pair exhibits greatest similarity deerelk deerhorse deermouse deerroof think answer question pretty obvious know elk means answer deer elk similar pair want ask think similar well know theyre kinds deer natural language system know well lets see remember wordnet tree segment showed ungulate well lets see animals words appear tree deer elk appear close fact one immediate hypernym one whereas horse pretty far want define similarity metric based wordnet simplest thing identify nodes different words appear count number links needed get one according metric elk deer similar theres one hop separates distance deer horse much larger go deer ruminant eventoed ungulate ungulate oddtoed ungulate equine horse thats total six hops distance deer horse six distance deer elk one need mention distance similarity related greater distance smaller similarity inverse relationship two want similarity pathlength go direction need put negative sign front pathlength first version path similarity want introduce today call version case similarity word v word w negation pathlength v w practical purposes turns better take logarithm pathlength version similarity metric minus logarithm pathlength v w theres problems approach first one may tree forest presentation specific domain example medical domain financial domain particular language example languages world wordnet type databases dozen specific words example term proper noun may tree may something new something way specific included database interesting problem hypernym edges also known isa ages elk deer ages equally apart similarity space sometimes two words two hops apart two edges apart still relatively similar semantically whereas another pair words exact distance may actually much different first pair lets talk advanced versions path similarity use wordnet trees version example developed philip resnik based minus logarithm something something pathlength rather probability observing word appears corpus lowest common subsumer two words trying compare lowest common subsumer mean well simply node tree ancestor nodes want compare want compare deer horse go way ungulate want compare deer elk need go far hierarchy deer lcs deer horses ungulate lcs deer elk deer want look probability pretty obvious given corpus probability ungulate larger probability deer ungulates common deer deer special case ungulates take logarithm probability two values going obtain similarity philip resnik defined theres another metric category based socalled information content node developed dekang lin late formula simple ic information content certain concept minus logarithm probability concept similarity two words defined according formula x logarithm probability lowest common subsumer divided sum logarithms probabilities individual words example dekang lins paper shows similarity hill coast x logarithm probability geological formation lowest common subsumer case numerator denominator logarithm probability hill logarithm logarithm probability cost simplify expression see similarity hill coast largest possible value want point lot algorithms implemented software exists versions pretty much major programming language including java c specifically slide shown links perl implementation called wordnet similarity ted peterson university minnesota duluth students also python version part nltk software talked explicitly slide hand nltk see computes similarity different pairs words called function linsimilarity implements dekang lin similarity metric showed earlier lin similarity dog cat using brown corpus knowledge base conditions similarity dog elephant smaller similarity dog elk even smaller concludes section methods word similarity based thesauri going continue next section text similarity using vector space model,Course3,W3-S1-L2,W3,S1,L2,-,3,1,2,okay next section text similar deal specif kind word similar method base thesauri wordnet let ask question look four pair word shown tell pair exhibit greatest similar deerelk deerhors deermous deerroof think answer question pretti obviou know elk mean answer deer elk similar pair want ask think similar well know theyr kind deer natur languag system know well let see rememb wordnet tree segment show ungul well let see anim word appear tree deer elk appear close fact one immedi hypernym one wherea hors pretti far want defin similar metric base wordnet simplest thing identifi node differ word appear count number link need get one accord metric elk deer similar there one hop separ distanc deer hors much larger go deer rumin evento ungul ungul oddto ungul equin hors that total six hop distanc deer hors six distanc deer elk one need mention distanc similar relat greater distanc smaller similar invers relationship two want similar pathlength go direct need put neg sign front pathlength first version path similar want introduc today call version case similar word v word w negat pathlength v w practic purpos turn better take logarithm pathlength version similar metric minu logarithm pathlength v w there problem approach first one may tree forest present specif domain exampl medic domain financi domain particular languag exampl languag world wordnet type databas dozen specif word exampl term proper noun may tree may someth new someth way specif includ databas interest problem hypernym edg also known isa age elk deer age equal apart similar space sometim two word two hop apart two edg apart still rel similar semant wherea anoth pair word exact distanc may actual much differ first pair let talk advanc version path similar use wordnet tree version exampl develop philip resnik base minu logarithm someth someth pathlength rather probabl observ word appear corpu lowest common subsum two word tri compar lowest common subsum mean well simpli node tree ancestor node want compar want compar deer hors go way ungul want compar deer elk need go far hierarchi deer lc deer hors ungul lc deer elk deer want look probabl pretti obviou given corpu probabl ungul larger probabl deer ungul common deer deer special case ungul take logarithm probabl two valu go obtain similar philip resnik defin there anoth metric categori base socal inform content node develop dekang lin late formula simpl ic inform content certain concept minu logarithm probabl concept similar two word defin accord formula x logarithm probabl lowest common subsum divid sum logarithm probabl individu word exampl dekang lin paper show similar hill coast x logarithm probabl geolog format lowest common subsum case numer denomin logarithm probabl hill logarithm logarithm probabl cost simplifi express see similar hill coast largest possibl valu want point lot algorithm implement softwar exist version pretti much major program languag includ java c specif slide shown link perl implement call wordnet similar ted peterson univers minnesota duluth student also python version part nltk softwar talk explicitli slide hand nltk see comput similar differ pair word call function linsimilar implement dekang lin similar metric show earlier lin similar dog cat use brown corpu knowledg base condit similar dog eleph smaller similar dog elk even smaller conclud section method word similar base thesauri go continu next section text similar use vector space model,[ 4 14 13 12 11]
259,Course3_W3-S1-L3_-_The_Vector_Space_Model_-_15_slides_09-53,okay going continue another method text similarity much general model vector space model used text similarity also many applications natural language processing information retrieval vector space model represents documents space multidimensional space term dimension used primarily representing full documents information retrieval scenarios terms could words appear document example cat dog elephant documents linear combinations vectors along axes document contains example three instances word cat one instance word dog one instance word elephant would represented vector threedimensional space another document four instances elephant instance two representation document similarity used information retrieval determine document similar given query one basic ideas document retrieval search engines query vector q shown middle slide two documents want determine whether better match query q note queries documents presented space use angle vectors proxy similarities example similarity q proportional angle alpha two vectors similarity q proportional theta angle two vectors slightly better version similarity based cosine angle cosine alpha versus cosine theta quadrant theyre related direction cosine smaller means angle also smaller also means similarity larger smaller angle means larger similarity lets see cosine similarities computed well mathematically cosine normalized dot product two vectors want compute similarity document query divide sum di qi different dimensions vector space sub ith component vector q sub ith component vector q need normalize dividing sum lengths two vectors lengths computed taking square dot product vector taking square root theres variant cosine called jaccard coefficient size intersection two vectors divided size union two vectors lets look examples suppose document presented cat dog dog query represented cat dog mouse mouse example first dimension cat second dimension dog third dimension mouse vector representation going vector representation q going compute similarity q multiply pairwise first components vectors x product second components x product third components x gives us numerator normalize product lengths two vectors length first vector square root squared plus square plus square square root length second vector square root sum square plus square plus square square root simplify formula divided square root times square root get cosine similarity q lets see happens compare vector cosine similarity x x x numerator denominator square length vectors vectors length square root square root squared simplify expression get overall cosine similarity largest possible similarity smallest possible similarity going think okay lets try example given three documents want compute cosine similarities dont compute similarities think values get think youre getting values next slide im going show answer answers similarity let go back previous slide remind documents similarity two aligned exact direction therefore angle corresponds largest possible cosine compare going get different similarity swapping two dimensions actually results different value case could much smaller value swapping two dimensions actually bad thing dont want whereas extending vector particular direction multiplying scalar actually something preserves document similarity one question range values cosine score take one correct answer theres another one somewhat correct depending context wont correct think carefully answer general mathematics cosine function rage plus however since dealing vectors include counts words first quadrant word counts nonnegative range cosine next item going discuss today vector space model applied text similarity particular first lets consider idea distributional similarity distributional similarity principle tells us two words appear similar contexts likely semantically related example went search engine searched names words appear together drive got schedule test drive investigate hondas financial options volkswagen debuted new version frontwheeldrive golf jeep reminded recent drive finally test drive took place wheel loaded ford el model see word drive appears sentences name car also appears honda volkswagen jeep ford related appear near word drive goes old principle firth says know word company keeps summery distributional similarity principle context though context word anything word target word word word within n words target word word specific syntactic relationship target word example head dependency subject sentence related target word also word within sentence even word within document example documents talk hospitals doctors nurses likely contain word patient somewhere thats means theyre semantically related going continue minutes next segment dimensionality reduction,Course3,W3-S1-L3,W3,S1,L3,-,3,1,3,okay go continu anoth method text similar much gener model vector space model use text similar also mani applic natur languag process inform retriev vector space model repres document space multidimension space term dimens use primarili repres full document inform retriev scenario term could word appear document exampl cat dog eleph document linear combin vector along axe document contain exampl three instanc word cat one instanc word dog one instanc word eleph would repres vector threedimension space anoth document four instanc eleph instanc two represent document similar use inform retriev determin document similar given queri one basic idea document retriev search engin queri vector q shown middl slide two document want determin whether better match queri q note queri document present space use angl vector proxi similar exampl similar q proport angl alpha two vector similar q proport theta angl two vector slightli better version similar base cosin angl cosin alpha versu cosin theta quadrant theyr relat direct cosin smaller mean angl also smaller also mean similar larger smaller angl mean larger similar let see cosin similar comput well mathemat cosin normal dot product two vector want comput similar document queri divid sum di qi differ dimens vector space sub ith compon vector q sub ith compon vector q need normal divid sum length two vector length comput take squar dot product vector take squar root there variant cosin call jaccard coeffici size intersect two vector divid size union two vector let look exampl suppos document present cat dog dog queri repres cat dog mous mous exampl first dimens cat second dimens dog third dimens mous vector represent go vector represent q go comput similar q multipli pairwis first compon vector x product second compon x product third compon x give us numer normal product length two vector length first vector squar root squar plu squar plu squar squar root length second vector squar root sum squar plu squar plu squar squar root simplifi formula divid squar root time squar root get cosin similar q let see happen compar vector cosin similar x x x numer denomin squar length vector vector length squar root squar root squar simplifi express get overal cosin similar largest possibl similar smallest possibl similar go think okay let tri exampl given three document want comput cosin similar dont comput similar think valu get think your get valu next slide im go show answer answer similar let go back previou slide remind document similar two align exact direct therefor angl correspond largest possibl cosin compar go get differ similar swap two dimens actual result differ valu case could much smaller valu swap two dimens actual bad thing dont want wherea extend vector particular direct multipli scalar actual someth preserv document similar one question rang valu cosin score take one correct answer there anoth one somewhat correct depend context wont correct think care answer gener mathemat cosin function rage plu howev sinc deal vector includ count word first quadrant word count nonneg rang cosin next item go discuss today vector space model appli text similar particular first let consid idea distribut similar distribut similar principl tell us two word appear similar context like semant relat exampl went search engin search name word appear togeth drive got schedul test drive investig honda financi option volkswagen debut new version frontwheeldr golf jeep remind recent drive final test drive took place wheel load ford el model see word drive appear sentenc name car also appear honda volkswagen jeep ford relat appear near word drive goe old principl firth say know word compani keep summeri distribut similar principl context though context word anyth word target word word word within n word target word word specif syntact relationship target word exampl head depend subject sentenc relat target word also word within sentenc even word within document exampl document talk hospit doctor nurs like contain word patient somewher that mean theyr semant relat go continu minut next segment dimension reduct,[ 2  4  5 14 13]
260,Course3_W3-S1-L4_-_Dimensionality_Reduction_-_38_slides_23-50,next topic text similarity called dimensionality reduction motivation behind dimensionality reduction document often many different topics relatively small number topics certainly smaller number words appear somehow collapse words semantic categories may able find better similarity measures example collapse documents patients doctors hospitals one place may consider semantically similar even though contained different words set simpler vector approaches similarity looked far problems many cases theres polysemy thats similarity actual similarity words smaller cosine similarity would make believe includes words multiple sentences bar bank jaguar hot opposite due synonymy thats actual similarity words larger cosine similarity would make believe word like building synonym edifice theyre different words therefore theyre going different contexts even though different contexts meaning want somehow know synonyms case overestimate cosines similarity matrix words sentences general sparse needs processed dimensionality reduction find hidden semantic dimensions lets look examples natural language processing literature example left gives multiple choice questions toefl test youre given words specifically levied youre asked word similar choices imposed b believed c requested correlated answer similar word levied imposed kind semantic relationship want discover text letter semantic analysis technique going introduce today going help us technique also used identify similar analogies example sat test youre given pair mason stone youre asked following five choices represents similar relationship one mason stone answers teacher chalk b carpenter wood c soldier gun photograph camera e book work correct answer carpenter wood like mason person works stone carpenter person works wood analogies different turns problem analogy similarity also resolved dimensionality reduction techniques lot work actually done peter turning papers last ten years let’s consider dimensionality reduction detail purpose method look hidden similarities data based matrix decomposition im going introduce giving example high school people measured heights weights students school scatter plot left shows different students measured xaxis represents lets say height yaxis represents weight find regression line explains data best thats shown middle red line appears diagonally turns third variable addition height weight explain differences height weight different students exactly regression line shows line corresponds dimension age turns sample students high school students age students across different classes age groups obviously students lower grades lighter shorter students upper grades collapse points diagonal axis represents regression line see theres actually nice trend students older taller heavier younger students process losing much information turns replace height weight age gain information appears data set left everything different two examples tells us particular student differs trends person tall age somebody whos short age reduce dimensionality data set two dimensions one dimension done practice well lets go back little bit linear algebra need remember vectors matrices work order understand dimensionality reduction matrix x n table objects case objects numbers row also column matrix vector matrices compatible dimensions theres special definition compatibility context matrices compatible dimensions multiplied together dont remember kind math go visit website explains multiply matrices come back try multiply two matrices example next slide im going show answer way matrices multiplied simple multiply values first row value first column add result goes cell corresponds first row first column product first row product x x x total second row x x x finally x x x equal product two matrices vector one important concept linear algebra related dimensionality reduction eignenvectors eigenvalues eigenvector implicit direction matrix multiply vector v eigenvector righthand side matrix going obtain result multiplied v lambda scalar like eigenvalue principle eigenvalue lambda complex number examples going look always real compute eigenvalues matrix need use following equation need find determinant matrix minus lambda unit matrix size matrix want square matrix obviously compute determinant want set determinant equal lets look example matrix minus minus lambda matrix shown right minus minus lambda first row followed minus lambda second row want compute determinant matrix need find product forward diagonal subtract value product numbers backwards diagonal gives us lambdalambda whole thing x solve quadratic equation lambda lambda squared going see two roots lambda lambda pick one eigenvalues replace equation top going get new matrix multiplied eigenvector x x want result equal well solve system equations find answer xx two dimensional vector x coordinates would satisfy second equation matrix square decomposed u lambda u inverse u matrix eigenvectors lambda diagonal matrix eigenvalues different transformations mathematically equivalent sigma u u lambda u inverse sigma u lambda sigma u lambda u inverse heres example original matrix added values lambda lambda decomposition going get u equal matrix inverse u equal matrix onehalf onehalf followed onehalf onehalf verify indeed cover original matrix multiplying u lambda u inverse multiplication sequence obtain original matrix happened able convert original matrix new space old eigen vectors point original space going represented point new representation case weight height would new dimension corresponds age another dimension corresponds deviation certain person given weight based trend line defined age matrix square use different technique called singular value decomposition care non square matrices well nlp information retrieval tasks matrices either documents terms words context features matrices definition necessarily square example vocabulary size million set million documents case going matrix million million clearly square case use another technique called singular value decomposition case matrix represented product u sigma v transposed u matrix orthogonal eigenvectors pole aa transposed v matrix orthogonal eigenvectors transpose components sigma eigenvalues transpose decomposition exists matrices whether theyre dense sparse estimate dimensionality different matrices example columns rows matrix u orthogonal eigenvectors transpose x matrix v matrix orthogonal eigenvectors transpose dimensionality x matlab octave use simple function svd give matrix input return tuple consisting u v match exactly u sigma v example lets look specific example collection seven documents nine terms look terms appear documents example document contains terms document contains terms also look posting list represent data form bipartite graph bipartite graph two components one left one right example correspond different types objects left hand side left hand mode corresponds documents right hand side corresponds terms example connection dont connection okay lets see compute singular body composition arbitrary matrix first reppresent whole form like example left one certain term appears certain document zero means absent document need normalize matrix dividing values length column vector part means first column two ones therefore length vector corresponds first column going square root divide values column square root going get nd column vector therefore length going square root divide square root going get see technique use compute normalized values normalize matrix compute singular value decomposition enter matrix matlab favorite software run svd library going get something like u going going x matrix v going x matrix going x matrix responds spread different axes data first dimension spread important dimension lower dimensionality representation second value singular values appear sigma matrix one thing reconstitute original matrix multiplying u sigma v transposed also produce different version specifically star lower dimensionality version instead multiplying u sigma v transpose multiply u sigma star v transpose sigma star keeps largest singular values original sigma matrix go back previous slide essentially delete th th line maybe without losing much information stored matrix keep four significant dimensions correspond four largest values sigma rank approximation sigma see zeros original matrix multiply u sigma v transpose going get different representation going beginning significantly different compute representations terms documents new semantics space done appropriate multiplications matrices example u times times v prime gives us representations documents terms respectively new semantic space also take step compute rank approximation original matrix preserving two largest values sigma matrix operators compute new value rank approximation rank approximation use u multiplied get word vector presentation concept space two concepts also use times v transposed find new concept representation document case two dimensions heres slide summarizes entire singular value decomposition method documents shown topleft terms topright new semantic space see two clusters documents terms im going focus one bottomleft area screen see appear near also appear near essentially kill two birds one stone firs found latent semantic similarity terms also similarity documents also able achieve something couldnt original space namely find documents six seven similar terms three seven new concept space look smallest circle purple see closest match entire cluster elements quadrant including also semantically connected actually something understand better look original example specifically graph representation see one documents appears purple cluster represented fact similar bit lot common terms continue expanding recursive see next small similar document followed exactly intuition get looking graph representation right patience formulas math lab allow seek translate individual documents terms concepts multiply either entire column vector entire row vector singular value matrix question document term matrix aa transpose transpose lets start aa transpose see aa transpose matrix surprising given original matrix multiplied transposed therefore result similarly transpose matrix seven seven surprising multiplied matrix matrix dimensionallities two products give hint mean turns transpose document document similarity matrix example tells similarity document document whereas similarity document one document four go previous slide see aa transpose term term similarity matrix similarity term term similarity term two term three zero based contexts documents appear terms appear lets wrap section discussed technique dimensionality reduction called latent semantic indexing lsi papers called lsa latent semantic analysis means pretty much thing dimensionality reduction used identify hidden latent concepts textual spaces used variety nlp tasks information retrieval tasks including limited query matching latent space two external pointers latent semantic indexing prefer latent semantic analysis two sites active field colorado university tennessee knoxville concludes sections text similarity using singular value decomposition dimensionality reduction next topic going text similarity using text kernels,Course3,W3-S1-L4,W3,S1,L4,-,3,1,4,next topic text similar call dimension reduct motiv behind dimension reduct document often mani differ topic rel small number topic certainli smaller number word appear somehow collaps word semant categori may abl find better similar measur exampl collaps document patient doctor hospit one place may consid semant similar even though contain differ word set simpler vector approach similar look far problem mani case there polysemi that similar actual similar word smaller cosin similar would make believ includ word multipl sentenc bar bank jaguar hot opposit due synonymi that actual similar word larger cosin similar would make believ word like build synonym edific theyr differ word therefor theyr go differ context even though differ context mean want somehow know synonym case overestim cosin similar matrix word sentenc gener spars need process dimension reduct find hidden semant dimens let look exampl natur languag process literatur exampl left give multipl choic question toefl test your given word specif levi your ask word similar choic impos b believ c request correl answer similar word levi impos kind semant relationship want discov text letter semant analysi techniqu go introduc today go help us techniqu also use identifi similar analog exampl sat test your given pair mason stone your ask follow five choic repres similar relationship one mason stone answer teacher chalk b carpent wood c soldier gun photograph camera e book work correct answer carpent wood like mason person work stone carpent person work wood analog differ turn problem analog similar also resolv dimension reduct techniqu lot work actual done peter turn paper last ten year let’ consid dimension reduct detail purpos method look hidden similar data base matrix decomposit im go introduc give exampl high school peopl measur height weight student school scatter plot left show differ student measur xaxi repres let say height yaxi repres weight find regress line explain data best that shown middl red line appear diagon turn third variabl addit height weight explain differ height weight differ student exactli regress line show line correspond dimens age turn sampl student high school student age student across differ class age group obvious student lower grade lighter shorter student upper grade collaps point diagon axi repres regress line see there actual nice trend student older taller heavier younger student process lose much inform turn replac height weight age gain inform appear data set left everyth differ two exampl tell us particular student differ trend person tall age somebodi who short age reduc dimension data set two dimens one dimens done practic well let go back littl bit linear algebra need rememb vector matric work order understand dimension reduct matrix x n tabl object case object number row also column matrix vector matric compat dimens there special definit compat context matric compat dimens multipli togeth dont rememb kind math go visit websit explain multipli matric come back tri multipli two matric exampl next slide im go show answer way matric multipli simpl multipli valu first row valu first column add result goe cell correspond first row first column product first row product x x x total second row x x x final x x x equal product two matric vector one import concept linear algebra relat dimension reduct eignenvector eigenvalu eigenvector implicit direct matrix multipli vector v eigenvector righthand side matrix go obtain result multipli v lambda scalar like eigenvalu principl eigenvalu lambda complex number exampl go look alway real comput eigenvalu matrix need use follow equat need find determin matrix minu lambda unit matrix size matrix want squar matrix obvious comput determin want set determin equal let look exampl matrix minu minu lambda matrix shown right minu minu lambda first row follow minu lambda second row want comput determin matrix need find product forward diagon subtract valu product number backward diagon give us lambdalambda whole thing x solv quadrat equat lambda lambda squar go see two root lambda lambda pick one eigenvalu replac equat top go get new matrix multipli eigenvector x x want result equal well solv system equat find answer xx two dimension vector x coordin would satisfi second equat matrix squar decompos u lambda u invers u matrix eigenvector lambda diagon matrix eigenvalu differ transform mathemat equival sigma u u lambda u invers sigma u lambda sigma u lambda u invers here exampl origin matrix ad valu lambda lambda decomposit go get u equal matrix invers u equal matrix onehalf onehalf follow onehalf onehalf verifi inde cover origin matrix multipli u lambda u invers multipl sequenc obtain origin matrix happen abl convert origin matrix new space old eigen vector point origin space go repres point new represent case weight height would new dimens correspond age anoth dimens correspond deviat certain person given weight base trend line defin age matrix squar use differ techniqu call singular valu decomposit care non squar matric well nlp inform retriev task matric either document term word context featur matric definit necessarili squar exampl vocabulari size million set million document case go matrix million million clearli squar case use anoth techniqu call singular valu decomposit case matrix repres product u sigma v transpos u matrix orthogon eigenvector pole aa transpos v matrix orthogon eigenvector transpos compon sigma eigenvalu transpos decomposit exist matric whether theyr dens spars estim dimension differ matric exampl column row matrix u orthogon eigenvector transpos x matrix v matrix orthogon eigenvector transpos dimension x matlab octav use simpl function svd give matrix input return tupl consist u v match exactli u sigma v exampl let look specif exampl collect seven document nine term look term appear document exampl document contain term document contain term also look post list repres data form bipartit graph bipartit graph two compon one left one right exampl correspond differ type object left hand side left hand mode correspond document right hand side correspond term exampl connect dont connect okay let see comput singular bodi composit arbitrari matrix first reppres whole form like exampl left one certain term appear certain document zero mean absent document need normal matrix divid valu length column vector part mean first column two one therefor length vector correspond first column go squar root divid valu column squar root go get nd column vector therefor length go squar root divid squar root go get see techniqu use comput normal valu normal matrix comput singular valu decomposit enter matrix matlab favorit softwar run svd librari go get someth like u go go x matrix v go x matrix go x matrix respond spread differ axe data first dimens spread import dimens lower dimension represent second valu singular valu appear sigma matrix one thing reconstitut origin matrix multipli u sigma v transpos also produc differ version specif star lower dimension version instead multipli u sigma v transpos multipli u sigma star v transpos sigma star keep largest singular valu origin sigma matrix go back previou slide essenti delet th th line mayb without lose much inform store matrix keep four signific dimens correspond four largest valu sigma rank approxim sigma see zero origin matrix multipli u sigma v transpos go get differ represent go begin significantli differ comput represent term document new semant space done appropri multipl matric exampl u time time v prime give us represent document term respect new semant space also take step comput rank approxim origin matrix preserv two largest valu sigma matrix oper comput new valu rank approxim rank approxim use u multipli get word vector present concept space two concept also use time v transpos find new concept represent document case two dimens here slide summar entir singular valu decomposit method document shown topleft term topright new semant space see two cluster document term im go focu one bottomleft area screen see appear near also appear near essenti kill two bird one stone fir found latent semant similar term also similar document also abl achiev someth couldnt origin space name find document six seven similar term three seven new concept space look smallest circl purpl see closest match entir cluster element quadrant includ also semant connect actual someth understand better look origin exampl specif graph represent see one document appear purpl cluster repres fact similar bit lot common term continu expand recurs see next small similar document follow exactli intuit get look graph represent right patienc formula math lab allow seek translat individu document term concept multipli either entir column vector entir row vector singular valu matrix question document term matrix aa transpos transpos let start aa transpos see aa transpos matrix surpris given origin matrix multipli transpos therefor result similarli transpos matrix seven seven surpris multipli matrix matrix dimensional two product give hint mean turn transpos document document similar matrix exampl tell similar document document wherea similar document one document four go previou slide see aa transpos term term similar matrix similar term term similar term two term three zero base context document appear term appear let wrap section discuss techniqu dimension reduct call latent semant index lsi paper call lsa latent semant analysi mean pretti much thing dimension reduct use identifi hidden latent concept textual space use varieti nlp task inform retriev task includ limit queri match latent space two extern pointer latent semant index prefer latent semant analysi two site activ field colorado univers tennesse knoxvil conclud section text similar use singular valu decomposit dimension reduct next topic go text similar use text kernel,[ 2  4 14 13 12]
261,Course3_W3-S1-L5_-_NLP_Tasks_1-3_-_16_slides_14-45,todays segment going talk long range tasks form core research development natural language processing simplest probably fundamental task called partofspeech tag understand sentence need understand part speech individual word lets look example swimmer getting ready run final race words obvious partofspeech example clearly article clearly verb getting clearly verb well ready adjective however words ambiguous lets look two first one word run context pretty obvious run noun verb preceded particle cases word run could preceded terminal considered noun word final noun adjective well obvious case final could noun final race looks like final adjective finally word race verb noun well could either one general example structure sentence follows article followed adjective much likely noun partofspeech tagging using rules like statistics understand parts speech individual words sentence run would label run noun run would label verb next task natural language processing parsing parsing takes sentence input produces syntactic representation lets look simple sentences english later going discuss context free grammar parsing detail want give inclusion parsing sentences subject myriam verb sentences subject verb verbs however different first verb slept know intransitive verb doesnt take direct object myriam wrote novel instance transitive verb wrote take direct object myriam gave sally flowers example ditransitive verb give takes case two nouns arguments give something somebody example give somebody something two nouns without preposition form examples showed earlier myriam ate pizza would different sorts prepositional phrases olives sally attached either newest noun pizza like case olive attached verb ate sally remorse parsing usually deals either constituent structure often phrase structure grammar like one im going describe known dependency grammar something going look minutes phrase structure grammar looks like two parts first part one left hand side rules nonterminals right hand side known lexicon rules terminals words lets see interpret rules left hand side want generate sentence want parse string sentence would look noun phrase followed verb phrase similarly noun phrase one two things determiner followed noun noun phrase followed prepositional phrase examples determiner would something like cat second one np goes nppp would something like eat pizza olives pizza olives would noun phrase olives prepositional phrase verb either vbd past tense verb could past tense verb followed noun phrase thats case transitive verb past tense verb followed two noun phrases also prepositional phrase case sally ate pizza pleasure case pleasure modifies verb pp stands prepositional phrase first tag prepositional phrase preposition labelled prp cases pr lets look lexicon particular grammar determiner noun child window car past tense verb either found ate saw finally three prepositions want produce parse tree sentence build entire representation like child saw car window sentence consists noun phrase verb phrase noun phrase child verb phrase everything else sentence noun phrase consists determinant noun turn get translated words child verb phrase turns vbd followed noun phrase prepositional phrase continue inference process little bit fill gaps complete sentence one example external tool use parsing wellknown stanford parser many others stanford parser comes nice demo go url type sentence get output parse tree well part speech tagged sequence words sentence see example output parser looks like example sentence left housing starts comma number new homes built comma continues right rose march annual rate units comma revised february comma commerce department set see side fairly complicated yet parse problem figuring internal structure lets look special things happen example commas punctuation labeled separate syntactic units separate parts speech things like adverbial phrases like revised february embedded clauses deep recursion many interesting phenomena get section parsing well see parsers work build likely parse given initial sentence lets switch interesting problem parsing mostly fun naclo problem known problem pretty easy notation mean means say part sentence two slashes sentence makes sense add last part get completely different sentence modification shorter sentence actually means something completely different lets try say problem pretty im essentially saying like problem beautiful add extra word changes meaning sentence completely case pretty modifies easy im really saying problem easy easy kind phenomenon parsing known garden path sentence falling properties point sentence stop interpret sentence one way syntactically continue way end sentence youll get different parse tree example actually motivated commercial phone company years ago idea something like want tell phones sell service provide pretty reliable youre going cut middle sentence youre making phone call happen one competitors phones would happen person listening call would hear beginning sentence get different impression trying say heard entire sentence heres examples garden path sentences dont bother coming versus dont bother coming early get interrupted dont bother coming go place heard full sentence would hear dont bother coming early means still come come time funny examples take turkey oven five order like instructions give somebody whos home hear part theyre going take turkey five really meant turkey oven five four different thing wait five oclock chances itll overcooked heres another one got canned something dont want hear phone maybe full sentence got canned peaches dinner clearly two sentences difference syntactic structures different semantic interpretations examples americans need buy house okay maybe yes maybe thats really intended intended americans need buy house lot money build parse tree two sentences realize different think examples essentially topic naclo problem subject line slide solutions available web naclo site theres lot examples however want emphasize criteria use judge criteria following bar two slashes complete sentence full sentence different meaning part slashes part slashes already ambiguous criteria met get good solution problem kind parsing common days called dependency parsing dependency parsing much interesting noun phrases verb phrases interested relationships words sentence without explicit constituent structure basic example shown sentence mary likes yellow apples dependency presentation shown always form tree rooted predicate sentence important word sentence considered main verb case likes thats top slide sentence mary likes yellow apples two words arguments verb likes liking event somebody liking somebody quote unquote receives liking recipient liking two slots center filled mary apples mary liker apples quote unquote liked terminology used linguistics finally still one word left represent yellow yellow modify likes modify mary thing modifies apples therefore draw part dependency tree child node apples dependency parsing start sentence produce dependency structure output something come handy heres example paper biological natural language processing case sentence converted dependency tree rules used dependency tree determine whether interaction two particular proteins sentence dependency structure made possible understand proteins related let us look sample output dependency parser see every line connects two words words numbered one guess last word sentence look pairs numbers dependency see construct entire dependency structure sentence need look number appears root sentence children know node second tier node course build entire dependency tree let figure entire tree homework concludes first part section nlp tasks includes morphology partofspeech tagging parsing going continue later additional nlp tasks,Course3,W3-S1-L5,W3,S1,L5,-,3,1,5,today segment go talk long rang task form core research develop natur languag process simplest probabl fundament task call partofspeech tag understand sentenc need understand part speech individu word let look exampl swimmer get readi run final race word obviou partofspeech exampl clearli articl clearli verb get clearli verb well readi adject howev word ambigu let look two first one word run context pretti obviou run noun verb preced particl case word run could preced termin consid noun word final noun adject well obviou case final could noun final race look like final adject final word race verb noun well could either one gener exampl structur sentenc follow articl follow adject much like noun partofspeech tag use rule like statist understand part speech individu word sentenc run would label run noun run would label verb next task natur languag process pars pars take sentenc input produc syntact represent let look simpl sentenc english later go discuss context free grammar pars detail want give inclus pars sentenc subject myriam verb sentenc subject verb verb howev differ first verb slept know intransit verb doesnt take direct object myriam wrote novel instanc transit verb wrote take direct object myriam gave salli flower exampl ditransit verb give take case two noun argument give someth somebodi exampl give somebodi someth two noun without preposit form exampl show earlier myriam ate pizza would differ sort preposit phrase oliv salli attach either newest noun pizza like case oliv attach verb ate salli remors pars usual deal either constitu structur often phrase structur grammar like one im go describ known depend grammar someth go look minut phrase structur grammar look like two part first part one left hand side rule nontermin right hand side known lexicon rule termin word let see interpret rule left hand side want gener sentenc want pars string sentenc would look noun phrase follow verb phrase similarli noun phrase one two thing determin follow noun noun phrase follow preposit phrase exampl determin would someth like cat second one np goe nppp would someth like eat pizza oliv pizza oliv would noun phrase oliv preposit phrase verb either vbd past tens verb could past tens verb follow noun phrase that case transit verb past tens verb follow two noun phrase also preposit phrase case salli ate pizza pleasur case pleasur modifi verb pp stand preposit phrase first tag preposit phrase preposit label prp case pr let look lexicon particular grammar determin noun child window car past tens verb either found ate saw final three preposit want produc pars tree sentenc build entir represent like child saw car window sentenc consist noun phrase verb phrase noun phrase child verb phrase everyth els sentenc noun phrase consist determin noun turn get translat word child verb phrase turn vbd follow noun phrase preposit phrase continu infer process littl bit fill gap complet sentenc one exampl extern tool use pars wellknown stanford parser mani other stanford parser come nice demo go url type sentenc get output pars tree well part speech tag sequenc word sentenc see exampl output parser look like exampl sentenc left hous start comma number new home built comma continu right rose march annual rate unit comma revis februari comma commerc depart set see side fairli complic yet pars problem figur intern structur let look special thing happen exampl comma punctuat label separ syntact unit separ part speech thing like adverbi phrase like revis februari embed claus deep recurs mani interest phenomena get section pars well see parser work build like pars given initi sentenc let switch interest problem pars mostli fun naclo problem known problem pretti easi notat mean mean say part sentenc two slash sentenc make sens add last part get complet differ sentenc modif shorter sentenc actual mean someth complet differ let tri say problem pretti im essenti say like problem beauti add extra word chang mean sentenc complet case pretti modifi easi im realli say problem easi easi kind phenomenon pars known garden path sentenc fall properti point sentenc stop interpret sentenc one way syntact continu way end sentenc youll get differ pars tree exampl actual motiv commerci phone compani year ago idea someth like want tell phone sell servic provid pretti reliabl your go cut middl sentenc your make phone call happen one competitor phone would happen person listen call would hear begin sentenc get differ impress tri say heard entir sentenc here exampl garden path sentenc dont bother come versu dont bother come earli get interrupt dont bother come go place heard full sentenc would hear dont bother come earli mean still come come time funni exampl take turkey oven five order like instruct give somebodi who home hear part theyr go take turkey five realli meant turkey oven five four differ thing wait five oclock chanc itll overcook here anoth one got can someth dont want hear phone mayb full sentenc got can peach dinner clearli two sentenc differ syntact structur differ semant interpret exampl american need buy hous okay mayb ye mayb that realli intend intend american need buy hous lot money build pars tree two sentenc realiz differ think exampl essenti topic naclo problem subject line slide solut avail web naclo site there lot exampl howev want emphas criteria use judg criteria follow bar two slash complet sentenc full sentenc differ mean part slash part slash alreadi ambigu criteria met get good solut problem kind pars common day call depend pars depend pars much interest noun phrase verb phrase interest relationship word sentenc without explicit constitu structur basic exampl shown sentenc mari like yellow appl depend present shown alway form tree root predic sentenc import word sentenc consid main verb case like that top slide sentenc mari like yellow appl two word argument verb like like event somebodi like somebodi quot unquot receiv like recipi like two slot center fill mari appl mari liker appl quot unquot like terminolog use linguist final still one word left repres yellow yellow modifi like modifi mari thing modifi appl therefor draw part depend tree child node appl depend pars start sentenc produc depend structur output someth come handi here exampl paper biolog natur languag process case sentenc convert depend tree rule use depend tree determin whether interact two particular protein sentenc depend structur made possibl understand protein relat let us look sampl output depend parser see everi line connect two word word number one guess last word sentenc look pair number depend see construct entir depend structur sentenc need look number appear root sentenc children know node second tier node cours build entir depend tree let figur entir tree homework conclud first part section nlp task includ morpholog partofspeech tag pars go continu later addit nlp task,[ 0  4 14  1 13]
262,Course3_W3-S1-L6_-_NLP_Tasks_2-3_-_20_slides_15-33,looked interesting nlp tasks already things like part speech tagging parsing going look others lets start information extraction information extraction task reading sentence extracting named entities people places organizations also relationships example ceo company let run specific example suppose want build database companies different rating agencies see first example wells fargo major bank cuts ppd inc market perform market perform rating company useful information investment people ppd incorporated name company cuts actual action taking place wells fargo lowering rating company next level lets look examples china southern air upgraded overweight neutral according hsbc bank shown red hsbc upgraded means change rating positive direction company company china southern air new rating overweight old rating neutral first time old rating look examples well get idea works ratings institution typically bank changes rating company rating rating see kind concept expressed many many different ways see colors move place also things like upper case lower case missing fields example recent example baird cuts kior incorporated underperform rating know new rating dont know old rating goal information extraction system would read sentences like understand different players understand different named entities different ratings able represent whole information form table later used people understand databases whatever decisions need make output information extraction would look like relation takes case seven arguments date time ticker stock ticker company company name source upgrade downgrade old rating new rating direction change direction change explicitly listed previous sentences inferred verb like cut upgrade okay another task natural language processing semantics semantics analysis semantics deals logical representations sentences first order logic also used represent inference example say x mother means x parent semantic analysis one hottest areas natural language processing days interesting see far go near future heres relevant problem semantics naclo called bertrand russell problem ben king download website youre done solving check solution solution bertrand russell problem shown another natural language processing task reading comprehension heres example paper anand et al task read document one shown titled mars polar lander couple short paragraphs followed reading comprehension questions example mars spacecraft supposed touch mars global surveyor goal kind research build systems ask us questions like understanding meaning paragraphs another example text understanding sort word puzzle used appear jury tests many years ago four bungalows culdesac theyre made four different materials constraints like mr scotts bungalow somewhere left wooden one third one along brick forth answer questions like lives bungalow made obviously takes sophisticated natural language processing able solve puzzles like another interesting task natural language processing word sense disambiguation said earlier words may multiple senses theyre used particular sentence figure sense intended see sentence like thieves took gold bars word bar mean pretty obvious human means pieces gold chunks gold computer doesnt know performs word sound disambiguation far concerned may stolen drinking establishments perhaps measures song task world sense disambiguation take word sentence look context determine senses wordnet dictionary meant remind words like bar ambiguous excerpt wordnet well discuss detail later shows different senses bar noun addition one mentioned also meaning legal association also mean prevent something happening blockade route word sense disambiguation important many different natural language tasks example machine translation often word ambiguous one language may translated differently target language lets look examples english word play ambiguous mean play sport paul plays soccer want translate sentence french would use au prepositionarticle combination indicate person playing sport translate differently playing musical instrument use different form structure use preposition de followed article la every time instrument use de every time sport use au case happens turned au lets look translation ambiguous word wall english german german basic translation word wall wand however case great wall china translation mauer die chinesische mauer need know advance particular sense wall english able translate sentence phrase properly german even spanish word wall translated many different ways depending whether internal wall external wall like wall separates several buildings next task natural language processing called named entity recognition named entity recognition sentence one shown left wolff currently journalist argentina played del bosque final years seventies real madrid want figure different named entities people wolff del bosque names organizations case soccer club real madrid also countries argentina output typical named entity recognition system shown right tells wolff person bper means beginning person next line comma label means something else means wolff single word person argentina labeled location single word del bosque labeled person labels named entity recognition system assigns individual words bper beginning person iper means inside person since next label know words part person finally real madrid organization first word labeled beginning organization second word inside organization stop interested topic detail cover later course meantime look two urls two online demos data entity recognition systems allow type entire sentences label output different named entities involved another demo system called abner developed university wisconsin specifically used named entity recognition biological domain see uses different colors indicate example names genes cells receptor proteins rna information extremely valuable want build system understand biomedical papers next task natural language processing called semantic role labeling turns verbs arguments required required arguments appear many different orders sentence verb accept multiple arguments important one acceptor person acceptance accepted thing thing accepted attribute also additional modifiers modality negation goal start sentence like wouldnt accept anything value writing recognize main verb accept identify words connected different arguments verb accept case acceptor word thing accepted anything value accepted writing talk semantic role labeling detail later semester meantime look online demo university illinois shown another task natural language processing core reference resolution core reference resolution understanding two phrases meant refer person entity typically used discourse structure avoid repetition first example barack obama visited china period could said barack obama met chinese counterpart would repetitive instead say us president met chinese counterpart mean us president refers barack obama thats im using color represent want build summarization system question answering system dont want think barack obama us president different people want two linked together ask met chinese counterpart would able say barack obama task coreference resolution identify expressions refer entity discourse link together actually tricky task addition pronouns typical example coreference resolution noun phrases like first example us president even complicated structures example cynthia went see aunt hospital period scheduled surgery monday pronoun could corefer either cynthia aunt well really use lot semantics word knowledge figure aunt scheduled surgery unlikely somebody whos scheduled surgery would go visit another person hospital would counterintuitive goal coreference resolution system would look ambiguous reference relate back aunt rather cynthia coreference two forms anaphoric cataphoric let explain two words means anaphoric means mention entity happens first another expression used refer back entity introduced previous example appears aunt cataphor cataphoric relation pronoun reference introduced first entity introduced happens less frequently aphora still exists computer systems understand deal example sick michael stayed home friday case michael entity refers used introducing entity cases tricky natural language processing systems deal many interesting aspects natural language processing addressed want build entire system example ellipses parallelism underspecification go together ellipsis certain word missing sentence implied understood context say chen speaks chinese period dont really mean chen speaks chinese dont speak chinese im using ellipsis skip speak chinese use parallelism two sentences understand words missing also use parallelism next example santa gave mary book johnny toy case infer santa gave johnny toy even though explicitly said sentence parallel structure end second section nlp tasks applications going continue next segment,Course3,W3-S1-L6,W3,S1,L6,-,3,1,6,look interest nlp task alreadi thing like part speech tag pars go look other let start inform extract inform extract task read sentenc extract name entiti peopl place organ also relationship exampl ceo compani let run specif exampl suppos want build databas compani differ rate agenc see first exampl well fargo major bank cut ppd inc market perform market perform rate compani use inform invest peopl ppd incorpor name compani cut actual action take place well fargo lower rate compani next level let look exampl china southern air upgrad overweight neutral accord hsbc bank shown red hsbc upgrad mean chang rate posit direct compani compani china southern air new rate overweight old rate neutral first time old rate look exampl well get idea work rate institut typic bank chang rate compani rate rate see kind concept express mani mani differ way see color move place also thing like upper case lower case miss field exampl recent exampl baird cut kior incorpor underperform rate know new rate dont know old rate goal inform extract system would read sentenc like understand differ player understand differ name entiti differ rate abl repres whole inform form tabl later use peopl understand databas whatev decis need make output inform extract would look like relat take case seven argument date time ticker stock ticker compani compani name sourc upgrad downgrad old rate new rate direct chang direct chang explicitli list previou sentenc infer verb like cut upgrad okay anoth task natur languag process semant semant analysi semant deal logic represent sentenc first order logic also use repres infer exampl say x mother mean x parent semant analysi one hottest area natur languag process day interest see far go near futur here relev problem semant naclo call bertrand russel problem ben king download websit your done solv check solut solut bertrand russel problem shown anoth natur languag process task read comprehens here exampl paper anand et al task read document one shown titl mar polar lander coupl short paragraph follow read comprehens question exampl mar spacecraft suppos touch mar global surveyor goal kind research build system ask us question like understand mean paragraph anoth exampl text understand sort word puzzl use appear juri test mani year ago four bungalow culdesac theyr made four differ materi constraint like mr scott bungalow somewher left wooden one third one along brick forth answer question like live bungalow made obvious take sophist natur languag process abl solv puzzl like anoth interest task natur languag process word sens disambigu said earlier word may multipl sens theyr use particular sentenc figur sens intend see sentenc like thiev took gold bar word bar mean pretti obviou human mean piec gold chunk gold comput doesnt know perform word sound disambigu far concern may stolen drink establish perhap measur song task world sens disambigu take word sentenc look context determin sens wordnet dictionari meant remind word like bar ambigu excerpt wordnet well discuss detail later show differ sens bar noun addit one mention also mean legal associ also mean prevent someth happen blockad rout word sens disambigu import mani differ natur languag task exampl machin translat often word ambigu one languag may translat differ target languag let look exampl english word play ambigu mean play sport paul play soccer want translat sentenc french would use au prepositionarticl combin indic person play sport translat differ play music instrument use differ form structur use preposit de follow articl la everi time instrument use de everi time sport use au case happen turn au let look translat ambigu word wall english german german basic translat word wall wand howev case great wall china translat mauer die chinesisch mauer need know advanc particular sens wall english abl translat sentenc phrase properli german even spanish word wall translat mani differ way depend whether intern wall extern wall like wall separ sever build next task natur languag process call name entiti recognit name entiti recognit sentenc one shown left wolff current journalist argentina play del bosqu final year seventi real madrid want figur differ name entiti peopl wolff del bosqu name organ case soccer club real madrid also countri argentina output typic name entiti recognit system shown right tell wolff person bper mean begin person next line comma label mean someth els mean wolff singl word person argentina label locat singl word del bosqu label person label name entiti recognit system assign individu word bper begin person iper mean insid person sinc next label know word part person final real madrid organ first word label begin organ second word insid organ stop interest topic detail cover later cours meantim look two url two onlin demo data entiti recognit system allow type entir sentenc label output differ name entiti involv anoth demo system call abner develop univers wisconsin specif use name entiti recognit biolog domain see use differ color indic exampl name gene cell receptor protein rna inform extrem valuabl want build system understand biomed paper next task natur languag process call semant role label turn verb argument requir requir argument appear mani differ order sentenc verb accept multipl argument import one acceptor person accept accept thing thing accept attribut also addit modifi modal negat goal start sentenc like wouldnt accept anyth valu write recogn main verb accept identifi word connect differ argument verb accept case acceptor word thing accept anyth valu accept write talk semant role label detail later semest meantim look onlin demo univers illinoi shown anoth task natur languag process core refer resolut core refer resolut understand two phrase meant refer person entiti typic use discours structur avoid repetit first exampl barack obama visit china period could said barack obama met chines counterpart would repetit instead say us presid met chines counterpart mean us presid refer barack obama that im use color repres want build summar system question answer system dont want think barack obama us presid differ peopl want two link togeth ask met chines counterpart would abl say barack obama task corefer resolut identifi express refer entiti discours link togeth actual tricki task addit pronoun typic exampl corefer resolut noun phrase like first exampl us presid even complic structur exampl cynthia went see aunt hospit period schedul surgeri monday pronoun could coref either cynthia aunt well realli use lot semant word knowledg figur aunt schedul surgeri unlik somebodi who schedul surgeri would go visit anoth person hospit would counterintuit goal corefer resolut system would look ambigu refer relat back aunt rather cynthia corefer two form anaphor cataphor let explain two word mean anaphor mean mention entiti happen first anoth express use refer back entiti introduc previou exampl appear aunt cataphor cataphor relat pronoun refer introduc first entiti introduc happen less frequent aphora still exist comput system understand deal exampl sick michael stay home friday case michael entiti refer use introduc entiti case tricki natur languag process system deal mani interest aspect natur languag process address want build entir system exampl ellips parallel underspecif go togeth ellipsi certain word miss sentenc impli understood context say chen speak chines period dont realli mean chen speak chines dont speak chines im use ellipsi skip speak chines use parallel two sentenc understand word miss also use parallel next exampl santa gave mari book johnni toy case infer santa gave johnni toy even though explicitli said sentenc parallel structur end second section nlp task applic go continu next segment,[13  4  8  0  9]
263,Course3_W3-S1-L7_-_NLP_Tasks_3-3_-_21_slides_16-21,okay continuing another set interesting applications next one question answering lets look example jeopardy game get questions form answers get something like antagonist stevensons treasure island youre supposed answer long john silver essentially asking antagonist stevensons treasure island long john silver answer one examples actually used game ibms watson system played tv give example powerful system powered racks ibm power servers terabytes ram cores operates teraflops kind system used play jeopardy back okay questions used jeopardy example december national newspaper raised newstand price answer usa today watsons answers correct incorrect including one really funny one would like bring really quickly category us cities question something like city two airports one named world war ii hero one named world war ii battle well correct answer chicago watson goofed answered toronto everybody knows toronto us city thing question two airports actually named world war ii heroes world war ii battles even united states case watson made mistake understanding category explicitly meant refer us cities still pretty well twoday winning streak two human contestants previously major players champions jeopardy much less youre interested system well talk question answering detail later semester also read lot online another task natural language processing sentiment analysis many websites people enter reviews products properties amazon reviews another website called epinions reviews picked example somebody saying thought camera bought statements camera like camera positive statement says later camera adjust images cropping ipad better yet better yet another positive thing camera theres sentence quality images specifically camera something specific also quite impressed quality images case example positive sentiment towards specific aspect object theyre describing goal sentiment analysis recognize object discussed understand sentiment whether positive negative challenges comments may object property object example quality images maybe screen size maybe zoom one hottest areas natural language processing machine translation imagine got email friend japan wrote text foreign happy new year dont know means easily copy paste text tool like google translate get back answer happy new year exist commercial systems machine translation google translate also several open source systems popular days moses heres small demo moses available statmtorg website illustrate machine translation works ran short document google translate im showing original text output translation english french text relatively difficult lot uncommon words yet google translate good job lets look examples highlighted boldface right hand side elephants social animals correctly translated foreign second paragraph starts long sentence translated accurately french even though sophisticated structure another example correctly translated sentence shown end foreign difficult sentence translate yet google translated perfectly however everything smooth also cases relatively bad job different reasons let look examples explain theyre problematic google translate arrived lets look second sentence first paragraph live families give hugs call using trunks trumpets english sentence subject elephant theres three verbs live families give hugs pull connected conjunction well french translation correctly translated example vive live third person plural matches however two verbs translated correctly translated fair appeler infinitive forms conjugated properly third person plural reason happening english give call forms third person plural also infinitive second paragraph highlighted words english sentence says elephants comma big brains survival savvy may among smartest animals planet translation french says foreign foreign singular one elephant expression maybe translated correctly translated french foreign must conjugated verb third highlighted text science news translated foreign probably correct translation usually people dont want translate names magazines like later theres example everyone translated foreign actually good translation theres mistake however following sentence plotnik est un psychologue comparative comparative french feminine form adjective person male adjective supposed used compartif f end singular finally last example foreign really mistranslation mislead person reading document original english text talks comparative psychologists talk study compares different psychologists clearly correct translation next slide shows general way machine translation systems work use known noisy channel model noisy channel model observe text foreign language labeled f example thats middle portion pipeline see little letter f f assumed generated english sentence encoder e foreign language observe f build decoder would translate back foreign language english end get sentence e prime may may correct sentence e originally translated foreign language way statistical machine translation works tries come possible sentences english e prime satisfy two criteria grammatical english also faithful foreign sentence faithful mean words english sentences somehow related words foreign translation well talk machine translation much detail towards end course okay another task natural language processing text summarization text summarization two different forms one case single document want produce short version document example display mobile device process text speech system dont time read entire document theres enough space show second example text summarization known multidocument summarization multidocument summarization input series connected documents example different news stories event summary contain information appears consensus also many cases focus differences input documents example single document summarization system need warn output youll see next slide actually produced existing text summarization system something would want see doesnt exist yet whats input talks health benefits diet rich vegetables explains eating vegetables healthy sort nutrients appear vegetables gives us sort specific examples ultimate summary want get im warning output existing system something like eating vegetables healthy well see later summarization systems work one first summarization systems used internet used news essence developed university michigan around since stopped existing many systems like available including systems columbia google yahoo many places another application natural language processing text speech im showing link external website play allows type arbitrary text specify rendered example male female voice native speaker english hispanic speaker english possibly different dialect different nationality whether want person sound angry happy definitely click link play website lot companies days produce text speech software im going mention one really quickly many others well one encourage take look another interesting task actual language processing paraphrasing entailments may remember paraphrasing different ways express exact concepts entailment decide difference im showing examples paper recent challenge recognizing entailment lets look fourth example lefthand side piece text says google files long awaited ipo one righthand side says google goes public one left called text one right called hypothesis question infer hypothesis text case answer yes company files ipo means going public even though goes public used original sentence something entailed original sentence lets look example number three says regaan attended ceremony washington commemorate landings normandy ceremony took place washington assumedly washington dc capital united states landings normandy end world war ii hypothesis washington located normandy well silly natural language processing system may make inference wouldnt correct washington located united states normandy located france particular case entailment text hypothesis actually false okay examples natural language tasks one dialogue systems lets look realistic scenario dialogue would like make reservation sorrento pm friday night availability pm pm sorry dont work difficult dialogue user either human makes reservations restaurant dialogue system computer program would try understand humans questions answer appropriately many applications im going enumerate well talk detail class based papers find website publication venues natural language processing things like spelling correction web search natural language interface databases example im looking employees make recently relocated state another state something easily express sql query want express natural language sentence natural language interface translate sql query examples natural language applications include parsing job postings example postings look people certain level experience certain state summarizing medical records example many patients undergone treatment hospital want look medical records extract information results experiments information extraction databases social network extraction text essay grading generating weather reports sports reports news stories exist many different forms commercial world research world concludes section different nlp applications next segment going look topic competition,Course3,W3-S1-L7,W3,S1,L7,-,3,1,7,okay continu anoth set interest applic next one question answer let look exampl jeopardi game get question form answer get someth like antagonist stevenson treasur island your suppos answer long john silver essenti ask antagonist stevenson treasur island long john silver answer one exampl actual use game ibm watson system play tv give exampl power system power rack ibm power server terabyt ram core oper teraflop kind system use play jeopardi back okay question use jeopardi exampl decemb nation newspap rais newstand price answer usa today watson answer correct incorrect includ one realli funni one would like bring realli quickli categori us citi question someth like citi two airport one name world war ii hero one name world war ii battl well correct answer chicago watson goof answer toronto everybodi know toronto us citi thing question two airport actual name world war ii hero world war ii battl even unit state case watson made mistak understand categori explicitli meant refer us citi still pretti well twoday win streak two human contest previous major player champion jeopardi much less your interest system well talk question answer detail later semest also read lot onlin anoth task natur languag process sentiment analysi mani websit peopl enter review product properti amazon review anoth websit call epinion review pick exampl somebodi say thought camera bought statement camera like camera posit statement say later camera adjust imag crop ipad better yet better yet anoth posit thing camera there sentenc qualiti imag specif camera someth specif also quit impress qualiti imag case exampl posit sentiment toward specif aspect object theyr describ goal sentiment analysi recogn object discuss understand sentiment whether posit neg challeng comment may object properti object exampl qualiti imag mayb screen size mayb zoom one hottest area natur languag process machin translat imagin got email friend japan wrote text foreign happi new year dont know mean easili copi past text tool like googl translat get back answer happi new year exist commerci system machin translat googl translat also sever open sourc system popular day mose here small demo mose avail statmtorg websit illustr machin translat work ran short document googl translat im show origin text output translat english french text rel difficult lot uncommon word yet googl translat good job let look exampl highlight boldfac right hand side eleph social anim correctli translat foreign second paragraph start long sentenc translat accur french even though sophist structur anoth exampl correctli translat sentenc shown end foreign difficult sentenc translat yet googl translat perfectli howev everyth smooth also case rel bad job differ reason let look exampl explain theyr problemat googl translat arriv let look second sentenc first paragraph live famili give hug call use trunk trumpet english sentenc subject eleph there three verb live famili give hug pull connect conjunct well french translat correctli translat exampl vive live third person plural match howev two verb translat correctli translat fair appel infinit form conjug properli third person plural reason happen english give call form third person plural also infinit second paragraph highlight word english sentenc say eleph comma big brain surviv savvi may among smartest anim planet translat french say foreign foreign singular one eleph express mayb translat correctli translat french foreign must conjug verb third highlight text scienc news translat foreign probabl correct translat usual peopl dont want translat name magazin like later there exampl everyon translat foreign actual good translat there mistak howev follow sentenc plotnik est un psychologu compar compar french feminin form adject person male adject suppos use compartif f end singular final last exampl foreign realli mistransl mislead person read document origin english text talk compar psychologist talk studi compar differ psychologist clearli correct translat next slide show gener way machin translat system work use known noisi channel model noisi channel model observ text foreign languag label f exampl that middl portion pipelin see littl letter f f assum gener english sentenc encod e foreign languag observ f build decod would translat back foreign languag english end get sentenc e prime may may correct sentenc e origin translat foreign languag way statist machin translat work tri come possibl sentenc english e prime satisfi two criteria grammat english also faith foreign sentenc faith mean word english sentenc somehow relat word foreign translat well talk machin translat much detail toward end cours okay anoth task natur languag process text summar text summar two differ form one case singl document want produc short version document exampl display mobil devic process text speech system dont time read entir document there enough space show second exampl text summar known multidocu summar multidocu summar input seri connect document exampl differ news stori event summari contain inform appear consensu also mani case focu differ input document exampl singl document summar system need warn output youll see next slide actual produc exist text summar system someth would want see doesnt exist yet what input talk health benefit diet rich veget explain eat veget healthi sort nutrient appear veget give us sort specif exampl ultim summari want get im warn output exist system someth like eat veget healthi well see later summar system work one first summar system use internet use news essenc develop univers michigan around sinc stop exist mani system like avail includ system columbia googl yahoo mani place anoth applic natur languag process text speech im show link extern websit play allow type arbitrari text specifi render exampl male femal voic nativ speaker english hispan speaker english possibl differ dialect differ nation whether want person sound angri happi definit click link play websit lot compani day produc text speech softwar im go mention one realli quickli mani other well one encourag take look anoth interest task actual languag process paraphras entail may rememb paraphras differ way express exact concept entail decid differ im show exampl paper recent challeng recogn entail let look fourth exampl lefthand side piec text say googl file long await ipo one righthand side say googl goe public one left call text one right call hypothesi question infer hypothesi text case answer ye compani file ipo mean go public even though goe public use origin sentenc someth entail origin sentenc let look exampl number three say regaan attend ceremoni washington commemor land normandi ceremoni took place washington assumedli washington dc capit unit state land normandi end world war ii hypothesi washington locat normandi well silli natur languag process system may make infer wouldnt correct washington locat unit state normandi locat franc particular case entail text hypothesi actual fals okay exampl natur languag task one dialogu system let look realist scenario dialogu would like make reserv sorrento pm friday night avail pm pm sorri dont work difficult dialogu user either human make reserv restaur dialogu system comput program would tri understand human question answer appropri mani applic im go enumer well talk detail class base paper find websit public venu natur languag process thing like spell correct web search natur languag interfac databas exampl im look employe make recent reloc state anoth state someth easili express sql queri want express natur languag sentenc natur languag interfac translat sql queri exampl natur languag applic includ pars job post exampl post look peopl certain level experi certain state summar medic record exampl mani patient undergon treatment hospit want look medic record extract inform result experi inform extract databas social network extract text essay grade gener weather report sport report news stori exist mani differ form commerci world research world conclud section differ nlp applic next segment go look topic competit,[8 9 4 2 6]
264,Course3_W4-S1-L1_-_Syntax_-_30_slides_31-02,okay welcome back course natural language processing today going start section syntax im going introduce grammars syntax next two segments one fundamental questions linguistics whether language bag words know rearrange words sentence going get time sense syntax turns mean grammatical rules exists apply categories groups works individual words example sentence include subject predicate subject typically noun phrase predicate verb phrase heres examples noun phrases cat samantha examples verb phrases example arrived went away dinner combine non phrases verb phrases form syntactically correct sentence example say cat went away samantha dinner people learn new word use sentence learn syntactic role example tell work noun immediately create sentences see two works dont like words doubted cluvious adjective immediately create sentence says something like dont like cluvious people im saying sentences make sense theyre least syntactically correct actually wanted come examples artificial words slide wug actually created gleason many years ago example cluvious comes program designed maclo years ago tried quite words however turns things came even though didnt sound like words english turned exist dictionaries slang many cases meant bad things dont want repeat front lets see define parts speech syntactic point view first thing nouns common type words common one properties nouns typically preceded word say cat house theyre verbs well verbs also different properties one cannot preceded verb like nouns preceded cant example sleep verb preceded word cant say cant sleep adjectives well adjectives words come noun mean appropriate adjectives one holds say clueless rug well slightly different high school definitions grade school definitions youre familiar case nouns defined things concepts ideas verbs actions syntactic point view interested way words form sentences rather meaning individual words similarly define categories parts speech example determiners prepositions seen one fundamental concepts syntax called constituent constituents continuous pieces text theyre noncrossing two constituents share word one two constituents must completely contain one word constituent rule constituents set sequence words part constituent replaced member constituent still form grammatically correct sentence example samantha cat considered noun phrases one simplest types constituents many types constituent tests find linguistics literature im going run give examples first one coordination test need warn tests used examples violated principal two words pass multiple tests chances high two words phrases guess belong constituent time coordination test tells us combine two constituents conjunction means theyre type also pronoun test replace small dog whole unit sentence word means unit constituent theres also interesting constituent test called question repetition suppose sentence seen blue elephants want find constituents sentence example sequence word seen blue constituent sequence words blue elephants constituent well constituent test going tell us answer lets try little dialogue seen blue elephants blue elephants well sounds like meaningful dialogue therefore blue elephants constituent imagine second dialogue seen blue elephants seen blue well doesnt look like plausible dialogue seen blue therefore constituent finally third dialogue would seen blue elephants seen blue elephants sounds actually fairly grammatical therefore seen blue elephants also valid constituent theres also topicalization test allows check something constituent fronting beginning sentence instead saying seen blue elephants say blue elephants seen therefore blue elephants also constituent theres also question test replace phrase question word like also indication phrase constituent seen blue elephants seen meaning blue elephants many constituent tests im going go lot detail example deletion test semantic test finally simple intuition test something looks intuitive lack constituent probably one lets see use tree structure generate sentences simplest algorithm following context grammar im going explain slides start called start symbol grammar generate tree structure matches grammar fill leaf nodes tree terminal symbols individual words lets see build sentence like simplest possible sentence english perhaps one consists noun verb something like birds fly notation screen tells tried generate sentence arrow indicates order generate sentence first produce noun verb use birds noun fly verb means birds fly correct grammatical sentence according grammar simplest grammar transforms n v n either samantha min jorge v either left sang walked produce following sentences samantha sang jorge left jorge sang obviously grammar simple going generate two word sentences need expand far intransitive verbs verbs dont direct object one possible rule want add grammar next allow transitive verbs remember ones take direct objects example say something like jorge saw samantha samantha direct object verb saw also want able include determiners example say something like cats determiner well turns want add possible combinations constituents sentence youre going end severe case combinatorial explosion many rules combine together produce way many sentences many even correct english even though grammatical according grammar need come ways combine words constituents define syntactic rules terms constituents rather words example expand idea noun noun phrase thing verbs expand verb phrases verb phrases would include example case individual intransitive verbs walked transitive verbs ditransitive verbs verbs prepositional phrases words multiple prepositional phrases lets expand grammar little bit instead noun verb noun phrase verb phrase goes np followed vp np sequence determiner noun vp sequence verb noun phrase determiners marked dt r either word word nouns child cat dog verbs took saw liked scared chased grammar going produce noun phrases start determiner example wont allow us produce word samantha either subject object two sample sentences grammar generates one dog chased cat one child saw dog theres way lefthand side transformation rule generate multiple righthand sides instead writing rule multiple times write one rule lefthand side constituents arrow possible expansions righthand side separated vertical bar vertical bar indicates choice alternative example one rule proper nouns one common nouns way handle samantha proper noun pn case cats dt followed cn cn stands common noun grammar far grown little bit produce np vp however two rules noun phrases determiner common noun proper noun verb phrase includes verb noun phrase determiners common nouns proper nouns well verbs previous example grammar allows us produce sentences child scared jorge min took child many optional categories grammatical rules means example noun phrases adjectives noun phrases without adjectives order take account possible optional categories introduce another type notation parenthesis put something parenthesis means optional lets look examples nouns one observation make whenever n allowed sentence replace syntactically following sequences one determiner noun adjective noun determiner adjective noun example allow cats allow cats allow small cats allow small cats use notation alternatives noun phrase produces either n determiner n adjective n determiner adjective n use parentheses say noun phrase sequence consists optional determiner followed one optional adjective followed noun rule equivalent four different rules rule n rule dt n rule jj n rule dt jj n lets see verb phrases many types verb phrases saw earlier lecture contain intransitive verbs ran samantha ran sentence intransitive verb prepositional phrase example samantha ran park also sentence particle samantha ran away case away single preposition also transitive verbs samantha bought cookie transitive verb direct object prepositional phase samantha bought cookie john overall structure verb phrase going something like always start verb optional noun phrase direct object also optional preposition things like away optional noun phrase follows preposition p np follows means would entire prepositional phrase part verb phrase grammar generate following sentences samantha saw cat also generate jorge gave cat min prepositional phrase prepositional phrases lets look examples mary bought book john bookstore bookstore sells magazines bookstore main st sells magazines mary ran away mary ran hill sentences see every time noun prepositional phrase follows john followed bookstore bookstore followed main street thing applies verbs every time verb followed preposition like ran away also followed prepositional phrase ran hill order accommodate prepositional phrases allow new constituent called pp embedded either np vp part sentence rule simple wherever preposition allowed followed noun phrase example ran versus ran street noun phase contain number prepositional phrases two noun phases case transitive verb mary gave john book revise grammar take account prepositional phrases well far rules goes np vp followed rule noun phrases rule verb phrases rule prepositional phrases prepositional phrases preposition followed optional noun phrase first time see something really important language look rule number two see noun phrase generate prepositional phrase look rule number four see prepositional phrase generate noun phrase therefore first instance recursion grammar means apply rules two four arbitrarily long sequence produce extremely long sentences good moment stop second explain difference made linguists something called performance competence im going go much detail principle basic idea following even though context free grammar produce arbitrarily long sentences lets say thousands prepositional phrases embedded noun phrases embedded prepositional phrases doesnt mean human sentences long people understand long sentences spend lot time never going put use expect people put use usually limit four five prepositional phrases noun phrase anything would make sentence completely incomprehensible certainly say something like saw john yesterday park perhaps add prepositional phrases end never going see four five lets see problem prepositional phrase ambiguity alluded earlier actually described context grammar lets look sentence boy saw woman telescope sentence ambiguous two interpretations first one boy used telescope see woman second interpretation woman carrying telescope seen boy case real ambiguity interpretations make sense different contexts lets look grammar used generate sentence prepositional phrase produce preposition fold noun verb phrase include prepositional phrase noun phrase include prepositional phrase actually two different parse trees correspond sentence produce pp either part noun phrase part verb phrase well look problem later semester understand deal one additional symbol used context grammar kleene star noted parentheses header slide used denote sequence constituents example jj means sequence zero adjectives number seven ten turns english noun phrases multiple adjectives socalled premodifiers precede noun inside noun phrase example say thin blue line case thin precedes blue blue precedes line general structure noun phrase something like determiner like followed number adjectives followed noun lets look examples say big red house really cannot say red big house sounds really awkward thats weve marked star interesting think order adjectives put premodifier section noun phase allows certain sequences others turns english adjective ordering noun phrase depends semantics meanings individual words lot work subject want run exercise help understand process works exercise going look titles famous songs books well try see find pattern little red riding hood three little pigs three musketeers steadfast tin soldier french connection old macdonald five golden rings ancient mariner think meaning individual word figure theres certain combination meanings appear certain order saw previous slide use size color cannot use color size kind semantic categories adjectives examples little size red color riding something like purpose designation determiner thats adjective determiner always come adjectives old age golden material french nationality steadfast quality finally tin material try come possible combinations categories realize cases specific ordering semantic categories adjectives heres example may exceptions close possible happens sentences determiner adjectives noun theyre likely appear order determiner first noun last adjectives middle following order number strength size age shape color origin material purpose actually infer order looking examples previous slide looking partial order create lets look problem nested sentences nested sentence something like birds fly versus believe birds fly second example birds fly nested inside larger sentence examples dont recall whether took dog know mall still open example first sentence embedded sentence nested sentence took dog order accommodate nested sentences need revise grammar little bit new rule one verb phrase produces verb followed zero one two noun phrases optional sequence c conjunction followed entire new sentence instance recursion general rule goes np vp rule vp produce recursively alternate vp produce arbitrarily sentences kind conjunctions go position c well socalled subordinating conjunctions things like whether okay sequence subordinating conjunction sentence appear inside non phase turns case also means c sequence part subject sentence lets look example whether win elections remains seen predicate sentence remains seen subject phrase whether win elections even verify syntacticy remains seen whether win elections whether win elections remains seen confirms whether win elections subject sentence lets go back topic recursion generate verb phrases verb phrases generate noun phrases generate prepositional phrases prepositional phrases generate noun phrases essentially recursion allows us produce really long sentences mentioned earlier really cannot designate based grammar longest sentence english fact grammar allow us produce infinitely long sentences cases recursion appearing grammar example conjunctions noun phrases noun phrase replaced sequence noun phrase noun phrase example like apples oranges apples oranges combined known phrase also apples known phrase oranges known phrase also conjunctions prepositional phrases pp transformed sequence pp pv thing exactly verb phrases example like walking running see meta patterns emerge im going mention topic briefly really germane course however think important understand term xbar theory means im going introduce second lets look example sentence transformed noun phrase verb phrase sequence noun phrase verb phrase propositional phrase rules metapattern well metapattern cases sort phrase whether noun phrase verb phrase prepositional phrase xp collective name phrases phrases replaced something called specifier comes main part phrase followed x bar sometimes denoted x opposed like example another portion tell us x bar would use x followed complement everything x bar specifier everything complement example noun case noun phrase produces determiner followed n bar youre interested x bar theory detail websites devoted well lot literature linguistics community lets look metarules conjunctions looked earlier examples noun phrase noun phrase verb phrase verb phrase general case category x generates x x kind rule expanded cover even entire sentences produces example sunny today go park things add grammar well one category particular importance auxiliaries want see auxiliaries past constituent tests looked earlier one thing interested sequence auxiliary verb constituent example sentence seen blue elephants remember forever two sequences auxiliary verb see structure sentence passes conjunction test say seen blue elephants period remember forever also say seen blue elephants remember forever seen blue elephants parallel remember forever gives us indication form constituents cursive rule going something like verb phrase produces auxiliary followed verb phrase sequence many auxiliaries want left hand side verb say raj may sleeping case three applications time generating additional auxiliary recursion limited well turns limited auxiliaries verb phrase lets look exercise grammar slightly different one recently rules np vp pp well embedded phrase want look two sentences try generate descriptions sentences using rules grammar small dog neighbors brought old tennis ball first sentence obviously sentence therefore need start role righthand side going use np vp cp vp well dont think also setting two sentences try come exact set rules used generate three sentences youre done understood whole idea parsing parsing going talk next section detail process taking sentence grammar coming known parse tree less set rules used grammar produce sentence concludes introduction syntax next segment going look parsing detail,Course3,W4-S1-L1,W4,S1,L1,-,4,1,1,okay welcom back cours natur languag process today go start section syntax im go introduc grammar syntax next two segment one fundament question linguist whether languag bag word know rearrang word sentenc go get time sens syntax turn mean grammat rule exist appli categori group work individu word exampl sentenc includ subject predic subject typic noun phrase predic verb phrase here exampl noun phrase cat samantha exampl verb phrase exampl arriv went away dinner combin non phrase verb phrase form syntact correct sentenc exampl say cat went away samantha dinner peopl learn new word use sentenc learn syntact role exampl tell work noun immedi creat sentenc see two work dont like word doubt cluviou adject immedi creat sentenc say someth like dont like cluviou peopl im say sentenc make sens theyr least syntact correct actual want come exampl artifici word slide wug actual creat gleason mani year ago exampl cluviou come program design maclo year ago tri quit word howev turn thing came even though didnt sound like word english turn exist dictionari slang mani case meant bad thing dont want repeat front let see defin part speech syntact point view first thing noun common type word common one properti noun typic preced word say cat hous theyr verb well verb also differ properti one cannot preced verb like noun preced cant exampl sleep verb preced word cant say cant sleep adject well adject word come noun mean appropri adject one hold say clueless rug well slightli differ high school definit grade school definit your familiar case noun defin thing concept idea verb action syntact point view interest way word form sentenc rather mean individu word similarli defin categori part speech exampl determin preposit seen one fundament concept syntax call constitu constitu continu piec text theyr noncross two constitu share word one two constitu must complet contain one word constitu rule constitu set sequenc word part constitu replac member constitu still form grammat correct sentenc exampl samantha cat consid noun phrase one simplest type constitu mani type constitu test find linguist literatur im go run give exampl first one coordin test need warn test use exampl violat princip two word pass multipl test chanc high two word phrase guess belong constitu time coordin test tell us combin two constitu conjunct mean theyr type also pronoun test replac small dog whole unit sentenc word mean unit constitu there also interest constitu test call question repetit suppos sentenc seen blue eleph want find constitu sentenc exampl sequenc word seen blue constitu sequenc word blue eleph constitu well constitu test go tell us answer let tri littl dialogu seen blue eleph blue eleph well sound like meaning dialogu therefor blue eleph constitu imagin second dialogu seen blue eleph seen blue well doesnt look like plausibl dialogu seen blue therefor constitu final third dialogu would seen blue eleph seen blue eleph sound actual fairli grammat therefor seen blue eleph also valid constitu there also topic test allow check someth constitu front begin sentenc instead say seen blue eleph say blue eleph seen therefor blue eleph also constitu there also question test replac phrase question word like also indic phrase constitu seen blue eleph seen mean blue eleph mani constitu test im go go lot detail exampl delet test semant test final simpl intuit test someth look intuit lack constitu probabl one let see use tree structur gener sentenc simplest algorithm follow context grammar im go explain slide start call start symbol grammar gener tree structur match grammar fill leaf node tree termin symbol individu word let see build sentenc like simplest possibl sentenc english perhap one consist noun verb someth like bird fli notat screen tell tri gener sentenc arrow indic order gener sentenc first produc noun verb use bird noun fli verb mean bird fli correct grammat sentenc accord grammar simplest grammar transform n v n either samantha min jorg v either left sang walk produc follow sentenc samantha sang jorg left jorg sang obvious grammar simpl go gener two word sentenc need expand far intransit verb verb dont direct object one possibl rule want add grammar next allow transit verb rememb one take direct object exampl say someth like jorg saw samantha samantha direct object verb saw also want abl includ determin exampl say someth like cat determin well turn want add possibl combin constitu sentenc your go end sever case combinatori explos mani rule combin togeth produc way mani sentenc mani even correct english even though grammat accord grammar need come way combin word constitu defin syntact rule term constitu rather word exampl expand idea noun noun phrase thing verb expand verb phrase verb phrase would includ exampl case individu intransit verb walk transit verb ditransit verb verb preposit phrase word multipl preposit phrase let expand grammar littl bit instead noun verb noun phrase verb phrase goe np follow vp np sequenc determin noun vp sequenc verb noun phrase determin mark dt r either word word noun child cat dog verb took saw like scare chase grammar go produc noun phrase start determin exampl wont allow us produc word samantha either subject object two sampl sentenc grammar gener one dog chase cat one child saw dog there way lefthand side transform rule gener multipl righthand side instead write rule multipl time write one rule lefthand side constitu arrow possibl expans righthand side separ vertic bar vertic bar indic choic altern exampl one rule proper noun one common noun way handl samantha proper noun pn case cat dt follow cn cn stand common noun grammar far grown littl bit produc np vp howev two rule noun phrase determin common noun proper noun verb phrase includ verb noun phrase determin common noun proper noun well verb previou exampl grammar allow us produc sentenc child scare jorg min took child mani option categori grammat rule mean exampl noun phrase adject noun phrase without adject order take account possibl option categori introduc anoth type notat parenthesi put someth parenthesi mean option let look exampl noun one observ make whenev n allow sentenc replac syntact follow sequenc one determin noun adject noun determin adject noun exampl allow cat allow cat allow small cat allow small cat use notat altern noun phrase produc either n determin n adject n determin adject n use parenthes say noun phrase sequenc consist option determin follow one option adject follow noun rule equival four differ rule rule n rule dt n rule jj n rule dt jj n let see verb phrase mani type verb phrase saw earlier lectur contain intransit verb ran samantha ran sentenc intransit verb preposit phrase exampl samantha ran park also sentenc particl samantha ran away case away singl preposit also transit verb samantha bought cooki transit verb direct object preposit phase samantha bought cooki john overal structur verb phrase go someth like alway start verb option noun phrase direct object also option preposit thing like away option noun phrase follow preposit p np follow mean would entir preposit phrase part verb phrase grammar gener follow sentenc samantha saw cat also gener jorg gave cat min preposit phrase preposit phrase let look exampl mari bought book john bookstor bookstor sell magazin bookstor main st sell magazin mari ran away mari ran hill sentenc see everi time noun preposit phrase follow john follow bookstor bookstor follow main street thing appli verb everi time verb follow preposit like ran away also follow preposit phrase ran hill order accommod preposit phrase allow new constitu call pp embed either np vp part sentenc rule simpl wherev preposit allow follow noun phrase exampl ran versu ran street noun phase contain number preposit phrase two noun phase case transit verb mari gave john book revis grammar take account preposit phrase well far rule goe np vp follow rule noun phrase rule verb phrase rule preposit phrase preposit phrase preposit follow option noun phrase first time see someth realli import languag look rule number two see noun phrase gener preposit phrase look rule number four see preposit phrase gener noun phrase therefor first instanc recurs grammar mean appli rule two four arbitrarili long sequenc produc extrem long sentenc good moment stop second explain differ made linguist someth call perform compet im go go much detail principl basic idea follow even though context free grammar produc arbitrarili long sentenc let say thousand preposit phrase embed noun phrase embed preposit phrase doesnt mean human sentenc long peopl understand long sentenc spend lot time never go put use expect peopl put use usual limit four five preposit phrase noun phrase anyth would make sentenc complet incomprehens certainli say someth like saw john yesterday park perhap add preposit phrase end never go see four five let see problem preposit phrase ambigu allud earlier actual describ context grammar let look sentenc boy saw woman telescop sentenc ambigu two interpret first one boy use telescop see woman second interpret woman carri telescop seen boy case real ambigu interpret make sens differ context let look grammar use gener sentenc preposit phrase produc preposit fold noun verb phrase includ preposit phrase noun phrase includ preposit phrase actual two differ pars tree correspond sentenc produc pp either part noun phrase part verb phrase well look problem later semest understand deal one addit symbol use context grammar kleen star note parenthes header slide use denot sequenc constitu exampl jj mean sequenc zero adject number seven ten turn english noun phrase multipl adject socal premodifi preced noun insid noun phrase exampl say thin blue line case thin preced blue blue preced line gener structur noun phrase someth like determin like follow number adject follow noun let look exampl say big red hous realli cannot say red big hous sound realli awkward that weve mark star interest think order adject put premodifi section noun phase allow certain sequenc other turn english adject order noun phrase depend semant mean individu word lot work subject want run exercis help understand process work exercis go look titl famou song book well tri see find pattern littl red ride hood three littl pig three musket steadfast tin soldier french connect old macdonald five golden ring ancient marin think mean individu word figur there certain combin mean appear certain order saw previou slide use size color cannot use color size kind semant categori adject exampl littl size red color ride someth like purpos design determin that adject determin alway come adject old age golden materi french nation steadfast qualiti final tin materi tri come possibl combin categori realiz case specif order semant categori adject here exampl may except close possibl happen sentenc determin adject noun theyr like appear order determin first noun last adject middl follow order number strength size age shape color origin materi purpos actual infer order look exampl previou slide look partial order creat let look problem nest sentenc nest sentenc someth like bird fli versu believ bird fli second exampl bird fli nest insid larger sentenc exampl dont recal whether took dog know mall still open exampl first sentenc embed sentenc nest sentenc took dog order accommod nest sentenc need revis grammar littl bit new rule one verb phrase produc verb follow zero one two noun phrase option sequenc c conjunct follow entir new sentenc instanc recurs gener rule goe np vp rule vp produc recurs altern vp produc arbitrarili sentenc kind conjunct go posit c well socal subordin conjunct thing like whether okay sequenc subordin conjunct sentenc appear insid non phase turn case also mean c sequenc part subject sentenc let look exampl whether win elect remain seen predic sentenc remain seen subject phrase whether win elect even verifi syntactici remain seen whether win elect whether win elect remain seen confirm whether win elect subject sentenc let go back topic recurs gener verb phrase verb phrase gener noun phrase gener preposit phrase preposit phrase gener noun phrase essenti recurs allow us produc realli long sentenc mention earlier realli cannot design base grammar longest sentenc english fact grammar allow us produc infinit long sentenc case recurs appear grammar exampl conjunct noun phrase noun phrase replac sequenc noun phrase noun phrase exampl like appl orang appl orang combin known phrase also appl known phrase orang known phrase also conjunct preposit phrase pp transform sequenc pp pv thing exactli verb phrase exampl like walk run see meta pattern emerg im go mention topic briefli realli german cours howev think import understand term xbar theori mean im go introduc second let look exampl sentenc transform noun phrase verb phrase sequenc noun phrase verb phrase proposit phrase rule metapattern well metapattern case sort phrase whether noun phrase verb phrase preposit phrase xp collect name phrase phrase replac someth call specifi come main part phrase follow x bar sometim denot x oppos like exampl anoth portion tell us x bar would use x follow complement everyth x bar specifi everyth complement exampl noun case noun phrase produc determin follow n bar your interest x bar theori detail websit devot well lot literatur linguist commun let look metarul conjunct look earlier exampl noun phrase noun phrase verb phrase verb phrase gener case categori x gener x x kind rule expand cover even entir sentenc produc exampl sunni today go park thing add grammar well one categori particular import auxiliari want see auxiliari past constitu test look earlier one thing interest sequenc auxiliari verb constitu exampl sentenc seen blue eleph rememb forev two sequenc auxiliari verb see structur sentenc pass conjunct test say seen blue eleph period rememb forev also say seen blue eleph rememb forev seen blue eleph parallel rememb forev give us indic form constitu cursiv rule go someth like verb phrase produc auxiliari follow verb phrase sequenc mani auxiliari want left hand side verb say raj may sleep case three applic time gener addit auxiliari recurs limit well turn limit auxiliari verb phrase let look exercis grammar slightli differ one recent rule np vp pp well embed phrase want look two sentenc tri gener descript sentenc use rule grammar small dog neighbor brought old tenni ball first sentenc obvious sentenc therefor need start role righthand side go use np vp cp vp well dont think also set two sentenc tri come exact set rule use gener three sentenc your done understood whole idea pars pars go talk next section detail process take sentenc grammar come known pars tree less set rule use grammar produc sentenc conclud introduct syntax next segment go look pars detail,[ 0  4 10 14 13]
265,Course3_W4-S1-L2_-_Parsing_-_22_slides_17-38,okay welcome back next segments class going parsing notice later parsing one important technologies used natural language processing many components text summarization question answering machine translation rely successful parsing succeed lets consider polar parsing im going start example parsing programming language c program reverse number youre familiar syntax programming languages include variables blocks statements however important computer languages syntax unambiguous case however human languages human languages different many different ways think ways theyre different far parsing concerned types words like computer languages know certain string variable comment natural language dont know whether certain word noun verb automatically brackets around phrases unlike programming languages four statements statements explicit bracketing around statement compose theres also lot ambiguity weve talked ambiguity level individual words level parses theres also lot implied information makes human language parsing even complicated example given dialog one participants conversation may referring object visually available part text also possible refer knowledge outside world obvious sentences text parsing parsing problem essentially associate sort structure often tree structure sentence done usually using grammar often context free grammar may exactly one tree structure given sentence grammar may many sentences case want pick one likely appropriate also case may none case particular sentence would parsed successfully grammar one thing keep mind grammars declarative applies grammars including context free grammars means use grammar describe sentence cannot automatically come methods convert sentence parse tree augment grammars using code words grammars sufficient specify parse tree going constructed talked tactic ambiguities let remind little bit prepositional phrase attachment sentence saw man telescope one interpretation use telescope see man another interpretation saw man carrying telescope pp attachment poem prepositional phrase telescope attach either verb sentence men direct object gaps example sentence mary likes physics hates chemistry clear subject second verb hates also mary however explicit structure successful parser able infer mary subject verbs first one coordination scope another interesting ambiguity sentence like small boys girls playing may two possible interpretations first one boys small girls age whereas second interpretation boys girls small example coordination ambiguity called way coordinating conjunction many cases certain word considered either particle preposition example say ran large bill word used particle phrase verb run ran large bill interpreted incurred large bill changed ran large hill case large hill prepositional phrase word attached directly verb ran rather head phrase large bill another example use words gerunds adjectives gerund verb form whereas adjective something completely different different part speech typical example would something like frightening kids cause trouble two interpretations one frightening adjective modifies kids second example gerund case frightening kids frighten kids action cause trouble lets see kind applications parsing first one grammar checking every time go favorite editor able see feedback type sentence doesnt look grammatical see underlined chance correct example want say want return shoes grammar checker parse sentence recognize ungrammatical suggest changes either want return shoes perhaps shoe shoes another example question answering question nature many people sales make k per year need parser able recognize youre looking record database record person k attribute another example machine translation know different languages different word order example language subject object verb needs translated language subject verb object would undergo syntactic transformation parsing next task information extraction information extraction want recognize different phrases relate also types sentence breaking bad takes place new mexico want recognize breaking bad name tv show new mexico name state many applications example speech generation speech understanding interpretation sentences next topic context grammars weve mentioned briefly past going go lot detail time around context free grammar context free grammar tuple consisting following symbols n sigma r well n set nonterminal symbols example symbols sentences prepositional phrases verb phrases sigma set terminal symbols words example mary john like assumed set terminal symbols distinct disjoint set non terminal symbols also set rules left hand side nonterminal symbol part set nonterminal symbols right hand side beta beta string combine symbols sigma n number zero large number finally specific designated start symbol n parse entire sentences happens sentence symbol general reason context grammar used parse syntactic constituents example nonphrases even entire paragraphs okay lets look example top line slide sentence want parse sentence child ate cake fork grammar context free grammar eight nonterminal symbols sentence np pp vp nonphrase prepositional phrase verb phrase respectively dt determiner article example n noun preposition finally past tense verbs see rules options example noun phrase either determiner followed noun recursively go turn noun phrase biprepositional phrase kind alternative rules would bring us multiple parses given sentence one thing wanted point phrases things p last symbol np pp vp considered headed constituents means one components important others surprisingly noun noun phrase preposition prepositional phrase verb verb phrase heads constituents important concept come later slide lets look examples understand phrasestructure grammars important parcer first thing need realize sentences bags words example sentence alice bought bob flowers distinct sentence bob bought alice flowers clearly parser would help us understand first sentence alice subject doer action whereas second sentence alice recipient action phasestructure grammars enforce known contextfree view language thats expression phrasestructure grammar expression contextfree grammar mean thing context view language tells us prepositional phrase look whether part subject nonphrase part verb phrase internal structure constituent order important said earlier languages subject verb object others subject object verb four combinations subjects verbs objects grammars include additional constituents example auxiliary verbs example dog may eaten homework may auxiliary verbs imperative sentences sentences describe orders example leave book table sentence doesnt explicit subject interrogative sentences end question marks example customer complaint two types interrogative questions yes questions like one wh questions example could also negative sentences main verb negated customer didnt complaint case didnt expression negation lets look longer example incorporates types sentences new rows first line sentence used noun phrase verb phrase two additional options figure changes made grammar make powerful previous example well answer next slide lets look changes one first line new rule turns sentence auxiliary nonphrase verb phrase example something like children arrived home also new concept nominal either noun nominal followed noun way create sequences nouns first one modifies second one wed moved prepositional phase noun phrase nominal different structures tactics whats also new verb phrase rules include row vp turning v example direct objects prepositional phrases known intransitive verb heres example penn treebank penn treebank large resource parsing information manually built pennsylvania extensively used training parsers last years well revisit later want give example realistic sentence included wall street journal see two sentences first sentence marked top second one bottom half page also marked first sentence noun phrase subject verb phrase second one look rest information slide see first sentence additional constituents adjectival phrases adj p case years old second sentence modal verb auxiliary verb like give example realistic sentence much unlike two examples looked far lets discuss idea leftmost derivation given grammar sentence unique way one parse sentence always expanding leftmost unexpanded nonterminal lets look example leftmost derivation sequence strings way sn start symbol sentences would sn last thing list includes terminal symbols words heres example start replace np vp applying row goes np vp replace np leftmost unexpanded symbol far determiner nounverb phrase forth finally get child ate cake fork everything terminal symbols lets look graphically beginning want expand sentence leftmost symbol goes np pp leftmost symbol np expand np determiner noun leftmost nonexpanded symbol determiner place word thats terminal stop leftmost unexpanded symbol n replace child terminal symbol word move vp vp gets expanded vp np first rule vp lefthand side second vp created needs expanded gives us verb ate one last thing expand thats noun phrase np first rule noun phrase grammar noun phrase prepositional phrase expand newly generated noun phrase get determiner noun expand determiner noun words prepositional phrase expand rule prepositional phrase something starts preposition followed noun phrase preposition becomes word nonphrase determiner followed noun finally determiner turn noun turn fork gives us leftmost derivation given grammar given sentence child ate cake fork realize necessarily correct semantic interpretation sentence one comes first follow leftmost derivation principle,Course3,W4-S1-L2,W4,S1,L2,-,4,1,2,okay welcom back next segment class go pars notic later pars one import technolog use natur languag process mani compon text summar question answer machin translat reli success pars succeed let consid polar pars im go start exampl pars program languag c program revers number your familiar syntax program languag includ variabl block statement howev import comput languag syntax unambigu case howev human languag human languag differ mani differ way think way theyr differ far pars concern type word like comput languag know certain string variabl comment natur languag dont know whether certain word noun verb automat bracket around phrase unlik program languag four statement statement explicit bracket around statement compos there also lot ambigu weve talk ambigu level individu word level pars there also lot impli inform make human languag pars even complic exampl given dialog one particip convers may refer object visual avail part text also possibl refer knowledg outsid world obviou sentenc text pars pars problem essenti associ sort structur often tree structur sentenc done usual use grammar often context free grammar may exactli one tree structur given sentenc grammar may mani sentenc case want pick one like appropri also case may none case particular sentenc would pars success grammar one thing keep mind grammar declar appli grammar includ context free grammar mean use grammar describ sentenc cannot automat come method convert sentenc pars tree augment grammar use code word grammar suffici specifi pars tree go construct talk tactic ambigu let remind littl bit preposit phrase attach sentenc saw man telescop one interpret use telescop see man anoth interpret saw man carri telescop pp attach poem preposit phrase telescop attach either verb sentenc men direct object gap exampl sentenc mari like physic hate chemistri clear subject second verb hate also mari howev explicit structur success parser abl infer mari subject verb first one coordin scope anoth interest ambigu sentenc like small boy girl play may two possibl interpret first one boy small girl age wherea second interpret boy girl small exampl coordin ambigu call way coordin conjunct mani case certain word consid either particl preposit exampl say ran larg bill word use particl phrase verb run ran larg bill interpret incur larg bill chang ran larg hill case larg hill preposit phrase word attach directli verb ran rather head phrase larg bill anoth exampl use word gerund adject gerund verb form wherea adject someth complet differ differ part speech typic exampl would someth like frighten kid caus troubl two interpret one frighten adject modifi kid second exampl gerund case frighten kid frighten kid action caus troubl let see kind applic pars first one grammar check everi time go favorit editor abl see feedback type sentenc doesnt look grammat see underlin chanc correct exampl want say want return shoe grammar checker pars sentenc recogn ungrammat suggest chang either want return shoe perhap shoe shoe anoth exampl question answer question natur mani peopl sale make k per year need parser abl recogn your look record databas record person k attribut anoth exampl machin translat know differ languag differ word order exampl languag subject object verb need translat languag subject verb object would undergo syntact transform pars next task inform extract inform extract want recogn differ phrase relat also type sentenc break bad take place new mexico want recogn break bad name tv show new mexico name state mani applic exampl speech gener speech understand interpret sentenc next topic context grammar weve mention briefli past go go lot detail time around context free grammar context free grammar tupl consist follow symbol n sigma r well n set nontermin symbol exampl symbol sentenc preposit phrase verb phrase sigma set termin symbol word exampl mari john like assum set termin symbol distinct disjoint set non termin symbol also set rule left hand side nontermin symbol part set nontermin symbol right hand side beta beta string combin symbol sigma n number zero larg number final specif design start symbol n pars entir sentenc happen sentenc symbol gener reason context grammar use pars syntact constitu exampl nonphras even entir paragraph okay let look exampl top line slide sentenc want pars sentenc child ate cake fork grammar context free grammar eight nontermin symbol sentenc np pp vp nonphras preposit phrase verb phrase respect dt determin articl exampl n noun preposit final past tens verb see rule option exampl noun phrase either determin follow noun recurs go turn noun phrase bipreposit phrase kind altern rule would bring us multipl pars given sentenc one thing want point phrase thing p last symbol np pp vp consid head constitu mean one compon import other surprisingli noun noun phrase preposit preposit phrase verb verb phrase head constitu import concept come later slide let look exampl understand phrasestructur grammar import parcer first thing need realiz sentenc bag word exampl sentenc alic bought bob flower distinct sentenc bob bought alic flower clearli parser would help us understand first sentenc alic subject doer action wherea second sentenc alic recipi action phasestructur grammar enforc known contextfre view languag that express phrasestructur grammar express contextfre grammar mean thing context view languag tell us preposit phrase look whether part subject nonphras part verb phrase intern structur constitu order import said earlier languag subject verb object other subject object verb four combin subject verb object grammar includ addit constitu exampl auxiliari verb exampl dog may eaten homework may auxiliari verb imper sentenc sentenc describ order exampl leav book tabl sentenc doesnt explicit subject interrog sentenc end question mark exampl custom complaint two type interrog question ye question like one wh question exampl could also neg sentenc main verb negat custom didnt complaint case didnt express negat let look longer exampl incorpor type sentenc new row first line sentenc use noun phrase verb phrase two addit option figur chang made grammar make power previou exampl well answer next slide let look chang one first line new rule turn sentenc auxiliari nonphras verb phrase exampl someth like children arriv home also new concept nomin either noun nomin follow noun way creat sequenc noun first one modifi second one wed move preposit phase noun phrase nomin differ structur tactic what also new verb phrase rule includ row vp turn v exampl direct object preposit phrase known intransit verb here exampl penn treebank penn treebank larg resourc pars inform manual built pennsylvania extens use train parser last year well revisit later want give exampl realist sentenc includ wall street journal see two sentenc first sentenc mark top second one bottom half page also mark first sentenc noun phrase subject verb phrase second one look rest inform slide see first sentenc addit constitu adjectiv phrase adj p case year old second sentenc modal verb auxiliari verb like give exampl realist sentenc much unlik two exampl look far let discuss idea leftmost deriv given grammar sentenc uniqu way one pars sentenc alway expand leftmost unexpand nontermin let look exampl leftmost deriv sequenc string way sn start symbol sentenc would sn last thing list includ termin symbol word here exampl start replac np vp appli row goe np vp replac np leftmost unexpand symbol far determin nounverb phrase forth final get child ate cake fork everyth termin symbol let look graphic begin want expand sentenc leftmost symbol goe np pp leftmost symbol np expand np determin noun leftmost nonexpand symbol determin place word that termin stop leftmost unexpand symbol n replac child termin symbol word move vp vp get expand vp np first rule vp lefthand side second vp creat need expand give us verb ate one last thing expand that noun phrase np first rule noun phrase grammar noun phrase preposit phrase expand newli gener noun phrase get determin noun expand determin noun word preposit phrase expand rule preposit phrase someth start preposit follow noun phrase preposit becom word nonphras determin follow noun final determin turn noun turn fork give us leftmost deriv given grammar given sentenc child ate cake fork realiz necessarili correct semant interpret sentenc one come first follow leftmost deriv principl,[ 0  4  9  8 14]
266,Course3_W4-S1-L3_-_Classic_Parsing_Methods_-_61_slides_25-05,okay going start talking classic parsing methods natural language processing techniques also used parsing nonhuman languages example programming languages thats developed many years ago say even though methods pretty old still used modifications expect even days important understand work people came coming lets look typical example parsing lefthand side parse tree sentence child ate cake fork righthand side grammar question arrive parse tree given grammar given original sentence string one approaches treat parsing search problem two types constraints constraints come input sentence know certain number words come certain order certain parts speech constraints come grammar know looking sentence noun phrase sentence noun phrase verb phrase order expect see adjective noun types constraints help constrain parse process two general approaches parsing search one topdown method starts sentence level tries expand true structure matches input string also socalled bottomup method start sentence try combine adjacent words constituents combine constituents together reach level top heres basic example topdown parsing righthand side grammar starting trying come parse sentence first thing try goes np vp thats kind unambiguous example noun phrase one thing try necessarily leftmost derivation reason want choose particular expansion np goes np pp turns eventually going conflict input sentence first two words child prepositional phrase nowhere found near beginning sentence therefore backtrack consider different interpretation noun phrase specifically determine noun later expand sentence particular derivation going match input sentence proceed inside grammar finish parsing entire sentence example topdown parsing successful case little bit backtrack lets look bottomup parsing bottomup parsing going start sentence child ate cake fork going try combine symbols starting lefthand side thing determiner child noun get parts speech move tried combine left right determiner something else build new nonterminal grammar determiner noun combined nicely noun phrase noun phrase verb phrase could combine two together combine another determiner noun noun phrase possibly third one combine preposition noun phrase form prepositional phrase combine noun phrase prepositional phrase form new noun phrase see every iteration picking two symbols combined yet combine together reach top level sentence say two grammar socalled binarized form case righthand side nonterminal always consists two new nonterminals necessarily case general remember grammar auxiliaries rules kind goes auxiliary noun phrase verb phrase case three things righthand side turns parsing important whether grammar binarized revisit next slides advantages bottomup topdown methods many disadvantages example bottomup parser significant problem explores options going lead full parse pretty obvious since starting sentences topdown method different problem explores options dont match full sentence may dead end starting top lot backtracking find correct expansion one technique solves problems based dynamic programming reminder dynamic programming method number iterations iteration combine partial solutions computed previous iterations form solution larger problem essentially cache lot intermediate results example particular parse noun phrase dont reparse later technical term cacheing memoization confused memorization memoization means low terminal already parsed store information needed parse restore memory instead recomputing every time need method used longest uses dynamic programming parsing socalled cockekasamiyounger parser cky mentioned based dynamic programming actually powerful long grammar binarized form going look specific techniques parsing going start shiftreduce parsing shiftreduce parsing bottomup parser goes left right sentence tries match righthand side production build sentence includes two operations name indicates shift operation reduce operation shift operation takes word input centers pushes onto stack example sentence child ate cake fork first well see word going pushed onto stack wait combined words happens perform socalled reduce operation checks top end words top stack match righthand side production popped stack replaced lefthand side production example word already stack next word child word determiner word child noun rule says determiner noun combined form noun phrase pushed child top stack popped back going replace symbol noun phrase theres stopping condition one pretty obvious method going stop things true namely input sentence processed way end popped stack theres nothing left stack let walk example shiftreduce parsing process see screen sentence want parse child ate cake star tells us far string come beginning obviously beginning string thing point recognize word word pushed stack using shift operation moving cursor forward word top stack replace determiner determiner part speech done using reduce operation point stack determiner top child ate cake remains string next thing want process word child word child gets pushed onto stack replaced noun shift reduce operation determiner noun top stack ate cake remainder string point perform second reduce operation merge determiner noun replace noun phrase next thing look word ate word ate goes stack gets replaced part speech verb noun phrase verb going combined together sequence shifts reduce operations get point stack three categories noun phrase verb determiner word cake word cake gets replaced noun point combine things onto top stack determiner noun noun phrase verb noun phrase verb phrase finally noun phrase verb phrase point satisfied stopping conditions declare parsing successful remind stopping conditions stack empty input string empty able get part process corresponding parse tree goes non phrase verb phrase next going talk cockekasamiyounger parsing method cky based dynamic programming reason using dynamic programming lot work normally repeated parsing bottom top parsing doesnt repeated going cache intermediate results order improve complexity parsing algorithm dynamic programming method works following way youre going build parse substring word word j based current parses k k j k somewhere j example something like child ate cake going parse phrase child cake based parses child parse ate cake combination substrings five word sequence complexity dynamic computing cubic recognizing finding single parse input string length n going look detail complexity cky slides cky method bottomup method requires grammar normalized specifically converted binary form binary grammar things allowed nonterminal going two nonterminals nonterminal going terminal later also going look early parser also based dynamic programming topdown parser little complicated cky importantly doesnt binarization requirement going back cky sentence want parse child ate cake fork grammar see binary form lets confirm goes np vp two nonterminals fine rules noun phrases include two nonterminals righthand side fine thing applies prepositional phrases verb phrases finally lexicon nonterminals determine noun preposition verb turn single terminals completely consistent definition binary grammar cky method works building dynamic table cells tells parse substring original sentence given word another given word example beginning want build parse word word looking lexicon determiner cell includes information word determiner going start filling table get boxes full order done columns going bottom next thing going produce determiner box child ate word child noun according lexicon next box one way combine determiner noun yes way creating rule noun phrases also keep back pointers tell us noun phrase created determiner noun next box needs expanded one right word ate well thats verb theres one possible way interpret word grammar theres nothing else column move next column word determiner boxes cannot filled point theres rule determiner righthand side word cake noun point combine determiner noun cake gives us noun phrase particular cell back pointers takes us parses individual constituents determiner noun continue filling rest column going verb phrase produced combining verb ate noun phrase cake see screen also go way cell goes cake case build sentence right sentence consists noun phrase child verb phrase ate cake one thing say point okay done weve parsed input sentence obviously done still words fork processed able identify sentence point child ate cake done continue find top righthand box chart okay need next need expand word thats preposition theres nothing preposition righthand side rest boxes column going remain blank next word word fork combine determiner noun fork get noun phrase combine noun phrase preposition form prepositional phrase finally combine noun phrase cake prepositional phrase fork form noun phrase corresponds substring cake fork continue going entire verb phrase obtained combining verb phrase ate cake prepositional phrase fork give us new verb phrase ate cake fork also combine verb phrase point merging ate verb noun phrase cake fork see verb phrase cell obtained two different ways obviously lead us two alternative parses sentence eventually verb phrase combined noun phrase child form sentence child ate cake fork noticed two different parses verb phrase ate cake fork look cky data structure see two bold lines tell verb phrase position eight obtained either combining verb noun phrase combining verb phrase prepositional phrase gives us two different parses want figure meaning two sentences one left one right whats difference two lets look syntactic tree one lefthand side prepositional phrase fork part cake fork part noun phrase whereas one righthand side fork level ate remind looked examples like one lefthand side example noun attachment low attachment one righthand side example high attachment also known verb attachment semantics sentence left cake includes fork whereas one righthand side says fork used eat cake pen treebank presentation sentences see first one goes np vp vp goes vp pp whereas second one less goes np vp verb phrase goes vnp pp generated part noun phrase instead directly part verbphrase youre interested theres nice online demo cky parser actually uses similar example one looked far feel free take look site return one thing want say complexity cky n square cells table specifically n times n plus one divided two cells still ordered n squared finding single parse linear lookup cell thats square algorithm n times total time complexity n cubed however want find parses total time complexity going exponential cubic theres reason well get sec lets look longer example grammar weve seen nonbinarized rules example possibility creating imperative sentences vp going reacted v auxiliaries noun phrases want parse sentence take book imperative sentence use grammar slides binarized cannot use cky first convert grammar binarized format nonbinary productions goes auxiliary noun phrase verb phrase nonbinarized three nonterminals righthand side goes vp np goes pronoun unary roles converted binary well leads us binarized form grammar technical term chomsky normal form cnf tells us grammar following form either non terminal going two non terminals non terminal going terminal convert grammar previous slide create new known terminals hybrid rules hybrid rules one mixture terminals nonterminals right hand side nary rules n unary rules n equal one heres example atis grammar describing jurafsky martin original grammar shown left two examples non binary productions goes auxillary noun phrase verb phrase goes verb phrase convert binary form well one possibility split three way righthand side auxiliary noun phrase verb phrase two binary roles introducing new nonterminal called x goes x vp x goes auxiliary vp way new grammar cnf unary cases complicated involve specific procedure going take possibly terminal symbols grammar turned context chomsky normal form case goes vp original grammar going replaced several new rules directly take terminals goes book include prefer others expand sentence two nonterminals exactly two known terminals goes verb noun phrase goes vp pp thing productions example np goes pronoun going replaced exactly np goes np goes proper noun going replaced directly terminals np goes either houston northwest airlines automated procedure takes original grammar binarizes turns chumsky normal form heres specific examples infinitival verb phrase verb phrase would become something like infinitival verb phrase followed prepositions nonterminal symbol new nonterminal symbol going converted terminal symbol goes auxiliary noun phrase verb phrase going replaced goes r x vp r goes auxiliary noun phrase finally unary rules converted rules lefthand side nonterminal right hand terminal issues cky start nonbinary grammar turns language generated new grammar weakly equivalent original language generates strengths using different derivations convert original grammar conjunctive normal form final parse tree going match original grammar additional processing need done convert something relatively straightforward needs done one issue cky parsing syntactic ambiguities deterministic cky method way perform syntactic disambiguation sentence child ate cake fork saw chart includes two possible parses parser way tell one best one would need use either probabilities additional external information choose one well see done later okay see cky reasonable parser however issues addressed using techniques going look techniques next segment,Course3,W4-S1-L3,W4,S1,L3,-,4,1,3,okay go start talk classic pars method natur languag process techniqu also use pars nonhuman languag exampl program languag that develop mani year ago say even though method pretti old still use modif expect even day import understand work peopl came come let look typic exampl pars lefthand side pars tree sentenc child ate cake fork righthand side grammar question arriv pars tree given grammar given origin sentenc string one approach treat pars search problem two type constraint constraint come input sentenc know certain number word come certain order certain part speech constraint come grammar know look sentenc noun phrase sentenc noun phrase verb phrase order expect see adject noun type constraint help constrain pars process two gener approach pars search one topdown method start sentenc level tri expand true structur match input string also socal bottomup method start sentenc tri combin adjac word constitu combin constitu togeth reach level top here basic exampl topdown pars righthand side grammar start tri come pars sentenc first thing tri goe np vp that kind unambigu exampl noun phrase one thing tri necessarili leftmost deriv reason want choos particular expans np goe np pp turn eventu go conflict input sentenc first two word child preposit phrase nowher found near begin sentenc therefor backtrack consid differ interpret noun phrase specif determin noun later expand sentenc particular deriv go match input sentenc proceed insid grammar finish pars entir sentenc exampl topdown pars success case littl bit backtrack let look bottomup pars bottomup pars go start sentenc child ate cake fork go tri combin symbol start lefthand side thing determin child noun get part speech move tri combin left right determin someth els build new nontermin grammar determin noun combin nice noun phrase noun phrase verb phrase could combin two togeth combin anoth determin noun noun phrase possibl third one combin preposit noun phrase form preposit phrase combin noun phrase preposit phrase form new noun phrase see everi iter pick two symbol combin yet combin togeth reach top level sentenc say two grammar socal binar form case righthand side nontermin alway consist two new nontermin necessarili case gener rememb grammar auxiliari rule kind goe auxiliari noun phrase verb phrase case three thing righthand side turn pars import whether grammar binar revisit next slide advantag bottomup topdown method mani disadvantag exampl bottomup parser signific problem explor option go lead full pars pretti obviou sinc start sentenc topdown method differ problem explor option dont match full sentenc may dead end start top lot backtrack find correct expans one techniqu solv problem base dynam program remind dynam program method number iter iter combin partial solut comput previou iter form solut larger problem essenti cach lot intermedi result exampl particular pars noun phrase dont repars later technic term cach memoiz confus memor memoiz mean low termin alreadi pars store inform need pars restor memori instead recomput everi time need method use longest use dynam program pars socal cockekasamiyoung parser cki mention base dynam program actual power long grammar binar form go look specif techniqu pars go start shiftreduc pars shiftreduc pars bottomup parser goe left right sentenc tri match righthand side product build sentenc includ two oper name indic shift oper reduc oper shift oper take word input center push onto stack exampl sentenc child ate cake fork first well see word go push onto stack wait combin word happen perform socal reduc oper check top end word top stack match righthand side product pop stack replac lefthand side product exampl word alreadi stack next word child word determin word child noun rule say determin noun combin form noun phrase push child top stack pop back go replac symbol noun phrase there stop condit one pretti obviou method go stop thing true name input sentenc process way end pop stack there noth left stack let walk exampl shiftreduc pars process see screen sentenc want pars child ate cake star tell us far string come begin obvious begin string thing point recogn word word push stack use shift oper move cursor forward word top stack replac determin determin part speech done use reduc oper point stack determin top child ate cake remain string next thing want process word child word child get push onto stack replac noun shift reduc oper determin noun top stack ate cake remaind string point perform second reduc oper merg determin noun replac noun phrase next thing look word ate word ate goe stack get replac part speech verb noun phrase verb go combin togeth sequenc shift reduc oper get point stack three categori noun phrase verb determin word cake word cake get replac noun point combin thing onto top stack determin noun noun phrase verb noun phrase verb phrase final noun phrase verb phrase point satisfi stop condit declar pars success remind stop condit stack empti input string empti abl get part process correspond pars tree goe non phrase verb phrase next go talk cockekasamiyoung pars method cki base dynam program reason use dynam program lot work normal repeat pars bottom top pars doesnt repeat go cach intermedi result order improv complex pars algorithm dynam program method work follow way your go build pars substr word word j base current pars k k j k somewher j exampl someth like child ate cake go pars phrase child cake base pars child pars ate cake combin substr five word sequenc complex dynam comput cubic recogn find singl pars input string length n go look detail complex cki slide cki method bottomup method requir grammar normal specif convert binari form binari grammar thing allow nontermin go two nontermin nontermin go termin later also go look earli parser also base dynam program topdown parser littl complic cki importantli doesnt binar requir go back cki sentenc want pars child ate cake fork grammar see binari form let confirm goe np vp two nontermin fine rule noun phrase includ two nontermin righthand side fine thing appli preposit phrase verb phrase final lexicon nontermin determin noun preposit verb turn singl termin complet consist definit binari grammar cki method work build dynam tabl cell tell pars substr origin sentenc given word anoth given word exampl begin want build pars word word look lexicon determin cell includ inform word determin go start fill tabl get box full order done column go bottom next thing go produc determin box child ate word child noun accord lexicon next box one way combin determin noun ye way creat rule noun phrase also keep back pointer tell us noun phrase creat determin noun next box need expand one right word ate well that verb there one possibl way interpret word grammar there noth els column move next column word determin box cannot fill point there rule determin righthand side word cake noun point combin determin noun cake give us noun phrase particular cell back pointer take us pars individu constitu determin noun continu fill rest column go verb phrase produc combin verb ate noun phrase cake see screen also go way cell goe cake case build sentenc right sentenc consist noun phrase child verb phrase ate cake one thing say point okay done weve pars input sentenc obvious done still word fork process abl identifi sentenc point child ate cake done continu find top righthand box chart okay need next need expand word that preposit there noth preposit righthand side rest box column go remain blank next word word fork combin determin noun fork get noun phrase combin noun phrase preposit form preposit phrase final combin noun phrase cake preposit phrase fork form noun phrase correspond substr cake fork continu go entir verb phrase obtain combin verb phrase ate cake preposit phrase fork give us new verb phrase ate cake fork also combin verb phrase point merg ate verb noun phrase cake fork see verb phrase cell obtain two differ way obvious lead us two altern pars sentenc eventu verb phrase combin noun phrase child form sentenc child ate cake fork notic two differ pars verb phrase ate cake fork look cki data structur see two bold line tell verb phrase posit eight obtain either combin verb noun phrase combin verb phrase preposit phrase give us two differ pars want figur mean two sentenc one left one right what differ two let look syntact tree one lefthand side preposit phrase fork part cake fork part noun phrase wherea one righthand side fork level ate remind look exampl like one lefthand side exampl noun attach low attach one righthand side exampl high attach also known verb attach semant sentenc left cake includ fork wherea one righthand side say fork use eat cake pen treebank present sentenc see first one goe np vp vp goe vp pp wherea second one less goe np vp verb phrase goe vnp pp gener part noun phrase instead directli part verbphras your interest there nice onlin demo cki parser actual use similar exampl one look far feel free take look site return one thing want say complex cki n squar cell tabl specif n time n plu one divid two cell still order n squar find singl pars linear lookup cell that squar algorithm n time total time complex n cube howev want find pars total time complex go exponenti cubic there reason well get sec let look longer exampl grammar weve seen nonbinar rule exampl possibl creat imper sentenc vp go react v auxiliari noun phrase want pars sentenc take book imper sentenc use grammar slide binar cannot use cki first convert grammar binar format nonbinari product goe auxiliari noun phrase verb phrase nonbinar three nontermin righthand side goe vp np goe pronoun unari role convert binari well lead us binar form grammar technic term chomski normal form cnf tell us grammar follow form either non termin go two non termin non termin go termin convert grammar previou slide creat new known termin hybrid rule hybrid rule one mixtur termin nontermin right hand side nari rule n unari rule n equal one here exampl ati grammar describ jurafski martin origin grammar shown left two exampl non binari product goe auxillari noun phrase verb phrase goe verb phrase convert binari form well one possibl split three way righthand side auxiliari noun phrase verb phrase two binari role introduc new nontermin call x goe x vp x goe auxiliari vp way new grammar cnf unari case complic involv specif procedur go take possibl termin symbol grammar turn context chomski normal form case goe vp origin grammar go replac sever new rule directli take termin goe book includ prefer other expand sentenc two nontermin exactli two known termin goe verb noun phrase goe vp pp thing product exampl np goe pronoun go replac exactli np goe np goe proper noun go replac directli termin np goe either houston northwest airlin autom procedur take origin grammar binar turn chumski normal form here specif exampl infinitiv verb phrase verb phrase would becom someth like infinitiv verb phrase follow preposit nontermin symbol new nontermin symbol go convert termin symbol goe auxiliari noun phrase verb phrase go replac goe r x vp r goe auxiliari noun phrase final unari rule convert rule lefthand side nontermin right hand termin issu cki start nonbinari grammar turn languag gener new grammar weakli equival origin languag gener strength use differ deriv convert origin grammar conjunct normal form final pars tree go match origin grammar addit process need done convert someth rel straightforward need done one issu cki pars syntact ambigu determinist cki method way perform syntact disambigu sentenc child ate cake fork saw chart includ two possibl pars parser way tell one best one would need use either probabl addit extern inform choos one well see done later okay see cki reason parser howev issu address use techniqu go look techniqu next segment,[ 0  4 10 14 13]
267,Course3_W4-S1-L4_-_Earley_Parser_-_20_slides_16-34,okay next segment going early parser early parser also known chart parser uses dynamic programming method advantages earlier cky parser earley parser created jay earley one major advantages cky theres need convert grammar chomsky normal form works parsing sentence left right looking partial completions nonterminals complexity often faster cubed going left right looks full partial constituents example goes auxnp vp tells us far weve able find auxiliary sentence later find noun phrase verb phrase adjacent auxiliary found sentence reading certain word k earleys parser already identified hypothesis consistent words k heres example later parser found noun phrase example change state goes auxiliary np period vp period indicates found auxiliary noun phrase still needs find verb phrase succeeds finding found entire advents data structure used earleys parser also dynamic programming table like cky uses columns course prone words seen far heres example entry column one brackets followed vp goes vppp means column one therefore still processing word number one parse shown right hand side corresponds words zero one words match verb phrase part verb phrase shown example later one end sentence found prepositional phrase means whole vp found dot separates completed known part one incomplete quite possibly unobtainable part three types entries chart theres entry called scan used individual words example child theres second one called predict used nonterminals finally one called complete used otherwise heres example grammar goes np vp auxiliary non phrase book phrase words things like book boys girls takes sentence want parse lets take book scanner tells us position zero position one word take position one position two word position two position three word book point start combining things also need acknowledge example created using nltk software lets look next lines chart less inside zero zero point hypothesising every possible rule grammar goes np vp goes auxiliary non phrase verb phrase putting stars left everything right hand side production havent seen anything yet point input screen words first bold faced items goes period star case npvp tells us potentially obtain sentence later found exactly noun phrase verb phrase order okay next set entries table deal one column means processed first word take theres several things happen example first bold faced item gives us rule v goes take star old entry v goes star take one v goes take star means already identified word called take consistent verb dont need find additional words complete particular production take zero one acceptable another thing take replace vp v star means verb far order complete verb phrase stop right away one rules grammar verb phrase turn verb another rule verb phrases one says verb phrase goes verb noun phrase one cannot complete point seen verb therefore put chart entry vp goes v star mp indicated seen verb yet look noun phrase complete verb phrase start rules start position one last two lines slide indicate rules one one mp goes stop one means find pronoun starting position one found nonphrase starts position one continue get full parse sentence give examples last three lines right hand side first one tells us verb phrase starts position zero thats first word ends position three last word verb phrase consists verb noun phrase would seen already dont need anything additional find verb phrase star end string also know point sentence consists verb phrase verb phrase already seen think three examples theyre pretty obvious one corresponds interpretation ive take book verb phrase one corresponds take book sentence last line right hand side tells us also possible take book beginning longer verb phrase complete verb phrase would need find additional prepositional phrase input problem words left parse therefore last row vp goes vp star vp never going completed since reached end input string stop two parses ones shown second third us lines either verb phrase take book samples take book one want one gives us final output early parser consists verb phrase consists non phrase verb example earley parser earley parser cky parser disadvantages need address using completely different techniques wanted talk next new topic issues context grammars method looked far cky earley dynamic programming base using context grammar saw differences also shared problems one problem agreement agreement happens lot english languages example want make sure sentence noun phrase verb phrase agree number subject chan single person want verb singular subject people plural noun want verb plural well dont want combinations people chen similarly agreement person first person example chen third person example tense chen reading versus chen reading versus chen reading want make sure theres agreement tense sentences case languages also problem example english theres much languages german russian greek agreement case gender much problem english languages french spanish german significant issues gender agreement one way deal problem incorporate special rules grammar allow us agreement turns however leads combinatorial explosion see next two examples want want simple way create special non terminals express agreement instead rule says goes noun phrase verb phrase create new order says goes first person singular noun phrase followed first person singular verb phrase rule going work well make sure agreement want work well create many rules nature example separate rule second person singular rule third person singular similarly plurals also expand idea constituents order get first person singular nouns create rules say first person singular noun phrase goes first person singular noun forth see leads serious case combinatorial explosion right way agreement graphing another issue agreement subcategorization frames rule says goes np vp vp goes verb vp goes auxiliary verb vp goes verb noun phrase follow context principle grammar going see problems different verbs different subcategorization frames take different sets arguments verbs take direct object example dog ate sausage verb ate one direct object sausage grammar would rule says vp goes verb noun phrase particular verb eat another example verbs take prepositional phrases arguments example mary left car garage case left takes direct object car also takes indirect object garage allow grammar third example predicative adjective example receptionist looked worried worried case adjective verb looked verb looked happens take predicative adjective argument another example would receptionist looked angry verbs take bare infinitives argument example helped buy place examples verbs take toinfinitives argument girl wanted alone verb entry type verb would verb phrase goes verb followed toinfinitive phrase another category verbs take participle phrases example stayed crying movie ended stayed takes case participle phrase argument thatclause ravi doesnt believe rain tomorrow verb believe takes entire thatclause argument finally questionform clauses example wondered go wondered takes argument questionform clause starts wh word go cfgs make lot independent assumptions well example nps half full industry distribution look bently bank noun phrases get expanded sequence noun phrases prepositional phrases get expanded determinable fold noun phrases get expanded prepositions however look non phrases top level sentence nine percent noun phrase prepositional phrase nine percent determiner noun whole preposition look noun phrases verb phrases see distribution different prepositions whereas full noun phrase prepositional phrases determiner nouns tells us whole idea context free grammars rather flawed assumption noun phrase would look exactly way whether verb phrase another noun phrase sandals see data case able model sort dependence structure noun phrase overall context example actually given dan klein computed bentley bank data set one possible solution problem independence using something called lexicalized grammars going talk lexicalized grammar later want give heads grammars productions dont look syntactic categories noun phrases verb phrases look heads syntactic categories example rules noun phrases head noun equal word cat conclude segment syntax helps us understand meaning sentence doesnt give us meaning tell us sentence bob gave alice flower bob subject giver action answer questions gave flower alice bob give alice another interesting observation contextfree grammars appropriate representation syntactic information within limits dynamic programming needed efficient parse saw cky earley parse takes typically cubic time find one parse takes exponential amount time find parses question think need exponential time find parses answer pretty obvious question take exponential amount time find parses even though takes cubic amount time find single parse well answer obvious number parses exponential matter algorithm use fact theres exponential number parses example attaching long sequence prepositional phrases many different ways tells us cannot faster exponential end segment next segment going talk important resource building parsers specifically bentley bank,Course3,W4-S1-L4,W4,S1,L4,-,4,1,4,okay next segment go earli parser earli parser also known chart parser use dynam program method advantag earlier cki parser earley parser creat jay earley one major advantag cki there need convert grammar chomski normal form work pars sentenc left right look partial complet nontermin complex often faster cube go left right look full partial constitu exampl goe auxnp vp tell us far weve abl find auxiliari sentenc later find noun phrase verb phrase adjac auxiliari found sentenc read certain word k earley parser alreadi identifi hypothesi consist word k here exampl later parser found noun phrase exampl chang state goe auxiliari np period vp period indic found auxiliari noun phrase still need find verb phrase succe find found entir advent data structur use earley parser also dynam program tabl like cki use column cours prone word seen far here exampl entri column one bracket follow vp goe vppp mean column one therefor still process word number one pars shown right hand side correspond word zero one word match verb phrase part verb phrase shown exampl later one end sentenc found preposit phrase mean whole vp found dot separ complet known part one incomplet quit possibl unobtain part three type entri chart there entri call scan use individu word exampl child there second one call predict use nontermin final one call complet use otherwis here exampl grammar goe np vp auxiliari non phrase book phrase word thing like book boy girl take sentenc want pars let take book scanner tell us posit zero posit one word take posit one posit two word posit two posit three word book point start combin thing also need acknowledg exampl creat use nltk softwar let look next line chart less insid zero zero point hypothesis everi possibl rule grammar goe np vp goe auxiliari non phrase verb phrase put star left everyth right hand side product havent seen anyth yet point input screen word first bold face item goe period star case npvp tell us potenti obtain sentenc later found exactli noun phrase verb phrase order okay next set entri tabl deal one column mean process first word take there sever thing happen exampl first bold face item give us rule v goe take star old entri v goe star take one v goe take star mean alreadi identifi word call take consist verb dont need find addit word complet particular product take zero one accept anoth thing take replac vp v star mean verb far order complet verb phrase stop right away one rule grammar verb phrase turn verb anoth rule verb phrase one say verb phrase goe verb noun phrase one cannot complet point seen verb therefor put chart entri vp goe v star mp indic seen verb yet look noun phrase complet verb phrase start rule start posit one last two line slide indic rule one one mp goe stop one mean find pronoun start posit one found nonphras start posit one continu get full pars sentenc give exampl last three line right hand side first one tell us verb phrase start posit zero that first word end posit three last word verb phrase consist verb noun phrase would seen alreadi dont need anyth addit find verb phrase star end string also know point sentenc consist verb phrase verb phrase alreadi seen think three exampl theyr pretti obviou one correspond interpret ive take book verb phrase one correspond take book sentenc last line right hand side tell us also possibl take book begin longer verb phrase complet verb phrase would need find addit preposit phrase input problem word left pars therefor last row vp goe vp star vp never go complet sinc reach end input string stop two pars one shown second third us line either verb phrase take book sampl take book one want one give us final output earli parser consist verb phrase consist non phrase verb exampl earley parser earley parser cki parser disadvantag need address use complet differ techniqu want talk next new topic issu context grammar method look far cki earley dynam program base use context grammar saw differ also share problem one problem agreement agreement happen lot english languag exampl want make sure sentenc noun phrase verb phrase agre number subject chan singl person want verb singular subject peopl plural noun want verb plural well dont want combin peopl chen similarli agreement person first person exampl chen third person exampl tens chen read versu chen read versu chen read want make sure there agreement tens sentenc case languag also problem exampl english there much languag german russian greek agreement case gender much problem english languag french spanish german signific issu gender agreement one way deal problem incorpor special rule grammar allow us agreement turn howev lead combinatori explos see next two exampl want want simpl way creat special non termin express agreement instead rule say goe noun phrase verb phrase creat new order say goe first person singular noun phrase follow first person singular verb phrase rule go work well make sure agreement want work well creat mani rule natur exampl separ rule second person singular rule third person singular similarli plural also expand idea constitu order get first person singular noun creat rule say first person singular noun phrase goe first person singular noun forth see lead seriou case combinatori explos right way agreement graph anoth issu agreement subcategor frame rule say goe np vp vp goe verb vp goe auxiliari verb vp goe verb noun phrase follow context principl grammar go see problem differ verb differ subcategor frame take differ set argument verb take direct object exampl dog ate sausag verb ate one direct object sausag grammar would rule say vp goe verb noun phrase particular verb eat anoth exampl verb take preposit phrase argument exampl mari left car garag case left take direct object car also take indirect object garag allow grammar third exampl predic adject exampl receptionist look worri worri case adject verb look verb look happen take predic adject argument anoth exampl would receptionist look angri verb take bare infinit argument exampl help buy place exampl verb take toinfinit argument girl want alon verb entri type verb would verb phrase goe verb follow toinfinit phrase anoth categori verb take participl phrase exampl stay cri movi end stay take case participl phrase argument thatclaus ravi doesnt believ rain tomorrow verb believ take entir thatclaus argument final questionform claus exampl wonder go wonder take argument questionform claus start wh word go cfg make lot independ assumpt well exampl np half full industri distribut look bentli bank noun phrase get expand sequenc noun phrase preposit phrase get expand determin fold noun phrase get expand preposit howev look non phrase top level sentenc nine percent noun phrase preposit phrase nine percent determin noun whole preposit look noun phrase verb phrase see distribut differ preposit wherea full noun phrase preposit phrase determin noun tell us whole idea context free grammar rather flaw assumpt noun phrase would look exactli way whether verb phrase anoth noun phrase sandal see data case abl model sort depend structur noun phrase overal context exampl actual given dan klein comput bentley bank data set one possibl solut problem independ use someth call lexic grammar go talk lexic grammar later want give head grammar product dont look syntact categori noun phrase verb phrase look head syntact categori exampl rule noun phrase head noun equal word cat conclud segment syntax help us understand mean sentenc doesnt give us mean tell us sentenc bob gave alic flower bob subject giver action answer question gave flower alic bob give alic anoth interest observ contextfre grammar appropri represent syntact inform within limit dynam program need effici pars saw cki earley pars take typic cubic time find one pars take exponenti amount time find pars question think need exponenti time find pars answer pretti obviou question take exponenti amount time find pars even though take cubic amount time find singl pars well answer obviou number pars exponenti matter algorithm use fact there exponenti number pars exampl attach long sequenc preposit phrase mani differ way tell us cannot faster exponenti end segment next segment go talk import resourc build parser specif bentley bank,[ 0  4 10 14 13]
268,Course3_W4-S1-L5_-_The_Penn_Treebank_-_21_slides_17-55,next segment going penn treebank one important resources used building parsers one thing would like mention penn treebank paper even though doesnt describe specific algorithm evaluation one cited papers entire history natural language processing penn treebank important penn treebank created early university pennsylvania designed people build trainable parsers idea take human annotators ask parse sentences hand use information features development parsers paper decided mitchell marcus beatrice santorini mary ann marcinkiewicz size penn treebank large training sentences hand test sentences give heads train parser penn treebank never look test data untill parser fully developed genre penn treebank mostly wall street journal news stories also spoken conversations summarize importance penn treebank single handedly helped launch modern automatic parsing methods pointers want explore penn treebanks available ldc catalogue ldc number theres older version available well heres website gives tokenization guidelines split sentences words finally theres another pointer similar resource called american national corpus also available online penn treebank tagset look like well next two slides going show different tags parse speech lets start examples cc stands coordinating conjunctions word cd cardinal number example determiner like marked dt adjectives jj modals md singular mass nouns marked nn add extra p end indicate plural also rb adverb verbs base form verb vb also special cases example vbd past tense verb took vbg gerund present participle vbn past participle example taken one important thing realize prepositions marked special symbol symbol used preposition used preposition subordinating conjunction except preposition tag name reason preposition also used lot infinitives special constructions significant difference rest prepositions heres example sentence wall street journal section penn treebank section sentence cd effective yield issued interest rates general declined sharply since part price dr blumenfeld paid premium additional amount top cds base value plus accrued interest represented cds increased market value took time read sentence entirety give appreciation complexity subject matter specific penn treebank beginning sentence presented exactly appears penn treebank recognize nontraditional labels example sbar corresponds fragment sentence starts another sentence sbar one corresponds time expression issued specific words example pp bottom screen labeled pp time tm stands temporal prepositional phrase expresses time thing relatively small font fit one screen full parse tree corresponds specific sentence see complicated way read three columns left right blank audio interesting factors example coordinating conjunction indicated cc two sentences conjoined first one one starts cd effective yield second one interest rates general declined sharply grammar allows coordinating conjunctions link entire sentences lets look peculiarities penn treebank includes things like complementizers example word includes gaps example word none example sentence says mary likes chemistry hates biology subject second verb hate also mary appears gap parse tree labeled string none also includes special category called sbar comes sbar theory example sentence starts complementizer example dont believe come tomorrow come tomorrow sbar tool use parse penn treebank allows search specific configurations nonterminals terminals heres operators less b gives sentences nonterminal immediately dominates parent b less less b means somewhere b parse tree necessarily parent see syntax pretty rich gives good opportunity find sentence use example less penn treebank looks like used well first disadvantages general idea using treebanks seems like nostarter ask takes lot work annotate sentences write grammar may superficial disadvantage actually advantages well use penn treebank count statistics different constituents phenomena example many times nonphrase turn specific righthand side part subject sentence part object sentence use train systems use evaluate systems automatic parser produce output compare output using welldefined statistical techniques manual annotations also possible use technology building multilingual versions penn treebank fact many versions exist many european languages well chinese korean lets see use something like penn treebank evaluating parses evaluation methodology general one used evaluating classifiers example binary classifier every object data set labeled either true false either positive negative traditional classification tasks document retrieval one example document query say whether document relevant query yes part speech tagging classification task two classes example word like round labeled noun verb adjective sequence parts speech showed earlier different parts speech parsing also considered classification task essentially set words labeled either sentence noun phrase verb phrase get class right get points get wrong lose points penn treebank split general training set test set general cases classification evaluation three sets training set used learn data looks like devtest set used test parsing method without touching official test set use devtest set go back retrain system see well works without going official test set finally official test set youre allowed look parser developed look would essentially overfit results going valid theyre going acceptable research community baselines used evaluation dumb baseline example label everything noun label everything noun phrase also intelligent baseline typically something straightforward example label every word likely part speech looking training data example word round either noun verb adjective noun sense round frequent one label noun every time give intelligent baseline one says label every word noun also human performance matrix tells accurate system expected humans dont agree performance means system expected better humans example part speech tagging task humans achieve accuracy expect part speech system go define new method example lets say statistical parser able compare baselines human performance using standard evaluation methods people typically use accuracy many times label predict match correct label precision recall looked earlier set slides tell us precision things labeled positive many actually positive according training data recall measure things could labeled positive based training data many actually labeled positive extensions accuracy precision recall take account fact may multiple references conflict one another example different humans disagreeing specific label case take account interjudge agreement interjudge agreement tells percentage times human judges pick label finally use matrix called kappa looks like kappa normalized performance system pa agreement system human judges multiple human judges pe expected agreement judges label something animal example kappa greater assumed integer agreement high somewhere much lower example means task welldefined probably consider matter results system achieves meaningful judges dont agree correct labels less evaluation done general question judge agreement binary classification task high enough good think well answer question consider formula kappa plug numbers probability agreement expected agreement chance given two classes compute kappa realize value kappa numerator divide denominator equal said previous slide far acceptable interjudge agreement case say task welldefined evaluate parsers well standard techniques evaluating based precision recall whether get constituents right also labeled precision recall difference precision recall labor position recall former case youre looking whether get right words bracketed together properly label precision also want make sure label nonterminal correct example label something phrase actually noun phrase youre going get full credit labeled precision recall even though would get full credit precision recall one metric used lot combine precision recall f f harmonic mean precision recall highest precision recall high low one high one low vice versa one specific twist evaluating parsers something called crossing brackets correct parse b b b c grouped together join system produces ab group together joining c youre going get crossing bracket error penn treebank corpus mentioned people usually train section use section development like deaf test finally evaluate performance section lets look example evaluation gold standard japanese industrial company know better output relatively state art parser charniak parser produces parse shown bottom see differences shown boldface example used different part speech word better instead labelling jjr labelled rbr going reduce labeled accuracy labeled precision recall labeled f overall output parse evaluation spare sentences bracketing recall correctly picked bracketing precision twothirds means three ones picked charniak parser incorrect f measure harmonic mean numbers complete match zero meaning sentence completely parsed correctly gets score crossing theyre crossing panels finally tagging accuracy words correctly labeled one mistake word better kind numbers would see reported papers bracketing measures tagging accuracy complete match crossing concludes section evaluating parsers next segment talking statistical parser,Course3,W4-S1-L5,W4,S1,L5,-,4,1,5,next segment go penn treebank one import resourc use build parser one thing would like mention penn treebank paper even though doesnt describ specif algorithm evalu one cite paper entir histori natur languag process penn treebank import penn treebank creat earli univers pennsylvania design peopl build trainabl parser idea take human annot ask pars sentenc hand use inform featur develop parser paper decid mitchel marcu beatric santorini mari ann marcinkiewicz size penn treebank larg train sentenc hand test sentenc give head train parser penn treebank never look test data until parser fulli develop genr penn treebank mostli wall street journal news stori also spoken convers summar import penn treebank singl handedli help launch modern automat pars method pointer want explor penn treebank avail ldc catalogu ldc number there older version avail well here websit give token guidelin split sentenc word final there anoth pointer similar resourc call american nation corpu also avail onlin penn treebank tagset look like well next two slide go show differ tag pars speech let start exampl cc stand coordin conjunct word cd cardin number exampl determin like mark dt adject jj modal md singular mass noun mark nn add extra p end indic plural also rb adverb verb base form verb vb also special case exampl vbd past tens verb took vbg gerund present participl vbn past participl exampl taken one import thing realiz preposit mark special symbol symbol use preposit use preposit subordin conjunct except preposit tag name reason preposit also use lot infinit special construct signific differ rest preposit here exampl sentenc wall street journal section penn treebank section sentenc cd effect yield issu interest rate gener declin sharpli sinc part price dr blumenfeld paid premium addit amount top cd base valu plu accru interest repres cd increas market valu took time read sentenc entireti give appreci complex subject matter specif penn treebank begin sentenc present exactli appear penn treebank recogn nontradit label exampl sbar correspond fragment sentenc start anoth sentenc sbar one correspond time express issu specif word exampl pp bottom screen label pp time tm stand tempor preposit phrase express time thing rel small font fit one screen full pars tree correspond specif sentenc see complic way read three column left right blank audio interest factor exampl coordin conjunct indic cc two sentenc conjoin first one one start cd effect yield second one interest rate gener declin sharpli grammar allow coordin conjunct link entir sentenc let look peculiar penn treebank includ thing like complement exampl word includ gap exampl word none exampl sentenc say mari like chemistri hate biolog subject second verb hate also mari appear gap pars tree label string none also includ special categori call sbar come sbar theori exampl sentenc start complement exampl dont believ come tomorrow come tomorrow sbar tool use pars penn treebank allow search specif configur nontermin termin here oper less b give sentenc nontermin immedi domin parent b less less b mean somewher b pars tree necessarili parent see syntax pretti rich give good opportun find sentenc use exampl less penn treebank look like use well first disadvantag gener idea use treebank seem like nostart ask take lot work annot sentenc write grammar may superfici disadvantag actual advantag well use penn treebank count statist differ constitu phenomena exampl mani time nonphras turn specif righthand side part subject sentenc part object sentenc use train system use evalu system automat parser produc output compar output use welldefin statist techniqu manual annot also possibl use technolog build multilingu version penn treebank fact mani version exist mani european languag well chines korean let see use someth like penn treebank evalu pars evalu methodolog gener one use evalu classifi exampl binari classifi everi object data set label either true fals either posit neg tradit classif task document retriev one exampl document queri say whether document relev queri ye part speech tag classif task two class exampl word like round label noun verb adject sequenc part speech show earlier differ part speech pars also consid classif task essenti set word label either sentenc noun phrase verb phrase get class right get point get wrong lose point penn treebank split gener train set test set gener case classif evalu three set train set use learn data look like devtest set use test pars method without touch offici test set use devtest set go back retrain system see well work without go offici test set final offici test set your allow look parser develop look would essenti overfit result go valid theyr go accept research commun baselin use evalu dumb baselin exampl label everyth noun label everyth noun phrase also intellig baselin typic someth straightforward exampl label everi word like part speech look train data exampl word round either noun verb adject noun sens round frequent one label noun everi time give intellig baselin one say label everi word noun also human perform matrix tell accur system expect human dont agre perform mean system expect better human exampl part speech tag task human achiev accuraci expect part speech system go defin new method exampl let say statist parser abl compar baselin human perform use standard evalu method peopl typic use accuraci mani time label predict match correct label precis recal look earlier set slide tell us precis thing label posit mani actual posit accord train data recal measur thing could label posit base train data mani actual label posit extens accuraci precis recal take account fact may multipl refer conflict one anoth exampl differ human disagre specif label case take account interjudg agreement interjudg agreement tell percentag time human judg pick label final use matrix call kappa look like kappa normal perform system pa agreement system human judg multipl human judg pe expect agreement judg label someth anim exampl kappa greater assum integ agreement high somewher much lower exampl mean task welldefin probabl consid matter result system achiev meaning judg dont agre correct label less evalu done gener question judg agreement binari classif task high enough good think well answer question consid formula kappa plug number probabl agreement expect agreement chanc given two class comput kappa realiz valu kappa numer divid denomin equal said previou slide far accept interjudg agreement case say task welldefin evalu parser well standard techniqu evalu base precis recal whether get constitu right also label precis recal differ precis recal labor posit recal former case your look whether get right word bracket togeth properli label precis also want make sure label nontermin correct exampl label someth phrase actual noun phrase your go get full credit label precis recal even though would get full credit precis recal one metric use lot combin precis recal f f harmon mean precis recal highest precis recal high low one high one low vice versa one specif twist evalu parser someth call cross bracket correct pars b b b c group togeth join system produc ab group togeth join c your go get cross bracket error penn treebank corpu mention peopl usual train section use section develop like deaf test final evalu perform section let look exampl evalu gold standard japanes industri compani know better output rel state art parser charniak parser produc pars shown bottom see differ shown boldfac exampl use differ part speech word better instead label jjr label rbr go reduc label accuraci label precis recal label f overal output pars evalu spare sentenc bracket recal correctli pick bracket precis twothird mean three one pick charniak parser incorrect f measur harmon mean number complet match zero mean sentenc complet pars correctli get score cross theyr cross panel final tag accuraci word correctli label one mistak word better kind number would see report paper bracket measur tag accuraci complet match cross conclud section evalu parser next segment talk statist parser,[ 4  0  1  2 14]
269,Course3_W5-S1-L1_-_Parsing_Introduction_and_recap-Parsing_noun_sequences_-_20_slides_15-19,welcome natural language processing course today going continue one interesting topics natural language processing namely parsing want remind last time looked parsing already wanted bring parsing human languages different parsing programming languages parse programming language parse statements punctuation variables however designed unambiguous example c program interpreted one possible way theres main function inside main function theres variable declarations loop gets converted binary code unambiguous way parsing human language hand different give examples show challenge first problem coordination scope look sentence small boys girls playing sentence two different interpretations adjective small attach either boys case boys small girls small playing alternative interpretation boys girls small playing second example known prepositional phrase attachments prepositional phrase phrase starts preposition name suggests sentence saw man telescope telescope prepositional phrase starts preposition lets try interpret sentence one interpretation man saw carrying telescope another interpretation saw man using telescope see two different interpretations difficult fact impossible tell one intended third example gaps sometimes sentence may skip words understood context sentence still make sense may difficult process computationally look example mary likes physics hates chemistry want find likes physics sentence tells us right away mary hates chemistry well still mary sentence doesnt say implicitly infer subject hates chemistry structure sentence fourth example use words either particles prepositions english word particle run example run bill case modifies ran also beginning prepositional phrase like large bill obviously large bill valid prepositional phrase another sentence ran large hill case large hill prepositional phrase know different interpretation well easy move large bill beginning sentence say large hill ran say large bill ran example hill opposite preposition example bill particle fifth example difference gerunds adjectives ing words english either gerunds verbal forms used adjectives example one interpretation playing cards expensive play cards expensive lose money play cards time another interpretation could playing cards specific type cards kind cards playing cards expansive parsing sentence represented using phrase structure formalism simple example buster chased cat buster proper noun labeled nnp pack set chased verb end verb stands past tense chase ed determiner cat common noun labeled nn gram start combining words phrases cat noun phrase consists determiner noun chased cat verb phrase consists verb chased noun phrase cat top level noun phrase buster combined verb phrase chased cat form sentence okay going look two specific problems parsing detail first problem call parsing noun sequences english often noun modify another noun example fish tank fish tank well fish tank kind tank specifically kind tank holds fish lets look similar construction different interpretation fish net fish net net used catch fish fish soup fish soup soup made fish fish oil oil extracted fish see even though four noun phrases similar structure superficially different interpretations even ambiguous fish sauce fish sauce sauce fish dishes sauce made fish could either one english nounnoun compound time head compound second word pair example college junior junior college look similar words however college junior kind junior specifically one goes college rather high school whereas junior college kind college opposed say senior college guided college well said english typically head two word noun phrase second noun exceptions head first rule also appear sometimes attorney general exception kind general rather kind attorney kind phrases relatively rare english adjectives cases words like college college junior considered adjectives modify second noun cases explicit adjectives part noun phrase example new mexico new clearly adjective general manger word general adjective rather noun turns english people dont always agree theyre asked label general manger often say adjective followed noun often say two nouns noun phrases consisting nouns limited two nouns look example bottom page luxury car dealership well two possible interpretations least two dealership luxury cars car dealership luxurious interpretations non phrase connected structure phrase lets look example detroit tigers clearly noun phrase consists two nouns however look detroit tigers general manager know really consists two noun phrases detroit tigers followed general manager combined noun phrase however four consecutive nouns many interpretations lets see first happens three words salt lake city three words either combine first salt lake interpretation case would city salt lake city thats called lake city somehow use word salt modify case pretty obvious first interpretation matters second one periods nobody would even think happens four words think way put parenthesis around four words get reasonable interpretation sentence answer next slide okay asked question interpretation salt lake city mayor obviously solution one see salt lake combined first salt lake combined city gives us kind city want finally salt lake city mayor mayor salt lake city representation using parenthesis common natural language processing allows us write compacted presentations sentences phrases dont refer phrase structure diagrams lets look second example detroit tigers general manager see putting parentheses different location first want group together detroit tigers general manager finally combine two pairs one next actually thats mistake leland stanford junior university question students phrase like leland stanford junior university way official title university west coast two possible interpretations one pretty obvious wrong second one less obvious correct see answer next slide okay question leland stanford junior university two possible interpretations well one leland stanford person leland stanford junior son leland stanford senior apparently finally university named leland stanford junior lets little bit combinatorics noticed noun phrase consists two nouns one possible interpretation b combined together case detroit tigers theres one way combine two three nouns two possible interpretations connects b first group b connects c alternatively combine b c together group together question four nouns many different interpretations one shown b connected c connected connected like detroit tigers think total number ways combine four nouns answer next slide well turns four nouns five different interpretations obvious answer people say either four six look five realize correct answer five detroit tigers general manager example thats one shown first line also group last two words add second one first one see really five different ways group four nouns okay question happens four nouns heres example five group b first group c add cluster cd finally group two smaller clusters group five nouns question many different groupings five consecutive nouns answers next slide well turns solution known nth catalan number catalan reference catalan language rather reference belgian mathematician whose last name catalan closed form solution c subscript n one divided n times n choose n n greater equal zero first catalan numbers shown one two thats cases n equal zero one n equal three two combinations n equals four five combinations saw earlier numbers increase rather fast note powers two factorials completely different sequence sequence commonly known index online encyclopedia integer sequences visit url investigate sequence others detail turns catalan numbers appear often math many different locations addition parses sequences words appear two interesting geometrical cases one left number different ways convex polygon n sides cut triangles connecting vertices straight lines see different ways n case thats n example right number monotonic paths along edges grid consists n x n square cells constraint path pass diagonal first example one go east east east east north north north north second one east east east northeast north north north get idea see ways get bottom left corner top right corner using set constraints see catalan numbers versatile appear many different places natural language parsing concludes section parsing noun phrase sequences next set slides prepositional phrase attachment,Course3,W5-S1-L1,W5,S1,L1,-,5,1,1,welcom natur languag process cours today go continu one interest topic natur languag process name pars want remind last time look pars alreadi want bring pars human languag differ pars program languag pars program languag pars statement punctuat variabl howev design unambigu exampl c program interpret one possibl way there main function insid main function there variabl declar loop get convert binari code unambigu way pars human languag hand differ give exampl show challeng first problem coordin scope look sentenc small boy girl play sentenc two differ interpret adject small attach either boy case boy small girl small play altern interpret boy girl small play second exampl known preposit phrase attach preposit phrase phrase start preposit name suggest sentenc saw man telescop telescop preposit phrase start preposit let tri interpret sentenc one interpret man saw carri telescop anoth interpret saw man use telescop see two differ interpret difficult fact imposs tell one intend third exampl gap sometim sentenc may skip word understood context sentenc still make sens may difficult process comput look exampl mari like physic hate chemistri want find like physic sentenc tell us right away mari hate chemistri well still mari sentenc doesnt say implicitli infer subject hate chemistri structur sentenc fourth exampl use word either particl preposit english word particl run exampl run bill case modifi ran also begin preposit phrase like larg bill obvious larg bill valid preposit phrase anoth sentenc ran larg hill case larg hill preposit phrase know differ interpret well easi move larg bill begin sentenc say larg hill ran say larg bill ran exampl hill opposit preposit exampl bill particl fifth exampl differ gerund adject ing word english either gerund verbal form use adject exampl one interpret play card expens play card expens lose money play card time anoth interpret could play card specif type card kind card play card expans pars sentenc repres use phrase structur formal simpl exampl buster chase cat buster proper noun label nnp pack set chase verb end verb stand past tens chase ed determin cat common noun label nn gram start combin word phrase cat noun phrase consist determin noun chase cat verb phrase consist verb chase noun phrase cat top level noun phrase buster combin verb phrase chase cat form sentenc okay go look two specif problem pars detail first problem call pars noun sequenc english often noun modifi anoth noun exampl fish tank fish tank well fish tank kind tank specif kind tank hold fish let look similar construct differ interpret fish net fish net net use catch fish fish soup fish soup soup made fish fish oil oil extract fish see even though four noun phrase similar structur superfici differ interpret even ambigu fish sauc fish sauc sauc fish dish sauc made fish could either one english nounnoun compound time head compound second word pair exampl colleg junior junior colleg look similar word howev colleg junior kind junior specif one goe colleg rather high school wherea junior colleg kind colleg oppos say senior colleg guid colleg well said english typic head two word noun phrase second noun except head first rule also appear sometim attorney gener except kind gener rather kind attorney kind phrase rel rare english adject case word like colleg colleg junior consid adject modifi second noun case explicit adject part noun phrase exampl new mexico new clearli adject gener manger word gener adject rather noun turn english peopl dont alway agre theyr ask label gener manger often say adject follow noun often say two noun noun phrase consist noun limit two noun look exampl bottom page luxuri car dealership well two possibl interpret least two dealership luxuri car car dealership luxuri interpret non phrase connect structur phrase let look exampl detroit tiger clearli noun phrase consist two noun howev look detroit tiger gener manag know realli consist two noun phrase detroit tiger follow gener manag combin noun phrase howev four consecut noun mani interpret let see first happen three word salt lake citi three word either combin first salt lake interpret case would citi salt lake citi that call lake citi somehow use word salt modifi case pretti obviou first interpret matter second one period nobodi would even think happen four word think way put parenthesi around four word get reason interpret sentenc answer next slide okay ask question interpret salt lake citi mayor obvious solut one see salt lake combin first salt lake combin citi give us kind citi want final salt lake citi mayor mayor salt lake citi represent use parenthesi common natur languag process allow us write compact present sentenc phrase dont refer phrase structur diagram let look second exampl detroit tiger gener manag see put parenthes differ locat first want group togeth detroit tiger gener manag final combin two pair one next actual that mistak leland stanford junior univers question student phrase like leland stanford junior univers way offici titl univers west coast two possibl interpret one pretti obviou wrong second one less obviou correct see answer next slide okay question leland stanford junior univers two possibl interpret well one leland stanford person leland stanford junior son leland stanford senior appar final univers name leland stanford junior let littl bit combinator notic noun phrase consist two noun one possibl interpret b combin togeth case detroit tiger there one way combin two three noun two possibl interpret connect b first group b connect c altern combin b c togeth group togeth question four noun mani differ interpret one shown b connect c connect connect like detroit tiger think total number way combin four noun answer next slide well turn four noun five differ interpret obviou answer peopl say either four six look five realiz correct answer five detroit tiger gener manag exampl that one shown first line also group last two word add second one first one see realli five differ way group four noun okay question happen four noun here exampl five group b first group c add cluster cd final group two smaller cluster group five noun question mani differ group five consecut noun answer next slide well turn solut known nth catalan number catalan refer catalan languag rather refer belgian mathematician whose last name catalan close form solut c subscript n one divid n time n choos n n greater equal zero first catalan number shown one two that case n equal zero one n equal three two combin n equal four five combin saw earlier number increas rather fast note power two factori complet differ sequenc sequenc commonli known index onlin encyclopedia integ sequenc visit url investig sequenc other detail turn catalan number appear often math mani differ locat addit pars sequenc word appear two interest geometr case one left number differ way convex polygon n side cut triangl connect vertic straight line see differ way n case that n exampl right number monoton path along edg grid consist n x n squar cell constraint path pass diagon first exampl one go east east east east north north north north second one east east east northeast north north north get idea see way get bottom left corner top right corner use set constraint see catalan number versatil appear mani differ place natur languag pars conclud section pars noun phrase sequenc next set slide preposit phrase attach,[ 0  4  9 14 13]
270,Course3_W5-S1-L2_-_Prepositional_phrase_attachment_1-3_-_15_slides_14-50,welcome back natural language processing going continue next topic parsing specifically something called prepositional phrase attachment let remind penn treebank representation sentence looks like famous sentence number one penn treebank represents sentence pierre vinken years old joined board nonexecutive director november th see list type structure represents syntactic structure sentence noun phrase marked subject namely pierre vinken years old verb phrase consists join board nonexecutive director november many prepositional phrases sentence well one one nonexecutive director word preposition penn treebank tag set prepositions marked code question preposition non executive director modify board modify join general prepositional phrases modify either noun closest verb noun second sentence penn treebank mr vinken chairman elsevier nv comma dutch publishing group prepositional phrase elsevier preposition one attach attach nearest noun specifically chairman attach well prepositional phrase attachment problem automatically figuring attachment two types prepositional phrase attachment first kind called high attachment verbal attachment thats prepositional phrase director attaches nearest verb case join second example low attachment nominal attachment example elsevier modifies chairman mean modify chairman means chairman associated elsevier verb associated elsevier like first example director way person joined board modification board lets look phrase structure sentence includes prepositional phrase jane caught butterfly net see net prepositional phrase case modifies verb caught modify butterfly actually denotes way butterfly caught reason butterfly carrying net would prepositional phrase net listed noun phrase butterfly lets look examples prepositional phrase attachment real life first example lucys plane leaves detroit monday high low attachment well monday doesnt modify chart modifies leaves high attachment second example jenna met mike concert high low well high concert modifies verb meet doesnt modify mike third example painting must cost millions dollars case dollars modifies millions rather cost low attachment question following six examples tell case high attachment low attachment im going read first two read rest alicia ate spaghetti italy alicia ate spaghetti meatballs think six attachments ill give answers next slide okay question classify six sentences high low attachment answers front first one alicia ate spaghetti italy low attachment italy modifies spaghetti alicia ate spaghetti meatballs likely low attachment meatballs modifies spaghetti four instances high attachment alicia ate spaghetti fork even though preposition previous example instance high attachment fork modifies verb ate next example probably little funny alicia ate spaghetti justin definitely want justin modify ate verb spaghetti case alicia would eaten justin along spaghetti fifth example alicia ate spaghetti delight instance high attachment delight modifies word ate finally alicia ate spaghetti friday instance high attachment friday modifies alicia weeks ago reading newspaper came across actual headline includes ambiguous propositional phrase police shoot man box cutters really funny example really difficult shoot man using box cutter box cutters typically used shoot people interpretation fairly simple human clear man carrying box cutters police using box cutters shoot man example two possible syntactic interpretations sentence first one correct one one box cutters modifies man sense man carrying box cutters second example deliberately marked question mark left indicates questionable interpretation sentence box cutters level verb shoot incorrect see two parse trees look different one left box cutters part noun phrase man one right incorrect one box cutters shot okay preposition phrase attachment interesting problem natural language processing first important parsing sentences syntactically second nice introduction problem binary classification im going use opportunity introduce binary classification problem formulate problem binary classification problem well straight forward set instances consists input output input prepositional phrase possibly surrounding context around rest sentence output binary label convention use zero high attachment one low attachment practice dont look context surrounding prepositional phrase fact dont look prepositional phrase entirety either fact context consists four words preposition verb preposition noun preposition noun preposition example segments penn treebank pr lincoln joining board nonexecutive director sentence would represented fourth poll join board director think keep four words representation instance ignore everything else think ill show answer next slide question represent prepositional phrase instance four features preposition noun one noun two verb well two reasons first dont really need rest context information fact almost needed classify prepositional phrase either high low attachment turns contained four features second reason using consistent tuples four features allows us consistent machine learning approach lets look sample tuples penn treebank table shows ten examples first column indicates sentence prepositional phrases extracted instance four features verb first noun preposition second noun last column shows particular tuple classified high attachment low attachment want remind high attachment verb low attachment noun lets look example led team researchers researchers modifies team doesnt modify led therefore going label noun attachment turns literature linguistics particularly psycholinguistics agreement humans attachment sentences one theory kimball favors socalled right association rule says given new phrase two choices attachment people tend attach new phrase recent rightmost portion sentence theory favors low attachment however theres alternative interpretation called minimal attachment principle frazier favors attachment results syntactic tree sentence fewer additional syntactic nodes therefore favoring high attachment remember previous slide diagram corresponds high attachment fewer internal nodes compared one low attachment well practice turns none theories correct instances high low attachment real occurring human text lets look data set used research prepositional phrase attachments im going refer data set rrr r corresponding initials three researchers first published data set includes prepositional phrases extracted penn treebank divided three groups used training rest used testing representation used data sets consist four features table verb two nouns preposition example sentence sort presented bring attention problem actually sentence although preliminary findings reported year ago latest results appear todays new england journal medicine form likely bring attention problem pretty obvious sentence relevant decision prepositional phrase attachments case classify prepositional phrase problem going introduce supervised learning method dealing prepositional phrase attachment specifically want focus evaluate supervised learning heres works take set instances example rrr data set manually label ask human evaluaters read instances label either high low attachment split labeled data training testing set look training data find patterns something encode rules use automatically label rest data apply patterns testing set evaluate accuracy algorithm using metric called surprisingly enough accuracy accuracy percentage correct labels algorithm assigned testing data correct label certain tuple v system labels v score point also score point correct label n label n two cases label v mislabel n vice versa actually dont score points typical performance using accuracy magic lets say two classes equally likely randomly guess get average accuracy important come new algorithm binary classification problem able compare using reasonable evaluation method accuracy simple baseline method simplest baseline method example think well simplest supervised baseline method find common class label training data assuming one frequent almost always going case unless perfectly equal split going use common label assign instances testing data set minutes going look different algorithms compare base line,Course3,W5-S1-L2,W5,S1,L2,-,5,1,2,welcom back natur languag process go continu next topic pars specif someth call preposit phrase attach let remind penn treebank represent sentenc look like famou sentenc number one penn treebank repres sentenc pierr vinken year old join board nonexecut director novemb th see list type structur repres syntact structur sentenc noun phrase mark subject name pierr vinken year old verb phrase consist join board nonexecut director novemb mani preposit phrase sentenc well one one nonexecut director word preposit penn treebank tag set preposit mark code question preposit non execut director modifi board modifi join gener preposit phrase modifi either noun closest verb noun second sentenc penn treebank mr vinken chairman elsevi nv comma dutch publish group preposit phrase elsevi preposit one attach attach nearest noun specif chairman attach well preposit phrase attach problem automat figur attach two type preposit phrase attach first kind call high attach verbal attach that preposit phrase director attach nearest verb case join second exampl low attach nomin attach exampl elsevi modifi chairman mean modifi chairman mean chairman associ elsevi verb associ elsevi like first exampl director way person join board modif board let look phrase structur sentenc includ preposit phrase jane caught butterfli net see net preposit phrase case modifi verb caught modifi butterfli actual denot way butterfli caught reason butterfli carri net would preposit phrase net list noun phrase butterfli let look exampl preposit phrase attach real life first exampl luci plane leav detroit monday high low attach well monday doesnt modifi chart modifi leav high attach second exampl jenna met mike concert high low well high concert modifi verb meet doesnt modifi mike third exampl paint must cost million dollar case dollar modifi million rather cost low attach question follow six exampl tell case high attach low attach im go read first two read rest alicia ate spaghetti itali alicia ate spaghetti meatbal think six attach ill give answer next slide okay question classifi six sentenc high low attach answer front first one alicia ate spaghetti itali low attach itali modifi spaghetti alicia ate spaghetti meatbal like low attach meatbal modifi spaghetti four instanc high attach alicia ate spaghetti fork even though preposit previou exampl instanc high attach fork modifi verb ate next exampl probabl littl funni alicia ate spaghetti justin definit want justin modifi ate verb spaghetti case alicia would eaten justin along spaghetti fifth exampl alicia ate spaghetti delight instanc high attach delight modifi word ate final alicia ate spaghetti friday instanc high attach friday modifi alicia week ago read newspap came across actual headlin includ ambigu proposit phrase polic shoot man box cutter realli funni exampl realli difficult shoot man use box cutter box cutter typic use shoot peopl interpret fairli simpl human clear man carri box cutter polic use box cutter shoot man exampl two possibl syntact interpret sentenc first one correct one one box cutter modifi man sens man carri box cutter second exampl deliber mark question mark left indic question interpret sentenc box cutter level verb shoot incorrect see two pars tree look differ one left box cutter part noun phrase man one right incorrect one box cutter shot okay preposit phrase attach interest problem natur languag process first import pars sentenc syntact second nice introduct problem binari classif im go use opportun introduc binari classif problem formul problem binari classif problem well straight forward set instanc consist input output input preposit phrase possibl surround context around rest sentenc output binari label convent use zero high attach one low attach practic dont look context surround preposit phrase fact dont look preposit phrase entireti either fact context consist four word preposit verb preposit noun preposit noun preposit exampl segment penn treebank pr lincoln join board nonexecut director sentenc would repres fourth poll join board director think keep four word represent instanc ignor everyth els think ill show answer next slide question repres preposit phrase instanc four featur preposit noun one noun two verb well two reason first dont realli need rest context inform fact almost need classifi preposit phrase either high low attach turn contain four featur second reason use consist tupl four featur allow us consist machin learn approach let look sampl tupl penn treebank tabl show ten exampl first column indic sentenc preposit phrase extract instanc four featur verb first noun preposit second noun last column show particular tupl classifi high attach low attach want remind high attach verb low attach noun let look exampl led team research research modifi team doesnt modifi led therefor go label noun attach turn literatur linguist particularli psycholinguist agreement human attach sentenc one theori kimbal favor socal right associ rule say given new phrase two choic attach peopl tend attach new phrase recent rightmost portion sentenc theori favor low attach howev there altern interpret call minim attach principl frazier favor attach result syntact tree sentenc fewer addit syntact node therefor favor high attach rememb previou slide diagram correspond high attach fewer intern node compar one low attach well practic turn none theori correct instanc high low attach real occur human text let look data set use research preposit phrase attach im go refer data set rrr r correspond initi three research first publish data set includ preposit phrase extract penn treebank divid three group use train rest use test represent use data set consist four featur tabl verb two noun preposit exampl sentenc sort present bring attent problem actual sentenc although preliminari find report year ago latest result appear today new england journal medicin form like bring attent problem pretti obviou sentenc relev decis preposit phrase attach case classifi preposit phrase problem go introduc supervis learn method deal preposit phrase attach specif want focu evalu supervis learn here work take set instanc exampl rrr data set manual label ask human evaluat read instanc label either high low attach split label data train test set look train data find pattern someth encod rule use automat label rest data appli pattern test set evalu accuraci algorithm use metric call surprisingli enough accuraci accuraci percentag correct label algorithm assign test data correct label certain tupl v system label v score point also score point correct label n label n two case label v mislabel n vice versa actual dont score point typic perform use accuraci magic let say two class equal like randomli guess get averag accuraci import come new algorithm binari classif problem abl compar use reason evalu method accuraci simpl baselin method simplest baselin method exampl think well simplest supervis baselin method find common class label train data assum one frequent almost alway go case unless perfectli equal split go use common label assign instanc test data set minut go look differ algorithm compar base line,[ 0  4 14 13 12]
271,Course3_W5-S1-L3_-_Prepositional_phrase_attachment_2-3_-_20_slides_17-22,okay going continue prepositional phrase attachment said earlier baseline algorithm try data simplest algorithm going number algorithm one label every single tuple test set low attachment default lets also consider known random baseline random baseline even simpler one showed would assign item data set random label zero one practice random performance lower bound evaluate every non random method lets see measure performance supervised baseline method named algorithm number one official training data set rrr rate occurrence one label low attachment slightly well claim accuracy baseline method equal reasonable thing say think answer previous question whether correct measure accuracy wrong accuracy computed accuracy computed testing set training set use training set learn rule evaluate algorithm testing set particular example performance training table favor one low attachment however using official split accuracy method testing set actually tuples well looks like got decent performance even higher expected really good result shouldnt proud difference performance training data testing data could actually gone well opposite direction resulting performance even worse random would disaster let make interesting observations baseline method simple testing set randomly done full data set data set large enough one expect accuracy testing set comparable one training set however lets note penn treebank data set drawn business news stories wanted train method method data set test completely different sentences example fictional sentences novel possible likely two sets would different characteristics method complicated likely fit training data may learn patterns way specific training data may even appear testing data even worse may associated opposite class training data testing data okay talked lower bounds look upper bound accuracy well typically papers classification people use human performance one possible upper bound pp attachment using four features mentioned earlier turns human accuracy hypothetical algorithm achieves accuracy actually happy even though doesnt get close scale lower bound upper bound actually way upper bound good algorithm would happy algorithm far weve looked algorithms involve random simple supervised base lines use linguistic information course natural language processing figure use linguistic information improve performance method example looking training data may notice preposition much likely associated low attachment high attachment fact training data set percentage instances preposition training data high attachment therefore preposition valuable feature two main reasons think two reasons well two reasons preposition useful feature first informative saw time connected low attachment class second reason frequent consists entire training set agree reason one alone sufficient informative feature rare may ever observe testing set feature would good one consider penn treebank data set used evaluation pp attachment algorithms since important come new algorithm know proceed methodologically results valid publish new algorithm allowed look training data set also add second development data set use knowledge extracted however official testing data set never used evaluation algorithm completely finished case youre allowed look testing data sets contrary repeatedly tuning algorithm based performance designated test test youre going train system youre going report performance level reproducible new data approaches allowed nlp research submit paper makes clear youve using approach paper rejected summarily lets look training data set see patterns use addition fat pattern preposition addition preposition look prepositions example appears times training set attached noun rest attached verb well ratio similar baseline clearly preposition useful gives us little new information furthermore total number recurrences future training corpus small less total number prepositional phrases prepositions use features looked one others well example preposition associated high attachment occurrences addition preposition also fairly frequent represents prepositions training set consists another knowledge build simple decision list algorithm originally introduced brill resnick looks like okay algorithm far consists two simple rules preposition going label tuple low going consider preposition thats going label tuple high else going go back low default algorithm lets see accurate algorithm training set first rule would fire cases correctly labeled low another incorrectly labeled high attachment second rule expected accuracy training set would result additional decisions correctly processed high attachment mislabeled low attachment everything else would fall default rule case gives us another would label correctly low incorrectly label remainder cases high attachment overall going tuples correctly labeled first rule correctly labeled second rule correctly labeled default rule total correct decisions instances data set gives us accuracy see using small amount linguistic information make new algorithm algorithm two better default algorithm algorithm one training data surprising expected accuracy new algorithm less worst expected accuracy rules case thats rule number three accuracy also likely higher baseline two additional rules exploit structure training data give us additional performance benefits look sophisticated algorithm example look nouns verb combination example verb got appears time high attachment time low attachment lets consider different algorithm going call algorithm algorithm number rule one rule two however default rule label tuple high may ask want kind algorithm well turns look specifically instances remaining data points actually likely instances high attachment low attachment algorithm actually going outperform method two algorithm particular going achieve correct classifications using rule number correct assignments using rule number also whopping correct assignments using default rule gives us accuracy training set clearly want prefer algorithm algorithm based training set alone also notice algorithms three rules theyre simple imagine classifier consists rules one training examples rules going kind form even preposition nouns verbs going classify data point actual class observed tuple training set okay algorithm going work well training set fact going get accuracy close reject performance algorithms training set test set lets compare algorithms one three algorithm one labels everything low attachment remember achieves accuracy training set performing test set expected similar fact roughly ball park slightly higher reason difference almost test set distributed exactly way training set example illustrates variability text random splits data cases example swapped training test set performance algorithm would gone would achieved training set testing set average build kind experiment many times expect performance test data similar one training data simple algorithm ample training data lets look algorithm number three one includes one rule instances training set first rule following preposition verb casting first noun cloud second noun economy going label phrase high attachment notice way memorize first tuple training set use else another instance preposition verb open first noun second noun worms going label phrase low rule doesnt apply going go rest rules would complicated algorithm would definitely fit training day algorithm number three particular going achieve high performance training data fact performance way upper bound achieved humans specific training data rules learned going apply test set turns combinations possible count words test set match combination previously training set words algorithm number three learned lot good rules failed learn enough rules work seamlessly testing data fact accuracy testing data improvise number three little bit better algorithm three way described also everything misses default attachment case noun attachment algorithm going call number actually going achieve performance slightly baseline turns good enough either clearly none algorithms three going get anything close accuracy even close upper bound interesting point algorithm going achieve matter designed turns even training set mutually inconsistent labels data point example instance verdict case appears twice training data labeled high attachment low attachment total discrepancies training set caused use inconsistent human annotators whereas others correctly labeled disagree however context needed example entire paragraph entire document corrected would great,Course3,W5-S1-L3,W5,S1,L3,-,5,1,3,okay go continu preposit phrase attach said earlier baselin algorithm tri data simplest algorithm go number algorithm one label everi singl tupl test set low attach default let also consid known random baselin random baselin even simpler one show would assign item data set random label zero one practic random perform lower bound evalu everi non random method let see measur perform supervis baselin method name algorithm number one offici train data set rrr rate occurr one label low attach slightli well claim accuraci baselin method equal reason thing say think answer previou question whether correct measur accuraci wrong accuraci comput accuraci comput test set train set use train set learn rule evalu algorithm test set particular exampl perform train tabl favor one low attach howev use offici split accuraci method test set actual tupl well look like got decent perform even higher expect realli good result shouldnt proud differ perform train data test data could actual gone well opposit direct result perform even wors random would disast let make interest observ baselin method simpl test set randomli done full data set data set larg enough one expect accuraci test set compar one train set howev let note penn treebank data set drawn busi news stori want train method method data set test complet differ sentenc exampl fiction sentenc novel possibl like two set would differ characterist method complic like fit train data may learn pattern way specif train data may even appear test data even wors may associ opposit class train data test data okay talk lower bound look upper bound accuraci well typic paper classif peopl use human perform one possibl upper bound pp attach use four featur mention earlier turn human accuraci hypothet algorithm achiev accuraci actual happi even though doesnt get close scale lower bound upper bound actual way upper bound good algorithm would happi algorithm far weve look algorithm involv random simpl supervis base line use linguist inform cours natur languag process figur use linguist inform improv perform method exampl look train data may notic preposit much like associ low attach high attach fact train data set percentag instanc preposit train data high attach therefor preposit valuabl featur two main reason think two reason well two reason preposit use featur first inform saw time connect low attach class second reason frequent consist entir train set agre reason one alon suffici inform featur rare may ever observ test set featur would good one consid penn treebank data set use evalu pp attach algorithm sinc import come new algorithm know proceed methodolog result valid publish new algorithm allow look train data set also add second develop data set use knowledg extract howev offici test data set never use evalu algorithm complet finish case your allow look test data set contrari repeatedli tune algorithm base perform design test test your go train system your go report perform level reproduc new data approach allow nlp research submit paper make clear youv use approach paper reject summarili let look train data set see pattern use addit fat pattern preposit addit preposit look preposit exampl appear time train set attach noun rest attach verb well ratio similar baselin clearli preposit use give us littl new inform furthermor total number recurr futur train corpu small less total number preposit phrase preposit use featur look one other well exampl preposit associ high attach occurr addit preposit also fairli frequent repres preposit train set consist anoth knowledg build simpl decis list algorithm origin introduc brill resnick look like okay algorithm far consist two simpl rule preposit go label tupl low go consid preposit that go label tupl high els go go back low default algorithm let see accur algorithm train set first rule would fire case correctli label low anoth incorrectli label high attach second rule expect accuraci train set would result addit decis correctli process high attach mislabel low attach everyth els would fall default rule case give us anoth would label correctli low incorrectli label remaind case high attach overal go tupl correctli label first rule correctli label second rule correctli label default rule total correct decis instanc data set give us accuraci see use small amount linguist inform make new algorithm algorithm two better default algorithm algorithm one train data surpris expect accuraci new algorithm less worst expect accuraci rule case that rule number three accuraci also like higher baselin two addit rule exploit structur train data give us addit perform benefit look sophist algorithm exampl look noun verb combin exampl verb got appear time high attach time low attach let consid differ algorithm go call algorithm algorithm number rule one rule two howev default rule label tupl high may ask want kind algorithm well turn look specif instanc remain data point actual like instanc high attach low attach algorithm actual go outperform method two algorithm particular go achiev correct classif use rule number correct assign use rule number also whop correct assign use default rule give us accuraci train set clearli want prefer algorithm algorithm base train set alon also notic algorithm three rule theyr simpl imagin classifi consist rule one train exampl rule go kind form even preposit noun verb go classifi data point actual class observ tupl train set okay algorithm go work well train set fact go get accuraci close reject perform algorithm train set test set let compar algorithm one three algorithm one label everyth low attach rememb achiev accuraci train set perform test set expect similar fact roughli ball park slightli higher reason differ almost test set distribut exactli way train set exampl illustr variabl text random split data case exampl swap train test set perform algorithm would gone would achiev train set test set averag build kind experi mani time expect perform test data similar one train data simpl algorithm ampl train data let look algorithm number three one includ one rule instanc train set first rule follow preposit verb cast first noun cloud second noun economi go label phrase high attach notic way memor first tupl train set use els anoth instanc preposit verb open first noun second noun worm go label phrase low rule doesnt appli go go rest rule would complic algorithm would definit fit train day algorithm number three particular go achiev high perform train data fact perform way upper bound achiev human specif train data rule learn go appli test set turn combin possibl count word test set match combin previous train set word algorithm number three learn lot good rule fail learn enough rule work seamlessli test data fact accuraci test data improvis number three littl bit better algorithm three way describ also everyth miss default attach case noun attach algorithm go call number actual go achiev perform slightli baselin turn good enough either clearli none algorithm three go get anyth close accuraci even close upper bound interest point algorithm go achiev matter design turn even train set mutual inconsist label data point exampl instanc verdict case appear twice train data label high attach low attach total discrep train set caus use inconsist human annot wherea other correctli label disagre howev context need exampl entir paragraph entir document correct would great,[ 4  0  5 14 13]
272,Course3_W5-S1-L4_-_Prepositional_phrase_attachment_3-3_-_18_slides_12-53,okay looked several simple algorithms prepositional phrase attachment lets compare using accuracy metric going start algorithm algorithm said earlier need measure performance test set rather training set lets first look performance algorithm test set consists items need classify rule correctly classifies instances accuracy rule kicks gets accuracy pose move rule one achieves performance slightly less random thats overall accuracy algorithm combine three different rules pose lets compare performance algorithm algorithm test data test data set algorithm actually outperforms algorithm rule three actually majority case pose accuracy overall accuracy algorithm testing set data points correct total prepare little table shows algorithms looked far first algorithm default one rule accuracy test set switching algorithm includes prepositions plus default rule getting accuracy use better default go high remember algorithm one memorizes everything training data lot rules compared training set accuracy really close except discrepancies however test set accuracy algorithm algorithm memorizes everything also default everything sees test never seen training set number rules case training set accuracy previous algorithm almost test accuracy significantly higher compared algorithm fact see even many rules even correct default achieves performance even high algorithm let alone algorithm memorizing everything really good idea going way many rules youre even going get bang buck okay whats next far come simple algorithms use limited linguistics particular use prepositions rules aand able get accuracy test set two simple rules sources information use improve algorithm hm heres ideas try come good linguistically motivated features example look prepositions perhaps verbs nouns another idea come clever ways deal missing information example see tuple testing set exactly like one seen training data similar one training data maybe use similar data point reference instead using default third option use lexical semantic information example synonyms tuple talks cats testing set tuple talks dogs animals general perhaps generalize one assume correct classification cases finally use additional context beyond four featured types used far example use rest sentence use information adjacent sentences perhaps use information genre document heres statistics different features potentially consider grouped four categories prepositions verbs nouns nouns ive shown category prepositions think would useful really great feature split training set thats essentially one two split thats good bad algorithms seen far feature unlikely give us lot additional performance feature hand seems pretty good large bias towards first type attachment versus overall appears times fairly decent number likely feature useful said feature overwhelmingly refers first kind attachment cases next feature good even though frequent appears times training set see frequency high low attachment feature going useful verbs general good prepositions features frequent theres small set prepositions whereas number verbs large unlikely occur frequently enough useful include verbs set would probably pick follow include fairly large discrepancies two different classes nouns nouns even less likely used things consider combination ideas far one thing done tried collins brooks method based principle called backoff somehow combination algorithms far backoff allows us use label tuple seen training data tuple present training data going back similar tuple training data example observe certain preposition noun one noun two verb training data exact words except verb different three features use training example point approach problem algorithm enough training data learn possible combinations features many training data points would need achieve good performance lets math look data points test set would need total billion combinations arrive number well simply product numbers respectively number verbs noun ones prepositions noun twos test set impossible obtain much training data even could done would still need billions combinations test set larger im going introduce collins brooks algorithm heres works going count frequency hypothesis verb noun phrase one noun one preposition noun two divide sum tuples going estimate probability hypothesis given tuple using maximum likelihood estimate however specific feature appear training set going go back combinations three features instead four would say three really six different ways could combine three four verb noun preposition noun well collins brooks always insist preposition match preposition dont find matching triple go back doubles keeping preposition plus one features either verb noun noun finally theyre doubles either would resort singleton says look instances preposition appears training set instances appears certain hypothesis high low attachment fails go default label everything else high attachment zero approaching conclusion lecture lets see many algorithms seen far algorithm one rule achieves accuracy three rules look best class per preposition going rules accuracy collins brooks goes algorithms introduce today knearest neighbors choose tumbl supervised learning method put numbers perspective human performance using tuples human performance using entire sentence cannot compare automatic methods performance allowed look rest sentence summary methods discuss detail read web zhao lin uses nearest neighbors looks similar examples given tuple achieves accuracy similar method described zavrel daelemans veenstra based technique called memorybased learning techniques based boosting abney et al methods use semantics stetina nagao one best performances graphbased method toutanova et al want remind papers mentioned slide elsewhere lecture available course website hope time next lecture look papers even briefly thank much attention,Course3,W5-S1-L4,W5,S1,L4,-,5,1,4,okay look sever simpl algorithm preposit phrase attach let compar use accuraci metric go start algorithm algorithm said earlier need measur perform test set rather train set let first look perform algorithm test set consist item need classifi rule correctli classifi instanc accuraci rule kick get accuraci pose move rule one achiev perform slightli less random that overal accuraci algorithm combin three differ rule pose let compar perform algorithm algorithm test data test data set algorithm actual outperform algorithm rule three actual major case pose accuraci overal accuraci algorithm test set data point correct total prepar littl tabl show algorithm look far first algorithm default one rule accuraci test set switch algorithm includ preposit plu default rule get accuraci use better default go high rememb algorithm one memor everyth train data lot rule compar train set accuraci realli close except discrep howev test set accuraci algorithm algorithm memor everyth also default everyth see test never seen train set number rule case train set accuraci previou algorithm almost test accuraci significantli higher compar algorithm fact see even mani rule even correct default achiev perform even high algorithm let alon algorithm memor everyth realli good idea go way mani rule your even go get bang buck okay what next far come simpl algorithm use limit linguist particular use preposit rule aand abl get accuraci test set two simpl rule sourc inform use improv algorithm hm here idea tri come good linguist motiv featur exampl look preposit perhap verb noun anoth idea come clever way deal miss inform exampl see tupl test set exactli like one seen train data similar one train data mayb use similar data point refer instead use default third option use lexic semant inform exampl synonym tupl talk cat test set tupl talk dog anim gener perhap gener one assum correct classif case final use addit context beyond four featur type use far exampl use rest sentenc use inform adjac sentenc perhap use inform genr document here statist differ featur potenti consid group four categori preposit verb noun noun ive shown categori preposit think would use realli great featur split train set that essenti one two split that good bad algorithm seen far featur unlik give us lot addit perform featur hand seem pretti good larg bia toward first type attach versu overal appear time fairli decent number like featur use said featur overwhelmingli refer first kind attach case next featur good even though frequent appear time train set see frequenc high low attach featur go use verb gener good preposit featur frequent there small set preposit wherea number verb larg unlik occur frequent enough use includ verb set would probabl pick follow includ fairli larg discrep two differ class noun noun even less like use thing consid combin idea far one thing done tri collin brook method base principl call backoff somehow combin algorithm far backoff allow us use label tupl seen train data tupl present train data go back similar tupl train data exampl observ certain preposit noun one noun two verb train data exact word except verb differ three featur use train exampl point approach problem algorithm enough train data learn possibl combin featur mani train data point would need achiev good perform let math look data point test set would need total billion combin arriv number well simpli product number respect number verb noun one preposit noun two test set imposs obtain much train data even could done would still need billion combin test set larger im go introduc collin brook algorithm here work go count frequenc hypothesi verb noun phrase one noun one preposit noun two divid sum tupl go estim probabl hypothesi given tupl use maximum likelihood estim howev specif featur appear train set go go back combin three featur instead four would say three realli six differ way could combin three four verb noun preposit noun well collin brook alway insist preposit match preposit dont find match tripl go back doubl keep preposit plu one featur either verb noun noun final theyr doubl either would resort singleton say look instanc preposit appear train set instanc appear certain hypothesi high low attach fail go default label everyth els high attach zero approach conclus lectur let see mani algorithm seen far algorithm one rule achiev accuraci three rule look best class per preposit go rule accuraci collin brook goe algorithm introduc today knearest neighbor choos tumbl supervis learn method put number perspect human perform use tupl human perform use entir sentenc cannot compar automat method perform allow look rest sentenc summari method discuss detail read web zhao lin use nearest neighbor look similar exampl given tupl achiev accuraci similar method describ zavrel daeleman veenstra base techniqu call memorybas learn techniqu base boost abney et al method use semant stetina nagao one best perform graphbas method toutanova et al want remind paper mention slide elsewher lectur avail cours websit hope time next lectur look paper even briefli thank much attent,[ 4  0  5 14 13]
273,Course3_W5-S1-L5_-_Statistical_Parsing_-_29_slides_12-52,next segment going statistical parsing techniques used statistical parsing significantly different least time ones used deterministic parsing still based dynamic programming techniques like cky earley based different kind grammar grammar going look called probabilistic contextfree grammar looks similar contextfree grammar probabilities associated rules first lets look need probabilistic parsing need classic example time flies like arrow decided many different parses associated particular sentence however equally feasible english languages english language sentences clearly likely others need way rank based score since using score use probabilities turns best approach definition context free grammar looks similar deterministic one tuple n sigma r n set nonterminal symbols things like noun phrase prepositional phrase sigma terminal symbols things like words like cat eight rules rules different deterministic context grammar nonterminal symbol producing beta string combine terminals nonterminals however probability p associated rule one important thing keep mind rules left hand side probabilities add one theyll start somewhere set one known terminals particularly sentence heres example set context grammar another example split rule left hand side separate lines sample speech typically done one two possible techniques one hand look rules left hand side determine percentage follow right hand side another way using training based maximum likelihood estimates corpus used alternate bold face nonbold face indicate left hand side rows p zero going associated rule goes np vp particular probability going equal one rule left hand side p p belong category left hand side symbol noun phrase therefore sum two numbers p p also equal one third set rules prepositional phrases probability p equal one way expand pp forth lets see compute probability parse tree given sentence grammar probability parse tree based productions used build lets assume n productions going multiply probabilities productions form probability parse tree likely parse given sentence given specific probability context grammar determined follows among possible ts associated sentence maximizes probability pt mathematically speaking arg max pt ts set trees consisted want compute probability sentence would sum probabilities parses specific example one possible parse tree associated sentence grammar another possible parse remind difference two first one fork attaches verb second case attaches noun use formula previous slide determine probability well equal probability going np pp times p probability np going dtn times p long sequence probabilities multiplied together similarly probability equal p previous example nt vp p np goes dt cases p p see probabilities cases different want compute likely two parse trees find probabilities different everything else irrelevant comparison p appears cases drop comparison thing every pair matching probabilities p p p crossed going two probabilities remaining p top tree p bottom tree really hinges difference two determine trees likely p associated nt cos ntpp production p associated vp cost vppp whichever one likely likely matching parse going many different things probabilistic contextfree grammars im going list later going discuss others given grammar g sentence let ts parse trees correspond task one going find tree among ts maximizes probability pt second task find probability sentence ps sum possible tree probabilities pt second part important train probability context grammar need numbers estimate maximum likelihood probabilities okay lets see fun probabilistic context grammars techniques exist parsing sentences probabilistically well early algorithm deterministic grammar similarly probabilistic earley algorithm topdown parser like dynamic programming table also probabilistic cockekasamiyounger algorithm bottomup parser dynamic programming table like whats new dynamic programming table going take account probabilities different productions learn probabilities well theyre typically trained corpus like bank treebank meaning different probabilities well probability twice large another one somehow indicate specific parse one associated first probability twice probable one second parse possible sort reranking many different parses associated given sentence figure additional properties parses would decide pick one doesnt largest probability generating process combine output probabilistic parser states example overall probability takes account probability parse also probability correct speech recognition probability machine translation lets see estimate probabilities corpus done using method called maximum likelihood use following way look parsed training set get counts form count alpha produces beta divided count alpha example may ten instances noun phrases six noun phrase producing n followed pp n followed pp numerator np denominator divide going get maximum likelihood estimate probability okay heres example case row goes np vp want divide number times produces np vp number times appears okay example inaudible grammar jurafsky martin main airline reservations see first three rules left hand side left hand side goes np vp probability goes auxiliary noun phrase verb phrase probability finally remaining probability mass assigned role vp similarly every group transitions left hand side symbol probabilities equal one also applies right hand side picture lexicon lexicon also probabilistic category determiner includes three words articles abilities respectively another determinator probability probabilities compute probability specific parse tree using probabilistic version cky algorithm chart exactly start parts speech individual words associate probabilities well lets start word word determiner probability probability child noun combine two noun phrase determine noun going multiply three numbers probability determiner probability noun also probability noun phrase turning determiner noun thats times times im going finish example fill rest boxes exactly way deterministic example end day want get top right hand box whatever probabilities associated corresponding parse trees want compute probability entire sentence would add probabilities difference parse streams lead x heres question compute probability entire sentence using probabilistic cky following steps described homework assignment hints dont forget may multiple parses need add corresponding probabilities end segment next segment going talk method called lexicalized parsing,Course3,W5-S1-L5,W5,S1,L5,-,5,1,5,next segment go statist pars techniqu use statist pars significantli differ least time one use determinist pars still base dynam program techniqu like cki earley base differ kind grammar grammar go look call probabilist contextfre grammar look similar contextfre grammar probabl associ rule first let look need probabilist pars need classic exampl time fli like arrow decid mani differ pars associ particular sentenc howev equal feasibl english languag english languag sentenc clearli like other need way rank base score sinc use score use probabl turn best approach definit context free grammar look similar determinist one tupl n sigma r n set nontermin symbol thing like noun phrase preposit phrase sigma termin symbol thing like word like cat eight rule rule differ determinist context grammar nontermin symbol produc beta string combin termin nontermin howev probabl p associ rule one import thing keep mind rule left hand side probabl add one theyll start somewher set one known termin particularli sentenc here exampl set context grammar anoth exampl split rule left hand side separ line sampl speech typic done one two possibl techniqu one hand look rule left hand side determin percentag follow right hand side anoth way use train base maximum likelihood estim corpu use altern bold face nonbold face indic left hand side row p zero go associ rule goe np vp particular probabl go equal one rule left hand side p p belong categori left hand side symbol noun phrase therefor sum two number p p also equal one third set rule preposit phrase probabl p equal one way expand pp forth let see comput probabl pars tree given sentenc grammar probabl pars tree base product use build let assum n product go multipli probabl product form probabl pars tree like pars given sentenc given specif probabl context grammar determin follow among possibl ts associ sentenc maxim probabl pt mathemat speak arg max pt ts set tree consist want comput probabl sentenc would sum probabl pars specif exampl one possibl pars tree associ sentenc grammar anoth possibl pars remind differ two first one fork attach verb second case attach noun use formula previou slide determin probabl well equal probabl go np pp time p probabl np go dtn time p long sequenc probabl multipli togeth similarli probabl equal p previou exampl nt vp p np goe dt case p p see probabl case differ want comput like two pars tree find probabl differ everyth els irrelev comparison p appear case drop comparison thing everi pair match probabl p p p cross go two probabl remain p top tree p bottom tree realli hing differ two determin tree like p associ nt co ntpp product p associ vp cost vppp whichev one like like match pars go mani differ thing probabilist contextfre grammar im go list later go discuss other given grammar g sentenc let ts pars tree correspond task one go find tree among ts maxim probabl pt second task find probabl sentenc ps sum possibl tree probabl pt second part import train probabl context grammar need number estim maximum likelihood probabl okay let see fun probabilist context grammar techniqu exist pars sentenc probabilist well earli algorithm determinist grammar similarli probabilist earley algorithm topdown parser like dynam program tabl also probabilist cockekasamiyoung algorithm bottomup parser dynam program tabl like what new dynam program tabl go take account probabl differ product learn probabl well theyr typic train corpu like bank treebank mean differ probabl well probabl twice larg anoth one somehow indic specif pars one associ first probabl twice probabl one second pars possibl sort rerank mani differ pars associ given sentenc figur addit properti pars would decid pick one doesnt largest probabl gener process combin output probabilist parser state exampl overal probabl take account probabl pars also probabl correct speech recognit probabl machin translat let see estim probabl corpu done use method call maximum likelihood use follow way look pars train set get count form count alpha produc beta divid count alpha exampl may ten instanc noun phrase six noun phrase produc n follow pp n follow pp numer np denomin divid go get maximum likelihood estim probabl okay here exampl case row goe np vp want divid number time produc np vp number time appear okay exampl inaud grammar jurafski martin main airlin reserv see first three rule left hand side left hand side goe np vp probabl goe auxiliari noun phrase verb phrase probabl final remain probabl mass assign role vp similarli everi group transit left hand side symbol probabl equal one also appli right hand side pictur lexicon lexicon also probabilist categori determin includ three word articl abil respect anoth determin probabl probabl comput probabl specif pars tree use probabilist version cki algorithm chart exactli start part speech individu word associ probabl well let start word word determin probabl probabl child noun combin two noun phrase determin noun go multipli three number probabl determin probabl noun also probabl noun phrase turn determin noun that time time im go finish exampl fill rest box exactli way determinist exampl end day want get top right hand box whatev probabl associ correspond pars tree want comput probabl entir sentenc would add probabl differ pars stream lead x here question comput probabl entir sentenc use probabilist cki follow step describ homework assign hint dont forget may multipl pars need add correspond probabl end segment next segment go talk method call lexic pars,[ 0  4 14 13 12]
274,Course3_W5-S1-L6_-_Lexicalized_Parsing_-_12_slides_09-06,next segment going lexicalized parsing lexicalized parsing one step context free parsing allows us capture regularities involved specific words specific heads phrases even considering well reason pcfgs cfgs general limitations example probabilities dont depend specific words verb like give verb like see know different structure give takes two arguments form give someone something versus verb see takes one argument see something context grammar possible take account kind argument structuralism also possible disambiguate sentences based semantic information example fact eating pizza pepperoni different eating pizza fork pepperoni food item fork tool cannot context grammar figure pepperoni likely associate low attachment versus fork high attachment basic idea lexicalized grammar use head phrase additional source information example instead rule says verb phrase gets transformed verb going rule says verb phrase whose head ate going transformed verb whose head ate well heres example parse tree previous segment sentence child ate cake fork want convert lexicalized form done starting lowest level parse tree terminal symbols words extracting heads every constituent recursive way reach top sentence going start individual words phrase cake noun phrase rule noun phrase says head noun phrase equal head noun since head noun cake head entire noun phrase two also cake something happens fork head noun phrase labeled number noun phrase three going fork main noun fork similarly head noun phrase one going child head noun child point still expand several heads head prepositional phrase fork definition preposition forgetting fork point start replacing head prepositional phrase word head verb phrase verb verb phrase ate cake going ate head ate going propagated one level next vp also going ate head two choices head entire sentence child head noun phrase ate head verb phrase case one think wins well rule head sentence head main verb phrase going propagate ate head entire sentence every production grammar rewritten lexicalized form example rule top going headed ate gives us np headed child followed vp headed ate one popular parsers involve lexicalization collins parser version one im going use example model generative foreign types rules lefthand side turned sequence ls h set rs ls things appear head rs things appear head generative model goes follows h gets generated first ls get generated right left n finally rs going left right probabilities estimated using maximum likelihood estimation penn treebank heres example want find conditional probability preposition given verb think often labelled preposition think labelled verb go ahead compute probability dividing actual occurrences word right head think divided total number symbols appear head think also sort smoothing data sparse smoothed probability involves lexicalized version given lexicalized version think going linear combination three things one actual counts think lexicalized available also look case lexicalized verb finally case none lexicalized helps us address issues lexicalized grammar specifically training data much sparse train lexicalized grammar many different combinations take account leads combinatorial explosion need parametrize grammar one thing want bring called discriminative reranking technique used often probabilistic context grammar many parses associated input sentence therefore similar clear wanted pick one whether one highest probability best one instead produce best list use additional information pick parses likely best ones considerations take account end best list things like parse tree depth want make sure dont get sentences trees deep shallow also take account whether instances left attachment right attachment example english likely right attachments left attachments would rule parse tree many left attachments also look discourse structure example certain sentence second one paragraph may want treat differently likely include enough word expression refer back previous sentence think features used reranking well lets see question considerations reranking addition parse tree depth attachment direction discourse structure well consistency across sentences example may word interpreted one way one sentence want assure interpreted exactly way next sentence also take account information comes stages natural language pipeline example part speech sequence constraints speech recognition one thing may wondering well parsers work well heres examples f measure computed sentences words less standard evaluation metric statistical parsers heres statistics ballpark low earliest parser wanted show charniak parser one achieves f theres modified versions better version charniak parsers charniak johnson achieves highest possible performance f namely concludes section statistical parsing next segment going look different type parsing called dependency parsing,Course3,W5-S1-L6,W5,S1,L6,-,5,1,6,next segment go lexic pars lexic pars one step context free pars allow us captur regular involv specif word specif head phrase even consid well reason pcfg cfg gener limit exampl probabl dont depend specif word verb like give verb like see know differ structur give take two argument form give someon someth versu verb see take one argument see someth context grammar possibl take account kind argument structur also possibl disambigu sentenc base semant inform exampl fact eat pizza pepperoni differ eat pizza fork pepperoni food item fork tool cannot context grammar figur pepperoni like associ low attach versu fork high attach basic idea lexic grammar use head phrase addit sourc inform exampl instead rule say verb phrase get transform verb go rule say verb phrase whose head ate go transform verb whose head ate well here exampl pars tree previou segment sentenc child ate cake fork want convert lexic form done start lowest level pars tree termin symbol word extract head everi constitu recurs way reach top sentenc go start individu word phrase cake noun phrase rule noun phrase say head noun phrase equal head noun sinc head noun cake head entir noun phrase two also cake someth happen fork head noun phrase label number noun phrase three go fork main noun fork similarli head noun phrase one go child head noun child point still expand sever head head preposit phrase fork definit preposit forget fork point start replac head preposit phrase word head verb phrase verb verb phrase ate cake go ate head ate go propag one level next vp also go ate head two choic head entir sentenc child head noun phrase ate head verb phrase case one think win well rule head sentenc head main verb phrase go propag ate head entir sentenc everi product grammar rewritten lexic form exampl rule top go head ate give us np head child follow vp head ate one popular parser involv lexic collin parser version one im go use exampl model gener foreign type rule lefthand side turn sequenc ls h set rs ls thing appear head rs thing appear head gener model goe follow h get gener first ls get gener right left n final rs go left right probabl estim use maximum likelihood estim penn treebank here exampl want find condit probabl preposit given verb think often label preposit think label verb go ahead comput probabl divid actual occurr word right head think divid total number symbol appear head think also sort smooth data spars smooth probabl involv lexic version given lexic version think go linear combin three thing one actual count think lexic avail also look case lexic verb final case none lexic help us address issu lexic grammar specif train data much spars train lexic grammar mani differ combin take account lead combinatori explos need parametr grammar one thing want bring call discrimin rerank techniqu use often probabilist context grammar mani pars associ input sentenc therefor similar clear want pick one whether one highest probabl best one instead produc best list use addit inform pick pars like best one consider take account end best list thing like pars tree depth want make sure dont get sentenc tree deep shallow also take account whether instanc left attach right attach exampl english like right attach left attach would rule pars tree mani left attach also look discours structur exampl certain sentenc second one paragraph may want treat differ like includ enough word express refer back previou sentenc think featur use rerank well let see question consider rerank addit pars tree depth attach direct discours structur well consist across sentenc exampl may word interpret one way one sentenc want assur interpret exactli way next sentenc also take account inform come stage natur languag pipelin exampl part speech sequenc constraint speech recognit one thing may wonder well parser work well here exampl f measur comput sentenc word less standard evalu metric statist parser here statist ballpark low earliest parser want show charniak parser one achiev f there modifi version better version charniak parser charniak johnson achiev highest possibl perform f name conclud section statist pars next segment go look differ type pars call depend pars,[ 0  4 14 13 12]
275,Course3_W5-S1-L7_-_Dependency_Parsing_-_28_slides_18-04,welcome back natural language processing course next section going dependency parsing everything talked parsing far deal constituent parsing interestingly enough many years perhaps decades prevalent kind parsing last year maybe ten years become fashionable use instead constituency parsing something called dependency parsing simpler method achieves good accuracy many different nlp tasks dependency parsing talk dependency parsing let explain dependency structure sentence lets consider simple noun phrase blue house noun phrase head noun house blue somehow modifies house entire phrase type house type blue dependency house blue house root tree blue modifier sort additional information gets added house specifier terminology used linguistics following word blue example called either modifier dependent child subordinate word house head also called governor parent regent see terms used different kinds literature lets look specific complicated sentence unionized workers usually better paid nonunion counterparts want build entire structure centers based dependencies adjacent awards word paid considered important word sentence main predicate main verb subject workers theres dependency paid workers paid head workers dependent workers modified word unionized shows kind workers talking fill entire chart adding dependencies cover words see word paid doesnt parent root sentence theres another notation people use dependencies verb top vbn example paid children labeled different types dependencies example prepositional phrase add verbal modifier subject also notice connection dependency structure sentence phrase structure sentence im going alternate two slides second phrase structure sentence turns noun phrase verb phrase heres equivalent dependency structure verb top turns methods take structure like one use called head rules convert dependency structure automatic dependency grammar grammar captures lexicalsyntactic dependencies words top level predicate sentence becomes root parse tree theyre much simpler parse context free grammars theyre useful languages free word order czech latin identify heads pair words well lets use terminology h head modifier general choose one head use following principles h determines syntactic category construct h determines semantic category construct h required whereas skipped example blue house house cannot skipped thats head blue skipped may also given specific type dependency maybe fixed linear position modifier respect heads example adjective modify noun right next case example list rules come michael collins future dissertation see every case paramount terminal list priority children lets look example example may following children constituents different kinds prepositions verb phrase another sentence bar adjectival phrase rules according collins pick first one list appears example choose verb phrase preposition going pick preposition prepositions choose lets say verb phrase sentence well pick verb phrase appears left also notice actions going way example adjectival phrase go left right verbial phrase go right left theres one candidate label among children going pick one comes right instead one comes left heres main techniques dependency parsing first type based dynamic programming use methods similar cky cubic complexity one famous paper jason eisner column shows case heres example sentence dependency representation mary likes cats predicate sentence likes two arguments two modifiers likes mary cats build kind tree using dynamic programming starting equivalent cky parser case propagate head top production example nsubj dependency direct object dependency would mapped labels parse tree another set techniques dependency parsing based constraint satisfaction kind technique introduced early maruyama karlsson others heres example set constraints words labels dependencies particular case rules says determiner modifies noun right label nmod previous line see position part speech general constraintbased methods problematic constraints function np complete problem need significant number heuristics make work practice general idea base represent sentence form constraint graph includes core grammar domains nodes constraints find assignment doesnt contradict constraints one assignment exists add additional constraints narrow choices parse third type category dependency parsing techniques based deterministic parsing done many different people including covington recently maltparser created joakim nivre colleagues maltparser method similar shiftreduce parser context free grammars reduce operator creates dependencies head either left right essentially creates dependency arcs left right also techniques based graphs specifically techniques based maximum spanning trees pioneered mcdonald pereira others around one issue dependency parsing projectivity nonprojectivity look sentence come last year see according one parses one shown sentence dont crossing links whereas parse bottom come connected year connected allow dependency trees include causing dependencies like known nonprojective dependency tree turns important languages three word orders russian czech much english thats english theres quite lot literature includes projective parts addition nonprojective ones lets talk specific examples dependency parsing example mcdonald et al paper idea mcdonald paper dependency parsing problem reduced equivalent problem searching maximum spanning tree directed graph already exist one methods chu liu edmonds work efficiently find maximum spanning tree directed graphs heres example sentence john saw mary present graph three words john saw mary plus additional root word represented nodes graph weights determined training data perform maximum spanning tree algorithm graph youre going get parse looks like root pointing saw main verb saw turn pointing two dependents john mary one popular techniques dependency parsing last years called maltparser introduced joakim nivre undergone many different changes years version im going describe one many similar method shiftreduce parsing includes components shiftreduce parse specifically stack buffer also includes set dependencies correspond dependency arcs reduce operations combines element stack one buffer arceager one versions maltparser includes shift reduce plus leftarc rightarc operation heres examples four operations shift example word gets removed sentence added stack reduce word top stack gets converted left right arc examples word first remaining buffer combined words top stack combine new dependency heres example sentence people want free beginning start style contains symbol root buffer contains people want free list arcs empty first operation shift operation would take word people move buffer stack leaving rest words want free buffer next operation leftarc operation remove word people top stack combine first word buffer want create new arc called nsubj labeled next iteration combine word want root thats rightarc operation removing word want stack combining root note actions selected using classifier looking features local current work theres search involved makes algorithm efficient final list arcs returned end parse full dependency tree sentence lets look evaluation metrics dependency parsing label dependency accuracy similar labeled constituent accuracy traditional parsing methods number correct dependencies divided number dependencies could heres example output dependency parser following standard representation first column word number second word word sentence third head dependency part speech word number head label dependency first example unionized modifies workers thats word number one modifying word number two name dependency nmod complexities different algorithms dependency parcing well projective cky method order n fifth power better version eisner cubic non projective method like mst using chuliuedmonds method maximum spanning trees quadratic complexity finally maltparser linear complexity turns dependency information useful information extraction tasks define different rules tell sort dependency subtrees connected specific relations heres example paper eric canedo goal identify interactions proteins medical literature dependency tree sentence top starting demonstrated two children results interacts full sentence results demonstrated kaic interacts arithmetically kaia kaib sasa see order identify important interactions example kaic kaia kaic sasa look paths connects dependency parse example kaic connected kaia using following paths first dependency named sub theres node called interacts theres dependency called prep finally goes sasa finally gets another dependency called conjunction finally node target kaia train system already labeled independencies identify new pairs independencies new text system would learn patterns kind another application depedency called dependency kernels dependency kernel described one previous segments technique decides similar two sentences based similar dependency structures example bunescu mooney wanted figure relations appear different sentences example one relations protesters located stations shortest path two words dependency graph goes protesters seized stations sets dependencies used identifying similar two sentences im going conclude segment pointers external links resources related dependency parsing first one data set evaluation methodology conllx shared task multilingual dependency parsing theres another interesting pointer one earliest dependency tree banks called prague dependency treebank next wiki page kinds methods dependency parsing pointer maltparser method nivre earlier dependency parsers dekang lins minipar link parser carnegie mellons dan sleator davy temper give illustration one sentence shows first sentence penn treebank presented dependency structure extracted viewer comes prague dependency treebank sentence pierre winken years old joined board non executive director november th see root connected join main predicate sentence modifiers join listed children example bored november recursively dependencies shown tree example modifies november nonexecutive modifies director little bit introduction dependency parsing going continue next set slides topic alternative syntactic representations,Course3,W5-S1-L7,W5,S1,L7,-,5,1,7,welcom back natur languag process cours next section go depend pars everyth talk pars far deal constitu pars interestingli enough mani year perhap decad preval kind pars last year mayb ten year becom fashion use instead constitu pars someth call depend pars simpler method achiev good accuraci mani differ nlp task depend pars talk depend pars let explain depend structur sentenc let consid simpl noun phrase blue hous noun phrase head noun hous blue somehow modifi hous entir phrase type hous type blue depend hous blue hous root tree blue modifi sort addit inform get ad hous specifi terminolog use linguist follow word blue exampl call either modifi depend child subordin word hous head also call governor parent regent see term use differ kind literatur let look specif complic sentenc union worker usual better paid nonunion counterpart want build entir structur center base depend adjac award word paid consid import word sentenc main predic main verb subject worker there depend paid worker paid head worker depend worker modifi word union show kind worker talk fill entir chart ad depend cover word see word paid doesnt parent root sentenc there anoth notat peopl use depend verb top vbn exampl paid children label differ type depend exampl preposit phrase add verbal modifi subject also notic connect depend structur sentenc phrase structur sentenc im go altern two slide second phrase structur sentenc turn noun phrase verb phrase here equival depend structur verb top turn method take structur like one use call head rule convert depend structur automat depend grammar grammar captur lexicalsyntact depend word top level predic sentenc becom root pars tree theyr much simpler pars context free grammar theyr use languag free word order czech latin identifi head pair word well let use terminolog h head modifi gener choos one head use follow principl h determin syntact categori construct h determin semant categori construct h requir wherea skip exampl blue hous hous cannot skip that head blue skip may also given specif type depend mayb fix linear posit modifi respect head exampl adject modifi noun right next case exampl list rule come michael collin futur dissert see everi case paramount termin list prioriti children let look exampl exampl may follow children constitu differ kind preposit verb phrase anoth sentenc bar adjectiv phrase rule accord collin pick first one list appear exampl choos verb phrase preposit go pick preposit preposit choos let say verb phrase sentenc well pick verb phrase appear left also notic action go way exampl adjectiv phrase go left right verbial phrase go right left there one candid label among children go pick one come right instead one come left here main techniqu depend pars first type base dynam program use method similar cki cubic complex one famou paper jason eisner column show case here exampl sentenc depend represent mari like cat predic sentenc like two argument two modifi like mari cat build kind tree use dynam program start equival cki parser case propag head top product exampl nsubj depend direct object depend would map label pars tree anoth set techniqu depend pars base constraint satisfact kind techniqu introduc earli maruyama karlsson other here exampl set constraint word label depend particular case rule say determin modifi noun right label nmod previou line see posit part speech gener constraintbas method problemat constraint function np complet problem need signific number heurist make work practic gener idea base repres sentenc form constraint graph includ core grammar domain node constraint find assign doesnt contradict constraint one assign exist add addit constraint narrow choic pars third type categori depend pars techniqu base determinist pars done mani differ peopl includ covington recent maltpars creat joakim nivr colleagu maltpars method similar shiftreduc parser context free grammar reduc oper creat depend head either left right essenti creat depend arc left right also techniqu base graph specif techniqu base maximum span tree pioneer mcdonald pereira other around one issu depend pars project nonproject look sentenc come last year see accord one pars one shown sentenc dont cross link wherea pars bottom come connect year connect allow depend tree includ caus depend like known nonproject depend tree turn import languag three word order russian czech much english that english there quit lot literatur includ project part addit nonproject one let talk specif exampl depend pars exampl mcdonald et al paper idea mcdonald paper depend pars problem reduc equival problem search maximum span tree direct graph alreadi exist one method chu liu edmond work effici find maximum span tree direct graph here exampl sentenc john saw mari present graph three word john saw mari plu addit root word repres node graph weight determin train data perform maximum span tree algorithm graph your go get pars look like root point saw main verb saw turn point two depend john mari one popular techniqu depend pars last year call maltpars introduc joakim nivr undergon mani differ chang year version im go describ one mani similar method shiftreduc pars includ compon shiftreduc pars specif stack buffer also includ set depend correspond depend arc reduc oper combin element stack one buffer arceag one version maltpars includ shift reduc plu leftarc rightarc oper here exampl four oper shift exampl word get remov sentenc ad stack reduc word top stack get convert left right arc exampl word first remain buffer combin word top stack combin new depend here exampl sentenc peopl want free begin start style contain symbol root buffer contain peopl want free list arc empti first oper shift oper would take word peopl move buffer stack leav rest word want free buffer next oper leftarc oper remov word peopl top stack combin first word buffer want creat new arc call nsubj label next iter combin word want root that rightarc oper remov word want stack combin root note action select use classifi look featur local current work there search involv make algorithm effici final list arc return end pars full depend tree sentenc let look evalu metric depend pars label depend accuraci similar label constitu accuraci tradit pars method number correct depend divid number depend could here exampl output depend parser follow standard represent first column word number second word word sentenc third head depend part speech word number head label depend first exampl union modifi worker that word number one modifi word number two name depend nmod complex differ algorithm depend parc well project cki method order n fifth power better version eisner cubic non project method like mst use chuliuedmond method maximum span tree quadrat complex final maltpars linear complex turn depend inform use inform extract task defin differ rule tell sort depend subtre connect specif relat here exampl paper eric canedo goal identifi interact protein medic literatur depend tree sentenc top start demonstr two children result interact full sentenc result demonstr kaic interact arithmet kaia kaib sasa see order identifi import interact exampl kaic kaia kaic sasa look path connect depend pars exampl kaic connect kaia use follow path first depend name sub there node call interact there depend call prep final goe sasa final get anoth depend call conjunct final node target kaia train system alreadi label independ identifi new pair independ new text system would learn pattern kind anoth applic deped call depend kernel depend kernel describ one previou segment techniqu decid similar two sentenc base similar depend structur exampl bunescu mooney want figur relat appear differ sentenc exampl one relat protest locat station shortest path two word depend graph goe protest seiz station set depend use identifi similar two sentenc im go conclud segment pointer extern link resourc relat depend pars first one data set evalu methodolog conllx share task multilingu depend pars there anoth interest pointer one earliest depend tree bank call pragu depend treebank next wiki page kind method depend pars pointer maltpars method nivr earlier depend parser dekang lin minipar link parser carnegi mellon dan sleator davi temper give illustr one sentenc show first sentenc penn treebank present depend structur extract viewer come pragu depend treebank sentenc pierr winken year old join board non execut director novemb th see root connect join main predic sentenc modifi join list children exampl bore novemb recurs depend shown tree exampl modifi novemb nonexecut modifi director littl bit introduct depend pars go continu next set slide topic altern syntact represent,[14  4  0 13 12]
276,Course3_W5-S1-L8_-_Alternative_Parsing_Formalisms_-_10_slides_10-03,going look specific categories grammars powerful fullfledged contextsensitive grammars would difficult parse time powerful regular contextfree grammars therefore capture types languages nth power b nth power essentially string followed string bs length something contextfree grammars cannot capture introduce let first mention briefly socalled tree substitution grammars actually contextfree grammars equivalent contextfree grammars move ones context free tree substitution grammar allows terminals generate entire tree fragments mentioned tree substitution grammars formerly equivalent contextfree grammars however next step tree substitution grammars socalled tree adjoining grammar powerful contextfree grammars example going look called combinatory categorial grammar ccg also powerful contextfree grammars lets look tags tags first theyre like tree substitution grammars allow adjunction adjunction suppose parse tree verb phrase embedded inside adjunction operator allows create new node put inside tree push existing subtree new verb phrase gets original verb phrase one descendents operation cannot modeled contextfree grammars used generate languages nth power b nth power c nth power even ww socalled crossserial dependencies wws string repeated twice example sentence mary gave book magazine chen mike example crossserial dependency book associated chen magazine associated mike kind sentence cannot generated contextfree grammar generated tag expressive power tags formally powerful cfgs time theyre powerful full contextsensitive grammars let show little bit diversion theres interesting game download ltaggame website bunch cards productions joining grammars combine together form sentences want something notice original set cards include words appropriate children therefore slightly pgrated version ltaggame designated families also download website category grammars called ccg popular days parsing generation involves introduction something called complex type complex type either xy xy complex types take argument type return object type x difference direction slash forward slash xy means appear right x xy means appear left x heres example simple grammar pronoun labelled noun phrase books another noun labelled noun phrase sleep type sleep well complex type says sleep kind thing combine noun phrase righthand side return sentence similarly lexicon antifour enjoy something combine right nonphrase return something complex type sleep heres example sentences sentence sleep parsed following way noun phrase sleep back word slash np two combined result sleep valid sentence according parse sentence enjoy books example transitive verb enjoy gets combined books replacing noun phrase right returning type left type enjoy books snp gets combined derive becomes interesting example shows point view enjoy books sleep exact syntactic structure theyre things combined subject form sentence expressive power ccgs generate language n b n c n n n greater something contextfree grammars cannot want acknowledge jonathan kummerfeld aleka blackwell patrick littell example next topic going brief semantic parsing everything weve discussed far syntactic parsing wanted build syntactic representation sender know subject verb direct object never discussed semantics sentence last years lot interest semantic parsing purposes questionanswering generation summarization semantic parsing look like well simple idea every grammar going associated syntactic structure also known compositional semantics compositional semantics meaning word word meaning combination words formula tells combine meanings individual words best illustrated example lets look simple sentence javier eats pizza drama javier associated noun semantics noun word object represented word javier pizza represented meaning word pizza eats verb represented logical formula corresponds longer function two arguments x predicate eat x entity eating entity receiving eating point combine lambda function arguments able come grounded expression entire sentence provided works well first combine eat pizza together going replace pizza known substitution formal logic going replace lambda function two arguments exy new function bound pizza still remaining unbound variable x still lambda expression looking missing argument combined happens next stage combine verb phrase node noun node javier get full meaning sentence eat predicate javier substituted x expression pizza remains entire semantic presentation sentence predicate called eat first argument javier second argument pizza im going go semantic parsing detail want encourage look recent literature acl topic seems one hottest areas next years im going conclude segment short link short set links follow parsing linguistics olympiad ones selected relevant section twodee two dimensional parser problem written jason eisner competition one two tree problem written noah smith kevin gimbel jason eisner problem set two problems related categorical combinatorial grammars ccg combining categories tok pisin used competition finally grammar rules simple problem contextfree grammars last years competition andrea schalley pat littell enjoy problems youve solved check solutions website enough one example parser skating figures also written pat littell one earliest competitions concludes section alternative grammatical formulisms parsing thank attention,Course3,W5-S1-L8,W5,S1,L8,-,5,1,8,go look specif categori grammar power fullfledg contextsensit grammar would difficult pars time power regular contextfre grammar therefor captur type languag nth power b nth power essenti string follow string bs length someth contextfre grammar cannot captur introduc let first mention briefli socal tree substitut grammar actual contextfre grammar equival contextfre grammar move one context free tree substitut grammar allow termin gener entir tree fragment mention tree substitut grammar formerli equival contextfre grammar howev next step tree substitut grammar socal tree adjoin grammar power contextfre grammar exampl go look call combinatori categori grammar ccg also power contextfre grammar let look tag tag first theyr like tree substitut grammar allow adjunct adjunct suppos pars tree verb phrase embed insid adjunct oper allow creat new node put insid tree push exist subtre new verb phrase get origin verb phrase one descend oper cannot model contextfre grammar use gener languag nth power b nth power c nth power even ww socal crossseri depend ww string repeat twice exampl sentenc mari gave book magazin chen mike exampl crossseri depend book associ chen magazin associ mike kind sentenc cannot gener contextfre grammar gener tag express power tag formal power cfg time theyr power full contextsensit grammar let show littl bit divers there interest game download ltaggam websit bunch card product join grammar combin togeth form sentenc want someth notic origin set card includ word appropri children therefor slightli pgrate version ltaggam design famili also download websit categori grammar call ccg popular day pars gener involv introduct someth call complex type complex type either xy xy complex type take argument type return object type x differ direct slash forward slash xy mean appear right x xy mean appear left x here exampl simpl grammar pronoun label noun phrase book anoth noun label noun phrase sleep type sleep well complex type say sleep kind thing combin noun phrase righthand side return sentenc similarli lexicon antifour enjoy someth combin right nonphras return someth complex type sleep here exampl sentenc sentenc sleep pars follow way noun phrase sleep back word slash np two combin result sleep valid sentenc accord pars sentenc enjoy book exampl transit verb enjoy get combin book replac noun phrase right return type left type enjoy book snp get combin deriv becom interest exampl show point view enjoy book sleep exact syntact structur theyr thing combin subject form sentenc express power ccg gener languag n b n c n n n greater someth contextfre grammar cannot want acknowledg jonathan kummerfeld aleka blackwel patrick littel exampl next topic go brief semant pars everyth weve discuss far syntact pars want build syntact represent sender know subject verb direct object never discuss semant sentenc last year lot interest semant pars purpos questionansw gener summar semant pars look like well simpl idea everi grammar go associ syntact structur also known composit semant composit semant mean word word mean combin word formula tell combin mean individu word best illustr exampl let look simpl sentenc javier eat pizza drama javier associ noun semant noun word object repres word javier pizza repres mean word pizza eat verb repres logic formula correspond longer function two argument x predic eat x entiti eat entiti receiv eat point combin lambda function argument abl come ground express entir sentenc provid work well first combin eat pizza togeth go replac pizza known substitut formal logic go replac lambda function two argument exi new function bound pizza still remain unbound variabl x still lambda express look miss argument combin happen next stage combin verb phrase node noun node javier get full mean sentenc eat predic javier substitut x express pizza remain entir semant present sentenc predic call eat first argument javier second argument pizza im go go semant pars detail want encourag look recent literatur acl topic seem one hottest area next year im go conclud segment short link short set link follow pars linguist olympiad one select relev section twode two dimension parser problem written jason eisner competit one two tree problem written noah smith kevin gimbel jason eisner problem set two problem relat categor combinatori grammar ccg combin categori tok pisin use competit final grammar rule simpl problem contextfre grammar last year competit andrea schalley pat littel enjoy problem youv solv check solut websit enough one exampl parser skate figur also written pat littel one earliest competit conclud section altern grammat formul pars thank attent,[ 0  4  1 14 13]
277,Course3_W6-S1-L1_-_Probabilities_-_17_slides_21-11,okay welcome back natural language processing today going start new section book probabilistic natural language processing information extraction familiar probabilities going start little overview probability theory related material probabilistic reasoning important natural language processing important let give example speech recognition suppose system recognizes speech converts audio signal text time able find perfect interpretation speech signal may come number alternatives reasonable others example say recognize speech possible system going hear something like wreck nice beach two strings sound similar speech recognition system maybe easy confuse obviously human different one reasonable one completely nonsensical figure build system natural language processing take account probabilities associated strings want probability first string high probability second string relatively low even speech recognition system chose two easy time figuring one correct heres another example machine translation translation frenchcanadian inaudible english expression attorney general attorney general people figured french correct translation avocat general however phrase french also mean general avocado french word avocat means lawyer avocado want able build system favor attorney general general avocado commonly used english one additional advantage using probabilities natural language processing possible combine evidence multiple sources systematic way example probability counts speech recognition system combine probability text understanding system able build better system way okay lets look probability theory little bit detail purpose probability theory predict likely something going happen one basic concepts probability theory idea experiment trial example throw coin lets assume coin fair chance heads chance tails every time throw coin performing experiment experiments different outcome example fair coin either heads tails sixsided die possible outcomes would one two three four five six also define term sample space sample spaces either discrete continuous case coins dice discrete discrete outcomes get two six also sample spaces continuous example want measure something real number probability every particular height lets say weight real number case continuous sample space event probability defined foreign way omega certain event something going happen sure example whether coin going either heads tails given coin fair theres third option event coin going heads tails certain also define impossible event denoted symbol phi impossible event coin heads tails simultaneously clear certain event high probability fact going probability one impossible event going probability zero event space possible events associated certain experiment probabilities defined numbers zero one zero corresponds impossible event one corresponds certain event define probability distribution takes probability one throughout sample space omega example case fair coin probability mass associated heads probability mass sorry associated tails lets look specific event example tossing fair coin three times interested probability getting three heads case important property independence coin throw going independent others therefore probability getting three heads row three tosses going product probability getting head trials since probability onehalf probability getting three heads back back onehalf cubed one eight complicated question probability getting two heads well little bit complicated said getting two heads three tosses happen three different ways first two throws heads last one tail two combinations tail middle tail first probability one eight three combinations overall probability getting exactly two heads going oneeighth time times three threeeighth conclude example probability getting one head also going equal three times oneeighth finally probably getting zero heads words three tails going probability getting three heads one eight different interpretations concept probability interesting one perhaps frequentist definition frequentist definition says observed certain event repeated overtime measure many times specific outcome example throw coin ten times turns heads five times give frequentist interpretation probability thats probability coin turning heads another interesting definition probability subjective definition case sort personal understanding reality certain event example believe coin fair able bet heads sure im going lose money long run knew example coin fair chance heads bet accordingly money heads tails okay let measure important properties probabilities already said probability impossible event zero probability certain event one also two events x say probability x less equal probability x subset example x throwing die six sides either probability corresponds outcomes one since x included therefore probability x smaller probability another important property probability union two events equal sum probabilities intersection empty set case disjoint events example x event coin going im sorry die going event die going probability probability union equal plus two events disjoints therefore intersection impossible another important concept probability theory conditional probability idea could two events measure probability one events measure probability second event given know outcome first event probability event get additional information additional variable called prior probability one get observe additional information called posterior probability conditional probability defined following way probability given b equal probability intersection b divided probability pb use venn diagram show possible combinations events left circle shows event right circle shows event b intersection two circles shown middle omega entire rectangle corresponds certain event case b none lets look example suppose fair sixsided die six sides numbers want figure whats probability even number well straightforward six possible outcomes three specifically correspond even numbers next slide going see actual number next thing want ask example probability die going come heres interesting example conditional probability probability die going even number given greater equal probability die odd number given greater equal even multiple conditions example whats probability die odd number given true greater equal less equal next line want show answers questions interested conditional probability gave questions probability die different numbers first one probability even number well probability greater equal also three possible outcomes thats conditional probability given dies number greater equal whats probability even well case three numbers greater equal three two specifically even probability even given greater equal similarly given greater equal probability ds odd number number falls category number notice last two lines sum probabilities equal equal actually coincidence given ten sides greater equal variable even odd two mutually exclusive events form partition possible events therefore total probability lets look example multiple conditions knowledge greater equal less equal well two numbers fall category numbers odd number therefore probability odd given conditions equal one important rule probability theory called chain rule allows us compute socalled joint probability multiple variables using simple representation lets say want compute probability n different events happening time usually difficult many different combinations going instead apply socalled chain rule works like compute joint probability n variables compute probability first variable example w multiply probability second variable given first one thats w given w times probability third variable given first two last term probability wn last variable given previous ones simplifies significantly computation joint probability p practical yet going look specific methods simplify chain rule used lot statistical actual language processing specifically markov models something going talk next lecture one important property probabilities idea independence two events independent product probabilities equal probability intersection unless pb equal equivalent saying probability equal conditional probability given b even knowledge outcome b going affect posterior understanding probability going prior probability pa completion two events independent going call dependent lets look interesting idea adding removing constraints probability formula adding constraints example interested following scenario going walk outside today given weather nice considering probability pwalkyesweathernice lets look slightly different version one whats probability going walk outside today given weather nice free time crowded outside two additional constraints restrict space possible outcomes given first equation second version accurate going give us better prediction much difficult estimate unlikely historical data going enough examples particular combination attributes weather freetime crowded opposite idea actually something often statistical processing idea removing constraints backoff lets start complicated formula probability going walk outside given weather nice free time crowded outside well remove one constraints even maybe remove probably imagine removing constraints makes less accurate compute probability also makes statistical feasible may instances trending data particular combination features one important observation possible adding removing constraints right hand side conditional probability give two examples possible backoff lefthand side condition example cannot replace probability walk equals yes comma e restaurant equals yes given condition remove one two formulas lefthand side thats allowed one thing introduce random variables random variables function takes omega space possible outcomes converts set real numbers numbers generated stochastic process certain probability distribution heres example lets look discrete random variable x sum faces two randomly thrown fair dice mean die six sides throw throw second time independently first one looking sum two numbers smallest possible value going two throws turn ones largest possible values going twelve lucky get two sixes number also possible example get score ten throwing two fives six four four six random variable corresponds sum two dice plotted range going look like curve corresponds probability outcomes going happen also define something called probability mass function tells us probability random variable specific numeric values example probability little x thats lets say probability sum two dice equal probability capital x probability distribution equals specific number going equal number times sum equal divided total number outcomes case six times six one definition random variable distributed according probability mass function p x write x tilde representation distributed p x two different versions random variables one discrete random variables case sum probabilities individual outcomes equal probability certain event one example continuous distribution going integral possible outcomes probabilities possible outcomes also equal one gives example discrete distribution probability throwing one fair die one six probability throwing two also one six probability distribution capital possible outcomes well going socalled uniform distribution one six one six six times corresponds possible values side dice conditional probability distribution given know threw odd number well case three possible odd numbers one three five obviously probability one third prior distribution case one shown second bottom line everywhere conditional posterior probability distribution going one shown bottom line okay lets go across example probability distributions lets talk little bit bayes theorem one fundamental techniques used statistical processing,Course3,W6-S1-L1,W6,S1,L1,-,6,1,1,okay welcom back natur languag process today go start new section book probabilist natur languag process inform extract familiar probabl go start littl overview probabl theori relat materi probabilist reason import natur languag process import let give exampl speech recognit suppos system recogn speech convert audio signal text time abl find perfect interpret speech signal may come number altern reason other exampl say recogn speech possibl system go hear someth like wreck nice beach two string sound similar speech recognit system mayb easi confus obvious human differ one reason one complet nonsens figur build system natur languag process take account probabl associ string want probabl first string high probabl second string rel low even speech recognit system chose two easi time figur one correct here anoth exampl machin translat translat frenchcanadian inaud english express attorney gener attorney gener peopl figur french correct translat avocat gener howev phrase french also mean gener avocado french word avocat mean lawyer avocado want abl build system favor attorney gener gener avocado commonli use english one addit advantag use probabl natur languag process possibl combin evid multipl sourc systemat way exampl probabl count speech recognit system combin probabl text understand system abl build better system way okay let look probabl theori littl bit detail purpos probabl theori predict like someth go happen one basic concept probabl theori idea experi trial exampl throw coin let assum coin fair chanc head chanc tail everi time throw coin perform experi experi differ outcom exampl fair coin either head tail sixsid die possibl outcom would one two three four five six also defin term sampl space sampl space either discret continu case coin dice discret discret outcom get two six also sampl space continu exampl want measur someth real number probabl everi particular height let say weight real number case continu sampl space event probabl defin foreign way omega certain event someth go happen sure exampl whether coin go either head tail given coin fair there third option event coin go head tail certain also defin imposs event denot symbol phi imposs event coin head tail simultan clear certain event high probabl fact go probabl one imposs event go probabl zero event space possibl event associ certain experi probabl defin number zero one zero correspond imposs event one correspond certain event defin probabl distribut take probabl one throughout sampl space omega exampl case fair coin probabl mass associ head probabl mass sorri associ tail let look specif event exampl toss fair coin three time interest probabl get three head case import properti independ coin throw go independ other therefor probabl get three head row three toss go product probabl get head trial sinc probabl onehalf probabl get three head back back onehalf cube one eight complic question probabl get two head well littl bit complic said get two head three toss happen three differ way first two throw head last one tail two combin tail middl tail first probabl one eight three combin overal probabl get exactli two head go oneeighth time time three threeeighth conclud exampl probabl get one head also go equal three time oneeighth final probabl get zero head word three tail go probabl get three head one eight differ interpret concept probabl interest one perhap frequentist definit frequentist definit say observ certain event repeat overtim measur mani time specif outcom exampl throw coin ten time turn head five time give frequentist interpret probabl that probabl coin turn head anoth interest definit probabl subject definit case sort person understand realiti certain event exampl believ coin fair abl bet head sure im go lose money long run knew exampl coin fair chanc head bet accordingli money head tail okay let measur import properti probabl alreadi said probabl imposs event zero probabl certain event one also two event x say probabl x less equal probabl x subset exampl x throw die six side either probabl correspond outcom one sinc x includ therefor probabl x smaller probabl anoth import properti probabl union two event equal sum probabl intersect empti set case disjoint event exampl x event coin go im sorri die go event die go probabl probabl union equal plu two event disjoint therefor intersect imposs anoth import concept probabl theori condit probabl idea could two event measur probabl one event measur probabl second event given know outcom first event probabl event get addit inform addit variabl call prior probabl one get observ addit inform call posterior probabl condit probabl defin follow way probabl given b equal probabl intersect b divid probabl pb use venn diagram show possibl combin event left circl show event right circl show event b intersect two circl shown middl omega entir rectangl correspond certain event case b none let look exampl suppos fair sixsid die six side number want figur what probabl even number well straightforward six possibl outcom three specif correspond even number next slide go see actual number next thing want ask exampl probabl die go come here interest exampl condit probabl probabl die go even number given greater equal probabl die odd number given greater equal even multipl condit exampl what probabl die odd number given true greater equal less equal next line want show answer question interest condit probabl gave question probabl die differ number first one probabl even number well probabl greater equal also three possibl outcom that condit probabl given die number greater equal what probabl even well case three number greater equal three two specif even probabl even given greater equal similarli given greater equal probabl ds odd number number fall categori number notic last two line sum probabl equal equal actual coincid given ten side greater equal variabl even odd two mutual exclus event form partit possibl event therefor total probabl let look exampl multipl condit knowledg greater equal less equal well two number fall categori number odd number therefor probabl odd given condit equal one import rule probabl theori call chain rule allow us comput socal joint probabl multipl variabl use simpl represent let say want comput probabl n differ event happen time usual difficult mani differ combin go instead appli socal chain rule work like comput joint probabl n variabl comput probabl first variabl exampl w multipli probabl second variabl given first one that w given w time probabl third variabl given first two last term probabl wn last variabl given previou one simplifi significantli comput joint probabl p practic yet go look specif method simplifi chain rule use lot statist actual languag process specif markov model someth go talk next lectur one import properti probabl idea independ two event independ product probabl equal probabl intersect unless pb equal equival say probabl equal condit probabl given b even knowledg outcom b go affect posterior understand probabl go prior probabl pa complet two event independ go call depend let look interest idea ad remov constraint probabl formula ad constraint exampl interest follow scenario go walk outsid today given weather nice consid probabl pwalkyesweathernic let look slightli differ version one what probabl go walk outsid today given weather nice free time crowd outsid two addit constraint restrict space possibl outcom given first equat second version accur go give us better predict much difficult estim unlik histor data go enough exampl particular combin attribut weather freetim crowd opposit idea actual someth often statist process idea remov constraint backoff let start complic formula probabl go walk outsid given weather nice free time crowd outsid well remov one constraint even mayb remov probabl imagin remov constraint make less accur comput probabl also make statist feasibl may instanc trend data particular combin featur one import observ possibl ad remov constraint right hand side condit probabl give two exampl possibl backoff lefthand side condit exampl cannot replac probabl walk equal ye comma e restaur equal ye given condit remov one two formula lefthand side that allow one thing introduc random variabl random variabl function take omega space possibl outcom convert set real number number gener stochast process certain probabl distribut here exampl let look discret random variabl x sum face two randomli thrown fair dice mean die six side throw throw second time independ first one look sum two number smallest possibl valu go two throw turn one largest possibl valu go twelv lucki get two six number also possibl exampl get score ten throw two five six four four six random variabl correspond sum two dice plot rang go look like curv correspond probabl outcom go happen also defin someth call probabl mass function tell us probabl random variabl specif numer valu exampl probabl littl x that let say probabl sum two dice equal probabl capit x probabl distribut equal specif number go equal number time sum equal divid total number outcom case six time six one definit random variabl distribut accord probabl mass function p x write x tild represent distribut p x two differ version random variabl one discret random variabl case sum probabl individu outcom equal probabl certain event one exampl continu distribut go integr possibl outcom probabl possibl outcom also equal one give exampl discret distribut probabl throw one fair die one six probabl throw two also one six probabl distribut capit possibl outcom well go socal uniform distribut one six one six six time correspond possibl valu side dice condit probabl distribut given know threw odd number well case three possibl odd number one three five obvious probabl one third prior distribut case one shown second bottom line everywher condit posterior probabl distribut go one shown bottom line okay let go across exampl probabl distribut let talk littl bit bay theorem one fundament techniqu use statist process,[ 4 14 13 12 11]
278,Course3_W6-S1-L2_-_Bayes_Theorem_-_8_slides_10-48,okay going start looking bayes theorem one important topics statistical natural language processing based formula joint probability joint probability two variables b equal probability one times probability one given first one example whats probability today im going wake late im going take walk well thats probability today im going wake late times probability im going take walk given woke late symmetry also add pab pbpab look two formulas see lefthand side therefore write set equations different format say conditional probability b given equal conditional probability given b times probability b comes second equation top divided probability equation bayes theorem used everywhere mentioned speech language processing also computer vision statistics insurance companies finance useful allows us compute condition probability given b know condition probability b given two things fact different circumstances important understand one also get one next heres example use bayesian formula going consider diagnostic test tells us whether certain person likely certain disease example disease could cancer certain test may look blood image person determine whether cancer problem kind test often make mistakes theyre really erroneous arent perfect either typical example test accuracy means probability person positive given dont disease example false positive words even person sick still pass positive five percent probability similarly type error socalled false negative given person disease also possible test going tell dont test turns negative also happens probability imagine two numbers missing computed two present probability positive given disease going one minus probability negative given disease similarly probability negative given disease going equal one minus probability positive given nondisease explain two numbers however doesnt case many tests different rates false positives false negatives dont need get going look example see apply bayesian formula practice one way think formula terms statistical natural language processing think problem part speech tagging part speech tagging youre given word figure part speech example word cat labeled noun looking word cat may able predict certain accuracy whether word noun going use exact mathematical methodology example im show lets look socalled joint probability table possible outcomes joint variable test disease conditional probability ab p random variable corresponds test value could either positive negative b corresponds random variable disease yes means person disease means person doesnt disease table plugged numbers previous slide see diagonal main diagonal two large numbers correspond true positives true negatives opposite diagonal two relatively small numbers correspond false positives false negatives important question youre trying answer whats probability person disease given test positive mean really likely scenario person walks hospital gets tested test turns positive want figure disease probability disease well point conditional probability disease given test available original table said minutes ago use bayes theorem invert directionality conditional probability instead probability positive given disease compute probability disease given positive lets see well actually straightforward write expression probability disease given positive according base formula probability positive given disease times probability disease divided probability positive similarly add probability person doesnt disease given test positive equal probability tested positive given dont disease times probability dont disease divided probability test positive interested following ratio two numbers previous two lines essentially much likely person disease given tested positive compared probability dont disease even though tested positive well number figure going enough answer questions turns dont need really care probability positive divide first performer slide going disappear long zero divide heres example probability person disease given tested positive divided probability dont disease given tested positive going equal probability positive given disease times probability disease thats first use bayes formula divided probability positive given dont disease times probability disease four variables first third one conditional get original diagnostic table two probabilities disease probability disease actually important dont yet numbers correspond well probability disease certain person going disease absent information test number turns really important baseline probability probability disease high posterior probability given test going even higher probability disease smaller small example order one percent one see fact test positive going increase significantly probability person disease may increase factor two four ten baseline probability lets say one thats still going enough evidence person disease lets see works suppose prior probability disease one means take random person streets theres chance disease trivial level probability random person disease going two add one plug numbers see probability somebody disease given theres positive divided probability dont disease given tested positive equal times one divided times simply formula youre going see overall probability two percent may sound shocking almost certainly mathematic error turns happens prior probability disease low even though one ratio positive negative test thats still enough compensate original prior ratio one thousand opposite direction going probability one thousand probability two percent thats increase still nowhere close two percent words walk clinic test positive disease test turns positive still two percent chance actually disease happens practice case wed need additional tests monitoring theres reason worried point little bit math since sum two numbers one one times larger one going see probability disease give positive still two percent end example patient next set slides going look mark models part speech time,Course3,W6-S1-L2,W6,S1,L2,-,6,1,2,okay go start look bay theorem one import topic statist natur languag process base formula joint probabl joint probabl two variabl b equal probabl one time probabl one given first one exampl what probabl today im go wake late im go take walk well that probabl today im go wake late time probabl im go take walk given woke late symmetri also add pab pbpab look two formula see lefthand side therefor write set equat differ format say condit probabl b given equal condit probabl given b time probabl b come second equat top divid probabl equat bay theorem use everywher mention speech languag process also comput vision statist insur compani financ use allow us comput condit probabl given b know condit probabl b given two thing fact differ circumst import understand one also get one next here exampl use bayesian formula go consid diagnost test tell us whether certain person like certain diseas exampl diseas could cancer certain test may look blood imag person determin whether cancer problem kind test often make mistak theyr realli erron arent perfect either typic exampl test accuraci mean probabl person posit given dont diseas exampl fals posit word even person sick still pass posit five percent probabl similarli type error socal fals neg given person diseas also possibl test go tell dont test turn neg also happen probabl imagin two number miss comput two present probabl posit given diseas go one minu probabl neg given diseas similarli probabl neg given diseas go equal one minu probabl posit given nondiseas explain two number howev doesnt case mani test differ rate fals posit fals neg dont need get go look exampl see appli bayesian formula practic one way think formula term statist natur languag process think problem part speech tag part speech tag your given word figur part speech exampl word cat label noun look word cat may abl predict certain accuraci whether word noun go use exact mathemat methodolog exampl im show let look socal joint probabl tabl possibl outcom joint variabl test diseas condit probabl ab p random variabl correspond test valu could either posit neg b correspond random variabl diseas ye mean person diseas mean person doesnt diseas tabl plug number previou slide see diagon main diagon two larg number correspond true posit true neg opposit diagon two rel small number correspond fals posit fals neg import question your tri answer what probabl person diseas given test posit mean realli like scenario person walk hospit get test test turn posit want figur diseas probabl diseas well point condit probabl diseas given test avail origin tabl said minut ago use bay theorem invert direction condit probabl instead probabl posit given diseas comput probabl diseas given posit let see well actual straightforward write express probabl diseas given posit accord base formula probabl posit given diseas time probabl diseas divid probabl posit similarli add probabl person doesnt diseas given test posit equal probabl test posit given dont diseas time probabl dont diseas divid probabl test posit interest follow ratio two number previou two line essenti much like person diseas given test posit compar probabl dont diseas even though test posit well number figur go enough answer question turn dont need realli care probabl posit divid first perform slide go disappear long zero divid here exampl probabl person diseas given test posit divid probabl dont diseas given test posit go equal probabl posit given diseas time probabl diseas that first use bay formula divid probabl posit given dont diseas time probabl diseas four variabl first third one condit get origin diagnost tabl two probabl diseas probabl diseas actual import dont yet number correspond well probabl diseas certain person go diseas absent inform test number turn realli import baselin probabl probabl diseas high posterior probabl given test go even higher probabl diseas smaller small exampl order one percent one see fact test posit go increas significantli probabl person diseas may increas factor two four ten baselin probabl let say one that still go enough evid person diseas let see work suppos prior probabl diseas one mean take random person street there chanc diseas trivial level probabl random person diseas go two add one plug number see probabl somebodi diseas given there posit divid probabl dont diseas given test posit equal time one divid time simpli formula your go see overal probabl two percent may sound shock almost certainli mathemat error turn happen prior probabl diseas low even though one ratio posit neg test that still enough compens origin prior ratio one thousand opposit direct go probabl one thousand probabl two percent that increas still nowher close two percent word walk clinic test posit diseas test turn posit still two percent chanc actual diseas happen practic case wed need addit test monitor there reason worri point littl bit math sinc sum two number one one time larger one go see probabl diseas give posit still two percent end exampl patient next set slide go look mark model part speech time,[ 4  6 14 13 12]
279,Course3_W6-S1-L3_-_Language_Modeling_1-3_-_18_slides_19-21,next lecture going language models lecture split three parts part one language model specifically probabilistic language model technique takes sentence input assigns probability probability sentence probability joint sequence events word one word two way word end last word sentence style technique natural age processing different deterministic methods looked using things like context free grammars idea want take possible sentences given language assign probability add one obviously difficult task possible write say sentence never pronounced come probability point language modeling also related point predicting next word sequence example start sentence saying lets meet times dotdotdot predict next word well probably several different things next likely one would word square lets meet times square example idea even though probability square general relatively low context lets meet times probability increases significantly posterior probability much higher prior probability another example general electric lost market blank well whats next word going square going square likely going something like share general electric lost market share idea context losing market something likely next word share formula predicting next word well conditional probability predicting probability n given words word follows word coming ngram website peter norvig list words follow huge corpus google documents word abilities follows cases word abilities follows count million see numbers pretty large also different one another language model something used lot speech recognition example want able probability sentence recognize speech significant larger probability wreck nice beach also use text generation example want generate sentence phrase three houses would probability sentence contains phrase three house spelling correction want probability cat eats fish come spelling correction system xat eats fish xat word english even could typo machine translation want probability blue house given lets say french expression larger probability house blue many uses example optical character recognition want text extracted image grammatical also summarization want summary produced automatically grammatical language model also used document classification case probability particular sentence coming english french figure two language models gives high probability therefore classify language language model corresponds sports news articles another one corresponds business articles figure whether document sports business based probability generated either one two models often language model coupled something called translation model minutes going talk translation models lets focus language models lets go back idea computing probability sentence well well one possibility find times particular sentence used large corpus use however sentences see corpus going novel theyre seen probability estimate going close zero certainly good idea never enough training sentences cover possible sentences english deal probabilities novel sentences lets see going looking estimate probability sentence probability joint distribution words sequence w wn rewrite formula using chain rule going get probability sentence equal probability first word times probability second word given first word way end probability last word given probability say correct formula means exactly gives result previous formula chain rule lose information okay lets look example suppose sentence want figure would like pepperoni spinach pizza probability sentence well straightforward going equal probability word followed probability would given previous word times probability like given previous two words would times pizza times probability pizza given previous words okay lets look something called n gram model n gram model allows us sacrifice accuracy prediction hand get good performance deal properly sparse training data trying put probability word based words example whats probability words lets meet times next word going square going use socalled markov assumption tells us probability word dependent entire history recent one two words look previous word alone current word going bigram model look two previous words plus current word going known trigram model word ngram covers unigrams context probability word square regardless words bigram example want complete probability square given previous word times trigram examples probability square given words times appearing order even trigram model going look words beyond anything left going irrelevant lets look random text generated socalled brown corpus one oldest important naturalized processing using ngrams different lengths gram random text looks like text actually brown corpus instead text generated automatically using bigram model trained brown corpus heres works pick first word random case word look word appears high probability word thats year old pick next word shea based probability shea appearing year old every word generated based previous word alone something similar trigrams example text case word county example computed probability follow words fulton word jail computed based probability appears fulton county thing four grams case text going look natural believe going guarantee least every consecutive sequence four words going something actually appeared previous text possible go tri grams four grams even five grams however often case larger ngrams including trigrams sparse estimate training data lets look examples specific groups download project gutenberg entire set works william shakespeare look unigrams words shakespeare theyd use linguistic inaudible means tokens correspond types linguistic term corresponds different types words average occurrences tokens per type little sidebar see entire set works shakespeare less one million words many orders magnitude smaller nowadays web lets see many bigrams shakespeare theres bigrams shakespeare theres one fewer bigram unigram token construction turns correspond different types type present three times data set many types present imagine vocabulary words squared thats million possible bigrams million appear data set orders magnitude less even one occurrence per time data extremely sparse going serious problem want estimate probabilities words based large corpora matter large even bigrams going presented frequently enough lets see actually estimate probabilities certain words large corporas compute conditional probabilities directly example probability certain word going follow certain word well really said earlier data sparse going instead use called markov assumption lets look sentence would like two tickets musical going use following approximation instead computing probability musical given entire string consists several words would like think going approximate bigram probability musical given word given word previous word turns practical purposes going degrade performance much going good trade given going much flexible robust system one looks entire previous history trigram counterpart example probability musical given would like two tickets going approximately equal probability musical given two previous words called trigram model previous example bigram model lets see estimate probabilities training data suppose really large corpus want figure probability word square given previous word times going see many times certain context appears training data many times certain conditional probability going appear heres unigram example word pizza appears times corpus ten million words maximum likelihood estimate probability pizza actually use term p prime p cap cases estimate maximum likelihood estimate probability pizza case going equal number times word appears total million possible times could occur ratio small number still zero lets look bigram example word appears times corpus phrase spinach appears times want compute probability spinach given possible contexts word appears word spinach maximum likelihood estimate probability spinach given one important thing keep mind learn probabilities one corpus theyre going valid corpora similar genre example learn probabilities corpus english language news expect probabilities accurate extent corpora english news cannot expect work languages even genres example fiction financial reports email important estimate probabilities use corpas comparable possible one youre going use testing heres example want compute probability sentence see monday one thing notice ive enclosed sample xml type tags symbol means start sentence symbol means end sentence turns important statistical language processing want treat beginning end sentences symbol sentence words come right right beginning end sentences going conditioned special symbols case bigram approximation see monday going probability word given beginning sentence times probability given previous word times probability see given previous word last thing going probability end sentence given previous word monday heres example set books written jane austen complete probability sentence elizabeth looked darcy lets see use information corpus compute probability going use maximum likelihood estimates ngram probabilities unigram maximum likelihood estimate pw sub equal number times count word divided v v size vocabulary set possible types words appearing corpus bigram probabilities going following form probability wi given wi minus going count wi minus wi appearing together divided count wi minus lets look specific values probability elizabeth im sorry converting decimal numbers corresponds times ten minus fourth power probability looked given elizabeth compute bigra using maximum likelihood estimates omitted beginning end sentence probabilities simplicity general included computation lets look bigram probability sentence probability elizabeth looked darcy bigram probability going number extremely low x power computing product numbers far also compare unigram probability probability different unigrams probability elizabeth times probability looks probability word times probability word darcy see x minus power see difference onethousandth fact onethousandth two probabilities words bigram probability sentence gives much higher value unigram probability makes sense bigram model uses additional context information unigram model doesnt include lets try estimate probability sentence looked darcy elizabeth think answer question going ill give second think well let give answer question probability sentence looked darcy elizabeth well turns unigram probability sentence going exactly one line unigram probability model doesnt take account word order theres going reordering four numbers product going exactly however bigram probability going something several orders magnitude smaller looked darcy darcy elizabeth elizabeth unlikely appeared training data possible probability going actually zero let stop example im going continue next set slides minute,Course3,W6-S1-L3,W6,S1,L3,-,6,1,3,next lectur go languag model lectur split three part part one languag model specif probabilist languag model techniqu take sentenc input assign probabl probabl sentenc probabl joint sequenc event word one word two way word end last word sentenc style techniqu natur age process differ determinist method look use thing like context free grammar idea want take possibl sentenc given languag assign probabl add one obvious difficult task possibl write say sentenc never pronounc come probabl point languag model also relat point predict next word sequenc exampl start sentenc say let meet time dotdotdot predict next word well probabl sever differ thing next like one would word squar let meet time squar exampl idea even though probabl squar gener rel low context let meet time probabl increas significantli posterior probabl much higher prior probabl anoth exampl gener electr lost market blank well what next word go squar go squar like go someth like share gener electr lost market share idea context lose market someth like next word share formula predict next word well condit probabl predict probabl n given word word follow word come ngram websit peter norvig list word follow huge corpu googl document word abil follow case word abil follow count million see number pretti larg also differ one anoth languag model someth use lot speech recognit exampl want abl probabl sentenc recogn speech signific larger probabl wreck nice beach also use text gener exampl want gener sentenc phrase three hous would probabl sentenc contain phrase three hous spell correct want probabl cat eat fish come spell correct system xat eat fish xat word english even could typo machin translat want probabl blue hous given let say french express larger probabl hous blue mani use exampl optic charact recognit want text extract imag grammat also summar want summari produc automat grammat languag model also use document classif case probabl particular sentenc come english french figur two languag model give high probabl therefor classifi languag languag model correspond sport news articl anoth one correspond busi articl figur whether document sport busi base probabl gener either one two model often languag model coupl someth call translat model minut go talk translat model let focu languag model let go back idea comput probabl sentenc well well one possibl find time particular sentenc use larg corpu use howev sentenc see corpu go novel theyr seen probabl estim go close zero certainli good idea never enough train sentenc cover possibl sentenc english deal probabl novel sentenc let see go look estim probabl sentenc probabl joint distribut word sequenc w wn rewrit formula use chain rule go get probabl sentenc equal probabl first word time probabl second word given first word way end probabl last word given probabl say correct formula mean exactli give result previou formula chain rule lose inform okay let look exampl suppos sentenc want figur would like pepperoni spinach pizza probabl sentenc well straightforward go equal probabl word follow probabl would given previou word time probabl like given previou two word would time pizza time probabl pizza given previou word okay let look someth call n gram model n gram model allow us sacrific accuraci predict hand get good perform deal properli spars train data tri put probabl word base word exampl what probabl word let meet time next word go squar go use socal markov assumpt tell us probabl word depend entir histori recent one two word look previou word alon current word go bigram model look two previou word plu current word go known trigram model word ngram cover unigram context probabl word squar regardless word bigram exampl want complet probabl squar given previou word time trigram exampl probabl squar given word time appear order even trigram model go look word beyond anyth left go irrelev let look random text gener socal brown corpu one oldest import natur process use ngram differ length gram random text look like text actual brown corpu instead text gener automat use bigram model train brown corpu here work pick first word random case word look word appear high probabl word that year old pick next word shea base probabl shea appear year old everi word gener base previou word alon someth similar trigram exampl text case word counti exampl comput probabl follow word fulton word jail comput base probabl appear fulton counti thing four gram case text go look natur believ go guarante least everi consecut sequenc four word go someth actual appear previou text possibl go tri gram four gram even five gram howev often case larger ngram includ trigram spars estim train data let look exampl specif group download project gutenberg entir set work william shakespear look unigram word shakespear theyd use linguist inaud mean token correspond type linguist term correspond differ type word averag occurr token per type littl sidebar see entir set work shakespear less one million word mani order magnitud smaller nowaday web let see mani bigram shakespear there bigram shakespear there one fewer bigram unigram token construct turn correspond differ type type present three time data set mani type present imagin vocabulari word squar that million possibl bigram million appear data set order magnitud less even one occurr per time data extrem spars go seriou problem want estim probabl word base larg corpora matter larg even bigram go present frequent enough let see actual estim probabl certain word larg corpora comput condit probabl directli exampl probabl certain word go follow certain word well realli said earlier data spars go instead use call markov assumpt let look sentenc would like two ticket music go use follow approxim instead comput probabl music given entir string consist sever word would like think go approxim bigram probabl music given word given word previou word turn practic purpos go degrad perform much go good trade given go much flexibl robust system one look entir previou histori trigram counterpart exampl probabl music given would like two ticket go approxim equal probabl music given two previou word call trigram model previou exampl bigram model let see estim probabl train data suppos realli larg corpu want figur probabl word squar given previou word time go see mani time certain context appear train data mani time certain condit probabl go appear here unigram exampl word pizza appear time corpu ten million word maximum likelihood estim probabl pizza actual use term p prime p cap case estim maximum likelihood estim probabl pizza case go equal number time word appear total million possibl time could occur ratio small number still zero let look bigram exampl word appear time corpu phrase spinach appear time want comput probabl spinach given possibl context word appear word spinach maximum likelihood estim probabl spinach given one import thing keep mind learn probabl one corpu theyr go valid corpora similar genr exampl learn probabl corpu english languag news expect probabl accur extent corpora english news cannot expect work languag even genr exampl fiction financi report email import estim probabl use corpa compar possibl one your go use test here exampl want comput probabl sentenc see monday one thing notic ive enclos sampl xml type tag symbol mean start sentenc symbol mean end sentenc turn import statist languag process want treat begin end sentenc symbol sentenc word come right right begin end sentenc go condit special symbol case bigram approxim see monday go probabl word given begin sentenc time probabl given previou word time probabl see given previou word last thing go probabl end sentenc given previou word monday here exampl set book written jane austen complet probabl sentenc elizabeth look darci let see use inform corpu comput probabl go use maximum likelihood estim ngram probabl unigram maximum likelihood estim pw sub equal number time count word divid v v size vocabulari set possibl type word appear corpu bigram probabl go follow form probabl wi given wi minu go count wi minu wi appear togeth divid count wi minu let look specif valu probabl elizabeth im sorri convert decim number correspond time ten minu fourth power probabl look given elizabeth comput bigra use maximum likelihood estim omit begin end sentenc probabl simplic gener includ comput let look bigram probabl sentenc probabl elizabeth look darci bigram probabl go number extrem low x power comput product number far also compar unigram probabl probabl differ unigram probabl elizabeth time probabl look probabl word time probabl word darci see x minu power see differ onethousandth fact onethousandth two probabl word bigram probabl sentenc give much higher valu unigram probabl make sens bigram model use addit context inform unigram model doesnt includ let tri estim probabl sentenc look darci elizabeth think answer question go ill give second think well let give answer question probabl sentenc look darci elizabeth well turn unigram probabl sentenc go exactli one line unigram probabl model doesnt take account word order there go reorder four number product go exactli howev bigram probabl go someth sever order magnitud smaller look darci darci elizabeth elizabeth unlik appear train data possibl probabl go actual zero let stop exampl im go continu next set slide minut,[ 4 14 13 12 11]
280,Course3_W6-S1-L4_-_Language_Modeling_1-3_contd_-__5_slides_03-05,okay lets look ngrams relate regular languages one important point would like mention ngrams similar regular languages fact hmms equivalent weighted finite state transitions used several earlier papers part speech tagging hmms going talk regular languages separate lecture stay tuned one thing need know ngram models considered generated models look unigram model first algorithm straightforward generate word distribution unigrams generate next one stop generate end sentence symbol slash actually unigram model looks like w w w rest words sentence wn final symbol different bigram model bigram model generate first special symbol start sentence conditional probability next word going generate next word based probability given generated generate second word based first one stop reach end sentence graphical model looks different though looks like conditional probabilities every pair words end one important engineer trick would like mention briefly maximum likelihood values estimate probabilities often order ten minus six less sometimes little ten minus multiply many numbers example sentence words may multiply many values would get number order power really tiny number computers delete arithmetic underflow therefore rounded zero therefore avoiding work done far talk problem theres one important engineering trick use think ill give answer second answer use logarithms specifically base logarthims instead something like minus power well get number like instead multiplying probabilities going add logarithms probabilities ad end need always exponentiate back get normal space something like minus becomes minus six use sums done concludes first part language modeling stay tuned next one,Course3,W6-S1-L4,W6,S1,L4,-,6,1,4,okay let look ngram relat regular languag one import point would like mention ngram similar regular languag fact hmm equival weight finit state transit use sever earlier paper part speech tag hmm go talk regular languag separ lectur stay tune one thing need know ngram model consid gener model look unigram model first algorithm straightforward gener word distribut unigram gener next one stop gener end sentenc symbol slash actual unigram model look like w w w rest word sentenc wn final symbol differ bigram model bigram model gener first special symbol start sentenc condit probabl next word go gener next word base probabl given gener gener second word base first one stop reach end sentenc graphic model look differ though look like condit probabl everi pair word end one import engin trick would like mention briefli maximum likelihood valu estim probabl often order ten minu six less sometim littl ten minu multipli mani number exampl sentenc word may multipli mani valu would get number order power realli tini number comput delet arithmet underflow therefor round zero therefor avoid work done far talk problem there one import engin trick use think ill give answer second answer use logarithm specif base logarthim instead someth like minu power well get number like instead multipli probabl go add logarithm probabl ad end need alway exponenti back get normal space someth like minu becom minu six use sum done conclud first part languag model stay tune next one,[ 4  1 14 13 12]
281,Course3_W6-S1-L5_-_Language_Modeling_2-3_-_13_slides_10-56,okay lets move next segment language modeling going discuss problem smoothing smoothing important vocabulary size large training data relatively small example typical application may vocabulary size million words want estimate frequencies unigrams training data set many millions words estimate possible unigrams thats lot parameters even unigram model maximumlikelihood estimate going assign values unseen event doesnt mean event impossible word training data still possible appear testing data assign score means going assign score entire sentence definitely want getting enough training data important unigrams even important bigrams trigrams bigram would expect million squared thats lot trending data trigrams would need something order million cubed much larger reasonable corpus train even corpus size would still lot gaps figure way come reasonable estimates probabilities unigrams bigrams trigrams even never appeared training data process called smoothing regularization idea behind assign probability mass unseen data basic way think want model occurrence novel words words seen training data possibly novel bigrams one techniques used smoothing called laplacian smoothing addone smoothing case going assume every word well see test data well see least training data even see word seen five times assume seen six times word never seen well assume seen exactly way word chance getting small amount probability training data set bigrams going estimate mathematical estimate probability word given word sub minus actual count bigram w wi normalization purposes need divide total number occurrences wi v v size vocabulary normalization ensure bigram probability addone smoothing still valid probability adds one problem method reassigns much probability mass unseen events one million still significantly larger would expect rare word possible something like addk instead addone similar problem practical purposes addone addk smoothing used dont work well practice instead people use advanced smoothing techniques im going go lot detail ill give one example common example goodturing used predict probabilities unseen events based probabilities seen events techniques example kneserney classbased ngrams class based ngrams collapse together words belong class example parts speech im sorry prepositions nouns verbs possibly days week persons numbers one class use estimate new member category based occurrences existing members category lets look example suppose corpus words appear order cat dog cat rabbit mouse said total words corpus goal predict probability next word going word cat well best estimate well maximum likelihood estimate straightforward word cat appeared times gives us probability cat going appear next iteration straightforward use simplest maximumlikelihood estimate however whats probability next word going species seen example elephant lets examples probability seeing mouse next iteration thats many times mouse appeared original corpus okay lets see happens elephant little trickier one estimate never seen would maximumlikelihood estimate elephant however know probability next animal unseen far actually even first examples many cases saw new species havent seen know something happen likely see future able allow elephants appear next iterations somehow discount probabilities animals already seen instead maximumlikelihood estimate mouse going change something heres good turing method comes place going take actual counts c particular word n sub r number ngrams occur exactly c times corpus n total number ngrams corpus example total words example occurred exactly going compute revised counts c based three parameters use good turing estimates probabilities general formula c c number ngrams occur exactly c times divided number ngrams occur exactly c times lets see works practice heres corpus whether count cat count dog count fish also count mouse rabbit also hamster fox turtle tiger lion compute n sub values n four animals appear exactly n n n cases exact counts number species appear certain number times quotes numbers came compute revised counts c c cat used going also formula estimate dog going equal c plus times onehalf number counts plus divided number counts c new value dog going times half equal less originally thing applies mouse multiplied twothirds also equal applies rabbit category mouse go next one fox fox appeared original data set new count times threequarters thats six quarters thing applies turtle tiger lion gives us estimates counts goodturing species also compute probability well see new species according formula nn number things seen exactly corpus far divided total number unigrams estimate elephant still need normalize entire set counts get probability distribution end goodturing example goodturing often used application speech processing natural language processing else deal sparse data two techniques used one called backoff one called interpolation lets look turn backoff backoff used certain ngram model give us enough data trying compute value bigram model possible counts given bigram small case want backoff another model lowerorder ngram example bigram go unigram trigram go either bigram unigram cases also go default value even unigram default value certain word learned parameters using training set also held set come values based development set technique called interpolation probability estimate trigram example sparse use linear combination bigram model unigram model combined trigram model lambda times trigram probability plus lambda times bigram probability plus lambda times unigram probability method works better backoff lambdas also learned based approximations held data set theres paper stanley chen josh goodman covers important techniques smoothing backoff interpolation look paper details concludes end second segment language model,Course3,W6-S1-L5,W6,S1,L5,-,6,1,5,okay let move next segment languag model go discuss problem smooth smooth import vocabulari size larg train data rel small exampl typic applic may vocabulari size million word want estim frequenc unigram train data set mani million word estim possibl unigram that lot paramet even unigram model maximumlikelihood estim go assign valu unseen event doesnt mean event imposs word train data still possibl appear test data assign score mean go assign score entir sentenc definit want get enough train data import unigram even import bigram trigram bigram would expect million squar that lot trend data trigram would need someth order million cube much larger reason corpu train even corpu size would still lot gap figur way come reason estim probabl unigram bigram trigram even never appear train data process call smooth regular idea behind assign probabl mass unseen data basic way think want model occurr novel word word seen train data possibl novel bigram one techniqu use smooth call laplacian smooth addon smooth case go assum everi word well see test data well see least train data even see word seen five time assum seen six time word never seen well assum seen exactli way word chanc get small amount probabl train data set bigram go estim mathemat estim probabl word given word sub minu actual count bigram w wi normal purpos need divid total number occurr wi v v size vocabulari normal ensur bigram probabl addon smooth still valid probabl add one problem method reassign much probabl mass unseen event one million still significantli larger would expect rare word possibl someth like addk instead addon similar problem practic purpos addon addk smooth use dont work well practic instead peopl use advanc smooth techniqu im go go lot detail ill give one exampl common exampl goodtur use predict probabl unseen event base probabl seen event techniqu exampl kneserney classbas ngram class base ngram collaps togeth word belong class exampl part speech im sorri preposit noun verb possibl day week person number one class use estim new member categori base occurr exist member categori let look exampl suppos corpu word appear order cat dog cat rabbit mous said total word corpu goal predict probabl next word go word cat well best estim well maximum likelihood estim straightforward word cat appear time give us probabl cat go appear next iter straightforward use simplest maximumlikelihood estim howev what probabl next word go speci seen exampl eleph let exampl probabl see mous next iter that mani time mous appear origin corpu okay let see happen eleph littl trickier one estim never seen would maximumlikelihood estim eleph howev know probabl next anim unseen far actual even first exampl mani case saw new speci havent seen know someth happen like see futur abl allow eleph appear next iter somehow discount probabl anim alreadi seen instead maximumlikelihood estim mous go chang someth here good ture method come place go take actual count c particular word n sub r number ngram occur exactli c time corpu n total number ngram corpu exampl total word exampl occur exactli go comput revis count c base three paramet use good ture estim probabl gener formula c c number ngram occur exactli c time divid number ngram occur exactli c time let see work practic here corpu whether count cat count dog count fish also count mous rabbit also hamster fox turtl tiger lion comput n sub valu n four anim appear exactli n n n case exact count number speci appear certain number time quot number came comput revis count c c cat use go also formula estim dog go equal c plu time onehalf number count plu divid number count c new valu dog go time half equal less origin thing appli mous multipli twothird also equal appli rabbit categori mous go next one fox fox appear origin data set new count time threequart that six quarter thing appli turtl tiger lion give us estim count goodtur speci also comput probabl well see new speci accord formula nn number thing seen exactli corpu far divid total number unigram estim eleph still need normal entir set count get probabl distribut end goodtur exampl goodtur often use applic speech process natur languag process els deal spars data two techniqu use one call backoff one call interpol let look turn backoff backoff use certain ngram model give us enough data tri comput valu bigram model possibl count given bigram small case want backoff anoth model lowerord ngram exampl bigram go unigram trigram go either bigram unigram case also go default valu even unigram default valu certain word learn paramet use train set also held set come valu base develop set techniqu call interpol probabl estim trigram exampl spars use linear combin bigram model unigram model combin trigram model lambda time trigram probabl plu lambda time bigram probabl plu lambda time unigram probabl method work better backoff lambda also learn base approxim held data set there paper stanley chen josh goodman cover import techniqu smooth backoff interpol look paper detail conclud end second segment languag model,[ 4 14 13 12 11]
282,Course3_W6-S1-L6_-_Language_Modeling_3-3_-_14_slides_15-06,welcome back natural language processing course going continue final section language modeling part three lets look additional issues related language models first evaluate quality language model well two types evaluations first one known extrinsic evaluation use language modern specific application example speech recognition machine translation partofspeech tagging second method based intrinsic evaluation properties language model much cheaper extrinsic evaluation done automatically however important use complete substitute extrinsic evaluation least use intrinsic evaluation also sort extrinsic evaluation able correlate two good idea two map continue using intrinsic method lets see intrinsic methods first commonly used method known perplexity perplexity given formula take probability words sentence take reciprocal take nth root number gives us perplexity estimate well model fits data good language model one going give high probability real sentence perplexity thought average branching factor predicting next work lower perplexity better correlated higher probability sentence formula n number words sentence lets look example suppose set words n equiprobable words probability k given formula perplexity compute value specific example value going k nth power power n nth root n n going cancel going get k power minus essentially original number k case happens perplexity exactly equal number equiprobable choices making lets say ten words choose perplexity going ten words equally likely another way assert perplexity like branching factor theres logarithmic version also used case base base logarithm previous formula power minus one n sum logarithms probability individual words give us exact value nonlogarithmic approach use perplexities related socalled shannon game named claude shannon lets see looks like shannon game try predict next word sequence based word far say something like new york governor andrew cuomo said want predict next word could try predict youll see easy task may say example next word example new york governor andrew cuomo said next item agenda may word example new york governor andrew cuomo said next day going travel however likely next word said cuomo word already used sentence also likely word usually doesnt follow word said example adjective shannon game inaudible predict next word lets try simple example whats perplexity guessing digit digits equally likely well imagine previous slide answer ten predicting next letter sequence well instance equal probable choices perplexity equal perplexity going get lower better understanding words already said numbers going work words may make sense example seen first letter word probability next letter also going going small probability next letter x also going small probably next letters h e r going larger case dont anymore equal probable distribution letters something different case average branching factor going smaller exactly perplexity useful heres example josh goodman paper mentioned earlier guessing one next words customer going say phone call customer service lets assume probability theyre going say word operator theyre going say word sales probability cases add probability onehalf example could names people work company case well take weighted sum numbers using harmonic mean thats going give us average branching factor corresponds particular language model measure perplexity cause distributions important problem naturalized often language model trained one particular set data tested completely different set data two drawn distribution perplexity however often case often people train example news stories test social media surprised see performance pretty low surprising perplexity tell us going happen turns two distributions different called called close anthropy two distributions going higher heres example training language model previous slides data however turns cases equally like options user say either sales operator case probability distribution going different instead one quarter one quarter values add onehalf going zero zero values add crossentropy equal log perplexity measured bits formula given populate distributions see topography distributions different cross entropy going large similar going minimum sample values perplexity real life natural language data computed wall street journal late corpus million words million tokens corresponds different words types perplexity computed separate sample million documents purpose unigrams level words letters bigrams went low would expect trigrams went even lower means even though total words corpus using unigram model youre going less choices average youre going trigrams go factor one method used validating language models socalled word error rate equal number insertions deletions substitutions two strings similar earlier called levenshtein edit distance remember something normalized sentence length heres example supposed one string governor andrew cuomo met mayor another sentence governor met senator figure edit distance two strings ill give answer minute see three deletions removed word andrew word cuomo word one insertion added word beginning one substitution replaced word mayor word senator thats pretty large word error rate five even normalize inaudible would still normalized word error rate one lets consider two issues come deal language models first one deal vocabulary words oov words words appear test data never seen training data purposes estimating probabilities split training data two parts label words part two part one unknown words estimates unknown words used estimates would deal testing data another thing something called cluster example combine information date expressions together monetary amounts together separately organizations years case conditional properties say probability certain word wi going appear year expression case year expressions combined together give additional strength prediction positive long language models dont model well named long distance dependencies ngram language models essentially fail definition theyre allowed look one two words back heres example may missing syntactic information example lets consider two sentences students participated game tired word conditional students subject subject verb sentence syntactically related however theres total five words intervene shorter six grams going miss dependency going make distinction sentence next one says student participated game tired instance agreement goes back five words trigram language models ging able deal information thing applies missing semantic information two sentences pizza last night tasty tasty case modifies pizza makes sense word tasty something expect pizza property change class last night interesting see tasty doesnt fit context yet trigram model assign sample ability cases two words night trigram anagram model case going miss semantic information long distance two words semantically related techniques deal kind problem example use syntactic language model looks recent words also recent words syntactically related current word need bind sentence figure pizza related tasty using dependency perhaps specific syntactic condition condition word tasty pizza similarly condition word interesting word class heres ideas use though mentioned syntactic model idea want condition words words appear specific syntactic relation also use something called caching model take advantage fact words appear bursts see work cuomo document likely word cuomo going appear document keep track words appeared recent history give high probability appearing future im going conclude section pointing external resources relevant modeling first sri language modeling toolkit available sri website second one cmu language modeling tool kit available cmu website popular think sri system popular days less thing allow train maximum likelihood estimates training data set sort length ngrams including many four five grams also use compute perplexity use label sequences also use estimate probabilities using interpolation would like also mention large corpus google n grams available internet data billions documents im going show later examples data data set theres also possibility look ngrams site proposed google allows track presence ngrams historically hundreds years heres example google ngrams see theyre large values bigrams conditioned word house see first example word appearing house frequency appears house times see large numbers used get reasonable estimates bigram probabilities use systems external links ngrams websites use ngrams randomly generate text example scientific papers poems country ban names really fun would encourage take look time concludes section language modeling,Course3,W6-S1-L6,W6,S1,L6,-,6,1,6,welcom back natur languag process cours go continu final section languag model part three let look addit issu relat languag model first evalu qualiti languag model well two type evalu first one known extrins evalu use languag modern specif applic exampl speech recognit machin translat partofspeech tag second method base intrins evalu properti languag model much cheaper extrins evalu done automat howev import use complet substitut extrins evalu least use intrins evalu also sort extrins evalu abl correl two good idea two map continu use intrins method let see intrins method first commonli use method known perplex perplex given formula take probabl word sentenc take reciproc take nth root number give us perplex estim well model fit data good languag model one go give high probabl real sentenc perplex thought averag branch factor predict next work lower perplex better correl higher probabl sentenc formula n number word sentenc let look exampl suppos set word n equiprob word probabl k given formula perplex comput valu specif exampl valu go k nth power power n nth root n n go cancel go get k power minu essenti origin number k case happen perplex exactli equal number equiprob choic make let say ten word choos perplex go ten word equal like anoth way assert perplex like branch factor there logarithm version also use case base base logarithm previou formula power minu one n sum logarithm probabl individu word give us exact valu nonlogarithm approach use perplex relat socal shannon game name claud shannon let see look like shannon game tri predict next word sequenc base word far say someth like new york governor andrew cuomo said want predict next word could tri predict youll see easi task may say exampl next word exampl new york governor andrew cuomo said next item agenda may word exampl new york governor andrew cuomo said next day go travel howev like next word said cuomo word alreadi use sentenc also like word usual doesnt follow word said exampl adject shannon game inaud predict next word let tri simpl exampl what perplex guess digit digit equal like well imagin previou slide answer ten predict next letter sequenc well instanc equal probabl choic perplex equal perplex go get lower better understand word alreadi said number go work word may make sens exampl seen first letter word probabl next letter also go go small probabl next letter x also go small probabl next letter h e r go larger case dont anymor equal probabl distribut letter someth differ case averag branch factor go smaller exactli perplex use here exampl josh goodman paper mention earlier guess one next word custom go say phone call custom servic let assum probabl theyr go say word oper theyr go say word sale probabl case add probabl onehalf exampl could name peopl work compani case well take weight sum number use harmon mean that go give us averag branch factor correspond particular languag model measur perplex caus distribut import problem natur often languag model train one particular set data test complet differ set data two drawn distribut perplex howev often case often peopl train exampl news stori test social media surpris see perform pretti low surpris perplex tell us go happen turn two distribut differ call call close anthropi two distribut go higher here exampl train languag model previou slide data howev turn case equal like option user say either sale oper case probabl distribut go differ instead one quarter one quarter valu add onehalf go zero zero valu add crossentropi equal log perplex measur bit formula given popul distribut see topographi distribut differ cross entropi go larg similar go minimum sampl valu perplex real life natur languag data comput wall street journal late corpu million word million token correspond differ word type perplex comput separ sampl million document purpos unigram level word letter bigram went low would expect trigram went even lower mean even though total word corpu use unigram model your go less choic averag your go trigram go factor one method use valid languag model socal word error rate equal number insert delet substitut two string similar earlier call levenshtein edit distanc rememb someth normal sentenc length here exampl suppos one string governor andrew cuomo met mayor anoth sentenc governor met senat figur edit distanc two string ill give answer minut see three delet remov word andrew word cuomo word one insert ad word begin one substitut replac word mayor word senat that pretti larg word error rate five even normal inaud would still normal word error rate one let consid two issu come deal languag model first one deal vocabulari word oov word word appear test data never seen train data purpos estim probabl split train data two part label word part two part one unknown word estim unknown word use estim would deal test data anoth thing someth call cluster exampl combin inform date express togeth monetari amount togeth separ organ year case condit properti say probabl certain word wi go appear year express case year express combin togeth give addit strength predict posit long languag model dont model well name long distanc depend ngram languag model essenti fail definit theyr allow look one two word back here exampl may miss syntact inform exampl let consid two sentenc student particip game tire word condit student subject subject verb sentenc syntact relat howev there total five word interven shorter six gram go miss depend go make distinct sentenc next one say student particip game tire instanc agreement goe back five word trigram languag model ging abl deal inform thing appli miss semant inform two sentenc pizza last night tasti tasti case modifi pizza make sens word tasti someth expect pizza properti chang class last night interest see tasti doesnt fit context yet trigram model assign sampl abil case two word night trigram anagram model case go miss semant inform long distanc two word semant relat techniqu deal kind problem exampl use syntact languag model look recent word also recent word syntact relat current word need bind sentenc figur pizza relat tasti use depend perhap specif syntact condit condit word tasti pizza similarli condit word interest word class here idea use though mention syntact model idea want condit word word appear specif syntact relat also use someth call cach model take advantag fact word appear burst see work cuomo document like word cuomo go appear document keep track word appear recent histori give high probabl appear futur im go conclud section point extern resourc relev model first sri languag model toolkit avail sri websit second one cmu languag model tool kit avail cmu websit popular think sri system popular day less thing allow train maximum likelihood estim train data set sort length ngram includ mani four five gram also use comput perplex use label sequenc also use estim probabl use interpol would like also mention larg corpu googl n gram avail internet data billion document im go show later exampl data data set there also possibl look ngram site propos googl allow track presenc ngram histor hundr year here exampl googl ngram see theyr larg valu bigram condit word hous see first exampl word appear hous frequenc appear hous time see larg number use get reason estim bigram probabl use system extern link ngram websit use ngram randomli gener text exampl scientif paper poem countri ban name realli fun would encourag take look time conclud section languag model,[ 4 12 14 13 11]
283,Course3_W6-S1-L7_-_Word_Sense_Disambiguation_-_15_slides_20-06,okay welcome back nlp going switch different topic word sense disambiguation says ambiguation imagine form name automatically recognizing multiple meanings ambiguous words used specific sentence words different properties seen one polysemy polysemy means many words multiple senses example lets drink bar word bar sentence means drinking establishment however general word bar many meanings example chocolate bar another meaning word bar study bar another meaning bar case means exam lawyers bring chocolate bar another property word homonymy example say may come lets meet may word may appears sentences pronounced exact way means something completely different different books make different distinctions polysemy homonymy purposes class going focus examples homonymy accident example word may verb auxiliary verb word may second sentence month clearly words different theyre different parts speech going worrying much mostly going worry words typically nouns multiple meanings going worry words like round verb adjective noun part well different senses bar according wordnet well theres first one bar also barroom saloon room establishment alcoholic drinks served counter sense counter obtain food drink actual physical piece furniture top get drink bar rigid piece metal wood used fastening obstruction weapon theyre like bars jail next thing bar music many many senses wont spend lot ten minutes going detail let show list see theres dozen lets go back word senses disambiguation said main goal take word like bar multiple senses numbered lets say given sentence tell us meanings used see could context thats typically get access entire sentence often entire paragraph document word appears word sense disambiguation important component natural language processing systems used example come better semantic representations sentences also question answering things like machine translation obviously word ambiguous one language doesnt mean necessarily ambiguous another language example want translate sentence word play english spanish need understand object played example english say play violin spanish translated verb tocar el violin sentence play tennis going use different word jugar al tenis tocar jugar two different translations play couldnt proper word sense disambiguation english wouldnt able translate sentences spanish correctly uses word sense disambiguation example accent restoration word cote french means different things pronounced differently depending accents appear could something like cote coast cote side uses text speech generation case may word multiple pronunciations depending meaning example lead lead english spelling correction want able distinguish words like aid aide e finally capitalization restoration word like turkey know meaning bird country properly capitalize text capitalized start let go common techniques used word sense disambiguation starting old method michael lesk socalled dictionary method idea behind dictionary method ambiguous words sentence appear together youre going find possible meanings words look dictionary definitions senses words look pairs dictionary definitions one ambiguous words overlap heres works going match sentence dictionary definition lets look sentence two ambiguous words first one plant according merriamwebster dictionary many senses first one living thing grows ground usually leaves flowers needs sun water survive second interpretation plant going call plant defined building factory something made example word leaf two definitions going consider leaf lateral outgrowth plant stem typically flattened expanded variably shaped greenish organ constitutes unit foliage functions primarily food manufacture photosynthesis leaf part book folded sheet containing page side supposed sentence like leaf food making factory green plants word leaf word plants want determine senses used sentence four possible combinations leaf plant leaf plant one going pick well going look overlap dictionary definitions pick one largest one okay method introduced early david yarowsky upenn time decision list method heres works based principle one sense per collocation previous lecture talked collocations say living plant word plant ambiguous fact appears near word living part collocation means first sense word plant study something like manufacturing plant clearly second sense plant question come list collocations give us hints disambiguation ambiguous words yarowsky method based idea going look two senses per word going try come ordered list rules collocation gives sense collocation ambiguous word automatically determine sense formula used ordering rules decision list following log probability sense given collocation divided probability sense b given collocation since decision list method sort words formula whenever new instance ambiguous noun going go list pick first item starting top matches label word accordingly stop heres example decision lists arovsky paper looks consist two word specific order also looks words within certain window many words example disintegrate word bass way also multiple pronunciations example bass bass going look following rules word fish appears within window going label bass doesnt hold going look next rule list collocation striped followed bass going label course bass next rule list guitar within window word bass word guitar nearby know second sense word bass bass ill examples bass player indicates bass sense number two play verb followed bass also bass two rules alternate bass bass lets look detailed list type features used yarowsky decision list algorithm mentioned collocations adjacent words one side word example word bar could chocolate bar bar exam bar stool bar fight words like aid could foreign aid one sense versus presidential aide sense also look position important realize collocation plant pesticide different collocation pesticide plant first one refers living thing sense plant second one refers factory sense plant position matter also look adjacent parts speech want know word follows verb adjective also look nearby words lets say within window plus five words entire sentence also look syntactic information example subject word object word example word play look specific object whether guitar tennis determine sense information topic text important one principles jarowski introduced one recent papers called one sentence per discourse principle said time ambiguous word appears paragraph document likely continue keep sense within entire discourse lets step back moment look general classification methods used word sense disambiguation word sense disambiguation science different many classification points naturalized points name key declassification class type classification discussed previously one common techniques used knearest neighbor classification method also known memorybased method idea look current problem example sense word context look history training data similar examples figure decision made time gives decision make current time well typically people use vector representation instance needs classified measure euclidean distance instance instances heres example according system put dimensions general case much dimensional vector red objects form one cluster example sense one blue points form another cluster sense two given new object want determine classify knearest neighbor algorithm going look nearest neighbor one nearest neighbor case red dot therefore going classify red also possible increase value k larger odd number example three five case k equals three going look three closest examples vector space one want classify look majority class among return correct answer example among three nearest neighbors two red examples one blue example therefore going label hollow example red well okay lets move another method also introduced david garofsky mids called bootstrapping bootstrapping example something called semisupervised learning become one influential algorithms word sentence disambiguation also many machine learning problems heres works going start one ambiguous word example word plant two senses lets say living thing manufacturing sense going come one strong collocation indicative senses example plant one going pick word leaf plant two going pick word factory going classifier looks labeled data sees word plant appears near leaf label sentence one sees factory label sense two since single examples classes label small percentage data lets say sake example one percent labeled plant one one percent labeled plant two last labeled yet okay boot strapping process works going build classifier going look objects classified previously going look additional collocations appear labeled examples example sentence word plant appears near leaf near living use leaf already feature learn living also good feature labels particular sense somehow process bootstrapping going expand first seeds small number labeled examples back features associated classes training supervised classify going get labeled data process going continue pretty much data ports classified one way bootstrapping summarize based two important principles introduced david yarofsky principle one sense per collocation given ambiguous word word appear nears may help ambiguate first one also one sense per discourse meaning instances word ambiguous word discourse unit likely sense methods mentioned trainable question comes get training data word sense disambiguation well several different places go one go sensevalsemcor datasets available public evaluation sense disambiguation systems obtained sensevalorg website senseval annual competition done two different events first event lexical sample task training data small set words things like bar plant board appear sentences annotated proper census second event second task socalled words disambiguation task thats words given document disambiguated cases senses manual annotation obtained wordnet senseval data recently senseval data available many different languages addition english languages like spanish romanian dutch another way train data word sense disambiguation socalled pseudowords principle pseudowords principle says take word ambiguous another word ambiguous combine new word instances unambiguous words example every time see word banana going replace single token stands banana door going thing every occurrence word door document tokens could either one train classifier know instance actual word used word sense disambiguation system going see collapsed word try guess correct one also multilingual corpora aligned sentence level example word play english aligned jugar tocar spanish use information training data word sense disambiguation brief results one earlier senseval evaluations metric used word sense disambiguation based following quantities number assigned senses c number words assigned correct senses total number test words evaluation precision c divided recall c divided basic precision recall evaluation metrics give idea results best performance senseval around precision recall whereas best human lexicographers could get high percents one interesting heuristic ordered list senses word common sense appearing first get decent performance precision recall still close best automatic systems still gives decent baseline one caveat however method works better text relatively homogeneous terms genre domain texts multiple genres domains method unlikely work common sense word changes according domain concludes section word sense disambiguation,Course3,W6-S1-L7,W6,S1,L7,-,6,1,7,okay welcom back nlp go switch differ topic word sens disambigu say ambigu imagin form name automat recogn multipl mean ambigu word use specif sentenc word differ properti seen one polysemi polysemi mean mani word multipl sens exampl let drink bar word bar sentenc mean drink establish howev gener word bar mani mean exampl chocol bar anoth mean word bar studi bar anoth mean bar case mean exam lawyer bring chocol bar anoth properti word homonymi exampl say may come let meet may word may appear sentenc pronounc exact way mean someth complet differ differ book make differ distinct polysemi homonymi purpos class go focu exampl homonymi accid exampl word may verb auxiliari verb word may second sentenc month clearli word differ theyr differ part speech go worri much mostli go worri word typic noun multipl mean go worri word like round verb adject noun part well differ sens bar accord wordnet well there first one bar also barroom saloon room establish alcohol drink serv counter sens counter obtain food drink actual physic piec furnitur top get drink bar rigid piec metal wood use fasten obstruct weapon theyr like bar jail next thing bar music mani mani sens wont spend lot ten minut go detail let show list see there dozen let go back word sens disambigu said main goal take word like bar multipl sens number let say given sentenc tell us mean use see could context that typic get access entir sentenc often entir paragraph document word appear word sens disambigu import compon natur languag process system use exampl come better semant represent sentenc also question answer thing like machin translat obvious word ambigu one languag doesnt mean necessarili ambigu anoth languag exampl want translat sentenc word play english spanish need understand object play exampl english say play violin spanish translat verb tocar el violin sentenc play tenni go use differ word jugar al teni tocar jugar two differ translat play couldnt proper word sens disambigu english wouldnt abl translat sentenc spanish correctli use word sens disambigu exampl accent restor word cote french mean differ thing pronounc differ depend accent appear could someth like cote coast cote side use text speech gener case may word multipl pronunci depend mean exampl lead lead english spell correct want abl distinguish word like aid aid e final capit restor word like turkey know mean bird countri properli capit text capit start let go common techniqu use word sens disambigu start old method michael lesk socal dictionari method idea behind dictionari method ambigu word sentenc appear togeth your go find possibl mean word look dictionari definit sens word look pair dictionari definit one ambigu word overlap here work go match sentenc dictionari definit let look sentenc two ambigu word first one plant accord merriamwebst dictionari mani sens first one live thing grow ground usual leav flower need sun water surviv second interpret plant go call plant defin build factori someth made exampl word leaf two definit go consid leaf later outgrowth plant stem typic flatten expand variabl shape greenish organ constitut unit foliag function primarili food manufactur photosynthesi leaf part book fold sheet contain page side suppos sentenc like leaf food make factori green plant word leaf word plant want determin sens use sentenc four possibl combin leaf plant leaf plant one go pick well go look overlap dictionari definit pick one largest one okay method introduc earli david yarowski upenn time decis list method here work base principl one sens per colloc previou lectur talk colloc say live plant word plant ambigu fact appear near word live part colloc mean first sens word plant studi someth like manufactur plant clearli second sens plant question come list colloc give us hint disambigu ambigu word yarowski method base idea go look two sens per word go tri come order list rule colloc give sens colloc ambigu word automat determin sens formula use order rule decis list follow log probabl sens given colloc divid probabl sens b given colloc sinc decis list method sort word formula whenev new instanc ambigu noun go go list pick first item start top match label word accordingli stop here exampl decis list arovski paper look consist two word specif order also look word within certain window mani word exampl disintegr word bass way also multipl pronunci exampl bass bass go look follow rule word fish appear within window go label bass doesnt hold go look next rule list colloc stripe follow bass go label cours bass next rule list guitar within window word bass word guitar nearbi know second sens word bass bass ill exampl bass player indic bass sens number two play verb follow bass also bass two rule altern bass bass let look detail list type featur use yarowski decis list algorithm mention colloc adjac word one side word exampl word bar could chocol bar bar exam bar stool bar fight word like aid could foreign aid one sens versu presidenti aid sens also look posit import realiz colloc plant pesticid differ colloc pesticid plant first one refer live thing sens plant second one refer factori sens plant posit matter also look adjac part speech want know word follow verb adject also look nearbi word let say within window plu five word entir sentenc also look syntact inform exampl subject word object word exampl word play look specif object whether guitar tenni determin sens inform topic text import one principl jarowski introduc one recent paper call one sentenc per discours principl said time ambigu word appear paragraph document like continu keep sens within entir discours let step back moment look gener classif method use word sens disambigu word sens disambigu scienc differ mani classif point natur point name key declassif class type classif discuss previous one common techniqu use knearest neighbor classif method also known memorybas method idea look current problem exampl sens word context look histori train data similar exampl figur decis made time give decis make current time well typic peopl use vector represent instanc need classifi measur euclidean distanc instanc instanc here exampl accord system put dimens gener case much dimension vector red object form one cluster exampl sens one blue point form anoth cluster sens two given new object want determin classifi knearest neighbor algorithm go look nearest neighbor one nearest neighbor case red dot therefor go classifi red also possibl increas valu k larger odd number exampl three five case k equal three go look three closest exampl vector space one want classifi look major class among return correct answer exampl among three nearest neighbor two red exampl one blue exampl therefor go label hollow exampl red well okay let move anoth method also introduc david garofski mid call bootstrap bootstrap exampl someth call semisupervis learn becom one influenti algorithm word sentenc disambigu also mani machin learn problem here work go start one ambigu word exampl word plant two sens let say live thing manufactur sens go come one strong colloc indic sens exampl plant one go pick word leaf plant two go pick word factori go classifi look label data see word plant appear near leaf label sentenc one see factori label sens two sinc singl exampl class label small percentag data let say sake exampl one percent label plant one one percent label plant two last label yet okay boot strap process work go build classifi go look object classifi previous go look addit colloc appear label exampl exampl sentenc word plant appear near leaf near live use leaf alreadi featur learn live also good featur label particular sens somehow process bootstrap go expand first seed small number label exampl back featur associ class train supervis classifi go get label data process go continu pretti much data port classifi one way bootstrap summar base two import principl introduc david yarofski principl one sens per colloc given ambigu word word appear near may help ambigu first one also one sens per discours mean instanc word ambigu word discours unit like sens method mention trainabl question come get train data word sens disambigu well sever differ place go one go sensevalsemcor dataset avail public evalu sens disambigu system obtain sensevalorg websit sensev annual competit done two differ event first event lexic sampl task train data small set word thing like bar plant board appear sentenc annot proper censu second event second task socal word disambigu task that word given document disambigu case sens manual annot obtain wordnet sensev data recent sensev data avail mani differ languag addit english languag like spanish romanian dutch anoth way train data word sens disambigu socal pseudoword principl pseudoword principl say take word ambigu anoth word ambigu combin new word instanc unambigu word exampl everi time see word banana go replac singl token stand banana door go thing everi occurr word door document token could either one train classifi know instanc actual word use word sens disambigu system go see collaps word tri guess correct one also multilingu corpora align sentenc level exampl word play english align jugar tocar spanish use inform train data word sens disambigu brief result one earlier sensev evalu metric use word sens disambigu base follow quantiti number assign sens c number word assign correct sens total number test word evalu precis c divid recal c divid basic precis recal evalu metric give idea result best perform sensev around precis recal wherea best human lexicograph could get high percent one interest heurist order list sens word common sens appear first get decent perform precis recal still close best automat system still give decent baselin one caveat howev method work better text rel homogen term genr domain text multipl genr domain method unlik work common sens word chang accord domain conclud section word sens disambigu,[ 4 14 13 12 11]
284,Course3_W7-S1-L1_-_Noisy_Channel_Model_-_8_slides_08-33,okay next topic related language modelling noisy channel models noisy channel model important natural language processing lets look example first suppose hypothetical system takes input written english lets call x sort encoder randomly garbles input converting sort garbled english output going call spoken english garbling well going convert original sequence words english written audio signal corresponds spoken english lets look examples convert grammatical english x english grammatical mistakes convert english x bit maps characters thats general case noisy channel model used determine connection joint probability two sequences product first sequence times probability second one given first one think process encoding decoding given foreign language chain want guess english language version assume e converted f using encoder converts e f want build decoder converts f e im using notation e apostrophe indicate estimate e doesnt mean original e best guess e determine e well e going value maximizes conditional probability english sentence given foreign sentence probability e given f according previous example bayesian formula write expression e maximizes product conditional probability f given e probability e two well pfe noisy channel model also known translation model pe language model already know language model looks like takes sequence words given language case english tells us likely particular sequence valid sentence going introduce translation model essentially tells us given certain sentence english whats probability particular sentence foreign language corresponds sentence english heres example french want translate la maison blanche french english dont know french means white house possible translations first one going translate cat plays piano obviously probability foreign string la maison blanche matched english string cat plays piano low going put minus box english language probability cat plays piano valid sentence going get given score low score next one little bit better possible translation house white well grammatical sentence least matches right words french going give positive score translation model negative score language model next one little bit better house white grammatical sentence english matches words french going give set scores previous example lets look two examples red house well red house valid sequence words english french doesnt match word red doesnt match words original sentence french small cat also valid sentence anything matches french words going give examples negative score translation mode finally sequence white house going obviously high score features going high language modeling probability valid nonphrase english going high translation score words original french sentence idea translation models like come possible sequences words english right length try figure based two probabilities two columns one best translation original sentence good translation going high score language model also high score translation model many phrases high scores going multiply two probabilities pick one highest product possible uses noisy channel model well mentioned machine translation many others example used handwriting recognition handwriting recognition input going bitmap write output going english sentence also used text generation text summarization machine translation spelling correction example spelling correction use model probability certain type mistake going made separate lecture edit distance text similarity going talk problem heres example spelling correction example comes peter norvig idea word thew thew clearly mistake theres word english want predict candidate words second column likely substitutions first one word next one leave word alone third one replace e get word thaw next one insert r would word threw finally replacement swaps two adjacent letters gets another nonsensical word thwe need figure conditions make substitutions possible first example looking probability going replace ew e probability small probability c large would get numbers well likely going insert w middle word word grammatical english going bit high school tfc next one thew thew gets typo ability first column means typo made however probability valid english small third example different numbers probability mistake going happen replace e probability one thousand probability thaw action english word actively high real english word add missing values cells table last column multiply two probabilities translation model probability language model probability client also going multiply billionth going numbers around one case first suggested spelling correction going replace thew going score second alternative keep score three substitutions possible much lower probabilities scores respectively one youre interested detail look peter norvigs section ngrams book url concludes section noise language models going continue next topic part speech tagging,Course3,W7-S1-L1,W7,S1,L1,-,7,1,1,okay next topic relat languag model noisi channel model noisi channel model import natur languag process let look exampl first suppos hypothet system take input written english let call x sort encod randomli garbl input convert sort garbl english output go call spoken english garbl well go convert origin sequenc word english written audio signal correspond spoken english let look exampl convert grammat english x english grammat mistak convert english x bit map charact that gener case noisi channel model use determin connect joint probabl two sequenc product first sequenc time probabl second one given first one think process encod decod given foreign languag chain want guess english languag version assum e convert f use encod convert e f want build decod convert f e im use notat e apostroph indic estim e doesnt mean origin e best guess e determin e well e go valu maxim condit probabl english sentenc given foreign sentenc probabl e given f accord previou exampl bayesian formula write express e maxim product condit probabl f given e probabl e two well pfe noisi channel model also known translat model pe languag model alreadi know languag model look like take sequenc word given languag case english tell us like particular sequenc valid sentenc go introduc translat model essenti tell us given certain sentenc english what probabl particular sentenc foreign languag correspond sentenc english here exampl french want translat la maison blanch french english dont know french mean white hous possibl translat first one go translat cat play piano obvious probabl foreign string la maison blanch match english string cat play piano low go put minu box english languag probabl cat play piano valid sentenc go get given score low score next one littl bit better possibl translat hous white well grammat sentenc least match right word french go give posit score translat model neg score languag model next one littl bit better hous white grammat sentenc english match word french go give set score previou exampl let look two exampl red hous well red hous valid sequenc word english french doesnt match word red doesnt match word origin sentenc french small cat also valid sentenc anyth match french word go give exampl neg score translat mode final sequenc white hous go obvious high score featur go high languag model probabl valid nonphras english go high translat score word origin french sentenc idea translat model like come possibl sequenc word english right length tri figur base two probabl two column one best translat origin sentenc good translat go high score languag model also high score translat model mani phrase high score go multipli two probabl pick one highest product possibl use noisi channel model well mention machin translat mani other exampl use handwrit recognit handwrit recognit input go bitmap write output go english sentenc also use text gener text summar machin translat spell correct exampl spell correct use model probabl certain type mistak go made separ lectur edit distanc text similar go talk problem here exampl spell correct exampl come peter norvig idea word thew thew clearli mistak there word english want predict candid word second column like substitut first one word next one leav word alon third one replac e get word thaw next one insert r would word threw final replac swap two adjac letter get anoth nonsens word thwe need figur condit make substitut possibl first exampl look probabl go replac ew e probabl small probabl c larg would get number well like go insert w middl word word grammat english go bit high school tfc next one thew thew get typo abil first column mean typo made howev probabl valid english small third exampl differ number probabl mistak go happen replac e probabl one thousand probabl thaw action english word activ high real english word add miss valu cell tabl last column multipli two probabl translat model probabl languag model probabl client also go multipli billionth go number around one case first suggest spell correct go replac thew go score second altern keep score three substitut possibl much lower probabl score respect one your interest detail look peter norvig section ngram book url conclud section nois languag model go continu next topic part speech tag,[ 8  4 10  3  1]
285,Course3_W7-S1-L2_-_Part_of_Speech_Tagging_-_15_slides_18-08,okay welcome back natural language processing next topic today going part speech tagging part speech tagging one fundamental applications statistics natural language processing lets define first pos part speech task give us example sentence got news yesterday bahrainis vote second round parliamentary election eight words words need associated part speech parts speech things like nouns verbs prepositions articles heres another example famous poem jabberwocky lewis carroll alice looking glass may seen poem really funny includes lot nonsensical words let read words twas brillig slithy toves gyre gimble wabe see words real words yet figure parts speech example word gyre gyre likely verb follows analogy word gimble gimble also verb word wabe likely noun follows word see get lot morphological part speech information words even theyre real words another example word toves noun know indicates plural verb know indicates third person fact set two words end first sentence actually ambiguous slithy toves mean adjective followed noun also mean noun followed verb make sense would difficult figure one without additional context information word mimsy well mimsy two reasons classified adjective first ending consistent many adjectives also follows word also indicates could adjective finally word borogroves clearly noun follows word parts speech used english well belong two different categories one socalled open class classes add new words time things correspond new words nouns nonmodal verbs adjectives adverbs cannot also closed class parts speech example prepositions modal verbs conjunctions particles determiners pronouns difficult invent new instances although possible case example recent use word ze genderneutral pronoun purpose class going use socalled penn treebank tagset examples tags use first example coordinating conjunction word like labeled cc second categorys cardinal numbers example labeled cd determiner existential foreign word typically first two characters use denote part speech example jj adjective third additional characters exist used give additional information part speech example jj stands adjective jjr comparative adjective example greener jjs superlative adjective example greenest similarly nn stands simplest type noun singular noun like table nns stands plural noun something ends typically like tables nnp stands proper singular noun like john finally nnps stands plural proper nouns vikings next slide penn treebank tagset examples example rb used adverb vb used base form verb interesting forms verbs vbd past tense verb thats necessarily ed form example regular words like take forms like took past tense would nevertheless labeled vbd vbg gerund present participle typically ing form vbn past participle normally ed form like vbd irregular words like take specific word like taken finally vbp used non third person singular present like take one interesting observation prepositions labelled label see previous slide except preposition gets label often used particle cases like go dont confused fact label indicate preposition indicates preposition except preposition tag used preposition even used particle little confusing good reasons people developed penn treebank set way lets make observations words english often ambiguous example word count noun also verb types brown corpus ambiguous however also tend frequent words look number tokens instead number types turns whole tokens brown corpus ambiguous heres example brown corpus word like many five different tags adp verb adj adv noun word present tagged adjective noun verb adverb examples words ambiguous english obviously previous one example first word look first time youre going think nonambiguous word however part two different parts speech noun verb also pronounced differently say transport verb transport interesting property applies words sequence say object something say achieved object discount something get discount address somebody live certain address another similar example word content adjective noun different meanings cases turns pattern speech important also pronunciation purposes example french many words pronounced differently depending pattern speech even spelling exactly three examples first one spelled est french two different words one third person singular verb case word pronounced foreign word east case case pronounced foreign one case verb pronounced one way case noun pronounced different way second example word president means president noun pronounced said president also verb case pronounced president ent silent corresponds third person plural verb preside preside final example word like fils pronounced differently said fils means sun fi fi means plural word fils train case additional morphological information noun whether singular plural tells us pronounce three main techniques part speech tagging going look next slides first one rule based technique second one based machine learning specifically tools like conditional random fields hidden mark models maximum marks entropy marks model third one something called transformation based learning methods valid used reallife part speech tagging however machine learning methods transformationbased methods powerful rulebased much easier scale languages domains trained automatically part speech tagging important applies parsing translation text speech word sense disambiguation pretty much major components natural language processing system depend proper proper part speech tagging lets look example bethlehem steel corporation hammered higher costs case word costs labeled noun plural nns compare bethlehem steel corporation hammered higher costs cost labeled verb given tags valid interpretations word costs know one pick sentence complete part speech assignment well obviously first example correct second one incorrect part speech system really know information one possible approach use baseline probabilities look number times costs appears training data see many times word labeled noun see many times labeled verb pick one frequent would essentially unigram part speech another possibility consider word example probability costs going noun given previous word word higher probability costs verb given previous word word higher clearly case likely first interpretation valid adjective likely see noun verb actually leads another model used example instead looking previous current word look part speech previous word current word say going pick interpretation costs consistent fact previous word adjective matter adjective looking three examples gave first unigram model second one bigram model looks words third one bigram model looks current word part speech previous word together part speech previous word label words sequence starting left right want consider part speech word current word would need right hand side going left sources information use label words example knowledge individual words useful fact costs noun verb look lexical information information available dictionary word look spelling example words end likely nouns like lets say word suitor vector words end est likely superlative forms adjectives also look capitalization things like ibm capital letters likely organization product lets say adjective common noun also use knowledge neighboring words examples mentioned look previous word word maybe two previous words previous word part speech word okay introduce specific techniques part speech tagging lets decide first going evaluate performance well turns varying part speech tagging straightforward however theres problem high baseline high baseline comes fact tag word likely tag tag outofvocabulary word noun baseline alone gives us accuracy predicting pattern speech next word automatic system come significantly higher useful current accuracy best part speech tags around english slightly upper bound expected human performance turns cases humans dont agree time happens adjective used noun example noun noun compound example word college senior two consecutive nouns cases first word noun phrase nonnoun component adjective used noun noun used adjective example senior class word senior used noun adjective humans dont necessarily agree part speech okay lets talk first type part speech tagging called rulebased method typically done using set finitestate automaton specifically finitestate transducers find possible parts speech given sequence words use disambiguation rules make transitions possible compared others example rule says article never followed verbs every time see transition article verb machine going remove possible set outputs define hundreds constraints like manually call finite state transducers heres example paper worked many years ago french part speech tagging sequence words lefthand side la teneur moyenne en uranium des riviere bien que delicate caculer stands average content uranium rivers even though difficult compute want tag sentence well first find possible parts speech associated words put second column beginning sentence marked special symbol carrot indicates beginning sentence word la many different things french pronoun noun next word teneur interesting noun cases mean different things feminine masculine example feminine correct time would nfs next word moyenne either adjective noun verb verb either first second third person forth sequence parts speech get rid said sequence article verb allowed lets look specific examples rule says third person subject personal pronoun cannot followed first person indirect personal pronoun example il nous faut stands need word il tag third person singular personal pronoun word nous several possible tags first person indirect personal pronouns direct personal pronouns get rid ones combine bs bi removes alternatives tags new keeps four heres another example constraint n followed k n stands noun k stands interrogative pronoun example sentence le fleuve qui river want label second word qui relative pronoun interrogative pronoun want get rid k tag use alternative e acceptable choice finally example gave article cannot followed verb example sentence contains words lappelle word appelle verb word l either article personal pronoun rule r v helps us get rid one eliminate article therefore unambiguously determining word pronoun small set examples large system hundreds rules imagine looking examples difficult build systems need lot linguists involved often dont necessarily agree whole process time consuming group based systems purpose sometimes useful especially unseen languages time part speech data done using completely automated methods hmms transformation based learning methods going look examples part speech tagging next slides,Course3,W7-S1-L2,W7,S1,L2,-,7,1,2,okay welcom back natur languag process next topic today go part speech tag part speech tag one fundament applic statist natur languag process let defin first po part speech task give us exampl sentenc got news yesterday bahraini vote second round parliamentari elect eight word word need associ part speech part speech thing like noun verb preposit articl here anoth exampl famou poem jabberwocki lewi carrol alic look glass may seen poem realli funni includ lot nonsens word let read word twa brillig slithi tove gyre gimbl wabe see word real word yet figur part speech exampl word gyre gyre like verb follow analog word gimbl gimbl also verb word wabe like noun follow word see get lot morpholog part speech inform word even theyr real word anoth exampl word tove noun know indic plural verb know indic third person fact set two word end first sentenc actual ambigu slithi tove mean adject follow noun also mean noun follow verb make sens would difficult figur one without addit context inform word mimsi well mimsi two reason classifi adject first end consist mani adject also follow word also indic could adject final word borogrov clearli noun follow word part speech use english well belong two differ categori one socal open class class add new word time thing correspond new word noun nonmod verb adject adverb cannot also close class part speech exampl preposit modal verb conjunct particl determin pronoun difficult invent new instanc although possibl case exampl recent use word ze genderneutr pronoun purpos class go use socal penn treebank tagset exampl tag use first exampl coordin conjunct word like label cc second categori cardin number exampl label cd determin existenti foreign word typic first two charact use denot part speech exampl jj adject third addit charact exist use give addit inform part speech exampl jj stand adject jjr compar adject exampl greener jj superl adject exampl greenest similarli nn stand simplest type noun singular noun like tabl nn stand plural noun someth end typic like tabl nnp stand proper singular noun like john final nnp stand plural proper noun vike next slide penn treebank tagset exampl exampl rb use adverb vb use base form verb interest form verb vbd past tens verb that necessarili ed form exampl regular word like take form like took past tens would nevertheless label vbd vbg gerund present participl typic ing form vbn past participl normal ed form like vbd irregular word like take specif word like taken final vbp use non third person singular present like take one interest observ preposit label label see previou slide except preposit get label often use particl case like go dont confus fact label indic preposit indic preposit except preposit tag use preposit even use particl littl confus good reason peopl develop penn treebank set way let make observ word english often ambigu exampl word count noun also verb type brown corpu ambigu howev also tend frequent word look number token instead number type turn whole token brown corpu ambigu here exampl brown corpu word like mani five differ tag adp verb adj adv noun word present tag adject noun verb adverb exampl word ambigu english obvious previou one exampl first word look first time your go think nonambigu word howev part two differ part speech noun verb also pronounc differ say transport verb transport interest properti appli word sequenc say object someth say achiev object discount someth get discount address somebodi live certain address anoth similar exampl word content adject noun differ mean case turn pattern speech import also pronunci purpos exampl french mani word pronounc differ depend pattern speech even spell exactli three exampl first one spell est french two differ word one third person singular verb case word pronounc foreign word east case case pronounc foreign one case verb pronounc one way case noun pronounc differ way second exampl word presid mean presid noun pronounc said presid also verb case pronounc presid ent silent correspond third person plural verb presid presid final exampl word like fil pronounc differ said fil mean sun fi fi mean plural word fil train case addit morpholog inform noun whether singular plural tell us pronounc three main techniqu part speech tag go look next slide first one rule base techniqu second one base machin learn specif tool like condit random field hidden mark model maximum mark entropi mark model third one someth call transform base learn method valid use reallif part speech tag howev machin learn method transformationbas method power rulebas much easier scale languag domain train automat part speech tag import appli pars translat text speech word sens disambigu pretti much major compon natur languag process system depend proper proper part speech tag let look exampl bethlehem steel corpor hammer higher cost case word cost label noun plural nn compar bethlehem steel corpor hammer higher cost cost label verb given tag valid interpret word cost know one pick sentenc complet part speech assign well obvious first exampl correct second one incorrect part speech system realli know inform one possibl approach use baselin probabl look number time cost appear train data see mani time word label noun see mani time label verb pick one frequent would essenti unigram part speech anoth possibl consid word exampl probabl cost go noun given previou word word higher probabl cost verb given previou word word higher clearli case like first interpret valid adject like see noun verb actual lead anoth model use exampl instead look previou current word look part speech previou word current word say go pick interpret cost consist fact previou word adject matter adject look three exampl gave first unigram model second one bigram model look word third one bigram model look current word part speech previou word togeth part speech previou word label word sequenc start left right want consid part speech word current word would need right hand side go left sourc inform use label word exampl knowledg individu word use fact cost noun verb look lexic inform inform avail dictionari word look spell exampl word end like noun like let say word suitor vector word end est like superl form adject also look capit thing like ibm capit letter like organ product let say adject common noun also use knowledg neighbor word exampl mention look previou word word mayb two previou word previou word part speech word okay introduc specif techniqu part speech tag let decid first go evalu perform well turn vari part speech tag straightforward howev there problem high baselin high baselin come fact tag word like tag tag outofvocabulari word noun baselin alon give us accuraci predict pattern speech next word automat system come significantli higher use current accuraci best part speech tag around english slightli upper bound expect human perform turn case human dont agre time happen adject use noun exampl noun noun compound exampl word colleg senior two consecut noun case first word noun phrase nonnoun compon adject use noun noun use adject exampl senior class word senior use noun adject human dont necessarili agre part speech okay let talk first type part speech tag call rulebas method typic done use set finitest automaton specif finitest transduc find possibl part speech given sequenc word use disambigu rule make transit possibl compar other exampl rule say articl never follow verb everi time see transit articl verb machin go remov possibl set output defin hundr constraint like manual call finit state transduc here exampl paper work mani year ago french part speech tag sequenc word lefthand side la teneur moyenn en uranium de rivier bien que delic cacul stand averag content uranium river even though difficult comput want tag sentenc well first find possibl part speech associ word put second column begin sentenc mark special symbol carrot indic begin sentenc word la mani differ thing french pronoun noun next word teneur interest noun case mean differ thing feminin masculin exampl feminin correct time would nf next word moyenn either adject noun verb verb either first second third person forth sequenc part speech get rid said sequenc articl verb allow let look specif exampl rule say third person subject person pronoun cannot follow first person indirect person pronoun exampl il nou faut stand need word il tag third person singular person pronoun word nou sever possibl tag first person indirect person pronoun direct person pronoun get rid one combin bs bi remov altern tag new keep four here anoth exampl constraint n follow k n stand noun k stand interrog pronoun exampl sentenc le fleuv qui river want label second word qui rel pronoun interrog pronoun want get rid k tag use altern e accept choic final exampl gave articl cannot follow verb exampl sentenc contain word lappel word appel verb word l either articl person pronoun rule r v help us get rid one elimin articl therefor unambigu determin word pronoun small set exampl larg system hundr rule imagin look exampl difficult build system need lot linguist involv often dont necessarili agre whole process time consum group base system purpos sometim use especi unseen languag time part speech data done use complet autom method hmm transform base learn method go look exampl part speech tag next slide,[ 4  0  1 14 13]
286,Course3_W7-S1-L3_-_Hidden_Markov_Models_1-2_-_30_slides_24-41,okay going continue new topic called hidden markov models going using hidden markov models many different nlp tasks including part speech tagging first lets see work first concept want introduce today idea markov model visible markov model hidden markov model well get hidden models later markov model sequence random variables independent example could weather reports weather report temperature given day depends temperature previous day text probability next word next letter depends previous word previous letter respectively heres properties markov models limited horizon probability observation time plus one depend probabilities recent history previous word previous words also time invariant probability seeing certain variable certain time depend time definition markov model terms transition matrix tells us probability moving one state another also initial state probabilities pi heres example six states b c e f define probability distribution transitions states specific numbers start state state look arcs go state go state probability go state c probability go state f probability state go back probability go state b probability thing remaining four states one thing pretty obvious given particular state sum probabilities outgoing transitions equal one makes sure theyre probability distribution want compute probability sequence states x xt use chain introduced earlier lecture gives us formula p first observation first state times probability second state given first state times probability third state given previous two times probability last state given states general formula joint distribution xy xt using mark assumption using bigram model change probability observing x times probability observing x given x times probability observing xt given x point ignoring history particularly x example looking two recent states final term xt given xt minus instead xt given everything xt compact form product xi plus one bigrams sequence starting equals one equals minus one whats probability example going see sequence dab well thats probability first state times probability second state given first state times probability third state b given previous one replace probabilities numbers specific example get first one one second one third one multiplied see sequence dab probability thing sequence probabilities particular example assume start state always state means sequence starts symbol going probability zero okay lets look hidden markov model something much appropriate language modeling visible markov model part speech tagging example observing sequence symbols dont know sequence states led generation symbols part speech tagging symbols observe actual words sequence states parts speech correspond words definition hidden markov model following q sequence states observed sequence observations drawn vocabulary q qf specific start final states set state transition probabilities like visible model also new set parameters called b known symbol emission probabilities essentially youre given state whats probability youre going emit certain symbol state also pi like visible model initial state probabilities mu union b pi complete probabilistic model determines hidden markov model many machine learning articles going look one goals find values mu equivalent saying find values b pi marker models use part speech tagging speech recognition gene sequencing many areas research used model state sequences also observation sequences example want find probability certain set states correlated certain set words bigram model essentially write product bigram probabilities going one state next times probability emitting certain symbol w sub state sub heres simple example sequence states sn state one produce w output symbol state two produce w way sn state sn produce word wn output hmm algorithm generative algorithm want start picking state initial distribution pi based probabilities pi set consecutive steps capital consecutive steps going move another state based previous state based probability given transition matrix going also emit observation based b b remind matrix emission probabilities heres example two states b connected certain probabilities specifically probability going b probability staying probability going b probability staying b already b remind probabilities come node state add one verify case also define start symbol start state starting probability case unambiguously going start state probability starting b zero lets look specific examples interested probability certain sequence observations time probability observation time equal k given currently certain state previous state given well emission probability matrix state much likely produce symbol x probability less likely produce z probabilities respectively yet still possible produce state b likely produce symbol probability also produce x z probabilities respectively parameters model well initial parameters probability given start probability b given start corresponds start transition earlier diagram transition probabilities given paa pab pba pbb emission probabilities pxa like previous slide see parameters model shown total parameters later talk learning parameters hmm going refer parameters collectively parameter set mu model mu lets look example want figure whats probability going observe sequence yz automaton going start state consider possible sequences states start go back start switch b move b next go finally go b stay b sequences states lead certain probability observation yz want consider probability yz given sequence two steps consider probability yz generated sequence aa probability yz generated sequence ab probability yz generated ba finally probability yz generated sequence bb sum four going give us full probability sequence observations yz first one four terms thats probability staying given started times probability emitting character state times probability staying second time thats times probability emitting z state equal second term get times matches previous line also probability going b times probability omitting z state b also thing rest formula im going give second think try fill numbers okay let give answers question probability producing yz given state sequence ba equal probability going b times probability producing state b times probability going b times probability generating z state last line similar times times probability going b b times probability emitting z state b add numbers youre going see overall probability sequence yz equal relatively speaking unlikely sequence given nine possible sequences x z one average would expected appear time particular one unlikely probability kind obvious look original example produce z none states b gives high probability would much likely given started state would produce sequence contains least one x let finish example similarly compute probability sequence zz yx xz probability sequences need add possible disjoint alternatives get observations start state little bit background states essentially used encode recent history dont necessarily include recent part speech include recent several parts speech within background model would encode recent part speech transitions encode likely sequences states example looked example article cannot followed verb therefore probability jumping article state verb state going low much likely go adjective noun probability state sequence adjective noun going relatively higher also possible sequences two words high probability example sequence articleadjectivenoun likely english therefore expect sequence two background probabilities high estimate transition probabilities use maximum likelihood estimate one possible way training corpus example part speech tagging look sequence adjacent parts speech use estimate probability transition lets consider case emission probabilities estimating emission probabilities actually little bit harder transition probabilities may novel uses specific combinations words parts speech specific word may appeared training data one part speech test set may appear completely different part speech heres suggestions used possible use standard smoothing like previous lecture also possible use heuristics example based spelling words okay lets look sequence observations often case hmm thats essentially reason hmms invented case observer see emitted symbols states led generation one thing may want compute observation likelihood given observation sequence model mu consists transition matrix emission matrix initial state probability want compute probability sequence given mu whats probability sequence generated particular model turns able compute probability observations sequence turns hmm language model language model way compute probability sequence sentence sequence observations important tasks hmms first one given probabilistic model b pi want find probability observation given model second task given observation maybe set observations mu want figure sequence states led observation finally given observation space possible mus want find mu best describes observations one important tasks hmm processing called decoding task finding likely sequence want tag token label also find observation likelihood classification sequences learn training models fit empirical data lets look one examples first want inference task inference find like sequence tags given sequence words essentially idea well use later parts speech tagging looking sequence words want predict sequence parts speech given possible ts sequences tags want pick one maximized probability ptw know model mu possible compute probability tw using generate model hmm values practice theres many combinations make feasible one possible solution use beam search essentially look partial hypothesis certain point time observation sequence maybe take top ten assume sequence reach end sentence probability skipped beam search method may work possible first words lead us one state discard valuable hypotheses low probability far find actually best hypothesis reach end sequence instead come much important algorithm called viterbi algorithm one fundamental algorithms natural language processing based dynamic programming used find best path specific observation apply entire sequence tell us observation probability entire sentence apply first word second word gives us best path point uses dynamic programming uses memorization remind memorization means essentially way store probabilities subsequence already computed dont compute future use similar technique parsing looked cky algorithm another important characteristic backpointers want keep track best path takes us certain state also got point heres example im going introduce called hmm trellis lattice heres looks like example four rows four columns column corresponds one time unit start state zero time zero go time one two three rows well first row corresponds first possible partofspeech thats start symbol second third row correspond two partsofspeech b finally one corresponds end state looking sequence takes us start node lower lefthand corner end state upper righthand corner going sequence bs column example sequence two words encoded second third column want find whether generated sequence aa sequence ab ba bb used solid circles states actually reached point time dotted circles states cannot really reached completeness lets see going define transitions given state another state probability node second column going equal probability observing symbol given state going transition state state b moving observation z observation finally get end sequence going transition state b state b lets look detail going start viterbi algorithm starting start state start state go either b certain probability probability time one well probability time one equal probability starting times probability given start state times probability admitting symbol given state similarly choose go b case want compute probability b times one product starting probability times probability getting b start states probability admitting b state next iteration want compute probability b time equals two well two possible ways get go start b go start b b b probability b time two going equal sum probabilities two paths first path probability time one times probability going b times probability emitting z state b second path product starting state b time one transferring state b times finally emitting z state b going pick value assign circle says b times going pick one larger two case block arrow likely get state b given observation yz went first state b stayed state b went first switched back b okay give back pointer goes state b time two state b time one back start time zero end want compute probability getting end state time equals three see ladder shows us possible ways together algorithm going help us compute probability making easier probability end state time three going maximum two possible ways get time two specifically going state end state going state b end state also case multiply probability emitting n symbol given end symbol given b whatever path best going included state corresponds end symbol times three going overall probability generating sequence yz end sentence given particular hmm set backpointers going tell us particular case likely ran sequence b b end concludes example okay went example viterbi algorithms using hmms hmm trellis going continue hmms next segment,Course3,W7-S1-L3,W7,S1,L3,-,7,1,3,okay go continu new topic call hidden markov model go use hidden markov model mani differ nlp task includ part speech tag first let see work first concept want introduc today idea markov model visibl markov model hidden markov model well get hidden model later markov model sequenc random variabl independ exampl could weather report weather report temperatur given day depend temperatur previou day text probabl next word next letter depend previou word previou letter respect here properti markov model limit horizon probabl observ time plu one depend probabl recent histori previou word previou word also time invari probabl see certain variabl certain time depend time definit markov model term transit matrix tell us probabl move one state anoth also initi state probabl pi here exampl six state b c e f defin probabl distribut transit state specif number start state state look arc go state go state probabl go state c probabl go state f probabl state go back probabl go state b probabl thing remain four state one thing pretti obviou given particular state sum probabl outgo transit equal one make sure theyr probabl distribut want comput probabl sequenc state x xt use chain introduc earlier lectur give us formula p first observ first state time probabl second state given first state time probabl third state given previou two time probabl last state given state gener formula joint distribut xy xt use mark assumpt use bigram model chang probabl observ x time probabl observ x given x time probabl observ xt given x point ignor histori particularli x exampl look two recent state final term xt given xt minu instead xt given everyth xt compact form product xi plu one bigram sequenc start equal one equal minu one what probabl exampl go see sequenc dab well that probabl first state time probabl second state given first state time probabl third state b given previou one replac probabl number specif exampl get first one one second one third one multipli see sequenc dab probabl thing sequenc probabl particular exampl assum start state alway state mean sequenc start symbol go probabl zero okay let look hidden markov model someth much appropri languag model visibl markov model part speech tag exampl observ sequenc symbol dont know sequenc state led gener symbol part speech tag symbol observ actual word sequenc state part speech correspond word definit hidden markov model follow q sequenc state observ sequenc observ drawn vocabulari q qf specif start final state set state transit probabl like visibl model also new set paramet call b known symbol emiss probabl essenti your given state what probabl your go emit certain symbol state also pi like visibl model initi state probabl mu union b pi complet probabilist model determin hidden markov model mani machin learn articl go look one goal find valu mu equival say find valu b pi marker model use part speech tag speech recognit gene sequenc mani area research use model state sequenc also observ sequenc exampl want find probabl certain set state correl certain set word bigram model essenti write product bigram probabl go one state next time probabl emit certain symbol w sub state sub here simpl exampl sequenc state sn state one produc w output symbol state two produc w way sn state sn produc word wn output hmm algorithm gener algorithm want start pick state initi distribut pi base probabl pi set consecut step capit consecut step go move anoth state base previou state base probabl given transit matrix go also emit observ base b b remind matrix emiss probabl here exampl two state b connect certain probabl specif probabl go b probabl stay probabl go b probabl stay b alreadi b remind probabl come node state add one verifi case also defin start symbol start state start probabl case unambigu go start state probabl start b zero let look specif exampl interest probabl certain sequenc observ time probabl observ time equal k given current certain state previou state given well emiss probabl matrix state much like produc symbol x probabl less like produc z probabl respect yet still possibl produc state b like produc symbol probabl also produc x z probabl respect paramet model well initi paramet probabl given start probabl b given start correspond start transit earlier diagram transit probabl given paa pab pba pbb emiss probabl pxa like previou slide see paramet model shown total paramet later talk learn paramet hmm go refer paramet collect paramet set mu model mu let look exampl want figur what probabl go observ sequenc yz automaton go start state consid possibl sequenc state start go back start switch b move b next go final go b stay b sequenc state lead certain probabl observ yz want consid probabl yz given sequenc two step consid probabl yz gener sequenc aa probabl yz gener sequenc ab probabl yz gener ba final probabl yz gener sequenc bb sum four go give us full probabl sequenc observ yz first one four term that probabl stay given start time probabl emit charact state time probabl stay second time that time probabl emit z state equal second term get time match previou line also probabl go b time probabl omit z state b also thing rest formula im go give second think tri fill number okay let give answer question probabl produc yz given state sequenc ba equal probabl go b time probabl produc state b time probabl go b time probabl gener z state last line similar time time probabl go b b time probabl emit z state b add number your go see overal probabl sequenc yz equal rel speak unlik sequenc given nine possibl sequenc x z one averag would expect appear time particular one unlik probabl kind obviou look origin exampl produc z none state b give high probabl would much like given start state would produc sequenc contain least one x let finish exampl similarli comput probabl sequenc zz yx xz probabl sequenc need add possibl disjoint altern get observ start state littl bit background state essenti use encod recent histori dont necessarili includ recent part speech includ recent sever part speech within background model would encod recent part speech transit encod like sequenc state exampl look exampl articl cannot follow verb therefor probabl jump articl state verb state go low much like go adject noun probabl state sequenc adject noun go rel higher also possibl sequenc two word high probabl exampl sequenc articleadjectivenoun like english therefor expect sequenc two background probabl high estim transit probabl use maximum likelihood estim one possibl way train corpu exampl part speech tag look sequenc adjac part speech use estim probabl transit let consid case emiss probabl estim emiss probabl actual littl bit harder transit probabl may novel use specif combin word part speech specif word may appear train data one part speech test set may appear complet differ part speech here suggest use possibl use standard smooth like previou lectur also possibl use heurist exampl base spell word okay let look sequenc observ often case hmm that essenti reason hmm invent case observ see emit symbol state led gener one thing may want comput observ likelihood given observ sequenc model mu consist transit matrix emiss matrix initi state probabl want comput probabl sequenc given mu what probabl sequenc gener particular model turn abl comput probabl observ sequenc turn hmm languag model languag model way comput probabl sequenc sentenc sequenc observ import task hmm first one given probabilist model b pi want find probabl observ given model second task given observ mayb set observ mu want figur sequenc state led observ final given observ space possibl mu want find mu best describ observ one import task hmm process call decod task find like sequenc want tag token label also find observ likelihood classif sequenc learn train model fit empir data let look one exampl first want infer task infer find like sequenc tag given sequenc word essenti idea well use later part speech tag look sequenc word want predict sequenc part speech given possibl ts sequenc tag want pick one maxim probabl ptw know model mu possibl comput probabl tw use gener model hmm valu practic there mani combin make feasibl one possibl solut use beam search essenti look partial hypothesi certain point time observ sequenc mayb take top ten assum sequenc reach end sentenc probabl skip beam search method may work possibl first word lead us one state discard valuabl hypothes low probabl far find actual best hypothesi reach end sequenc instead come much import algorithm call viterbi algorithm one fundament algorithm natur languag process base dynam program use find best path specif observ appli entir sequenc tell us observ probabl entir sentenc appli first word second word give us best path point use dynam program use memor remind memor mean essenti way store probabl subsequ alreadi comput dont comput futur use similar techniqu pars look cki algorithm anoth import characterist backpoint want keep track best path take us certain state also got point here exampl im go introduc call hmm trelli lattic here look like exampl four row four column column correspond one time unit start state zero time zero go time one two three row well first row correspond first possibl partofspeech that start symbol second third row correspond two partsofspeech b final one correspond end state look sequenc take us start node lower lefthand corner end state upper righthand corner go sequenc bs column exampl sequenc two word encod second third column want find whether gener sequenc aa sequenc ab ba bb use solid circl state actual reach point time dot circl state cannot realli reach complet let see go defin transit given state anoth state probabl node second column go equal probabl observ symbol given state go transit state state b move observ z observ final get end sequenc go transit state b state b let look detail go start viterbi algorithm start start state start state go either b certain probabl probabl time one well probabl time one equal probabl start time probabl given start state time probabl admit symbol given state similarli choos go b case want comput probabl b time one product start probabl time probabl get b start state probabl admit b state next iter want comput probabl b time equal two well two possibl way get go start b go start b b b probabl b time two go equal sum probabl two path first path probabl time one time probabl go b time probabl emit z state b second path product start state b time one transfer state b time final emit z state b go pick valu assign circl say b time go pick one larger two case block arrow like get state b given observ yz went first state b stay state b went first switch back b okay give back pointer goe state b time two state b time one back start time zero end want comput probabl get end state time equal three see ladder show us possibl way togeth algorithm go help us comput probabl make easier probabl end state time three go maximum two possibl way get time two specif go state end state go state b end state also case multipli probabl emit n symbol given end symbol given b whatev path best go includ state correspond end symbol time three go overal probabl gener sequenc yz end sentenc given particular hmm set backpoint go tell us particular case like ran sequenc b b end conclud exampl okay went exampl viterbi algorithm use hmm hmm trelli go continu hmm next segment,[10  4  1  7 14]
287,Course3_W7-S1-L4_-_Hidden_Markov_Models_2-2_-_9_slides_05-28,going continue topic hmms looked example lets look else hmms given multiple hidden markov models may want compute hmm likely generated particular observation think useful well turns different hmms different languages decide whether document sentence particular language based likelihood sentence generated particular hmms naive solution try possible sequences hmms something going work practice high complexity instead want something similar viterbi algorithm sense uses dynamic programming something called forward algorithm forward algorithm uses trellis called forward trellis encodes possible state paths im going go math algorithm want tell based markov assumption probability state given time depends probabilities particular state previous time point okay three different types learning algorithms hmm first one called supervised learning thats luxury training sequences labeled parts speech second class belongs socalled unsupervised learning category case training sequences like sequences words sentences dont sequences labels parts speech case thing need know many states expect hmm know number states hmm part speech tagging corresponds number parts speech plus possibly start end something easily get third category matters hmm learning called semisupervised learning semisupervised learning label training data data labeled example hundred sentences manually labeled parts speech millions tens millions sentences labeled lets first look supervised hmm learning want estimate transition probabilities mission probabilities using maximum likelihood estimates simple method use maximum likelihood count many times certain set two states appears divide total number instances first state observation probabilities counting number times certain word certain part speech appear together divide number times particular part speech appears use smoothing unseen conditional probabilities interesting method called unsupervised hmm training case given set observation sequences goal build hmm particular build new model consists b pi matrices general technique used hmm training without supervision called em algorithm em stands expectation maximization specific implementation em hmm training called baumwelch algorithm forwardbackward algorithm baumwelch guaranteed find exact solution best model maximizes probability observation given model however often reaches solution acceptable im going go outline baumwelch heres works initially set parameters hmm random values case believe different parameters going perform set steps set parameters converges two steps e step step e step expectation step used determine probability various state sequences generating observations step maximization step used reestimate parameters based probabilities observed e step often happens small number iterations probably dozen even less set parameters converges stop notes em algorithm algorithm guarantees iteration likelihood data increases also important stopped point time give us reasonably acceptable partial solution dont need wait converges also guaranteed converge local maximum let finish little bit outline methods used hmms natural language processing next lecture going look additional ways part speech tagging,Course3,W7-S1-L4,W7,S1,L4,-,7,1,4,go continu topic hmm look exampl let look els hmm given multipl hidden markov model may want comput hmm like gener particular observ think use well turn differ hmm differ languag decid whether document sentenc particular languag base likelihood sentenc gener particular hmm naiv solut tri possibl sequenc hmm someth go work practic high complex instead want someth similar viterbi algorithm sens use dynam program someth call forward algorithm forward algorithm use trelli call forward trelli encod possibl state path im go go math algorithm want tell base markov assumpt probabl state given time depend probabl particular state previou time point okay three differ type learn algorithm hmm first one call supervis learn that luxuri train sequenc label part speech second class belong socal unsupervis learn categori case train sequenc like sequenc word sentenc dont sequenc label part speech case thing need know mani state expect hmm know number state hmm part speech tag correspond number part speech plu possibl start end someth easili get third categori matter hmm learn call semisupervis learn semisupervis learn label train data data label exampl hundr sentenc manual label part speech million ten million sentenc label let first look supervis hmm learn want estim transit probabl mission probabl use maximum likelihood estim simpl method use maximum likelihood count mani time certain set two state appear divid total number instanc first state observ probabl count number time certain word certain part speech appear togeth divid number time particular part speech appear use smooth unseen condit probabl interest method call unsupervis hmm train case given set observ sequenc goal build hmm particular build new model consist b pi matric gener techniqu use hmm train without supervis call em algorithm em stand expect maxim specif implement em hmm train call baumwelch algorithm forwardbackward algorithm baumwelch guarante find exact solut best model maxim probabl observ given model howev often reach solut accept im go go outlin baumwelch here work initi set paramet hmm random valu case believ differ paramet go perform set step set paramet converg two step e step step e step expect step use determin probabl variou state sequenc gener observ step maxim step use reestim paramet base probabl observ e step often happen small number iter probabl dozen even less set paramet converg stop note em algorithm algorithm guarante iter likelihood data increas also import stop point time give us reason accept partial solut dont need wait converg also guarante converg local maximum let finish littl bit outlin method use hmm natur languag process next lectur go look addit way part speech tag,[ 4 10  1 14 13]
288,Course3_W7-S1-L5_-_Statistical_POS_Tagging_-_16_slides_09-28,okay welcome back natural language processing last segments looked problem partofspeech tagging looked hmms going see combine two together lecture statistical methods partofspeech tagging particular remind many different types tagging methods rulebase already went stochastic based hmm maximum entropy mark models difference two hmms generated models maximum entropy mark models discriminated class going look hmms finally still cover transformationbased methods part speech tagging lets see hmm tagging first remember previous segment looking tags maximizes probability sequence tags given sequence words decomposed way tn corresponds sequence tags bayes theorem say ptw ptpwt pw since maximizing right hand side equation ignore probability w doesnt depend computer argmax put terminology p power probability sequence tags p w given called likelihood words given tags okay lets see compute probabilities entire sequence probability sequence tags times probability sequence words given sequence tags going large product want compute probability possible sequences words given previous words previous tags also sequence tags without considering words make two simplifications one going use emission probabilities wi given sub also going look bigram sequence tags bigram approximation turns looking max p given w product emission probabilities sequence times bigram transition probabilities sequence maximum likelihood estimates lets look examples compute example transition probabilities interested probability noun following adjective thats equal ratio count adjectives nouns appearing together order divided number times adjectives occur corpus total instances adjectives followed noun instances adjectives probability noun following adjective admission probabilities admission probability word given tag determiner equal joint count determiner word divided count determiner example divided similarly compute rest probabilities use smoothing interpolation necessary lets try apply idea specific sentence sentence rich like travel travel marked verb also noun dictionary want consider two possible sequences times determiner adjective verb preposition followed noun followed end sentence sequence determiner adjective verb preposition verb travel okay already know build partofspeech tagger lets see evaluate one many classification problems data set consists training set possibly development set also test set going use actual evaluation metric used tagging tagging accuracy essentially many times get correct tag given word mentioned typical accuracy backed data whats interesting notice accuracy unknown words really hurts overall performance ranges according different tagger tagger model actually doesnt anything good unknown words performance unknown words usually order upper bound still mostly caused errors inconsistencies data example labeling nouns adjectives vice versa okay final method part speech tagging going introduce class called transformation based learning developed eric brill mid heres idea prior probability example emission like noun given sleep norrow probability verb given sleep equal rule says want change noun verb previous tag preposition types rules used transformationbased learning following kind preceding possibly following word specific part speech tag word two left two right particular tag one two preceding following words tagged certain way maybe one three preceding words tagged certain way maybe preceding word tagged one way following word tagged different way possible things learned transformation based tagger heres slides paper eric brill commonly used known nonlexicalized transformations nonlexicalized means refer parts speech without looking individual words first rule says want change noun tag verb tag previous tag preposition third line says want change noun verb one previous two tags modal verb lets say example number says want change preposition determiner next tag noun heres examples rules transformation based tagger example interested words words current word tagger previous word tagger following words heres examples unknown words default label certain word noun change plural noun suffix default noun change character certain character like case period indicates number rules given bill paper look rest understand make sense typically reflect important morphological properties words okay conclude section partofspeech tagging want bring interesting thoughts one new domains turns difficult port part speech taggers new domains especially domains lot words novel example porting tagger news domain biomedical domain usually bad idea get lower performance find ways deal example training data domain rather different domain another interesting idea idea distribution clustering want combine statistics semantically related words example companies numbers use combine statistics build better predictions next word examples semantic categories include days week animals names companies im going conclude section pointing external website john hopkins university jason eisner developed nice interactive spreadsheet teaches learn parameters hmm additional teaching materials site find useful well concludes section partofspeech tagging going continue minutes next segment,Course3,W7-S1-L5,W7,S1,L5,-,7,1,5,okay welcom back natur languag process last segment look problem partofspeech tag look hmm go see combin two togeth lectur statist method partofspeech tag particular remind mani differ type tag method rulebas alreadi went stochast base hmm maximum entropi mark model differ two hmm gener model maximum entropi mark model discrimin class go look hmm final still cover transformationbas method part speech tag let see hmm tag first rememb previou segment look tag maxim probabl sequenc tag given sequenc word decompos way tn correspond sequenc tag bay theorem say ptw ptpwt pw sinc maxim right hand side equat ignor probabl w doesnt depend comput argmax put terminolog p power probabl sequenc tag p w given call likelihood word given tag okay let see comput probabl entir sequenc probabl sequenc tag time probabl sequenc word given sequenc tag go larg product want comput probabl possibl sequenc word given previou word previou tag also sequenc tag without consid word make two simplif one go use emiss probabl wi given sub also go look bigram sequenc tag bigram approxim turn look max p given w product emiss probabl sequenc time bigram transit probabl sequenc maximum likelihood estim let look exampl comput exampl transit probabl interest probabl noun follow adject that equal ratio count adject noun appear togeth order divid number time adject occur corpu total instanc adject follow noun instanc adject probabl noun follow adject admiss probabl admiss probabl word given tag determin equal joint count determin word divid count determin exampl divid similarli comput rest probabl use smooth interpol necessari let tri appli idea specif sentenc sentenc rich like travel travel mark verb also noun dictionari want consid two possibl sequenc time determin adject verb preposit follow noun follow end sentenc sequenc determin adject verb preposit verb travel okay alreadi know build partofspeech tagger let see evalu one mani classif problem data set consist train set possibl develop set also test set go use actual evalu metric use tag tag accuraci essenti mani time get correct tag given word mention typic accuraci back data what interest notic accuraci unknown word realli hurt overal perform rang accord differ tagger tagger model actual doesnt anyth good unknown word perform unknown word usual order upper bound still mostli caus error inconsist data exampl label noun adject vice versa okay final method part speech tag go introduc class call transform base learn develop eric brill mid here idea prior probabl exampl emiss like noun given sleep norrow probabl verb given sleep equal rule say want chang noun verb previou tag preposit type rule use transformationbas learn follow kind preced possibl follow word specif part speech tag word two left two right particular tag one two preced follow word tag certain way mayb one three preced word tag certain way mayb preced word tag one way follow word tag differ way possibl thing learn transform base tagger here slide paper eric brill commonli use known nonlexic transform nonlexic mean refer part speech without look individu word first rule say want chang noun tag verb tag previou tag preposit third line say want chang noun verb one previou two tag modal verb let say exampl number say want chang preposit determin next tag noun here exampl rule transform base tagger exampl interest word word current word tagger previou word tagger follow word here exampl unknown word default label certain word noun chang plural noun suffix default noun chang charact certain charact like case period indic number rule given bill paper look rest understand make sens typic reflect import morpholog properti word okay conclud section partofspeech tag want bring interest thought one new domain turn difficult port part speech tagger new domain especi domain lot word novel exampl port tagger news domain biomed domain usual bad idea get lower perform find way deal exampl train data domain rather differ domain anoth interest idea idea distribut cluster want combin statist semant relat word exampl compani number use combin statist build better predict next word exampl semant categori includ day week anim name compani im go conclud section point extern websit john hopkin univers jason eisner develop nice interact spreadsheet teach learn paramet hmm addit teach materi site find use well conclud section partofspeech tag go continu minut next segment,[ 1  4  0 14 13]
289,Course3_W7-S1-L6_-_Information_Extraction_-_10_slides_05-37,welcome back natural language processing next slide site going information extraction information extraction well one interesting problems natural language processing also one practically useful usually start unstructured semistructured data unstructured refers plain text semistructured refers text may tables metadata examples unstructured data include news stories scientific papers maybe resumes weve tried extract entities events relations documents example want figure may able use information build knowledge base example companies develop kind products papers particular scientific topics named entity well different types common categories people locations organizations organizations partly divided things like sports teams newspapers lets say new york times companies like ibm microsoft also geopolitical entities like countries continents named entities ambiguous example word london person like last name city country country autonomy remember one earlier lectures say something like london decided get involved war london stands country capital named entities useful used interfaces databases question answering systems addition named entities want able extract things like times events times absolute expressions like example january th relative expressions example like last night next thursday also want able extract events lot information extraction tasks modeled sequence labeling problems part speech tagging another sequence labeling problem named entity recognition one semantic role labeling going talk future also sequence labeling problem sequence labelling input sequence words example w w w output set labelled words example w noun corresponds person maybe w w combined sequence two nouns correspond person time people use classification method sequence labelling use categories previous tokens features classify next one direction matters sometimes better left right case may better right left also direction going directions time one interesting points information extraction named entity recognition ner known literature consists two parts segmentation classification segmentation determining words belong specific named entity sentence brazilian football legend peles condition improved according thursday evening statement sao paulo hospital three named entities theyre segmented properly second task name entity recognition classification segmented text named entities rest sentence want label named entities type different ways use gazetteers session long lists geographical locations peoples names look spelling individual word look words adjacent presence specific words example president mrs mr hints classify named entity particular sentence labeled following way pele person thursday evening time expression finally sao paulo location heres examples organizations albert einstein hospital organization thursday night time expression event pele relocated case action verb relocate certain set arguments case pele person relocated information extraction entity recognition commonly used biomedical domain examples include gene labeling heres example sentence brca brca human genes produce tumor suppressor proteins case want label two instances genes brca brca genes end section information extraction going continue additional topics related information extraction next segment,Course3,W7-S1-L6,W7,S1,L6,-,7,1,6,welcom back natur languag process next slide site go inform extract inform extract well one interest problem natur languag process also one practic use usual start unstructur semistructur data unstructur refer plain text semistructur refer text may tabl metadata exampl unstructur data includ news stori scientif paper mayb resum weve tri extract entiti event relat document exampl want figur may abl use inform build knowledg base exampl compani develop kind product paper particular scientif topic name entiti well differ type common categori peopl locat organ organ partli divid thing like sport team newspap let say new york time compani like ibm microsoft also geopolit entiti like countri contin name entiti ambigu exampl word london person like last name citi countri countri autonomi rememb one earlier lectur say someth like london decid get involv war london stand countri capit name entiti use use interfac databas question answer system addit name entiti want abl extract thing like time event time absolut express like exampl januari th rel express exampl like last night next thursday also want abl extract event lot inform extract task model sequenc label problem part speech tag anoth sequenc label problem name entiti recognit one semant role label go talk futur also sequenc label problem sequenc label input sequenc word exampl w w w output set label word exampl w noun correspond person mayb w w combin sequenc two noun correspond person time peopl use classif method sequenc label use categori previou token featur classifi next one direct matter sometim better left right case may better right left also direct go direct time one interest point inform extract name entiti recognit ner known literatur consist two part segment classif segment determin word belong specif name entiti sentenc brazilian footbal legend pele condit improv accord thursday even statement sao paulo hospit three name entiti theyr segment properli second task name entiti recognit classif segment text name entiti rest sentenc want label name entiti type differ way use gazett session long list geograph locat peopl name look spell individu word look word adjac presenc specif word exampl presid mr mr hint classifi name entiti particular sentenc label follow way pele person thursday even time express final sao paulo locat here exampl organ albert einstein hospit organ thursday night time express event pele reloc case action verb reloc certain set argument case pele person reloc inform extract entiti recognit commonli use biomed domain exampl includ gene label here exampl sentenc brca brca human gene produc tumor suppressor protein case want label two instanc gene brca brca gene end section inform extract go continu addit topic relat inform extract next segment,[13  4  1 14 12]
290,Course3_W7-S1-L7_-_Relation_Extraction_-_28_slides_21-26,welcome back natural language processing going continue topic information extraction focusing relation extraction relation extraction well entities often interested links entities example certain person works certain company certain company manufactures certain product certain company located certain location one earliest research challenges done area called muc muc stands message understanding conference multiple iterations early annual competition extracting events news stories events things like terrorist events joint ventures management changes different scenario pretty much every year evaluation metrics things like precision recall fmeasure filling slots different fields heres example muc story muc english joint ventures tasks talks bridgestone sports corporation said friday set joint venture taiwan local concern japanese trading house produce golf clubs shipped japan particular implementation muc challenge participants given set articles like identify companies joint venture fill set slots particular event slide little bit difficult read look detail see includes things like name company initiating joint venture name partner company location joint venture day effective expected output mentioned systems evaluated based often correctly identified slots values examples information regulation extraction things like job announcements job announcements want find things like location job title job starting date possibly list qualifications starting salary seminar announcements interested filling slots like time title location speaker medical papers want able extract things like drug disease geneprotein cell line species substance fill templates fields filled text document say youre extracting entire string text example name company name gene directly document others predefined values example whether certain attempt successful successful merger merger unsuccessful fields allow one value allow multiple values common approaches used information extraction specifically relation extraction well information extraction viewed sequence labeling problem people use hmms also use patterns example regular expressions regular expressions something comes lot natural language processing would like spend little bit time talk next set slides people also use features example capitalization words caps initial caps contain digits kind suffixes contains kind punctuation contains heres regular expressions used perl programming language similar expressions common languages like java python caret example perl used match beginning string also used within set square brackets means complement rest symbols mentioned example want find anything letter would include pair square brackets caret dollar sign matches end string period matches character string except new line character star matches occurrences symbol plus matches occurrences question mark matches occurrences vertical bar allows search alternatives example may looking megahertz gigahertz property computer would say something like megahertz vertical bar gigahertz also things like grouping memory essentially able replace entire sequences characters means zero occurrences something curly brace number closing curly brace means want exactly occurrences thing put curly braces something like comma number youre looking least many occurrences put two numbers separated comma looking least n occurrences particular symbol special symbols match things like new lines tabs carriage returns letter number anything letter anything number back slash b matches word boundary back slash b matches anything word boundary also ranges things example say left square bracket ab closing square bracket would tell match character b inclusive z would match character alphabet long lowercase want match character alphabet either lowercase uppercase would need say something like left brace az followed az followed closing square bracket section regular expressions automata find way expressions used computed heres patterns used information extraction example identify prices something like use expression says look special symbol dollar sign followed either anything symbol comma repeated many times necessary followed sequence starts period includes exactly two occurrences digits second expression going match like written also match without comma going match period three digits question mark end regular expression tells fractional portion optional heres example regular expression matches date format year month day something like caret beginning string either th st century respectively followed two digits followed hyphen followed two digit sequence month specific format valid months acceptable finally last two digits correspond days month anywhere expression obviously doesnt enough information determine days days february days may july least good job heres example expression matches email addresses something like see fairly sophisticated expression important things include sign somewhere end extension starts period anywhere two four characters cover things like us com info think nowadays domains four characters expression would always need revised match person include things like example sequence two words first one starts capital letter second one starts capital letter obviously going match persons people three names one name also possible come patterns link html code used scrape websites example extract price information websites like amazon ebay also give part speech information example may looking noun followed adjective also include wordnet information example may searching sequence second word sequence something belongs organization subtree wordnet would include things like company newspaper heres simple sound point good format named entity recognition par santos penn treebank hugo favanew years old former chairman consolidated gold fields plc named nonexecutive director british industrial conglomerate example im going show called iob format common format many natural language processing tasks including part speech tagging name entity recognition semantical labeling iob stand stands fact every word sequence labeled either stands b followed label followed label b stands beginning label stands inside lets look example first name entity rudolph agnew first word labeled bperson beginning person next one labeled iperson part person beginning next thing comma labeled anything interested consolidated gold fields plc company name see word consolidated labeled beginning symbol organization rest words organization gold fields plc labeled iorg imagine possible perform character recognition two steps first step recognize consolidated gold fields plc sort named entity use standard classification method determine specific type named entity evaluate template based information extraction well straightforward test document lets say news story scientific paper got first many correct template extractions happen example task identify mergers companies given document doesnt mergers correct number template extractions zero end zero means made mistake next thing figure many slot value pairs extracted different templates case management succession example mark name person whos replaced organization name person whos replacing previous person get correctly would get two points miss one would get half points finally want get number extracted slotvalue pairs actually correct extracted relation extraction many different relations exist text example relation two people parentof marriedto manages relations person organization example person work certain organization also relations two different organizations example organization part organization b also organizationlocation relations example organization headquartered location leads another evaluation used early mostly following mark socalled ace evaluation automatic content extraction evaluation task use set newspaper articles identify entities belong following categories person organization facility location geopolitical entity also identify relations entities example role part located near social relation extraction general important core nlp task used building knowledge bases question answering input sentence example mazda north american operations headquartered irvine california output supposed tupal consists organization case mazda north american operations location case irvine california label relation specifically example headquartered predicate form tupal isheadquarteredin first argument mazda north american operations second argument city irvine california see relation common many organizations organization specific location headquarters easy build databases use standard database techniques querying databases performed information extraction relation extraction steps okay different techniques used relation extraction involve using patterns example regular expressions gazetteers others fall usual categories supervised learning semisupervised learning going look examples semisupervised learning minute lets first look examples extracting relations specifically isa relations using patterns example comes paper marti hearst see extract relations find patterns say something like x x x including x especially x general category x less general category example evolutionary relationships platypus mammals case expression gives us hint platypus x mammal specifically platypus kind mammal supervised relation extraction well case want look sentences two entities know part target relation look words sentence especially ones two entities build classifier looks clue words target words help us classify entire tuple heres example english sentence like beethoven born december bonn born expression links together two entities beethoven ways express supervised relation however example born beethoven birth december beethoven grew musical family see always case clue phrase like case born birth appear two words trying link appear outside usually appear somewhere nearby heres one example ludwig van beethoven birth year year death another expression may want look forward find instances relation person born certain year heres one example evidence supports case december beethovens date birth date birth clue phrase turns techniques used nonenglish languages well examples spanish german german ludwig van beethoven foreign case foreign expression indicates born particular date also foreign another way say thing also foreign birthday ludwig van beethoven next name person year spanish examples foreign spanish means born connects named entity person named entity time foreign foreign means born bonn beethoven ludwig van beethoven foreign foreign case means born december third method relation extraction semisupervised example training data example seed expressions like beethoven born december bonn know sentence represents valid instance tuple x born year next thing need figure sentences contain beethoven probability many different ways express relation two expect words appear nearby beethoven ones carry meaning relation maybe going learn birth date example want start looking expressions appear nearby like birthdate going use expressions find instances containing people birthdates sentences documents okay lets look different ways going evaluate relation extraction well relation extraction essentially classification task want measure things like precision number correctly extracted relations divided number extracted relations recall number correctly extracted relations divided existing relations also combine two f measure harmonic mean precision recall metrics work well annotated data theres annotated data possible measure recall dont know missing measure precision okay conclude probabilistic nlp important one crucial examples probabilistic nlp action part speech tagging hidden markov models information extraction also uses kind techniques probabilistic natural language processing theres one technique used information extraction called conditional random fields cover look textbook concludes section information extraction thank attention,Course3,W7-S1-L7,W7,S1,L7,-,7,1,7,welcom back natur languag process go continu topic inform extract focus relat extract relat extract well entiti often interest link entiti exampl certain person work certain compani certain compani manufactur certain product certain compani locat certain locat one earliest research challeng done area call muc muc stand messag understand confer multipl iter earli annual competit extract event news stori event thing like terrorist event joint ventur manag chang differ scenario pretti much everi year evalu metric thing like precis recal fmeasur fill slot differ field here exampl muc stori muc english joint ventur task talk bridgeston sport corpor said friday set joint ventur taiwan local concern japanes trade hous produc golf club ship japan particular implement muc challeng particip given set articl like identifi compani joint ventur fill set slot particular event slide littl bit difficult read look detail see includ thing like name compani initi joint ventur name partner compani locat joint ventur day effect expect output mention system evalu base often correctli identifi slot valu exampl inform regul extract thing like job announc job announc want find thing like locat job titl job start date possibl list qualif start salari seminar announc interest fill slot like time titl locat speaker medic paper want abl extract thing like drug diseas geneprotein cell line speci substanc fill templat field fill text document say your extract entir string text exampl name compani name gene directli document other predefin valu exampl whether certain attempt success success merger merger unsuccess field allow one valu allow multipl valu common approach use inform extract specif relat extract well inform extract view sequenc label problem peopl use hmm also use pattern exampl regular express regular express someth come lot natur languag process would like spend littl bit time talk next set slide peopl also use featur exampl capit word cap initi cap contain digit kind suffix contain kind punctuat contain here regular express use perl program languag similar express common languag like java python caret exampl perl use match begin string also use within set squar bracket mean complement rest symbol mention exampl want find anyth letter would includ pair squar bracket caret dollar sign match end string period match charact string except new line charact star match occurr symbol plu match occurr question mark match occurr vertic bar allow search altern exampl may look megahertz gigahertz properti comput would say someth like megahertz vertic bar gigahertz also thing like group memori essenti abl replac entir sequenc charact mean zero occurr someth curli brace number close curli brace mean want exactli occurr thing put curli brace someth like comma number your look least mani occurr put two number separ comma look least n occurr particular symbol special symbol match thing like new line tab carriag return letter number anyth letter anyth number back slash b match word boundari back slash b match anyth word boundari also rang thing exampl say left squar bracket ab close squar bracket would tell match charact b inclus z would match charact alphabet long lowercas want match charact alphabet either lowercas uppercas would need say someth like left brace az follow az follow close squar bracket section regular express automata find way express use comput here pattern use inform extract exampl identifi price someth like use express say look special symbol dollar sign follow either anyth symbol comma repeat mani time necessari follow sequenc start period includ exactli two occurr digit second express go match like written also match without comma go match period three digit question mark end regular express tell fraction portion option here exampl regular express match date format year month day someth like caret begin string either th st centuri respect follow two digit follow hyphen follow two digit sequenc month specif format valid month accept final last two digit correspond day month anywher express obvious doesnt enough inform determin day day februari day may juli least good job here exampl express match email address someth like see fairli sophist express import thing includ sign somewher end extens start period anywher two four charact cover thing like us com info think nowaday domain four charact express would alway need revis match person includ thing like exampl sequenc two word first one start capit letter second one start capit letter obvious go match person peopl three name one name also possibl come pattern link html code use scrape websit exampl extract price inform websit like amazon ebay also give part speech inform exampl may look noun follow adject also includ wordnet inform exampl may search sequenc second word sequenc someth belong organ subtre wordnet would includ thing like compani newspap here simpl sound point good format name entiti recognit par santo penn treebank hugo favanew year old former chairman consolid gold field plc name nonexecut director british industri conglomer exampl im go show call iob format common format mani natur languag process task includ part speech tag name entiti recognit semant label iob stand stand fact everi word sequenc label either stand b follow label follow label b stand begin label stand insid let look exampl first name entiti rudolph agnew first word label bperson begin person next one label iperson part person begin next thing comma label anyth interest consolid gold field plc compani name see word consolid label begin symbol organ rest word organ gold field plc label iorg imagin possibl perform charact recognit two step first step recogn consolid gold field plc sort name entiti use standard classif method determin specif type name entiti evalu templat base inform extract well straightforward test document let say news stori scientif paper got first mani correct templat extract happen exampl task identifi merger compani given document doesnt merger correct number templat extract zero end zero mean made mistak next thing figur mani slot valu pair extract differ templat case manag success exampl mark name person who replac organ name person who replac previou person get correctli would get two point miss one would get half point final want get number extract slotvalu pair actual correct extract relat extract mani differ relat exist text exampl relat two peopl parentof marriedto manag relat person organ exampl person work certain organ also relat two differ organ exampl organ part organ b also organizationloc relat exampl organ headquart locat lead anoth evalu use earli mostli follow mark socal ace evalu automat content extract evalu task use set newspap articl identifi entiti belong follow categori person organ facil locat geopolit entiti also identifi relat entiti exampl role part locat near social relat extract gener import core nlp task use build knowledg base question answer input sentenc exampl mazda north american oper headquart irvin california output suppos tupal consist organ case mazda north american oper locat case irvin california label relat specif exampl headquart predic form tupal isheadquarteredin first argument mazda north american oper second argument citi irvin california see relat common mani organ organ specif locat headquart easi build databas use standard databas techniqu queri databas perform inform extract relat extract step okay differ techniqu use relat extract involv use pattern exampl regular express gazett other fall usual categori supervis learn semisupervis learn go look exampl semisupervis learn minut let first look exampl extract relat specif isa relat use pattern exampl come paper marti hearst see extract relat find pattern say someth like x x x includ x especi x gener categori x less gener categori exampl evolutionari relationship platypu mammal case express give us hint platypu x mammal specif platypu kind mammal supervis relat extract well case want look sentenc two entiti know part target relat look word sentenc especi one two entiti build classifi look clue word target word help us classifi entir tupl here exampl english sentenc like beethoven born decemb bonn born express link togeth two entiti beethoven way express supervis relat howev exampl born beethoven birth decemb beethoven grew music famili see alway case clue phrase like case born birth appear two word tri link appear outsid usual appear somewher nearbi here one exampl ludwig van beethoven birth year year death anoth express may want look forward find instanc relat person born certain year here one exampl evid support case decemb beethoven date birth date birth clue phrase turn techniqu use nonenglish languag well exampl spanish german german ludwig van beethoven foreign case foreign express indic born particular date also foreign anoth way say thing also foreign birthday ludwig van beethoven next name person year spanish exampl foreign spanish mean born connect name entiti person name entiti time foreign foreign mean born bonn beethoven ludwig van beethoven foreign foreign case mean born decemb third method relat extract semisupervis exampl train data exampl seed express like beethoven born decemb bonn know sentenc repres valid instanc tupl x born year next thing need figur sentenc contain beethoven probabl mani differ way express relat two expect word appear nearbi beethoven one carri mean relat mayb go learn birth date exampl want start look express appear nearbi like birthdat go use express find instanc contain peopl birthdat sentenc document okay let look differ way go evalu relat extract well relat extract essenti classif task want measur thing like precis number correctli extract relat divid number extract relat recal number correctli extract relat divid exist relat also combin two f measur harmon mean precis recal metric work well annot data there annot data possibl measur recal dont know miss measur precis okay conclud probabilist nlp import one crucial exampl probabilist nlp action part speech tag hidden markov model inform extract also use kind techniqu probabilist natur languag process there one techniqu use inform extract call condit random field cover look textbook conclud section inform extract thank attent,[13  4 14 12 11]
291,Course3_W8-S1-L1_-_Question_Answering_-_21_slides_21-39,welcome back natural language processing next lecture going one interesting areas natural language processing specifically question answering youre familiar question asking different apps websites probably television one popular applications question answering speech systems like siri created apple interface snapshot siri ask question whether text speech example population sri lanka gives answer million people many websites systems deal question answering using natural language processing one popular ones called ask jeeves formerly known askcom website allows ask questions natural language form example played linus lost gives information person ask jeeves somewhere standard search engine returns mostly documents pure question answering system still allows ask questions form natural language questions another example wolfram alpha used mathematical computations form natural language dialog ask question area circle radius five feet give answer also give mathematical derivation image shows answer pi square feet computed probably famous least relevant class ibms watson system little bit background ibms watson system question answering system used television play jeopardy game beginning snapshot tv show shows two best humans time facing watson watson really embodied represented avatar logo globe form sort light bulb see snapshot final question round jeopardy watson ahead huge margin compared two humans way way behind watson jeopardy heres sample questions example december national newspaper raised newsstand price cents typical question jeopardy formulated slightly differently traditional questions really formulated answer youre supposed answer using question case supposed say something like usa today name newspaper notice question answer format used jeopardy really equivalent normal question answer little twist added formulate questions answers opposite order heres lot questions used actual jeopardy round former first lady published memoirs spoken heart youre supposed answer laura bush questions look given competition watson two human participants see watson videos heres recommend theyre available youtube external websites want care question answering natural language processing class well first people ask questions online analyze large corpus queries search engines realize many queries form natural language questions one earliest analysis query corporal called excite corpus million queries one days worth simple analysis show form natural language questions even though search engine really supposed understand human natural language questions means users expecting search engine understand human language questions even though officially designated half factual questions example country code belgium asks procedure example get debt types questions note factual questions much easier deal procedure questions procedure questions require lot context example user sort background factual questions typically single answer may vary time general single answer perhaps one small set possible answers much easier find heres questions excite corpus year baseball become official sport fairly straightforward question assumption obviously united states question asked different country perhaps answer would different get debit mentioned one already heres others example super bowl well mentioned questions type multiple answers depend time years super bowl couple weeks next years super bowl going different location different date answer question vary depending asked things goes next question whos california district state senator completely possible answer question depends location certainly depends time heres sets questions different systems one systems called morax one first research systems built mid julian kupiec xerox parc supposed answer questions based large encyclopedia questions appropriate kind system nature us city junction allegheny monongahela rivers wrote across river trees married actress nancy davis see fairly factual questions specifically related corpus case encyclopedia english examples corpus questions published one called aol corpus published years ago see people ask different types questions example cerebrocortical atrophy mean fraction closest pi highest calories consumed person hour period notice questions mistyped common query logs people dont always ask questions grammatical way possible look one thing people research natural language processing question answering called question type taxonomy want build ontology question types systems question answering built efficiently question types well different distinctions made one distinction socalled yes questions require name says either yes answer example barack obama president united states yes contrasted socalled wh questions wh questions another distinction factual procedural questions factual questions require usually short single answer thats factually based procedural questions usually require lot understanding context user typically require much sophisticated detailed answer third dimension single answer multiple answer questions example ask whats states flower oklahoma ill get one answer ask question example representatives state oklahoma get definition multiple answers ask states first sign us constitution would get multiple answers another dimension objective subjective questions example ask movie golden globe would get objective answer versus asked best movie year youre probably going get subjective answer note legitimate types questions questions people ask people expect get answers search engine however ways systems deal questions different one dimension contextspecific generic questions generic question may something doesnt depend user background type queries theyve asked recently whereas generic question would opposite another dimension whether theres known answer collection yes many search evaluations question answering assumed questions used evaluation answer somewhere collection question find answer perhaps sentence contains general case answer question necessarily collection able say dont think theres answer question collection would get points correctly say theres answer would lose points think theres answer fact one whats state art question answer work factual shortanswer questions system architectures typically include following components ir component question converted query search engine search engine returns documents likely contain answers questions query systems use statistical approaches lots data cases terabytes use relatively little knowledge sense dont detailed inference procedures detailed knowledge representation use mostly surface patterns im going go ahead give overview systems built years question answer im going start oldest systems way im going define oldest precede called trec evaluation trec evaluation happened going quite years since lets start old systems perhaps first system relates question answering socalled baseball system built long time ago domain questions baseball statistics little small vocabulary small set questions could answer second system marginally classified question answering system eliza introduced weizenbaum dialogue agent simulates use psychologist therapist third one would like discuss detail minutes shrdlu built terry winograd early one deal questions simulated world blocks red blocks green blocks square blocks round blocks system able manipulate put blocks top asking answering questions next system would like mention system created bill woods early system called lunar based data expeditions moon returned large collection lunar rocks questions could ask example many rocks certain property give detail systems next slides first general domain system question answering left julian kupiec based encyclopedia open domain already showed questions able handle minutes ago first web based system called start designed boris katz mit mid completely open domain used web corpus backend want mention briefly systems example system called deep red mitre lynette hirschman et al used answering reading comprehension questions school level text would like mention also one first spoken question answering systems victor zue group also mit late early called jupiter lets look systems little bit detail mentioned eliza really question answering system would like mention engage dialog influential development future dialog systems plays therapist doesnt really answer questions little joke therapists maybe go therapist ask question theyre really come answers ask questions ask think problems uses simple pattern matching converts questions user asks questions asks clarification example lets look following dialogue eliza says psychotherapist please describe problems user says im always tired system says say youre always tired doesnt really give useful information asks additional questions cannot sleep cannot sleep came way convert surface patterns users questions additional questions follow questions seem indicate sort intelligence even though really yes help please point system doesnt know starts new thread dialog reminding user something said earlier earlier said cannot sleep next system lunar lunar stands lunar sciences natural language information system based collection moon rocks returned marth apollo lunar able ask questions english restrictive domain included following components first component general purpose meta transition network grammar english atn grammar atn essentially finite state machine transitions labeled jump one section finite state machine another uses proceduralsemantics framework semantic interpretation able convert syntactic information question sort semantic representation specific domain maps questions logical representation uses word dictionary entries knowledge base evaluation would like report lunar able answer correctly sentences humans actual professional geologists wanted ask system happened long time ago back lunar rocks conference heres sample questions lunar able answer many breccias contain olivine specific questions rocks collection give idea atn looks like snapshot bill woods paper see cannot really read fonts get idea fairly complicated finite state automaton used represent users questions next system want mention today shrdlu daniel winograd designed answer questions blocks world heres simple example blocks world point time rendered kind measures lines box right hand side lot blocks form pyramids cubes shapes either outside box possibly top system simulated robot able recognize shapes able place top move box remove box gives sample dialogue person says pick big red block system says ok means understood correctly theres one big red block available able find move next thing person says grasp pyramid computer says dont understand pyramid mean context dialogue may multiple pyramids computer needs little bit context person says okay forget pyramid find block taller one youre holding put box see questions involve commands also inference system able identify blocks world figure ones taller ones holding determine unambiguous isnt unambiguous actually perform action move block box okay heres computer said assume mean block taller one holding confirmation computer understands person asking computer goes says ok thats exactly want saying ok computer performed action person intended person asks box contain computer says well box contains blue pyramid blue block good representation world point time holding whats surface top another block whats box whats outside box last question pyramid supported box means box contains blue pyramid blue block blue pyramid top blue block instead directly sitting top box many blocks box four least one narrower one told pick yes red cube youre interested system little bit detail go official website many transcripts also simulation software lets manipulate blocks little bit better graphics ones used years ago next system want discuss today called start heres looks like currently allows ask arbitrary natural language questions keeps interesting ones log look questions category geography science reference asked past start open domain system like example lunar shrdlu specific domains uses web back end mostly continuously online since makes oldest web based system little bit introduction question answering systems going continue next set examples next section,Course3,W8-S1-L1,W8,S1,L1,-,8,1,1,welcom back natur languag process next lectur go one interest area natur languag process specif question answer your familiar question ask differ app websit probabl televis one popular applic question answer speech system like siri creat appl interfac snapshot siri ask question whether text speech exampl popul sri lanka give answer million peopl mani websit system deal question answer use natur languag process one popular one call ask jeev formerli known askcom websit allow ask question natur languag form exampl play linu lost give inform person ask jeev somewher standard search engin return mostli document pure question answer system still allow ask question form natur languag question anoth exampl wolfram alpha use mathemat comput form natur languag dialog ask question area circl radiu five feet give answer also give mathemat deriv imag show answer pi squar feet comput probabl famou least relev class ibm watson system littl bit background ibm watson system question answer system use televis play jeopardi game begin snapshot tv show show two best human time face watson watson realli embodi repres avatar logo globe form sort light bulb see snapshot final question round jeopardi watson ahead huge margin compar two human way way behind watson jeopardi here sampl question exampl decemb nation newspap rais newsstand price cent typic question jeopardi formul slightli differ tradit question realli formul answer your suppos answer use question case suppos say someth like usa today name newspap notic question answer format use jeopardi realli equival normal question answer littl twist ad formul question answer opposit order here lot question use actual jeopardi round former first ladi publish memoir spoken heart your suppos answer laura bush question look given competit watson two human particip see watson video here recommend theyr avail youtub extern websit want care question answer natur languag process class well first peopl ask question onlin analyz larg corpu queri search engin realiz mani queri form natur languag question one earliest analysi queri corpor call excit corpu million queri one day worth simpl analysi show form natur languag question even though search engin realli suppos understand human natur languag question mean user expect search engin understand human languag question even though offici design half factual question exampl countri code belgium ask procedur exampl get debt type question note factual question much easier deal procedur question procedur question requir lot context exampl user sort background factual question typic singl answer may vari time gener singl answer perhap one small set possibl answer much easier find here question excit corpu year basebal becom offici sport fairli straightforward question assumpt obvious unit state question ask differ countri perhap answer would differ get debit mention one alreadi here other exampl super bowl well mention question type multipl answer depend time year super bowl coupl week next year super bowl go differ locat differ date answer question vari depend ask thing goe next question who california district state senat complet possibl answer question depend locat certainli depend time here set question differ system one system call morax one first research system built mid julian kupiec xerox parc suppos answer question base larg encyclopedia question appropri kind system natur us citi junction allegheni monongahela river wrote across river tree marri actress nanci davi see fairli factual question specif relat corpu case encyclopedia english exampl corpu question publish one call aol corpu publish year ago see peopl ask differ type question exampl cerebrocort atrophi mean fraction closest pi highest calori consum person hour period notic question mistyp common queri log peopl dont alway ask question grammat way possibl look one thing peopl research natur languag process question answer call question type taxonomi want build ontolog question type system question answer built effici question type well differ distinct made one distinct socal ye question requir name say either ye answer exampl barack obama presid unit state ye contrast socal wh question wh question anoth distinct factual procedur question factual question requir usual short singl answer that factual base procedur question usual requir lot understand context user typic requir much sophist detail answer third dimens singl answer multipl answer question exampl ask what state flower oklahoma ill get one answer ask question exampl repres state oklahoma get definit multipl answer ask state first sign us constitut would get multipl answer anoth dimens object subject question exampl ask movi golden globe would get object answer versu ask best movi year your probabl go get subject answer note legitim type question question peopl ask peopl expect get answer search engin howev way system deal question differ one dimens contextspecif gener question gener question may someth doesnt depend user background type queri theyv ask recent wherea gener question would opposit anoth dimens whether there known answer collect ye mani search evalu question answer assum question use evalu answer somewher collect question find answer perhap sentenc contain gener case answer question necessarili collect abl say dont think there answer question collect would get point correctli say there answer would lose point think there answer fact one what state art question answer work factual shortansw question system architectur typic includ follow compon ir compon question convert queri search engin search engin return document like contain answer question queri system use statist approach lot data case terabyt use rel littl knowledg sens dont detail infer procedur detail knowledg represent use mostli surfac pattern im go go ahead give overview system built year question answer im go start oldest system way im go defin oldest preced call trec evalu trec evalu happen go quit year sinc let start old system perhap first system relat question answer socal basebal system built long time ago domain question basebal statist littl small vocabulari small set question could answer second system margin classifi question answer system eliza introduc weizenbaum dialogu agent simul use psychologist therapist third one would like discuss detail minut shrdlu built terri winograd earli one deal question simul world block red block green block squar block round block system abl manipul put block top ask answer question next system would like mention system creat bill wood earli system call lunar base data expedit moon return larg collect lunar rock question could ask exampl mani rock certain properti give detail system next slide first gener domain system question answer left julian kupiec base encyclopedia open domain alreadi show question abl handl minut ago first web base system call start design bori katz mit mid complet open domain use web corpu backend want mention briefli system exampl system call deep red mitr lynett hirschman et al use answer read comprehens question school level text would like mention also one first spoken question answer system victor zue group also mit late earli call jupit let look system littl bit detail mention eliza realli question answer system would like mention engag dialog influenti develop futur dialog system play therapist doesnt realli answer question littl joke therapist mayb go therapist ask question theyr realli come answer ask question ask think problem use simpl pattern match convert question user ask question ask clarif exampl let look follow dialogu eliza say psychotherapist pleas describ problem user say im alway tire system say say your alway tire doesnt realli give use inform ask addit question cannot sleep cannot sleep came way convert surfac pattern user question addit question follow question seem indic sort intellig even though realli ye help pleas point system doesnt know start new thread dialog remind user someth said earlier earlier said cannot sleep next system lunar lunar stand lunar scienc natur languag inform system base collect moon rock return marth apollo lunar abl ask question english restrict domain includ follow compon first compon gener purpos meta transit network grammar english atn grammar atn essenti finit state machin transit label jump one section finit state machin anoth use proceduralsemant framework semant interpret abl convert syntact inform question sort semant represent specif domain map question logic represent use word dictionari entri knowledg base evalu would like report lunar abl answer correctli sentenc human actual profession geologist want ask system happen long time ago back lunar rock confer here sampl question lunar abl answer mani breccia contain olivin specif question rock collect give idea atn look like snapshot bill wood paper see cannot realli read font get idea fairli complic finit state automaton use repres user question next system want mention today shrdlu daniel winograd design answer question block world here simpl exampl block world point time render kind measur line box right hand side lot block form pyramid cube shape either outsid box possibl top system simul robot abl recogn shape abl place top move box remov box give sampl dialogu person say pick big red block system say ok mean understood correctli there one big red block avail abl find move next thing person say grasp pyramid comput say dont understand pyramid mean context dialogu may multipl pyramid comput need littl bit context person say okay forget pyramid find block taller one your hold put box see question involv command also infer system abl identifi block world figur one taller one hold determin unambigu isnt unambigu actual perform action move block box okay here comput said assum mean block taller one hold confirm comput understand person ask comput goe say ok that exactli want say ok comput perform action person intend person ask box contain comput say well box contain blue pyramid blue block good represent world point time hold what surfac top anoth block what box what outsid box last question pyramid support box mean box contain blue pyramid blue block blue pyramid top blue block instead directli sit top box mani block box four least one narrow one told pick ye red cube your interest system littl bit detail go offici websit mani transcript also simul softwar let manipul block littl bit better graphic one use year ago next system want discuss today call start here look like current allow ask arbitrari natur languag question keep interest one log look question categori geographi scienc refer ask past start open domain system like exampl lunar shrdlu specif domain use web back end mostli continu onlin sinc make oldest web base system littl bit introduct question answer system go continu next set exampl next section,[ 9  4 14 13 12]
292,Course3_W8-S1-L2_-_Evaluation_of_QA__System_Architecture_-_21_slides_21-59,okay previous segment talked question answering systems general new segment im going talk evaluation question answering systems figure system works well improve let start first focusing specific type question answer evaluation socalled trec evaluation trec text retrieval evaluation conference going years includes many different competitions every year involves document retrieval others involve log retrieval spoken document retrieval starting new evaluation called qa spearheaded lot research question answering systems next years trec run nist national institute standards technology united states nice description system available papers alan voorhees tice evaluation specifically designated deal factual question answer using specific corpus using web corpus news gb worth corpus known aquaint corpus eventually evaluation first year fact based questions next year work evaluation essentially involved fact extraction dealt questions kind unambiguous answers example lincolns secretary state peugeot company manufacture assumptions following questions based text corpus graded human analysts actually read documents found sentences contain answers questions systems told advance answers guaranteed inside corpus systems supposed return short passages necessarily exact answer short passages one tasks consecutive bytes task return consecutive bytes years starting single passage bytes supposed returned annotated confidence score used computing confidence based evaluation system also possible system could return answer nil specific question sure answer question corpus evaluations inference required although systems small set systems use sort inference lets look trec first questions corpus im going go real quick date east germany open berlin wall theres single unambiguous answer november th johnny mathis high school track coach famous question shown many papers topic somebody called lou vasquez shape porpoises tooth spade shaped see different wh type questions answers short passages dont change time look questions test questions information author book iron lady biography margaret thatcher monetary value nobel peace prize way entire corpus available trec website nisgov youre interested research evaluating system corpus download okay introduced corpus general task description lets focus evaluation metric used important metric used something called mrr stands mean reciprocal rank correct answer among ranked list answers heres formula mrr take rank correct answer take reciprocal rank one point first place half point second place average questions corpus metric first introduced trec popular qa community years heres example suppose question capital canada system returns following five answers ranked order first answer toronto second answer ottawa third answer albany fourth philadelphia fifth ottawa mrr youre looking highest ranked correct answer case would second answer point would get score points dont get additional credit getting ottawa second time fifth place theres different version mrr called trr total reciprocal rank gives extra credit additional correct answers case little difficult answers questions multiple correct answers case makes sense get credit correct answers case ottawa second fifth place system would get trr score points case maximum larger obviously later years confidenceweighted score prevalent case getting points got correct answer also certain wouldnt get many points got right answer werent certain could lose potentially lot points gave high confidence wrong answer lets look performances wanted show best systems participating evaluation first three years best systems cymphony buffalo smu att ibm xerox europe university maryland smu isi university waterloo ibm queens college insightsoft russia lcc group spun smu oracle isi ones including ask national university singapore first systems participated years every year participating systems types questions possible latter evaluations check following introduced definitional questions example boll weevil list questions example states signed us declaration independence also crosslingual questions example questions could spanish documents could english finally series questions one initial question depending answer question get different follow questions get one wrong youre essentially unlikely get rest correct youre essentially going tangent heres examples series questions first question first series prions made well answer question next question discovered prions question thread researchers worked prions next category lead singermusician nirvana band members point question doesnt even mention nirvana anymore figure part thread band formed biggest hit albums style music play third series industry rhom haas company located annual revenue many employees finally kind insect boll weevil type plant damage states problems boll weevils lets move next topic within question number six specific typical architecture system answers questions one observation would like start many questions answered traditional search engines example go yahoo search engine ask question capital nicaragua even though yahoo search engine officially question answering engine understand question give answer capital nicaragua managua one answers even search engine cannot necessarily retrieve answer question still possible return document contains answer maybe snippet document contains answer see hits modern search engines associated short snippet many cases contains answer question directly lets see approaches question answering components involved system building heres question largest city northern afghanistan would answer question like well first could probably look map figure way afghanistan figure cities northern part country go perhaps table populations figure populations cities northern afghanistan figure one largest population go search engine type directly question largest city northern afghanistan case let show example sent query search engine able get top seven documents snippets system picked representative documents given query top page see easily set snippets contains lot answers correct incorrect certainly contains lot city names heres one kabul capital afghanistan right answer question way heres panama city even afghanistan kano also afghanistan somewhere nigeria one instance kabul one gudermes city russia incorrect answers even though cities first time fifth passage mazari sharif correct answer also appears albeit different spelling seventh passage okay question run query search engine gives passages come learning algorithm classify candidate answers good ones bad ones least rank based likely answer question asked components typical system includes addition ir component far first step source identification database looking using general web corpus perhaps something specific question comes movies actors maybe better go send question something like imdb wikipedia rather entire web depends also whether answers likely contained textual source semi structured perhaps structured source example database questions example population certain city unemployment rate certain country certain year probably better obtained certain sort semi structured structured data set whereas general questions probably better answered plain text unstructured data text sources query modulation important step want describe detail next two slides heres basic idea want convert natural language question query search engine search engines actually good answering natural language questions example easily get confused fact wh words may also automatically drop important stop words whereas actually important answer ask question said string consists entirely stop words search engine likely drop answering query case going get correct answers query modulation also concerned correct syntax specific search engine example search engine allows include alternative ways ask words example using vertical bars convert question wrote hamlet either author wrote followed hamlet case figure person wrote certain book person author book therefore going looking documents contain name book also contain one multiple ways least one multiple ways one describe author book document retrieval finding documents match queries sentence ranking means identified document contained answers want find passages possibly sentences paragraphs likely contain answer usually done sort end gap overlap form formula dont copy formula going discuss information retrieval section next thing identified right passages sentences want identify answers original questions somewhere sentences passages answer extraction involves process called question type classification question person want identify names persons possible answer sentences also involves something called phrase chunking example lets say something new york dont want separate word new word york want keep together part answer finally answer ranking answer ranking bunch candidate answers satisfy criteria far correct type appear sentences documents relevant query want figure order present user score points example dont want kabul canno good airmask appear top list want mazaresharif features use find answers include question type proximity recruiting words also importantly frequency get lets say passages returned search engine relevant query six contain exact answer independently well chances likely correct answer opposed something appeared lets go entire pipeline example question largest city northern afghanistan go first query modulation converted largest biggest keep word city drop stop words example add quotation marks around northern afghanistan indicate phrase dont want cities northern part southern afghanistan northern neighbor afghanistan want actual phrase northern afghanistan appear okay send modulated query search engine perform document retrieval get bunch urls rank sentences documents get ones likely contain correct answer first perform answer extraction identify candidate non phrases case likely contain answer finally perform answer ranking using machine learning identify mazeresharif actually better candidate lead gudermes second place ideal representation pipeline lets look stages little bit detail first one question type classification question type classification want identify type named entity match question asked example question author book going looking answers people authors writers theyre going looking names organizations names football teams heres example wrote anna karenina looking person individual writer categories lets look two different taxonomies question types first one called synclasses ibms answer selection system ansel includes categories categorized qa token question answering tokens first place answers question example category rocky mountains next country country answer question specific question country case answer could united kingdom see mapping actually ambiguous questions input mapped either places countries states perhaps category questions mapped persons roles organizations even though look question word still dont know exactly category question youre looking allow multiple possible categories included answer selection process let show one taxonomy real quickly university illinois urbana champagne questiontype taxonomy includes quite question types ibm system first major categorys entities includes things like animals body organs colors theres another category abbreviations descriptions different types humans locations finally different numeric expressions example dates distances money ranks examples university illinois corpus rococo painting architecture flourish well category numerical subcategory date countrys national passenger rail system called via well automatically classified location country invented makeup well human specifically individual look examples details way classification actually done look uiuc papers question classification specifically recommend ones listed page li roth one li roth referred specific data set think theres training set test set thousand different questions labels manually selected corpus used lot papers comparative evaluations going stop continue next segment techniques question classification,Course3,W8-S1-L2,W8,S1,L2,-,8,1,2,okay previou segment talk question answer system gener new segment im go talk evalu question answer system figur system work well improv let start first focus specif type question answer evalu socal trec evalu trec text retriev evalu confer go year includ mani differ competit everi year involv document retriev other involv log retriev spoken document retriev start new evalu call qa spearhead lot research question answer system next year trec run nist nation institut standard technolog unit state nice descript system avail paper alan voorhe tice evalu specif design deal factual question answer use specif corpu use web corpu news gb worth corpu known aquaint corpu eventu evalu first year fact base question next year work evalu essenti involv fact extract dealt question kind unambigu answer exampl lincoln secretari state peugeot compani manufactur assumpt follow question base text corpu grade human analyst actual read document found sentenc contain answer question system told advanc answer guarante insid corpu system suppos return short passag necessarili exact answer short passag one task consecut byte task return consecut byte year start singl passag byte suppos return annot confid score use comput confid base evalu system also possibl system could return answer nil specif question sure answer question corpu evalu infer requir although system small set system use sort infer let look trec first question corpu im go go real quick date east germani open berlin wall there singl unambigu answer novemb th johnni mathi high school track coach famou question shown mani paper topic somebodi call lou vasquez shape porpois tooth spade shape see differ wh type question answer short passag dont chang time look question test question inform author book iron ladi biographi margaret thatcher monetari valu nobel peac prize way entir corpu avail trec websit nisgov your interest research evalu system corpu download okay introduc corpu gener task descript let focu evalu metric use import metric use someth call mrr stand mean reciproc rank correct answer among rank list answer here formula mrr take rank correct answer take reciproc rank one point first place half point second place averag question corpu metric first introduc trec popular qa commun year here exampl suppos question capit canada system return follow five answer rank order first answer toronto second answer ottawa third answer albani fourth philadelphia fifth ottawa mrr your look highest rank correct answer case would second answer point would get score point dont get addit credit get ottawa second time fifth place there differ version mrr call trr total reciproc rank give extra credit addit correct answer case littl difficult answer question multipl correct answer case make sens get credit correct answer case ottawa second fifth place system would get trr score point case maximum larger obvious later year confidenceweight score preval case get point got correct answer also certain wouldnt get mani point got right answer werent certain could lose potenti lot point gave high confid wrong answer let look perform want show best system particip evalu first three year best system cymphoni buffalo smu att ibm xerox europ univers maryland smu isi univers waterloo ibm queen colleg insightsoft russia lcc group spun smu oracl isi one includ ask nation univers singapor first system particip year everi year particip system type question possibl latter evalu check follow introduc definit question exampl boll weevil list question exampl state sign us declar independ also crosslingu question exampl question could spanish document could english final seri question one initi question depend answer question get differ follow question get one wrong your essenti unlik get rest correct your essenti go tangent here exampl seri question first question first seri prion made well answer question next question discov prion question thread research work prion next categori lead singermusician nirvana band member point question doesnt even mention nirvana anymor figur part thread band form biggest hit album style music play third seri industri rhom haa compani locat annual revenu mani employe final kind insect boll weevil type plant damag state problem boll weevil let move next topic within question number six specif typic architectur system answer question one observ would like start mani question answer tradit search engin exampl go yahoo search engin ask question capit nicaragua even though yahoo search engin offici question answer engin understand question give answer capit nicaragua managua one answer even search engin cannot necessarili retriev answer question still possibl return document contain answer mayb snippet document contain answer see hit modern search engin associ short snippet mani case contain answer question directli let see approach question answer compon involv system build here question largest citi northern afghanistan would answer question like well first could probabl look map figur way afghanistan figur citi northern part countri go perhap tabl popul figur popul citi northern afghanistan figur one largest popul go search engin type directli question largest citi northern afghanistan case let show exampl sent queri search engin abl get top seven document snippet system pick repres document given queri top page see easili set snippet contain lot answer correct incorrect certainli contain lot citi name here one kabul capit afghanistan right answer question way here panama citi even afghanistan kano also afghanistan somewher nigeria one instanc kabul one guderm citi russia incorrect answer even though citi first time fifth passag mazari sharif correct answer also appear albeit differ spell seventh passag okay question run queri search engin give passag come learn algorithm classifi candid answer good one bad one least rank base like answer question ask compon typic system includ addit ir compon far first step sourc identif databas look use gener web corpu perhap someth specif question come movi actor mayb better go send question someth like imdb wikipedia rather entir web depend also whether answer like contain textual sourc semi structur perhap structur sourc exampl databas question exampl popul certain citi unemploy rate certain countri certain year probabl better obtain certain sort semi structur structur data set wherea gener question probabl better answer plain text unstructur data text sourc queri modul import step want describ detail next two slide here basic idea want convert natur languag question queri search engin search engin actual good answer natur languag question exampl easili get confus fact wh word may also automat drop import stop word wherea actual import answer ask question said string consist entir stop word search engin like drop answer queri case go get correct answer queri modul also concern correct syntax specif search engin exampl search engin allow includ altern way ask word exampl use vertic bar convert question wrote hamlet either author wrote follow hamlet case figur person wrote certain book person author book therefor go look document contain name book also contain one multipl way least one multipl way one describ author book document retriev find document match queri sentenc rank mean identifi document contain answer want find passag possibl sentenc paragraph like contain answer usual done sort end gap overlap form formula dont copi formula go discuss inform retriev section next thing identifi right passag sentenc want identifi answer origin question somewher sentenc passag answer extract involv process call question type classif question person want identifi name person possibl answer sentenc also involv someth call phrase chunk exampl let say someth new york dont want separ word new word york want keep togeth part answer final answer rank answer rank bunch candid answer satisfi criteria far correct type appear sentenc document relev queri want figur order present user score point exampl dont want kabul canno good airmask appear top list want mazaresharif featur use find answer includ question type proxim recruit word also importantli frequenc get let say passag return search engin relev queri six contain exact answer independ well chanc like correct answer oppos someth appear let go entir pipelin exampl question largest citi northern afghanistan go first queri modul convert largest biggest keep word citi drop stop word exampl add quotat mark around northern afghanistan indic phrase dont want citi northern part southern afghanistan northern neighbor afghanistan want actual phrase northern afghanistan appear okay send modul queri search engin perform document retriev get bunch url rank sentenc document get one like contain correct answer first perform answer extract identifi candid non phrase case like contain answer final perform answer rank use machin learn identifi mazeresharif actual better candid lead guderm second place ideal represent pipelin let look stage littl bit detail first one question type classif question type classif want identifi type name entiti match question ask exampl question author book go look answer peopl author writer theyr go look name organ name footbal team here exampl wrote anna karenina look person individu writer categori let look two differ taxonomi question type first one call synclass ibm answer select system ansel includ categori categor qa token question answer token first place answer question exampl categori rocki mountain next countri countri answer question specif question countri case answer could unit kingdom see map actual ambigu question input map either place countri state perhap categori question map person role organ even though look question word still dont know exactli categori question your look allow multipl possibl categori includ answer select process let show one taxonomi real quickli univers illinoi urbana champagn questiontyp taxonomi includ quit question type ibm system first major categori entiti includ thing like anim bodi organ color there anoth categori abbrevi descript differ type human locat final differ numer express exampl date distanc money rank exampl univers illinoi corpu rococo paint architectur flourish well categori numer subcategori date countri nation passeng rail system call via well automat classifi locat countri invent makeup well human specif individu look exampl detail way classif actual done look uiuc paper question classif specif recommend one list page li roth one li roth refer specif data set think there train set test set thousand differ question label manual select corpu use lot paper compar evalu go stop continu next segment techniqu question classif,[ 9  4  2 14 13]
293,Course3_W8-S1-L3_-_System_Architecture_-_9_slides_08-03,okay discussed one important problems question answers specifically question classification brief lets look specific techniques use question classification well first name indicates question classification classification task standard machine learning task possible use standard techniques instance svms naive bayesian techniques also possible use regular expressions example question country fairly obvious answer looking country question classify expression question person state state query formulation query modulation essentially process natural language question converted information retrieval query examples one michigan papers case question country biggest producer tungsten question converted different queries depending target search engine example removing double quotes replacing names words example tungsten replaced wolfram synonym element tungsten also replaced atomic number done using database like wordmap biggest producer also replaced largest producer country replaced region geographical area rural area whats important depending target search engine may important use right connector example word indicate disjunction query words perhaps vertical bar purpose use double quotes indicate phrases passage retrieval features use passage retrieval proper nouns match query example question wrote tarzan looking passages contain word tarzan want words near multiple proper nouns question want appear near answer well also want entities match expected answer type looking person author like wrote tarzan looking sentences contain persons didnt person sentence likely correct passage return answer retrieval use standard name entity recognition systems identify matching phrases example label january date features used answer retrieval distance query words example name author listed within words name book thats better listed words away case also looking answer type match question wordnet similarity example looking author want able get person instead writer also looking redundancy words appear multiple times answer set likely correct answer lets look redundancy little bit detail fairly obvious many different ways one express relation using text example relation madrid city capital spain spain country expressed following ways simple statement madrid capital spain also things like en route spains capital madrid dot dot dot madrid spains capital city situated almost geographical epicenter country capital spain madrid different paraphrase original madrid spains sunny capital madrid became spains capital referring back madrid previous sentence elevated status spains capital city important able recognize passages really paraphrases one another take advantage existing redundancy boost madrid candidate answer list example idea french revolutionaries storm bastille answers storming bastille occurred paris morning th july storming bastille july storming bastille prison july th slightly different spelling theres one year french revolutionaries storm bastille forth one example killed mahatma gandhi heres one answer gandhi assassinated dot dot dot nathuram godse nathuram godse killed gandhi godse killed gandhi gandhi assassinated shot dot dot dot person let appreciate much paraphrasing variability language important tasks like question answer systems mentioned based local corpus news theyre rather based arbitrary web documents including blogs encyclopedia entries theres important distinction cocalled traditional corpusbased systems openended webbased systems first systems web significantly larger corpus know web billions billions documents impossible sort preannotation local corpus even lets say several hundred gigabytes possible go advance identify named entities data expressions possible web document potential answer question possible preprocess data advance search engines necessarily useful time example may remove stop words may disregard question types example question type wrote hamlet word also going considered stop word going translated person name part query process search engines also impose arbitrary restrictions queries example allow queries certain length many issues researchers deal related reliability timeliness documents dont want return answer accurate year ago longer correct also presence inaccurate answers creep web many different reasons work research question answering deal issues,Course3,W8-S1-L3,W8,S1,L3,-,8,1,3,okay discuss one import problem question answer specif question classif brief let look specif techniqu use question classif well first name indic question classif classif task standard machin learn task possibl use standard techniqu instanc svm naiv bayesian techniqu also possibl use regular express exampl question countri fairli obviou answer look countri question classifi express question person state state queri formul queri modul essenti process natur languag question convert inform retriev queri exampl one michigan paper case question countri biggest produc tungsten question convert differ queri depend target search engin exampl remov doubl quot replac name word exampl tungsten replac wolfram synonym element tungsten also replac atom number done use databas like wordmap biggest produc also replac largest produc countri replac region geograph area rural area what import depend target search engin may import use right connector exampl word indic disjunct queri word perhap vertic bar purpos use doubl quot indic phrase passag retriev featur use passag retriev proper noun match queri exampl question wrote tarzan look passag contain word tarzan want word near multipl proper noun question want appear near answer well also want entiti match expect answer type look person author like wrote tarzan look sentenc contain person didnt person sentenc like correct passag return answer retriev use standard name entiti recognit system identifi match phrase exampl label januari date featur use answer retriev distanc queri word exampl name author list within word name book that better list word away case also look answer type match question wordnet similar exampl look author want abl get person instead writer also look redund word appear multipl time answer set like correct answer let look redund littl bit detail fairli obviou mani differ way one express relat use text exampl relat madrid citi capit spain spain countri express follow way simpl statement madrid capit spain also thing like en rout spain capit madrid dot dot dot madrid spain capit citi situat almost geograph epicent countri capit spain madrid differ paraphras origin madrid spain sunni capit madrid becam spain capit refer back madrid previou sentenc elev statu spain capit citi import abl recogn passag realli paraphras one anoth take advantag exist redund boost madrid candid answer list exampl idea french revolutionari storm bastil answer storm bastil occur pari morn th juli storm bastil juli storm bastil prison juli th slightli differ spell there one year french revolutionari storm bastil forth one exampl kill mahatma gandhi here one answer gandhi assassin dot dot dot nathuram gods nathuram gods kill gandhi gods kill gandhi gandhi assassin shot dot dot dot person let appreci much paraphras variabl languag import task like question answer system mention base local corpu news theyr rather base arbitrari web document includ blog encyclopedia entri there import distinct cocal tradit corpusbas system openend webbas system first system web significantli larger corpu know web billion billion document imposs sort preannot local corpu even let say sever hundr gigabyt possibl go advanc identifi name entiti data express possibl web document potenti answer question possibl preprocess data advanc search engin necessarili use time exampl may remov stop word may disregard question type exampl question type wrote hamlet word also go consid stop word go translat person name part queri process search engin also impos arbitrari restrict queri exampl allow queri certain length mani issu research deal relat reliabl timeli document dont want return answer accur year ago longer correct also presenc inaccur answer creep web mani differ reason work research question answer deal issu,[ 9  4  2 13 14]
294,Course3_W8-S1-L4_-_Question_Answering_Systems_1-2_-_14_slides_14-23,okay lets continue topic question answering next segment going important q systems developed years already gave examples systems trec im going continue systems developed trec trec starting first system ansel john prager et al ibm designed specifically trec following components includes something called predictive annotation means every named entity document collection corpus labeled candidate answer type person location uses standard machine learning technique logistic regression compute scores candidate answers heres example one questions yemen reunified way predictive annotation works somewhere two extremes question answer one exchanges something purely knowledgebased nlpbased example using parse tree sentence something completely bagofwords type example ir approach case vector representations documents queries sentences youre looking passage contains words similar query predictive annotation somewhere middle labels every named identity appropriate category time expression person location heres example works practice one trec questions author book iron lady biography margaret thatcher question converted internal presentation iron lady phrase biography margaret thatcher phrase gives different weights different types words example biography book author green middle screen see socalled syn categories expected answer questions corresponds persons organizations names roles named entity space next lines show sample document continued system label middle overall ir score given next one passages document shown bottom page see multiple named entities one biography margaret thatcher labeled ner agent name next thing hugo young labeled person farrar straus giroux marked organization finally margaret thatcher labeled person really need identify entities types match expected types question going skip example example actually plays deaf list question types expected going focus four actually correspond four categories green important observations kind work document contains answers question query words tend appear close proximity example author name book name person like margaret thatcher appear near answer sentences another observation answers fact seeking questions usually phrases consistent idea predictive annotation phrases categorize question type phrases identified using simple pattern matching techniques commonly use name editing recognition case candidate answers match question types kind features use rank pick one want first one answer following features used average distance several others im going show next seconds average distance average distance words beginning passage adjective words queries also appear passage example question johnny mathis high school track coach passage tim odonohue woodbridge high schools varsity baseball coach resigned monday replaced assistant johnny ceballos comma athletic director david cowen said example passage considering candidate answer original question tim odonahue want compute far phrase tim odonahue words appear also question question words high school appears words later sentence word track doesnt appear word coach appears six words right average distance tim odonahue words query eight another feature query query reflects number words passage appear query example one candidate answers woodbridge high school going notinq score one words high school appear query whereas word woodbridge doesnt third feature frequency number times given passage appears hit list next one sscore search engine relevance score essentially relevant particular sentence original query according underlying search engine fifth feature number position span shorttext passage considering among spans returned example lou vasquez first example returned passages next one relative span number position span among spans returned within current passage example rspanno value tim odonahue one would would two finally count number spans span class retrieved within current passage example tim odonahuewoodbridge high school monday johnny ceballos athletic director dave cowen five would answer question final feature type position span type list potential span ties particular question questions answers expected person organization name role order actually ordered person likely one organization second likely want candidate answer span person get higher score one organization name role continue full table features left hand side candidates spans candidate answer passages type value different feature finally composite score computed logistic regression function sort passages based value get lou vasquez gets highest score therefore returned first place system tim donohue second place one whether correct answer really relevant whats important understand system works okay ibm system next system att research system developed abney et al called ionaut used available online anymore based passage retrieval uses salton buckley start system one classic information retrieval systems back end performs entity recognition using abneys cass parser parser recognizes chunks nonincursive nonphrases example names people dates uses like ibm system entity classification question types next system want mention developed kwok university washington researchers first largescale web qa system used full web index involves several shelf natural language components example maximum entropy parser charniak group brown university uses pckimmo antworths system part speech tagging morphological analysis unknown words uses old version dependency link parser sleator temperley uses google underlying search engine performs tokenization identifies phrases quotes performs query transformations example question like nixon visit china expected appear original text nixon visited china opposed verb infinitive question word actually expecting text corpus different syntactically form question type convert questions format automatically fourth system university michigan called nsir based probabilistic phrase reranking idea behind every candidate named entity span signature sequence parts speech example two nouns noun followed propositional phrase training compass determine probability certain question type associated specific signature example signature two consecutive proper nouns n p n p probability name person high heres example bill gates name person pnnp likely sequence parts speech corresponds person answers uses shelf search engines backend time google also old search engines alltheweb northern light alta vista next system askmsr michelle banko et al microsoft research based assumption question important means somebody already answered question web components shown diagram directly paper heres typical flow looks like question louver museum located first component rewrite query component original question converted likely patterns louvre museum located louvre museum louvre museum near louvre museum finally word louvre museum near queries output query query modulation component sent parallel search engine passages returned search engine collected summaries snippets get return document search engine considered ngrams example two three four word long ngrams collected passages filtered tiled tiling process means one diagram consists words b another one consists words b c tiling component going merge together trigraph b c case example paris could one bigram returned search engine whereas paris france could come completely different bigram two tiled together trigram paris france see simple example three candidate answers one correct one ranks first place one example tiling mr charles charles dickens could combined mr charles dickens okay last system going talk segment echihabi marcu introduces first time noisychannel model one hand questions hand sentences contain answers questions idea pick answer sentence maximizes probability given question turns kind work requires sentence simplification possible learn probabilistic model accurately next segment going look systems question answer,Course3,W8-S1-L4,W8,S1,L4,-,8,1,4,okay let continu topic question answer next segment go import q system develop year alreadi gave exampl system trec im go continu system develop trec trec start first system ansel john prager et al ibm design specif trec follow compon includ someth call predict annot mean everi name entiti document collect corpu label candid answer type person locat use standard machin learn techniqu logist regress comput score candid answer here exampl one question yemen reunifi way predict annot work somewher two extrem question answer one exchang someth pure knowledgebas nlpbase exampl use pars tree sentenc someth complet bagofword type exampl ir approach case vector represent document queri sentenc your look passag contain word similar queri predict annot somewher middl label everi name ident appropri categori time express person locat here exampl work practic one trec question author book iron ladi biographi margaret thatcher question convert intern present iron ladi phrase biographi margaret thatcher phrase give differ weight differ type word exampl biographi book author green middl screen see socal syn categori expect answer question correspond person organ name role name entiti space next line show sampl document continu system label middl overal ir score given next one passag document shown bottom page see multipl name entiti one biographi margaret thatcher label ner agent name next thing hugo young label person farrar strau giroux mark organ final margaret thatcher label person realli need identifi entiti type match expect type question go skip exampl exampl actual play deaf list question type expect go focu four actual correspond four categori green import observ kind work document contain answer question queri word tend appear close proxim exampl author name book name person like margaret thatcher appear near answer sentenc anoth observ answer fact seek question usual phrase consist idea predict annot phrase categor question type phrase identifi use simpl pattern match techniqu commonli use name edit recognit case candid answer match question type kind featur use rank pick one want first one answer follow featur use averag distanc sever other im go show next second averag distanc averag distanc word begin passag adject word queri also appear passag exampl question johnni mathi high school track coach passag tim odonohu woodbridg high school varsiti basebal coach resign monday replac assist johnni ceballo comma athlet director david cowen said exampl passag consid candid answer origin question tim odonahu want comput far phrase tim odonahu word appear also question question word high school appear word later sentenc word track doesnt appear word coach appear six word right averag distanc tim odonahu word queri eight anoth featur queri queri reflect number word passag appear queri exampl one candid answer woodbridg high school go notinq score one word high school appear queri wherea word woodbridg doesnt third featur frequenc number time given passag appear hit list next one sscore search engin relev score essenti relev particular sentenc origin queri accord underli search engin fifth featur number posit span shorttext passag consid among span return exampl lou vasquez first exampl return passag next one rel span number posit span among span return within current passag exampl rspanno valu tim odonahu one would would two final count number span span class retriev within current passag exampl tim odonahuewoodbridg high school monday johnni ceballo athlet director dave cowen five would answer question final featur type posit span type list potenti span tie particular question question answer expect person organ name role order actual order person like one organ second like want candid answer span person get higher score one organ name role continu full tabl featur left hand side candid span candid answer passag type valu differ featur final composit score comput logist regress function sort passag base valu get lou vasquez get highest score therefor return first place system tim donohu second place one whether correct answer realli relev what import understand system work okay ibm system next system att research system develop abney et al call ionaut use avail onlin anymor base passag retriev use salton buckley start system one classic inform retriev system back end perform entiti recognit use abney cass parser parser recogn chunk nonincurs nonphras exampl name peopl date use like ibm system entiti classif question type next system want mention develop kwok univers washington research first largescal web qa system use full web index involv sever shelf natur languag compon exampl maximum entropi parser charniak group brown univers use pckimmo antworth system part speech tag morpholog analysi unknown word use old version depend link parser sleator temperley use googl underli search engin perform token identifi phrase quot perform queri transform exampl question like nixon visit china expect appear origin text nixon visit china oppos verb infinit question word actual expect text corpu differ syntact form question type convert question format automat fourth system univers michigan call nsir base probabilist phrase rerank idea behind everi candid name entiti span signatur sequenc part speech exampl two noun noun follow proposit phrase train compass determin probabl certain question type associ specif signatur exampl signatur two consecut proper noun n p n p probabl name person high here exampl bill gate name person pnnp like sequenc part speech correspond person answer use shelf search engin backend time googl also old search engin alltheweb northern light alta vista next system askmsr michel banko et al microsoft research base assumpt question import mean somebodi alreadi answer question web compon shown diagram directli paper here typic flow look like question louver museum locat first compon rewrit queri compon origin question convert like pattern louvr museum locat louvr museum louvr museum near louvr museum final word louvr museum near queri output queri queri modul compon sent parallel search engin passag return search engin collect summari snippet get return document search engin consid ngram exampl two three four word long ngram collect passag filter tile tile process mean one diagram consist word b anoth one consist word b c tile compon go merg togeth trigraph b c case exampl pari could one bigram return search engin wherea pari franc could come complet differ bigram two tile togeth trigram pari franc see simpl exampl three candid answer one correct one rank first place one exampl tile mr charl charl dicken could combin mr charl dicken okay last system go talk segment echihabi marcu introduc first time noisychannel model one hand question hand sentenc contain answer question idea pick answer sentenc maxim probabl given question turn kind work requir sentenc simplif possibl learn probabilist model accur next segment go look system question answer,[ 9  4  2 13  0]
295,Course3_W8-S1-L5_-_Question_Answering_Systems_2-2_-_9_slides_10-55,okay going continue system descriptions question answering next system want discuss briefly system developed language computer corporation harabagiu moldovan others around thats one system thats different others involves deep semantic analysis sentences converts logical forms heres example paper sentence like heavy selling standard poors stock index futures chicago relentlessly beat stocks downward going present logical form using different elements attributes example heavy adjective relates x selling also related x preposition connects x x system uses different semantic axioms inference example uses lexical chains word net example word game related recreation recreation related sport able use lexical chain information find answers even dont contain words original question one system university michigan called qasm also based noisy channel model want convert natural language question query different system marco ishihari mapping question sentence example question country biggest producer tungsten want convert something like biggest largest biggest largest synonyms keep content words produce tungsten drop content words chasm involves number channel noisy channel operators example deletions deleting prepositions stop words example replacing noun phrase disjunction includes multiple wordnet synonyms replace third system segment ravinchandran hovy isi based automatically learning surface patterns sentences likely contain answers given question starts seed queries web finds patterns contain question answer terms example question wrote hamlet knows correct answer shakespeare search documents contain word hamlet word shakespeare try identify patterns contain expressions look words connect together heres one example mozart born kind pattern system going recognize born example links together name person birthday takes much recent system watson system david ferrucci ibm thats system participated jeopardy stretch imagination largest q system published extensively covered press probably system jeopardy interesting features system architecture first architecture based technique called deepqa thats technology enables computers precisely answer natural language questions using different types knowledge sources structured data also inference engines knowledge representations powerful hardware implementation involves racks ibm servers running linux terabytes ram ram almost cores operating backbreaking speed teraflops written java also little bit c prolog components integrated using ibms constructor data uima system overview ferrucci et al system ai magazine fall theres article pc magazine includes lot information background performance kind knowledge sources watson use uses million pages structured unstructured content total four terabytes disk storage includes things like wikipedia encyclopedias dictionaries news articles also includes things like wordnet knowledge representation sources one interesting aspect watson system much related natural language processing certainly worth mentioning betting strategy way jeopardy works get right answer also buzz everybody else buzz youre supposed answer right away get selected answer get answer wrong means actually lose points youre really certain answer best strategy buzz quickly possible youre sure may better wait little bit maybe let somebody else answer possibly get wrong case get additional information try second one guess get right way betting strategy works sort confidence associated every answer getting least certain try buzz wait well supposed answer questions got correct wrong famous example incorrect answer category us cities one questions city two airports named world war ii one named battle one named person watson thought buzzed first gave answer toronto incorrect guess missed fact category us cities correct answer case chicago midway ohare airports however even though incorrect answers still managed win game huge margin two human performance winners many previous contests ken jennings brad rutter walked away really small winnings compared watson watson well covered press im going spend time included several interesting pointers future reference okay question types watson uses theres actually really large taxonomy theyre way systems turns according law diminishing returns question types really common rest appear twice entire history jeopardy games years way talk archive actually available internet go check every single jeopardy question asked last years including ones watson answer lets switch slightly different question challenges question answering would need solved order make question answering even successful one obvious problem word sense disambiguation many words english languages multiple senses want able use state art word sense disambiguation understand next one coreference resolution often answer question maybe sentence doesnt contain original named entity instead maybe second sentence paragraph introduced means pronoun named entity aforenamed entity case cannot really identify answer easily obviously advances coreference go long way towards making qa systems better third component semantic role labeling topic going discuss one future segments semantic role labeling deal identifying main predicates sentences attributes example action buying something semantic roles associated buying person buying object bought price location answers likely improve question answering one important topic question answering temporal questions deal answers change time example whos president united states currently answer barack obama years ago george w bush years somebody else able understand correctly time question asked time answer given document jeopardys specific concern use categories correctly mentioned watson made mistake category us cities giving canadian city obvious didnt realize answers supposed us cities okay going switch slightly different topic well hear concluding question answer,Course3,W8-S1-L5,W8,S1,L5,-,8,1,5,okay go continu system descript question answer next system want discuss briefli system develop languag comput corpor harabagiu moldovan other around that one system that differ other involv deep semant analysi sentenc convert logic form here exampl paper sentenc like heavi sell standard poor stock index futur chicago relentlessli beat stock downward go present logic form use differ element attribut exampl heavi adject relat x sell also relat x preposit connect x x system use differ semant axiom infer exampl use lexic chain word net exampl word game relat recreat recreat relat sport abl use lexic chain inform find answer even dont contain word origin question one system univers michigan call qasm also base noisi channel model want convert natur languag question queri differ system marco ishihari map question sentenc exampl question countri biggest produc tungsten want convert someth like biggest largest biggest largest synonym keep content word produc tungsten drop content word chasm involv number channel noisi channel oper exampl delet delet preposit stop word exampl replac noun phrase disjunct includ multipl wordnet synonym replac third system segment ravinchandran hovi isi base automat learn surfac pattern sentenc like contain answer given question start seed queri web find pattern contain question answer term exampl question wrote hamlet know correct answer shakespear search document contain word hamlet word shakespear tri identifi pattern contain express look word connect togeth here one exampl mozart born kind pattern system go recogn born exampl link togeth name person birthday take much recent system watson system david ferrucci ibm that system particip jeopardi stretch imagin largest q system publish extens cover press probabl system jeopardi interest featur system architectur first architectur base techniqu call deepqa that technolog enabl comput precis answer natur languag question use differ type knowledg sourc structur data also infer engin knowledg represent power hardwar implement involv rack ibm server run linux terabyt ram ram almost core oper backbreak speed teraflop written java also littl bit c prolog compon integr use ibm constructor data uima system overview ferrucci et al system ai magazin fall there articl pc magazin includ lot inform background perform kind knowledg sourc watson use use million page structur unstructur content total four terabyt disk storag includ thing like wikipedia encyclopedia dictionari news articl also includ thing like wordnet knowledg represent sourc one interest aspect watson system much relat natur languag process certainli worth mention bet strategi way jeopardi work get right answer also buzz everybodi els buzz your suppos answer right away get select answer get answer wrong mean actual lose point your realli certain answer best strategi buzz quickli possibl your sure may better wait littl bit mayb let somebodi els answer possibl get wrong case get addit inform tri second one guess get right way bet strategi work sort confid associ everi answer get least certain tri buzz wait well suppos answer question got correct wrong famou exampl incorrect answer categori us citi one question citi two airport name world war ii one name battl one name person watson thought buzz first gave answer toronto incorrect guess miss fact categori us citi correct answer case chicago midway ohar airport howev even though incorrect answer still manag win game huge margin two human perform winner mani previou contest ken jen brad rutter walk away realli small win compar watson watson well cover press im go spend time includ sever interest pointer futur refer okay question type watson use there actual realli larg taxonomi theyr way system turn accord law diminish return question type realli common rest appear twice entir histori jeopardi game year way talk archiv actual avail internet go check everi singl jeopardi question ask last year includ one watson answer let switch slightli differ question challeng question answer would need solv order make question answer even success one obviou problem word sens disambigu mani word english languag multipl sens want abl use state art word sens disambigu understand next one corefer resolut often answer question mayb sentenc doesnt contain origin name entiti instead mayb second sentenc paragraph introduc mean pronoun name entiti aforenam entiti case cannot realli identifi answer easili obvious advanc corefer go long way toward make qa system better third compon semant role label topic go discuss one futur segment semant role label deal identifi main predic sentenc attribut exampl action buy someth semant role associ buy person buy object bought price locat answer like improv question answer one import topic question answer tempor question deal answer chang time exampl who presid unit state current answer barack obama year ago georg w bush year somebodi els abl understand correctli time question ask time answer given document jeopardi specif concern use categori correctli mention watson made mistak categori us citi give canadian citi obviou didnt realiz answer suppos us citi okay go switch slightli differ topic well hear conclud question answer,[ 9  4 13 14 12]
296,Course3_W9-S1-L1_-_Summarization_-_16_slides_11-50,welcome back natural language processing today going start new section text summarization one interesting parts natural language processing simple introduction text summarization want able get input set documents example health bunch sentences eating vegetables fruits reasons theyre healthy often users dont time read lot details want able produce summary two reasons one read summary instead original documents dont enough time even realistically show summary get idea document theyve summary decide whether want go read full set documents example summary would like get although theres summarization system currently want get short summary like eating vegetables healthy well extreme example text summarization said practical feasible point many instances existing summarization systems work could use meaningful summaries actually useful users lets look examples kind summaries wont able produce one news summarization wont get example cluster documents related produce short summary like one appears middle page everything happened set related documents also put book summaries example something like cliff notes probably everybody knows cliff notes short descriptions happens large book plus additional comments characters plot techniques used book heres fun example theres website called book minute people spent time summarize famous books really really short versions heres one guess book boys crash island ralph says repeatedly need fire make fire goes happens times jack says forget fire lets kill boys say yeah kill end tongue cheek summary famous book guess one yeah book obviously lord flies golding author condensed use term david packer samuel stoddard rinkworkscom examples summaries movie summaries like titanic short summaries example beginning genuine footage departure titanic fateful voyage epic movie tells events tragic night perspective fictional survivor rose old lady recounts story duty love disaster salvage crew searching lost gem obviously many different summaries produce us given obviously go website like imdb get hundreds summaries written different people see summaries often look different one another yet goal namely capture gist movie book piece another type summaries search engines snippets difference ones looked previously cookie based search engines return little passages retrieved documents similar query heres example search cloud atlas science fiction book see top hits returned google accompanied short passages snippets words cloud atlas name author highlighted many different genres summaries also headlines headline construed short summary document new story example outline produced many different genres documents could outline book outline meeting outline encyclopedia entry paper also minutes meetings biographies people obituaries plain biographies abridgments abridgments books typically shorter versions different audiences perhaps younger children people dont much time sound bites small snippets audio interview event movie summaries chronologies want give credit taxonomy gidget mani mark maybury paper types summaries well already saw distinguish different types summaries well one factors whats input single document multiple documents grammatical text output grammatical sentence keywords speech text whats purpose intended replace original document indicative case tells original document doesnt give details also something called critical summaries summarizing example book movie youre also giving subjective information feel form either extractive abstractive extracts usually like representative paragraphs sentences phrases document abstracts reformulated using different words general quote paice paper concise summary sample subject matter document dimensions take account document summaries based single multiple document input context important query specific example document related lets say trade talks versus generic whats generically good summary document regardless context okay typical summarization system three stages typically following first stage content identification given input document determine information want preserve pass next stage could specific sentences named entities facts next thing organize information want combine information multiple documents want preserve entire sentences want reorder finally realization realization deal additional issues example take two documents one sentence may read nicely next possible realization stage would include generation connectives example example therefore contrast sentences came different sources tied together coherently realization include generation referring expression person heres example extractive summarizer takes input news story see news story consists ten sentences want summary include important facts underlined shown red see purely extractive summarizer would underline passages sentences present summary case realization practical nonexistent preserving information original documents appears exactly order one important thing mention extracting summarization come different kinds first kind full sentences like example last sentence example portions sentences still counts extracted summarizor even youre extracting entire sentences turns summarization something humans years example professional abstractors read scientific articles create manual abstract go different diplomatic databases heres nice quote many years ago years ago ashworth professional abstractors quote unquote take original article understand pack neatly nutshell without loss substance clarity presents challenge many felt worth taking joys achievement alone characteristics art form passage clearly indicates summarization difficult task humans may may good involves significant amount craftmanship okay lets focus specific types summaries example extractive summarization mentioned extractive summarization selecting units original text presenting order appear units usually sentences one common scenarios simplification sentences allowed youre allowed skip portions replacing words others rewriting allowed turns genres documents important baseline hard meet extractor summarization called leadbase baseline leadbased summaries certain amount text allowed produce part summary example lets say equivalent two sentences five sentences baseline extract many sentences beginning document allowed produce output example turns many innovation metrics many genres text first sentences document fact informative sentences well luckily researchers field case genres possible come techniques even better going continue additional considerations organizations next segment,Course3,W9-S1-L1,W9,S1,L1,-,9,1,1,welcom back natur languag process today go start new section text summar one interest part natur languag process simpl introduct text summar want abl get input set document exampl health bunch sentenc eat veget fruit reason theyr healthi often user dont time read lot detail want abl produc summari two reason one read summari instead origin document dont enough time even realist show summari get idea document theyv summari decid whether want go read full set document exampl summari would like get although there summar system current want get short summari like eat veget healthi well extrem exampl text summar said practic feasibl point mani instanc exist summar system work could use meaning summari actual use user let look exampl kind summari wont abl produc one news summar wont get exampl cluster document relat produc short summari like one appear middl page everyth happen set relat document also put book summari exampl someth like cliff note probabl everybodi know cliff note short descript happen larg book plu addit comment charact plot techniqu use book here fun exampl there websit call book minut peopl spent time summar famou book realli realli short version here one guess book boy crash island ralph say repeatedli need fire make fire goe happen time jack say forget fire let kill boy say yeah kill end tongu cheek summari famou book guess one yeah book obvious lord fli gold author condens use term david packer samuel stoddard rinkworkscom exampl summari movi summari like titan short summari exampl begin genuin footag departur titan fate voyag epic movi tell event tragic night perspect fiction survivor rose old ladi recount stori duti love disast salvag crew search lost gem obvious mani differ summari produc us given obvious go websit like imdb get hundr summari written differ peopl see summari often look differ one anoth yet goal name captur gist movi book piec anoth type summari search engin snippet differ one look previous cooki base search engin return littl passag retriev document similar queri here exampl search cloud atla scienc fiction book see top hit return googl accompani short passag snippet word cloud atla name author highlight mani differ genr summari also headlin headlin constru short summari document new stori exampl outlin produc mani differ genr document could outlin book outlin meet outlin encyclopedia entri paper also minut meet biographi peopl obituari plain biographi abridg abridg book typic shorter version differ audienc perhap younger children peopl dont much time sound bite small snippet audio interview event movi summari chronolog want give credit taxonomi gidget mani mark mayburi paper type summari well alreadi saw distinguish differ type summari well one factor what input singl document multipl document grammat text output grammat sentenc keyword speech text what purpos intend replac origin document indic case tell origin document doesnt give detail also someth call critic summari summar exampl book movi your also give subject inform feel form either extract abstract extract usual like repres paragraph sentenc phrase document abstract reformul use differ word gener quot paic paper concis summari sampl subject matter document dimens take account document summari base singl multipl document input context import queri specif exampl document relat let say trade talk versu gener what gener good summari document regardless context okay typic summar system three stage typic follow first stage content identif given input document determin inform want preserv pass next stage could specif sentenc name entiti fact next thing organ inform want combin inform multipl document want preserv entir sentenc want reorder final realiz realiz deal addit issu exampl take two document one sentenc may read nice next possibl realiz stage would includ gener connect exampl exampl therefor contrast sentenc came differ sourc tie togeth coher realiz includ gener refer express person here exampl extract summar take input news stori see news stori consist ten sentenc want summari includ import fact underlin shown red see pure extract summar would underlin passag sentenc present summari case realiz practic nonexist preserv inform origin document appear exactli order one import thing mention extract summar come differ kind first kind full sentenc like exampl last sentenc exampl portion sentenc still count extract summarizor even your extract entir sentenc turn summar someth human year exampl profession abstractor read scientif articl creat manual abstract go differ diplomat databas here nice quot mani year ago year ago ashworth profession abstractor quot unquot take origin articl understand pack neatli nutshel without loss substanc clariti present challeng mani felt worth take joy achiev alon characterist art form passag clearli indic summar difficult task human may may good involv signific amount craftmanship okay let focu specif type summari exampl extract summar mention extract summar select unit origin text present order appear unit usual sentenc one common scenario simplif sentenc allow your allow skip portion replac word other rewrit allow turn genr document import baselin hard meet extractor summar call leadbas baselin leadbas summari certain amount text allow produc part summari exampl let say equival two sentenc five sentenc baselin extract mani sentenc begin document allow produc output exampl turn mani innov metric mani genr text first sentenc document fact inform sentenc well luckili research field case genr possibl come techniqu even better go continu addit consider organ next segment,[ 2  4 13 14 12]
297,Course3_W9-S1-L2_-_Summarization_Techniques_1-3_-_26_slides_19-49,okay going continue specific summarization techniques next segments want start one classic papers baxendale introduced called positional method works well specific genres text example scientific papers analyzed paragraphs figured first last sentences paragraph ones contain useful information thats topic sentences typically located according style used type documents naive yet reasonable approach given state art years ago paper always cited first paper automatic summarization influential field see many papers follow use kind technique starting point next paper luhn ibm also summarizing technical documents also one first people stemming automatic stemming words removal stop words using features frequency content terms according methodology words frequent ones stop words least frequent ones ones appear document informative ones e metric shown diagram reaches maximum words middle range based frequency sentences contain words deemed worthy inclusion summary theres sentence level significance factor based presence words high e values wanted pick sentences large concentration salient content terms example sentence represented top slide four significant words appear within span words score particular cluster significant words going squared number significant words squared divided length span appear see reasonable metric focuses two important things summaries significant words also high concentration near rather dispersed around document recent paper edmundson also uses technical documents uses features position frequency previous work also looks cue words cue words categorized bonus words stigma words bonus words things like significant accomplishments stigma words hardly impossible words typical scientific papers typically indicate portions paper informative edmundson also uses document structure feature sentence title heading section paper right one example first sentence document first sentence specific section subsection combines four categories features using linear combination pick important sentences papers early based sentence extraction technical papers completely different approach done jared dejong part thesis university illinois system called frump one first knowledge based summarization systems idea would automatically process sequence news articles upi recognize sort scenario theyre discussing figure scenario discussed particular story would set slots need filled correspond scenario use sentences include slot fillers summary sentences created collection socalled sketchy scripts correspond different situations often discussed news matched scripts based manually selected keywords theyre manually selected difficult port domains turns evaluated realized set scripts nearly enough cover possible inputs expect get news heres example one scenarios called demonstration category script category involves following events demonstration going first thing happened demonstrators arrived location demonstration march police arrives scene demonstrators communicate target demonstration example mayor politician organization attack target demonstration attack police see fairly specific scenario happens match existing news story valuable theres far go kind approach given wide diversity event types occur news nevertheless paper highly influential summarization community knowledge representation communities okay recent paper john paice essentially survey work text summarization point discusses lot detail techniques worked many actually failed discusses techniques really provide good results example use syntactic criteria pick sentences syntactic patterns presences indicator phrases discourse structure phrases also focused problems extracts idea would use extractive summary document contains sentences adjacent original text may run issues discourse coherence also lack balance example fairly balanced document describes points view two opposing factions end summary describes one thereby experiencing lack balance problem another example lack cohesion first fact sentence may start pronoun definite anaphora clear dont pick sentence contains antecedent anaphoric expression refers even possible get incorrect outputs example three sentences first one talking second one b third one saying something b using another fork expression example drop second sentence youre going first third sentence remaining therefore going appear refer back first sentence rather b second sentence therefore lead confusion incorrect outputs one big things liked discussed detail idea deal lack balance recently dealt using rhetorical structure texts specifically work daniel marcu im going discuss one later slides lack cohesion addressed using techniques discourse analysis example want deal issues anaphoric reference presence definite anaphora use lexical definite reference able understand use correct rhetorical connectives generate output sentences possible recognize anaphoric use early work done liz liddy example want determine word used anaphorically could classifier looks context word tells case whether used anaphorically example word preceded research verb example demonstrate nonanaphoric else followed pronoun article quantifier also nonanaphoric else appears first ten words sentence external meaning unaphoric refers reference previous sentence finally internal unaphoric appears first ten words sentence papers time period one brandow news articles change previous work summarization deal scientific articles system called anes included commercial news large number publications able evaluate different techniques intelligence summarization showed lead summarization ones picked first sentences document much acceptable automatic summaries essentially negative result research yet turned later based work people problem lead outperforming intelligence summarizers applicable news certain genres news journalists instructed write stories socalled pyramid style automatically expected include important information first sentences brandow et al paper looked documents looked words based tfidf value tfidf concept going discuss information table section class suffice say point words high values metric important words rather stop words lower tfidf values used sentence based features example presence signature words location sentence paragraph presence absence anaphora words length abstract sentences signature words included appear two selected sentences one way deal problems anaphora reference evaluated output word length used nontask driven evaluation asked people rank summaries whether theyre acceptable one interesting papers julien croupier okay person behind murax question answering system paper appeared sigrear first use trainable machine learning techniques sentence extractions specifically system used naive based classifier target extract input collection documents scientific journals back scientific papers features use addition ones already discussed include sentence length sentence fewer five words included presence uppercase words example names places organizations people except common acronyms use thematic words set manually fixed phrases last set features deal sentences position paragraph thats numeric feature number sentences paragraph heres naive bayesian classifier works features like example f fk like ones previous slide little specific sentence capital summary try compute probability sentence going put summary given set specific features using bayesian formula convert probability set features given sentence summary times probability sentence summary divide prior probability particular set features using statistical independence use chain rule simplify little bit value probability sentence summary given features going product probabilities individual features sentences period summary performance kupiecs system getting precision compared goal standard summaries level actually getting improvement leadbased summary producing shorter summaries encouraging research brandel paper another paper mckeown radev also system called summons first paper introduced problem multidocument summarization work single news articles single scientific papers put set news story topic come possibly variety different sources approach extraction knowledge based approach first convert input document socalled muc template corresponds specific event type muc message understanding conference competition midtolate early systems built filling slots correspond specific scenarios example mergers acquisitions joint ventures terrorist events types news templates filled mock system next step perform text generation actually identifies generate sentences contain slot fillers muc template corresponds practice things like perpetrator event names companies involved merger date amount merger heres example output summons output generated multiple input documents see reported statements stuff green correspond sources information also indicators agreement disagreement different sources specific slot filled particular document use specific sentence explain theres information particular slot heres example muc template used input summons example terrorist events different slots filled muc systems example bbm participants date event location event type event participating entities may present may missing summons takes input cluster templates correspond different input documents combines together using domain ontology performs multiple additional stages really relevant point going discuss detail talk ten generation stages paragraph planner tells information want include paragraphs generate within paragraph sentence planner essentially determines information go individual sentence lexical chooser tells words use express specific concept input template finally uses offtheshelf sentence generator called surge converts logical representation actual grammatical sentence grammatical paragraph one related example case input consists four different templates heres output multiple events discussed generate specific time expressions example sunday next day second incident later day made summary readable coherent heres rules used summons example two templates location time second template time first template sources first template different source second template least one slot differs combine two templates using contradiction operator later would tell generation component certain connective expression used heres operators used summons change perspective precondition change perspective source reports change small number slots reuters gives one account one day later day actually changes account something different operator contradiction multiple sources report contradictory values small number slots opposed previous example source different times another operator refinement slot gets filled point even though empty also agreement thats multiple sources corroborate information theres also ones generalization output two different templates want combine one let stop going continue mitra paper next segment,Course3,W9-S1-L2,W9,S1,L2,-,9,1,2,okay go continu specif summar techniqu next segment want start one classic paper baxendal introduc call posit method work well specif genr text exampl scientif paper analyz paragraph figur first last sentenc paragraph one contain use inform that topic sentenc typic locat accord style use type document naiv yet reason approach given state art year ago paper alway cite first paper automat summar influenti field see mani paper follow use kind techniqu start point next paper luhn ibm also summar technic document also one first peopl stem automat stem word remov stop word use featur frequenc content term accord methodolog word frequent one stop word least frequent one one appear document inform one e metric shown diagram reach maximum word middl rang base frequenc sentenc contain word deem worthi inclus summari there sentenc level signific factor base presenc word high e valu want pick sentenc larg concentr salient content term exampl sentenc repres top slide four signific word appear within span word score particular cluster signific word go squar number signific word squar divid length span appear see reason metric focus two import thing summari signific word also high concentr near rather dispers around document recent paper edmundson also use technic document use featur posit frequenc previou work also look cue word cue word categor bonu word stigma word bonu word thing like signific accomplish stigma word hardli imposs word typic scientif paper typic indic portion paper inform edmundson also use document structur featur sentenc titl head section paper right one exampl first sentenc document first sentenc specif section subsect combin four categori featur use linear combin pick import sentenc paper earli base sentenc extract technic paper complet differ approach done jare dejong part thesi univers illinoi system call frump one first knowledg base summar system idea would automat process sequenc news articl upi recogn sort scenario theyr discuss figur scenario discuss particular stori would set slot need fill correspond scenario use sentenc includ slot filler summari sentenc creat collect socal sketchi script correspond differ situat often discuss news match script base manual select keyword theyr manual select difficult port domain turn evalu realiz set script nearli enough cover possibl input expect get news here exampl one scenario call demonstr categori script categori involv follow event demonstr go first thing happen demonstr arriv locat demonstr march polic arriv scene demonstr commun target demonstr exampl mayor politician organ attack target demonstr attack polic see fairli specif scenario happen match exist news stori valuabl there far go kind approach given wide divers event type occur news nevertheless paper highli influenti summar commun knowledg represent commun okay recent paper john paic essenti survey work text summar point discuss lot detail techniqu work mani actual fail discuss techniqu realli provid good result exampl use syntact criteria pick sentenc syntact pattern presenc indic phrase discours structur phrase also focus problem extract idea would use extract summari document contain sentenc adjac origin text may run issu discours coher also lack balanc exampl fairli balanc document describ point view two oppos faction end summari describ one therebi experienc lack balanc problem anoth exampl lack cohes first fact sentenc may start pronoun definit anaphora clear dont pick sentenc contain anteced anaphor express refer even possibl get incorrect output exampl three sentenc first one talk second one b third one say someth b use anoth fork express exampl drop second sentenc your go first third sentenc remain therefor go appear refer back first sentenc rather b second sentenc therefor lead confus incorrect output one big thing like discuss detail idea deal lack balanc recent dealt use rhetor structur text specif work daniel marcu im go discuss one later slide lack cohes address use techniqu discours analysi exampl want deal issu anaphor refer presenc definit anaphora use lexic definit refer abl understand use correct rhetor connect gener output sentenc possibl recogn anaphor use earli work done liz liddi exampl want determin word use anaphor could classifi look context word tell case whether use anaphor exampl word preced research verb exampl demonstr nonanaphor els follow pronoun articl quantifi also nonanaphor els appear first ten word sentenc extern mean unaphor refer refer previou sentenc final intern unaphor appear first ten word sentenc paper time period one brandow news articl chang previou work summar deal scientif articl system call ane includ commerci news larg number public abl evalu differ techniqu intellig summar show lead summar one pick first sentenc document much accept automat summari essenti neg result research yet turn later base work peopl problem lead outperform intellig summar applic news certain genr news journalist instruct write stori socal pyramid style automat expect includ import inform first sentenc brandow et al paper look document look word base tfidf valu tfidf concept go discuss inform tabl section class suffic say point word high valu metric import word rather stop word lower tfidf valu use sentenc base featur exampl presenc signatur word locat sentenc paragraph presenc absenc anaphora word length abstract sentenc signatur word includ appear two select sentenc one way deal problem anaphora refer evalu output word length use nontask driven evalu ask peopl rank summari whether theyr accept one interest paper julien croupier okay person behind murax question answer system paper appear sigrear first use trainabl machin learn techniqu sentenc extract specif system use naiv base classifi target extract input collect document scientif journal back scientif paper featur use addit one alreadi discuss includ sentenc length sentenc fewer five word includ presenc uppercas word exampl name place organ peopl except common acronym use themat word set manual fix phrase last set featur deal sentenc posit paragraph that numer featur number sentenc paragraph here naiv bayesian classifi work featur like exampl f fk like one previou slide littl specif sentenc capit summari tri comput probabl sentenc go put summari given set specif featur use bayesian formula convert probabl set featur given sentenc summari time probabl sentenc summari divid prior probabl particular set featur use statist independ use chain rule simplifi littl bit valu probabl sentenc summari given featur go product probabl individu featur sentenc period summari perform kupiec system get precis compar goal standard summari level actual get improv leadbas summari produc shorter summari encourag research brandel paper anoth paper mckeown radev also system call summon first paper introduc problem multidocu summar work singl news articl singl scientif paper put set news stori topic come possibl varieti differ sourc approach extract knowledg base approach first convert input document socal muc templat correspond specif event type muc messag understand confer competit midtol earli system built fill slot correspond specif scenario exampl merger acquisit joint ventur terrorist event type news templat fill mock system next step perform text gener actual identifi gener sentenc contain slot filler muc templat correspond practic thing like perpetr event name compani involv merger date amount merger here exampl output summon output gener multipl input document see report statement stuff green correspond sourc inform also indic agreement disagr differ sourc specif slot fill particular document use specif sentenc explain there inform particular slot here exampl muc templat use input summon exampl terrorist event differ slot fill muc system exampl bbm particip date event locat event type event particip entiti may present may miss summon take input cluster templat correspond differ input document combin togeth use domain ontolog perform multipl addit stage realli relev point go discuss detail talk ten gener stage paragraph planner tell inform want includ paragraph gener within paragraph sentenc planner essenti determin inform go individu sentenc lexic chooser tell word use express specif concept input templat final use offtheshelf sentenc gener call surg convert logic represent actual grammat sentenc grammat paragraph one relat exampl case input consist four differ templat here output multipl event discuss gener specif time express exampl sunday next day second incid later day made summari readabl coher here rule use summon exampl two templat locat time second templat time first templat sourc first templat differ sourc second templat least one slot differ combin two templat use contradict oper later would tell gener compon certain connect express use here oper use summon chang perspect precondit chang perspect sourc report chang small number slot reuter give one account one day later day actual chang account someth differ oper contradict multipl sourc report contradictori valu small number slot oppos previou exampl sourc differ time anoth oper refin slot get fill point even though empti also agreement that multipl sourc corrobor inform there also one gener output two differ templat want combin one let stop go continu mitra paper next segment,[ 4  2  5  0 14]
298,Course3_W9-S1-L3_-_Summarization_Techniques_2-3_-_27_slides_20-34,okay going continue papers summarization first one mitraallen group cornell university first paper use graphbased summarization techniques using first time corpus encyclopedia articles idea want represent sentence encyclopedia article node graph connect nodes using semantic hyperlinks content sentences overlaps idea overlap defined lexical similarity threshold example multiple words common idea paths link highly connected paragraphs likely contain information central topic articles kind representation start first sentence document follow paths connect highly connected paragraphs youre going produce reasonable summary encyclopedia article okay going continue techniques late next one mani bloedorn miter essentially second time people looked multidocument summarization also use graphbased methods identifying similarities differences documents based single event sequence events nodes document entities relations correspond edges different relation types example entitys adjacent appears near alpha related semantics wordnet coref two coreferent heres example entity second paragraph leader entity called leaders another called tupac amaru connected adjacency chief victor polay connected adjacency chief leader connected alpha relation type kind graph used identify two documents contain entities summarized separately used technique called spreading activation similar one used mitraallen identify important nodes need highlighted summaries next paper yvgeny barzilay michael elhadad based idea lexical chains whats lexical chain lexical chain sequence words semantically connected synonyms hyponyms appear together heres example paper mr kenny person invented anesthetic machine uses microcomputers control rate anesthetic pumped blood period machines see machines second time nothing new device device word similar machine uses two microcomputers word used achieve much closer monitoring pump pump similar pump feeding anesthetic related word previous sentence finally patient patient related person several lexical chains one consistent machine machines device another one pumped pump idea identify longest lexical chains pick sentences contain many chains possible relations count extra strong corresponds extra repetitions exact word others considered strong example pseudonyms hypernyms others mediumstong links synsets link longer one scoring chains simple formula used based length homogeneity index homogeneity index tells many words come lexical chain sentences contain highest scores produced part output interesting work daniel marco part phd thesis university toronto later isi late approach summarization based text coherence specifically based called rhetorical structure theory mann thompson rhetorical structure theory connections adjacent utterances adjacent sentences sort rhetorical connection example say like cats period make feel happy second sentence make feel happy gives additional information reasons like cats rhetorical structure theory kinds relationships called rhetorical relations usually consist nucleus satellite nucleus important piece relation satellite optional piece relation example evidence nucleus first words followed satellite nucleus truth pressure smoke junior high greater time ones life thats claim know teens start smoking day satellite easily see nucleus contains important information one cannot omitted summary whereas satellite optional information omitted necessary based kind framework marco built rhetorical boxer used text summarization example tenant rhetorical structure theory says nucleus satellite combination increases readers belief nucleus purpose satellite make nucleus stronger lets see use technique automatic summarization short article weather conditions mars separated ten individual chunks honesty doesnt work level sentence rather works level clauses utterances thats example sentences split multiple utterances read story left right going one ten see want build rhetorical parse tree corresponds document see use summarize summary going look like want pick one two three perhaps utterances contain important information document going use rhetorical structure document determine ones important okay heres slightly heavy shuffled version document easily build rhetorical tree still idea read document left right numbers okay whats connection utterance utterance well turns instance background justification number sentence number nucleus sentence number satellite similarly sentences five six combined using evidence cause nucleus five contrast binuclear relation thats propagate four five next level binuclear relation input clauses nuclei elaboration consists centers number three nucleus whats left four five six satellite join two three using elaboration relation two nucleus right hand side combine seven eight using concession eight nucleus combine nine ten using antithesis relation ten nucleus combine concession eight antithesis ten example relation eight nucleus finally well combine two existing topmost nodes two elaboration eight example using another instance elaboration previous one nucleus number two see root document sentence number two wanted produce summary consists one sentence would go reverse tree starting root going going pick sentence number two summary case thats sentence mars experiences frigid weather conditions according model sentence contains information one picked limited summary size one let explain little bit detail ive used following notation dashed box dashed line indicates satellite relation solid boxes solid line indicates nucleus relation first sentence picked want produce one sentence summary said sentence number two get sentence traversing nucleus edges starting root want pick next sentences allowing one satellite addition nuclei takes us sentences number three number eight sentence three satellite nucleus sentence number eight nucleus satellite sentences second important document okay next type approaches summarization deal noisy channel models mentioned noisy channel models different section going briefly remember noisy channel model concept source target language example english french want consider one garbled version one garbled version obtained kind coding process example converts english french want translate back french english decode string get back original sentence english noisy channel becomes french recover decode get likely english sentence corresponds french sentence papers around vigumita berger michael woodblock people something called ultra summarization headline generation consider document one languages headline title document shorter version translated version document one systems called ocelot used type summarization called gisting idea find gist maximizes probability gist given document using bayesian model corresponds product two models one language model gists want gist grammatical english sentence also want gist highly correlated document therefore contain words important indicative document paper idea training set summary document pairs testing thousand pairs using viterbi decoding defined likely translation valuation used word overlap original headlines documents test sent heres example output summarizations system given document birding society savannah headline produced fairly large document keeping important words yet still preserving grammatical output possible one paper time period harvey carbonell jay goldstein circa introduced idea maximal marginal relevance mmr confused mrr metric question answering mmr tells even two documents relevant query two documents similar dont want show user youve shown one user value marginal value second one actually small dont want get example ten copies document output query even though fairly relevant similar would rather different documents individually may relevant would still different enough previous ones give diversity mmr essentially greedy selection method applies querybased summaries based idea diminishing returns law dont want get many good documents similar heres example suppose c collection documents q user query r like relevance based search engine given query collection documents already retrieved example lets say already retrieved one relevant document want decide document want retrieve say document essentially information scenario metric applied summarization deals sentences case first sentence similar query going stage going pick second sentence return cases document sentence want pick something relevant query different sentences documents already produced mmr value actually computed looks like documents sub belongs relevant set r given documents already retrieved see mixture two different scores one similar document original query thats sim sub query second one similar sentence sentences already selected varying product alpha one put less emphasis diversity value important principle influential information retrieval summerization many applications resulted whole research area diversity based reranking also appears machine learning community user retrieval communities another paper time period mead developed around general purpose framework extractive summarization based salience salience mead means idea centroid collection sentences im going explain next slides want say mead extractive works well many different languages heres centroid based method works suppose set statuses different documents presented dots vector space want plaster topics lets say one cluster left one cluster right red circles represent called centroids centers mass individual clusters pick cluster sentences similar centroid possibly performing diversity based reranking using mmr pick sentences remain similar centroids different one another method works single multidocument summarization mead general uses additional features centroid also uses position example whether sentence first one document length sentence short dont want include also uses technique called crossdocument structure theory want explain little bit later way mead one commonly used methods extracting summarization includes open source library downloaded wwwsummarizationcom lets see centroid works principle vector space presentation documents sentences alpha measure similarity given document sentence centroid cluster similarity measured using standard cosine similarity formula document centroid essentially normalized doc product vector corresponding document vector corresponding centroid input mead cluster documents n sentences compression rate equal r output nr sentences cluster ones highest scores linear combination centroid positional features mead used early newsinessense system summarized news articles web snapshot system appeared see left contains summary right large block white background text middle right hand side different clusters correspond different topics day systems around time built web based news summarization im going show next slides one newsblaster columbia mckeown et al still existence another one started around time google news also still existence day okay going conclude current segment text summarization segments later,Course3,W9-S1-L3,W9,S1,L3,-,9,1,3,okay go continu paper summar first one mitraallen group cornel univers first paper use graphbas summar techniqu use first time corpu encyclopedia articl idea want repres sentenc encyclopedia articl node graph connect node use semant hyperlink content sentenc overlap idea overlap defin lexic similar threshold exampl multipl word common idea path link highli connect paragraph like contain inform central topic articl kind represent start first sentenc document follow path connect highli connect paragraph your go produc reason summari encyclopedia articl okay go continu techniqu late next one mani bloedorn miter essenti second time peopl look multidocu summar also use graphbas method identifi similar differ document base singl event sequenc event node document entiti relat correspond edg differ relat type exampl entiti adjac appear near alpha relat semant wordnet coref two corefer here exampl entiti second paragraph leader entiti call leader anoth call tupac amaru connect adjac chief victor polay connect adjac chief leader connect alpha relat type kind graph use identifi two document contain entiti summar separ use techniqu call spread activ similar one use mitraallen identifi import node need highlight summari next paper yvgeni barzilay michael elhadad base idea lexic chain what lexic chain lexic chain sequenc word semant connect synonym hyponym appear togeth here exampl paper mr kenni person invent anesthet machin use microcomput control rate anesthet pump blood period machin see machin second time noth new devic devic word similar machin use two microcomput word use achiev much closer monitor pump pump similar pump feed anesthet relat word previou sentenc final patient patient relat person sever lexic chain one consist machin machin devic anoth one pump pump idea identifi longest lexic chain pick sentenc contain mani chain possibl relat count extra strong correspond extra repetit exact word other consid strong exampl pseudonym hypernym other mediumstong link synset link longer one score chain simpl formula use base length homogen index homogen index tell mani word come lexic chain sentenc contain highest score produc part output interest work daniel marco part phd thesi univers toronto later isi late approach summar base text coher specif base call rhetor structur theori mann thompson rhetor structur theori connect adjac utter adjac sentenc sort rhetor connect exampl say like cat period make feel happi second sentenc make feel happi give addit inform reason like cat rhetor structur theori kind relationship call rhetor relat usual consist nucleu satellit nucleu import piec relat satellit option piec relat exampl evid nucleu first word follow satellit nucleu truth pressur smoke junior high greater time one life that claim know teen start smoke day satellit easili see nucleu contain import inform one cannot omit summari wherea satellit option inform omit necessari base kind framework marco built rhetor boxer use text summar exampl tenant rhetor structur theori say nucleu satellit combin increas reader belief nucleu purpos satellit make nucleu stronger let see use techniqu automat summar short articl weather condit mar separ ten individu chunk honesti doesnt work level sentenc rather work level claus utter that exampl sentenc split multipl utter read stori left right go one ten see want build rhetor pars tree correspond document see use summar summari go look like want pick one two three perhap utter contain import inform document go use rhetor structur document determin one import okay here slightli heavi shuffl version document easili build rhetor tree still idea read document left right number okay what connect utter utter well turn instanc background justif number sentenc number nucleu sentenc number satellit similarli sentenc five six combin use evid caus nucleu five contrast binuclear relat that propag four five next level binuclear relat input claus nuclei elabor consist center number three nucleu what left four five six satellit join two three use elabor relat two nucleu right hand side combin seven eight use concess eight nucleu combin nine ten use antithesi relat ten nucleu combin concess eight antithesi ten exampl relat eight nucleu final well combin two exist topmost node two elabor eight exampl use anoth instanc elabor previou one nucleu number two see root document sentenc number two want produc summari consist one sentenc would go revers tree start root go go pick sentenc number two summari case that sentenc mar experi frigid weather condit accord model sentenc contain inform one pick limit summari size one let explain littl bit detail ive use follow notat dash box dash line indic satellit relat solid box solid line indic nucleu relat first sentenc pick want produc one sentenc summari said sentenc number two get sentenc travers nucleu edg start root want pick next sentenc allow one satellit addit nuclei take us sentenc number three number eight sentenc three satellit nucleu sentenc number eight nucleu satellit sentenc second import document okay next type approach summar deal noisi channel model mention noisi channel model differ section go briefli rememb noisi channel model concept sourc target languag exampl english french want consid one garbl version one garbl version obtain kind code process exampl convert english french want translat back french english decod string get back origin sentenc english noisi channel becom french recov decod get like english sentenc correspond french sentenc paper around vigumita berger michael woodblock peopl someth call ultra summar headlin gener consid document one languag headlin titl document shorter version translat version document one system call ocelot use type summar call gist idea find gist maxim probabl gist given document use bayesian model correspond product two model one languag model gist want gist grammat english sentenc also want gist highli correl document therefor contain word import indic document paper idea train set summari document pair test thousand pair use viterbi decod defin like translat valuat use word overlap origin headlin document test sent here exampl output summar system given document bird societi savannah headlin produc fairli larg document keep import word yet still preserv grammat output possibl one paper time period harvey carbonel jay goldstein circa introduc idea maxim margin relev mmr confus mrr metric question answer mmr tell even two document relev queri two document similar dont want show user youv shown one user valu margin valu second one actual small dont want get exampl ten copi document output queri even though fairli relev similar would rather differ document individu may relev would still differ enough previou one give divers mmr essenti greedi select method appli querybas summari base idea diminish return law dont want get mani good document similar here exampl suppos c collect document q user queri r like relev base search engin given queri collect document alreadi retriev exampl let say alreadi retriev one relev document want decid document want retriev say document essenti inform scenario metric appli summar deal sentenc case first sentenc similar queri go stage go pick second sentenc return case document sentenc want pick someth relev queri differ sentenc document alreadi produc mmr valu actual comput look like document sub belong relev set r given document alreadi retriev see mixtur two differ score one similar document origin queri that sim sub queri second one similar sentenc sentenc alreadi select vari product alpha one put less emphasi divers valu import principl influenti inform retriev summer mani applic result whole research area divers base rerank also appear machin learn commun user retriev commun anoth paper time period mead develop around gener purpos framework extract summar base salienc salienc mead mean idea centroid collect sentenc im go explain next slide want say mead extract work well mani differ languag here centroid base method work suppos set status differ document present dot vector space want plaster topic let say one cluster left one cluster right red circl repres call centroid center mass individu cluster pick cluster sentenc similar centroid possibl perform divers base rerank use mmr pick sentenc remain similar centroid differ one anoth method work singl multidocu summar mead gener use addit featur centroid also use posit exampl whether sentenc first one document length sentenc short dont want includ also use techniqu call crossdocu structur theori want explain littl bit later way mead one commonli use method extract summar includ open sourc librari download wwwsummarizationcom let see centroid work principl vector space present document sentenc alpha measur similar given document sentenc centroid cluster similar measur use standard cosin similar formula document centroid essenti normal doc product vector correspond document vector correspond centroid input mead cluster document n sentenc compress rate equal r output nr sentenc cluster one highest score linear combin centroid posit featur mead use earli newsinessens system summar news articl web snapshot system appear see left contain summari right larg block white background text middl right hand side differ cluster correspond differ topic day system around time built web base news summar im go show next slide one newsblast columbia mckeown et al still exist anoth one start around time googl news also still exist day okay go conclud current segment text summar segment later,[ 2  4 11 13  8]
299,Course3_W9-S1-L4_-_Summarization_Techniques_3-3_-_17_slides_10-23,okay going continue additional techniques text summarization im going go briefly papers frist one john conroy diane oleary uses hidden markov models text summarization idea want take account local dependencies sentences idea dont want include sentences summary randomly independent one another often put sentence decide whether sentence also included features used things like position number terms similarity document terms similar centroid idea mead paper hmm alternates summary nonsummary states portability staying summary state leaving summary state going nonsummary state four possible combinations heres example conroy oleary paper green blue sentences tell whether want include sentence summary next paper miles osborne first one take account fact features used previous papers actually dependent techniques like necessarily best ones case used maxent loglinear model take account independence different features got better performance naive bayes features used sentence length sentence position whether sentence inside introduction document whether inside conclusion next paper erkan radev published journal artificial intelligence research jair first paper metal cobased random walks multidocument summarization technique also works single document summaries radev used something called lexical centrality lexical centrality means sentence likely visited random process similarity graph corresponding sentences set documents sentence worth putting summary steps following present text graph sentences connected lot words common use standard graph centrality metric example centrality vector centrality determine top sentences one components lexrank graph clustering want pick central sentences want also segment graph units correspond different themes example collection sentences different documents correspond event first one ds means sentence one document one second one sentence one document two total build similarity matrix corresponds different pairs sentences input obvious diagonal entries ones however also interested high values diagonal example theres value sentences sentence sentence going strongly connected graph lets see compute cosines centrality graph using cosine cutoff notes corresponds one sentences note sentence pairs similarity connected see graph still fairly disconnected theres much useful information gained structure lower cutoff cosine similarity going see much better structure fact would obvious point side notes ds highly connected rest graph whereas sentences like ds ds highly connected keep lowering threshold going get situation almost everything connected everything dont want go far ekran radev paper found threshold gives best information value graph theres approximately half connections actually present half present graph like want sentences vote central sentence essentially passing messages along edges graph ds central sentence want produce part summary im going discuss little bit advanced material skip part dont feel comfortable linear algebra used heres lexrank works lexrank lexical strategy method used ekran radev paper based square connectivity matrix node corresponds sentence either directed undirected eigenvalue square matrix scalar lambda exists vector x vector product ax matrix vector equal product scalar lambda vector thats sort implicit direction matrix normalized eigenvector associated largest lambda called principal eigenvector alpha matrix called stochastic matrix sum entries row sum none negative form probability distribution theorem says stochastic matrices principal eigenvector connectivity matrix kind setup similar one used pagerank document ranking thats system behind google also known reducible one use integrative power method compute principal eigenvector pretty much arbitrarily large matrices eigenvector corresponds stationary value markov stochastic process described connectivity matrix since random walk north matrix proportion weight edges stationary value markov matrixs computed power method power method something straightforward p vector values correspond centralities nodes e transposed transpose connectivity matrix eigenvector formula p etransposed p also write etransposed p matrix diagonal everywhere else pagerank also added twist dead end pages end note doesnt outgoing edges possible probability ɛ start randomly different page value node p v vertices equal minus epsilon divided n probability teleportation random jump plus epsilon sum normalized values centrality adjacent nodes pr nodes connected v eigenvector centrality computed following way paths random walk weighted centrality nodes path connects general lexon method found successful evaluation summarization based doc new summarization compass official evaluation used many years mid next paper want mention briefly gong liu first paper uses latent semantic analysis lsa something talked past class works single multidocument summarization cases doesnt use explicit semantics linguistics example wordnet document represented word sentence matrix row corresponds word column corresponds sentence weight matrix based tf idf value words lsa remember previous lecture based singular value decomposition want represent matrix product u sigma v transposed rows v independent topics correspond documents want pick sentences contain independent topics thats summary gong liu method works going continue evaluation summarization next session,Course3,W9-S1-L4,W9,S1,L4,-,9,1,4,okay go continu addit techniqu text summar im go go briefli paper frist one john conroy dian oleari use hidden markov model text summar idea want take account local depend sentenc idea dont want includ sentenc summari randomli independ one anoth often put sentenc decid whether sentenc also includ featur use thing like posit number term similar document term similar centroid idea mead paper hmm altern summari nonsummari state portabl stay summari state leav summari state go nonsummari state four possibl combin here exampl conroy oleari paper green blue sentenc tell whether want includ sentenc summari next paper mile osborn first one take account fact featur use previou paper actual depend techniqu like necessarili best one case use maxent loglinear model take account independ differ featur got better perform naiv bay featur use sentenc length sentenc posit whether sentenc insid introduct document whether insid conclus next paper erkan radev publish journal artifici intellig research jair first paper metal cobas random walk multidocu summar techniqu also work singl document summari radev use someth call lexic central lexic central mean sentenc like visit random process similar graph correspond sentenc set document sentenc worth put summari step follow present text graph sentenc connect lot word common use standard graph central metric exampl central vector central determin top sentenc one compon lexrank graph cluster want pick central sentenc want also segment graph unit correspond differ theme exampl collect sentenc differ document correspond event first one ds mean sentenc one document one second one sentenc one document two total build similar matrix correspond differ pair sentenc input obviou diagon entri one howev also interest high valu diagon exampl there valu sentenc sentenc sentenc go strongli connect graph let see comput cosin central graph use cosin cutoff note correspond one sentenc note sentenc pair similar connect see graph still fairli disconnect there much use inform gain structur lower cutoff cosin similar go see much better structur fact would obviou point side note ds highli connect rest graph wherea sentenc like ds ds highli connect keep lower threshold go get situat almost everyth connect everyth dont want go far ekran radev paper found threshold give best inform valu graph there approxim half connect actual present half present graph like want sentenc vote central sentenc essenti pass messag along edg graph ds central sentenc want produc part summari im go discuss littl bit advanc materi skip part dont feel comfort linear algebra use here lexrank work lexrank lexic strategi method use ekran radev paper base squar connect matrix node correspond sentenc either direct undirect eigenvalu squar matrix scalar lambda exist vector x vector product ax matrix vector equal product scalar lambda vector that sort implicit direct matrix normal eigenvector associ largest lambda call princip eigenvector alpha matrix call stochast matrix sum entri row sum none neg form probabl distribut theorem say stochast matric princip eigenvector connect matrix kind setup similar one use pagerank document rank that system behind googl also known reduc one use integr power method comput princip eigenvector pretti much arbitrarili larg matric eigenvector correspond stationari valu markov stochast process describ connect matrix sinc random walk north matrix proport weight edg stationari valu markov matrix comput power method power method someth straightforward p vector valu correspond central node e transpos transpos connect matrix eigenvector formula p etranspos p also write etranspos p matrix diagon everywher els pagerank also ad twist dead end page end note doesnt outgo edg possibl probabl ɛ start randomli differ page valu node p v vertic equal minu epsilon divid n probabl teleport random jump plu epsilon sum normal valu central adjac node pr node connect v eigenvector central comput follow way path random walk weight central node path connect gener lexon method found success evalu summar base doc new summar compass offici evalu use mani year mid next paper want mention briefli gong liu first paper use latent semant analysi lsa someth talk past class work singl multidocu summar case doesnt use explicit semant linguist exampl wordnet document repres word sentenc matrix row correspond word column correspond sentenc weight matrix base tf idf valu word lsa rememb previou lectur base singular valu decomposit want repres matrix product u sigma v transpos row v independ topic correspond document want pick sentenc contain independ topic that summari gong liu method work go continu evalu summar next session,[ 4  2  5 14 13]
300,Course3_W9-S1-L5_-_Summarization_Evaluation_-_24_slides_25-49,okay moving summary evaluation summarization interesting part natural language processing sub summarization interesting evaluation evaluate good summaries well many different criteria right length dont want something thats way short long faithful original documents balance capture salient information grammatical nonredundant wellformed referentially means right connectives doesnt make false implicatures structured properly coherent important evaluation criteria work research community specific summarization example grammaticality coherence also apply text generation general also machine translation even question answering whereas length salience others specific text summarization lets step little bit back think would ideal evaluation summary summary really achieve well idea compression ratio retention ratio compression ratio essentially percentage original document preserved summary retention ratio percentage information document preserved summary want compress much possible retaining much information possible ideally want retention ratio larger possibly much larger compression ratio summary preserves original content would really good summarizer different evaluation methods used well several categories first one extrinsic technique taskbased techniques example give one set users full documents ask classify using different categories give another set users summaries documents give task users access shorter documents able classification task well access full documents perhaps take even less time means summary done good job task could something different could question answering routing another category techniques evaluation summaries called intrinsic techniques case take output summarizer compare possibly golden standard summary another summary important technique intrinsic evaluation based precision recall probably going see description multiple times class precision recall important word disambiguation parsing organization many tasks let give little bit background two two table two rows first row corresponds cases system thinks certain document relevant query second row matches documents system thinks relevant query first column talks documents relevant second one one relevant example gave document retrieval relate summarization well case gold summary example two sentences speak human judges two sentence automatic summary consists possibly sentences case relevant corresponds sentences gold standard summary nonrelevant means sentences system relevant means sentence produced part output summarizer system nonrelevant corresponds sentences produced summarizer numbers give us good performance cases high b c low better system going okay metrics going use precision recall precision p divided ab ab sentences would produce part summary want see many goal standard recall divided ac sentences goal standard many pick also define f f harmonic mean precision recall purpose using f single metric dont try optimize either p r expense one another technique used summary evaluation rouge rouge introduced chinyew lin ed hovy automatic method easy prototype even though number flaws widely adopted summarization community convenient easy prototype based papinenis paper bleu another metric based content used machine translation let explain difference two bleu youre trying get get high score output high precision terms one grams bigrams trigrams compared reference text rouge high score recall original content gold standard content high r rouge actually stands recall rougen variant rouge used measure ngram overlap automatic summary set reference summaries idea dont expect one great gold standard summary allow possibility different human judges going produce different summaries going different one another want system exactly particular one human reference summaries rather similar time rougel uses instead ngram overlap uses longest common subsequence words automatic summary reference summaries rouge useful shown correlate manual evaluations least averaged many different sentences important well manual evaluations expensive whereas rouge automatic computed easily thing need gold standard summaries multiple humans rouge quality easily gamed something much concern another metric related rouge precision recall relative utility relative utility takes account fact may multiple correct summaries multiple goal summaries set documents let give example suppose one reference summary two sentences picked ten thats ideal column example gold standard suppose two systems system one system two system one picks two sentences one two system two picks sentences two four use precision recall system one going get precision recall picks exactly sentences whereas system two going get precision recall picks one two sentences one two sentences picks correct sense well say system two way worse system one may may case suppose hypothetically summary really consisted sentence one sentences two four level importance ideal summarizer supposed pick two sentences picked one two another evaluator perhaps evaluator asked would picked different set maybe turns somehow training us picking two sentences ten case dont want penalize system two much lucky didnt pick exactly sentences ideal summary came pretty close lets look specific example ask judges give us plus minus yes decision sentence rather give us utility score sentence important summary lets see changes picture right hand side left column ideal summary consists two sentences judge explicitly said utility sentence one ten utility sentence two eight also given utility scores sentences three four see still best sentences one picked sentence four instead sentence two maybe instead sentence one total utilities selected two sentences would bad may think pick sentences one four get utility points compared utility points picked sentence one sentence two youre essentially getting thats pretty good shouldnt penalized pick sentence four instead sentence two penalty get case one point gets eight seven thats basic idea relative utility utility actually percentage ideal utility system summary picks utility came expanded concept relative utility takes account also difficulty task many problems sort lower bound random performance system pick two random sentences ten score would get also upper bound corresponds human judges get others gold standards pick two sentences ten okay lets see compute relative utility suppose three judges judge one two three four sentences want judge pick two sentences summary numbers table represent utilities judge gives different sentences judge gives utilities ten eight two five respectively pick two sentences would pick sentences one two judge scores five eight four nine pick two sentences would pick sentence two sentence four experiment pick judge gold standard measure performance judge pick judge reference judge performance judge lets look latter example compute relative utility heres best utility get comparing judge points pick sentences two four judge would pick sentences one two however two sentences according judges utility judgements get score relative utility judge gets judge since three judges six pairs relativity scores computed way compute performance judge judges either judge judge picked exact two sentences therefore scores one judge would get judge would judge judge average values would get judges average performance judge number two judge gets slightly lower performance average numbers together get called interjudge agreement utility selection would give upward bound systems performance dont expect system perform better judges okay get punch line relative utility going compute following formula srjr r random performance want explain next slide essentially lower bound performance automatic system system performance actual utility performance system current data set j average judge scores normalized system performance lets see special cases j means system good interjudge agreement srjr equal one highest possible value expect get sr hand means numerator formula zero divided something thats nonzero going get normalized performance zero system going get performance zero one cases gets bit lower random thats possible theory going assume lucky even worst performance random get something higher judges also ignore cannot evaluate something works better judges agree compute random performance straight forward take possible systems pick r n average systems shown top example compression rate means two sentences four six possible random outputs one responds sentences compute utility score average get value r heres example sentence system picks sentences one four going thats utility three judges pick sentences one four random performance interjudge agreement according relativity formula get average performance another example system pick sentences formula would give relativity score better quantities nice way visualize system picks sentences performance thats middle dot scale agreement upper dot performance lower dot scale range careful three dots going appear follows r going zero j going equal one going relativity normalized relativity system actually pretty good much similar judges random performance lets look one issue introduced mead paper idea subsumption across doctrines news story sentences another story sentences similar topic day different source one thing notice sentences lefthand side somehow match sentences righthand side sentence one left gives almost information sentence one right sentences two three four left combined give information sentence umber two right sentences three four right combined give information sentence nine left idea behind example two sentences contain roughly information according diversity ranking principle dont want include summary thats people compute performance information extraction summarization systems take account redundancy lets see related subsumption equivalence subsumption information content sentence contained within sentence b becomes redundant context b information content subsumed information content b skip sentence sentence b included summary hand sentence b included summary value still also define equivalence subsumed b b subsumed say sentences b equivalent either one would equally good summary okay example sentence one john doe found guilty murder sentence court found john doe guilty murder jane doe last august sentenced life see second sentence includes four five different facts whereas first one includes one one fact definitely included second sentence well say sentence subsumes sentence sentence subsumption used evaluating summaries lets look previous example speaking sentences instead judges columns going different documents essentially set different sentences want determine ones pick summary wanted pick two sentences right ones would pick well pretty obvious would want pick sentence article sentence article total utility points imagine sentence one article one subsumes sentence one article two similarly sentence four article two subsumes sentences therefore article three case two sentences going pick wanted produce two sentence summary well probably still going pick sentence one article one score dont want pick sentence one article two second sentence irrelevant useless instead going pick either sentence two article two maybe sentence four article three would give us nine points instead ten least thats best get presence redundancy modifying formula mean sum different features sentence position frequency want also discount redundant sentences ones subsumed others formula used mead based jacquard coefficient two sentences another metric evaluating summaries socalled pyramid method pyramid method introduced ani nenkova rebecca passonneau used multiple document summarization based idea semantic content units semantic content units well deal different statements facts realized using different formulations really two different ways say exact thing heres example four inputs b c lines four difference sentences text underlined repeated one content unit scu appears four sentences fact two libyans officially accused lockerbie bombing appears four sentences second scu scu number appears three four sentences fact indictment two lockerbie suspects happened weight scu weight scu equal means want produce summary summary happens mention fact two libyans officially accused lockerbie bombing get four points instead tells indited get three points everything include unit included summary gets points proportion number reference documents appears heres example nenkova passonneau paper kind pyramid scus appear documents shown top pyramids ones appear ones bottom pyramid ones appear one reference documents optimal summaries include four scus well clearly every optimal summary would include two scus weight four since room two seus would pick two ones weight three two four seus weight three would equally good shouldnt penalized pick different set one used reference sum variant sense relative utility much knowledgebased based actual understanding document whereas based importance entire sentences okay conclude section want mention important corpora available text summarization used many different papers evaluation purposes first ones mention ones used duc tac duc tac niche based evaluations similar trek remember trek text retrieval conference whereas duc document understanding conference superseded tac text analysis conference popular last ten years corpora use include single multidocument summarization corpora available duc tac respective websites another corpus sumbank includes english chinese language documents summaries manual automatic valuations based relativity precision recall many metrics one available summarizationcom theres also summac corpus new york times corpus collection news documents non extractive summaries documents corpus available linguistic data consortium university pennsylvania many corpora available text summarization evaluation would like focus think influential ones concludes section evaluation summarization next segment going sentence compression,Course3,W9-S1-L5,W9,S1,L5,-,9,1,5,okay move summari evalu summar interest part natur languag process sub summar interest evalu evalu good summari well mani differ criteria right length dont want someth that way short long faith origin document balanc captur salient inform grammat nonredund wellform referenti mean right connect doesnt make fals implicatur structur properli coher import evalu criteria work research commun specif summar exampl grammat coher also appli text gener gener also machin translat even question answer wherea length salienc other specif text summar let step littl bit back think would ideal evalu summari summari realli achiev well idea compress ratio retent ratio compress ratio essenti percentag origin document preserv summari retent ratio percentag inform document preserv summari want compress much possibl retain much inform possibl ideal want retent ratio larger possibl much larger compress ratio summari preserv origin content would realli good summar differ evalu method use well sever categori first one extrins techniqu taskbas techniqu exampl give one set user full document ask classifi use differ categori give anoth set user summari document give task user access shorter document abl classif task well access full document perhap take even less time mean summari done good job task could someth differ could question answer rout anoth categori techniqu evalu summari call intrins techniqu case take output summar compar possibl golden standard summari anoth summari import techniqu intrins evalu base precis recal probabl go see descript multipl time class precis recal import word disambigu pars organ mani task let give littl bit background two two tabl two row first row correspond case system think certain document relev queri second row match document system think relev queri first column talk document relev second one one relev exampl gave document retriev relat summar well case gold summari exampl two sentenc speak human judg two sentenc automat summari consist possibl sentenc case relev correspond sentenc gold standard summari nonrelev mean sentenc system relev mean sentenc produc part output summar system nonrelev correspond sentenc produc summar number give us good perform case high b c low better system go okay metric go use precis recal precis p divid ab ab sentenc would produc part summari want see mani goal standard recal divid ac sentenc goal standard mani pick also defin f f harmon mean precis recal purpos use f singl metric dont tri optim either p r expens one anoth techniqu use summari evalu roug roug introduc chinyew lin ed hovi automat method easi prototyp even though number flaw wide adopt summar commun conveni easi prototyp base papineni paper bleu anoth metric base content use machin translat let explain differ two bleu your tri get get high score output high precis term one gram bigram trigram compar refer text roug high score recal origin content gold standard content high r roug actual stand recal rougen variant roug use measur ngram overlap automat summari set refer summari idea dont expect one great gold standard summari allow possibl differ human judg go produc differ summari go differ one anoth want system exactli particular one human refer summari rather similar time rougel use instead ngram overlap use longest common subsequ word automat summari refer summari roug use shown correl manual evalu least averag mani differ sentenc import well manual evalu expens wherea roug automat comput easili thing need gold standard summari multipl human roug qualiti easili game someth much concern anoth metric relat roug precis recal rel util rel util take account fact may multipl correct summari multipl goal summari set document let give exampl suppos one refer summari two sentenc pick ten that ideal column exampl gold standard suppos two system system one system two system one pick two sentenc one two system two pick sentenc two four use precis recal system one go get precis recal pick exactli sentenc wherea system two go get precis recal pick one two sentenc one two sentenc pick correct sens well say system two way wors system one may may case suppos hypothet summari realli consist sentenc one sentenc two four level import ideal summar suppos pick two sentenc pick one two anoth evalu perhap evalu ask would pick differ set mayb turn somehow train us pick two sentenc ten case dont want penal system two much lucki didnt pick exactli sentenc ideal summari came pretti close let look specif exampl ask judg give us plu minu ye decis sentenc rather give us util score sentenc import summari let see chang pictur right hand side left column ideal summari consist two sentenc judg explicitli said util sentenc one ten util sentenc two eight also given util score sentenc three four see still best sentenc one pick sentenc four instead sentenc two mayb instead sentenc one total util select two sentenc would bad may think pick sentenc one four get util point compar util point pick sentenc one sentenc two your essenti get that pretti good shouldnt penal pick sentenc four instead sentenc two penalti get case one point get eight seven that basic idea rel util util actual percentag ideal util system summari pick util came expand concept rel util take account also difficulti task mani problem sort lower bound random perform system pick two random sentenc ten score would get also upper bound correspond human judg get other gold standard pick two sentenc ten okay let see comput rel util suppos three judg judg one two three four sentenc want judg pick two sentenc summari number tabl repres util judg give differ sentenc judg give util ten eight two five respect pick two sentenc would pick sentenc one two judg score five eight four nine pick two sentenc would pick sentenc two sentenc four experi pick judg gold standard measur perform judg pick judg refer judg perform judg let look latter exampl comput rel util here best util get compar judg point pick sentenc two four judg would pick sentenc one two howev two sentenc accord judg util judgement get score rel util judg get judg sinc three judg six pair rel score comput way comput perform judg judg either judg judg pick exact two sentenc therefor score one judg would get judg would judg judg averag valu would get judg averag perform judg number two judg get slightli lower perform averag number togeth get call interjudg agreement util select would give upward bound system perform dont expect system perform better judg okay get punch line rel util go comput follow formula srjr r random perform want explain next slide essenti lower bound perform automat system system perform actual util perform system current data set j averag judg score normal system perform let see special case j mean system good interjudg agreement srjr equal one highest possibl valu expect get sr hand mean numer formula zero divid someth that nonzero go get normal perform zero system go get perform zero one case get bit lower random that possibl theori go assum lucki even worst perform random get someth higher judg also ignor cannot evalu someth work better judg agre comput random perform straight forward take possibl system pick r n averag system shown top exampl compress rate mean two sentenc four six possibl random output one respond sentenc comput util score averag get valu r here exampl sentenc system pick sentenc one four go that util three judg pick sentenc one four random perform interjudg agreement accord rel formula get averag perform anoth exampl system pick sentenc formula would give rel score better quantiti nice way visual system pick sentenc perform that middl dot scale agreement upper dot perform lower dot scale rang care three dot go appear follow r go zero j go equal one go rel normal rel system actual pretti good much similar judg random perform let look one issu introduc mead paper idea subsumpt across doctrin news stori sentenc anoth stori sentenc similar topic day differ sourc one thing notic sentenc lefthand side somehow match sentenc righthand side sentenc one left give almost inform sentenc one right sentenc two three four left combin give inform sentenc umber two right sentenc three four right combin give inform sentenc nine left idea behind exampl two sentenc contain roughli inform accord divers rank principl dont want includ summari that peopl comput perform inform extract summar system take account redund let see relat subsumpt equival subsumpt inform content sentenc contain within sentenc b becom redund context b inform content subsum inform content b skip sentenc sentenc b includ summari hand sentenc b includ summari valu still also defin equival subsum b b subsum say sentenc b equival either one would equal good summari okay exampl sentenc one john doe found guilti murder sentenc court found john doe guilti murder jane doe last august sentenc life see second sentenc includ four five differ fact wherea first one includ one one fact definit includ second sentenc well say sentenc subsum sentenc sentenc subsumpt use evalu summari let look previou exampl speak sentenc instead judg column go differ document essenti set differ sentenc want determin one pick summari want pick two sentenc right one would pick well pretti obviou would want pick sentenc articl sentenc articl total util point imagin sentenc one articl one subsum sentenc one articl two similarli sentenc four articl two subsum sentenc therefor articl three case two sentenc go pick want produc two sentenc summari well probabl still go pick sentenc one articl one score dont want pick sentenc one articl two second sentenc irrelev useless instead go pick either sentenc two articl two mayb sentenc four articl three would give us nine point instead ten least that best get presenc redund modifi formula mean sum differ featur sentenc posit frequenc want also discount redund sentenc one subsum other formula use mead base jacquard coeffici two sentenc anoth metric evalu summari socal pyramid method pyramid method introduc ani nenkova rebecca passonneau use multipl document summar base idea semant content unit semant content unit well deal differ statement fact realiz use differ formul realli two differ way say exact thing here exampl four input b c line four differ sentenc text underlin repeat one content unit scu appear four sentenc fact two libyan offici accus lockerbi bomb appear four sentenc second scu scu number appear three four sentenc fact indict two lockerbi suspect happen weight scu weight scu equal mean want produc summari summari happen mention fact two libyan offici accus lockerbi bomb get four point instead tell indit get three point everyth includ unit includ summari get point proport number refer document appear here exampl nenkova passonneau paper kind pyramid scu appear document shown top pyramid one appear one bottom pyramid one appear one refer document optim summari includ four scu well clearli everi optim summari would includ two scu weight four sinc room two seu would pick two one weight three two four seu weight three would equal good shouldnt penal pick differ set one use refer sum variant sens rel util much knowledgebas base actual understand document wherea base import entir sentenc okay conclud section want mention import corpora avail text summar use mani differ paper evalu purpos first one mention one use duc tac duc tac nich base evalu similar trek rememb trek text retriev confer wherea duc document understand confer supersed tac text analysi confer popular last ten year corpora use includ singl multidocu summar corpora avail duc tac respect websit anoth corpu sumbank includ english chines languag document summari manual automat valuat base rel precis recal mani metric one avail summarizationcom there also summac corpu new york time corpu collect news document non extract summari document corpu avail linguist data consortium univers pennsylvania mani corpora avail text summar evalu would like focu think influenti one conclud section evalu summar next segment go sentenc compress,[ 4  2  0 14 13]
301,Course3_W9-S1-L6_-_Sentence_Simplification_-_8_slides_05-30,next segment going sentence simplification sentence simplification one challenges text summarization far everything discussed involved extractive summarization take entire sentences put output going briefly look going beyond sentence extraction specifically taking entire sentences cutting pieces make shorter parts sentences could potentially go summaries may removed variety reasons one want space available important content order figure portion sentences least informative prior research found include things like quotes appositions remind apposition something like barack obama comma president united states comma second part sentence apposition ultimately removed without hurting information content summary things removed many cases adjectives adverbs embedded clauses often first go attribution clauses example said want sentence simplification many different applications one things like subtitling often theres enough space screen include entire sentence possible include portions instead headline generation mobile devices may want produce shorter version sentence display large font part summary mobile device user wants read click sentence instead text also multiple instances people built applications visually impaired people also involves sentence simplification im going go examples work sentence simplification starting paper kevin knight daniel marcu paper use syntactic information sentences compare two different approaches one noisy channel base model one based decision kind rules based constituent tree sentence example may want convert rule noun phrase goes determiner adjective noun one says noun phrase goes determiner noun thereby dropping adjective set rules like used approach corpus simplified sentences computer manual called ziff davis corpus heres examples corpus full sentence like documentation typical epson quality excellent also shorter version part training data documentation excellent see removed individual words example articles adjectives prepositional phrases case heres examples design goals achieved delivered performance matches speed underlying device full sentence short sentence design goals achieved missing possessive missing portions conjunctive clause also prepositional phrase okay examples im going read let read one beyond basic level operations three products vary widely skipping entire quote beyond basic level get operations three products vary widely one possible source information building sentence compression sentence simplification applications comes large corpus called simple english wikipedia let show looks like slide im going show entry person taken directly english wikipedia full length sentences see lot information long sentences lot additional information example oppositions prepositional phrases convey lot detail spend little bit time slide understand general structure document compare matching entry person simple english wikipedia would nice see side side figure often compare sentences one documents matches sentence one use alignment simplify sentences automatically many projects text simplification course going look going switch different topic,Course3,W9-S1-L6,W9,S1,L6,-,9,1,6,next segment go sentenc simplif sentenc simplif one challeng text summar far everyth discuss involv extract summar take entir sentenc put output go briefli look go beyond sentenc extract specif take entir sentenc cut piec make shorter part sentenc could potenti go summari may remov varieti reason one want space avail import content order figur portion sentenc least inform prior research found includ thing like quot apposit remind apposit someth like barack obama comma presid unit state comma second part sentenc apposit ultim remov without hurt inform content summari thing remov mani case adject adverb embed claus often first go attribut claus exampl said want sentenc simplif mani differ applic one thing like subtitl often there enough space screen includ entir sentenc possibl includ portion instead headlin gener mobil devic may want produc shorter version sentenc display larg font part summari mobil devic user want read click sentenc instead text also multipl instanc peopl built applic visual impair peopl also involv sentenc simplif im go go exampl work sentenc simplif start paper kevin knight daniel marcu paper use syntact inform sentenc compar two differ approach one noisi channel base model one base decis kind rule base constitu tree sentenc exampl may want convert rule noun phrase goe determin adject noun one say noun phrase goe determin noun therebi drop adject set rule like use approach corpu simplifi sentenc comput manual call ziff davi corpu here exampl corpu full sentenc like document typic epson qualiti excel also shorter version part train data document excel see remov individu word exampl articl adject preposit phrase case here exampl design goal achiev deliv perform match speed underli devic full sentenc short sentenc design goal achiev miss possess miss portion conjunct claus also preposit phrase okay exampl im go read let read one beyond basic level oper three product vari wide skip entir quot beyond basic level get oper three product vari wide one possibl sourc inform build sentenc compress sentenc simplif applic come larg corpu call simpl english wikipedia let show look like slide im go show entri person taken directli english wikipedia full length sentenc see lot inform long sentenc lot addit inform exampl opposit preposit phrase convey lot detail spend littl bit time slide understand gener structur document compar match entri person simpl english wikipedia would nice see side side figur often compar sentenc one document match sentenc one use align simplifi sentenc automat mani project text simplif cours go look go switch differ topic,[ 4  0  2  3 14]
