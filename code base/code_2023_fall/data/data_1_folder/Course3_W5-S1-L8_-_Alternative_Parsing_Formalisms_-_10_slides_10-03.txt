So we're now going to look at some
specific categories of grammars
that are not as powerful as full-fledged
context-sensitive grammars.
Because that would be too
difficult to parse and
at the same time are more powerful
than regular context-free grammars.
Therefore, they can capture some types
of languages such as a to the nth power,
b to the nth power, which is essentially
a string of a's followed by a string of
b's of the same length.
Which is something that context-free
grammars cannot capture.
So before I introduce those,
let me first mention briefly,
the so-called Tree Substitution Grammars.
Which are actually context-free grammars,
equivalent to context-free grammars.
Before I move on to the ones
that are not context free.
So Tree Substitution Grammar.
It allows you to have terminals that
can generate entire tree fragments.
And as I mentioned,
Tree Substitution Grammars are formerly
equivalent to context-free grammars.
However, the next step up from Tree
Substitution Grammars is the so-called
Tree Adjoining Grammar, which is more
powerful than context-free grammars.
And the other example that we're going
to look at is called Combinatory
Categorial Grammar, or CCG which is also
more powerful than context-free grammars.
Let's look at TAGs or tags first.
They're like tree substitution
grammars but they allow adjunction.
So what is adjunction?
Suppose that you have a parse tree with
the verb phrase embedded inside it.
The adjunction operator allows
you to create a new node and
put it inside the tree and
push the existing sub-tree further down.
So you have a new verb phrase that gets
the original verb phrase
as one of its descendents.
So this operation cannot be
modeled in context-free grammars.
And it can be used to generate
languages such as a to the nth,
power b to the nth power,
c to the nth power, or
even ww, which are the so-called
cross-serial dependencies.
So ww's a string that is repeated twice.
So for example, the sentence Mary gave
a book and a magazine to Chen and Mike.
This is an example of
a cross-serial dependency,
because book is associated with Chen and
magazine is associated with Mike.
So this kind of sentence cannot be
generated by context-free grammar but
it can be generated by TAG.
So the expressive power is that TAGs
are formally more powerful than CFGs.
But at the same time,
they're not as powerful as full
context-sensitive grammars.
Let me show you a little
bit of a diversion here.
There's a very interesting game that you
can download from the ltaggame website.
It's a bunch of cards with productions and
joining grammars that you can
combine together to form sentences.
I want something to notice here is that
the original set of cards include some
words that are not appropriate for
children.
Therefore, there is a slightly
more PG-rated version of ltaggame
designated for families.
You can also download
from the same website.
So the other category of
grammars is called CCG.
It's very popular these days for
both parsing and for generation.
It involves the introduction of
something called a complex type.
So complex type is either X/Y or X\Y.
So those complex types take an argument
of type Y and return an object of type X.
And the difference between the direction
of the slash is that if you have
a forward slash, X/Y, that means that
Y should appear on the right of X.
And if you have X\Y, that means that
Y should appear on the left of X.
So here's an example.
You have this simple grammar here.
I is the pronoun labelled as noun phrase.
Books is another noun,
labelled as noun phrase.
And now we have sleep.
So what is the type of sleep?
Well, it's a complex type that
says that sleep is the kind of
thing that if you combine it with
a noun phrase on the right-hand side,
it will return a sentence.
Similarly the Lexicon Antifour Enjoy
is something that
if you combine it on
the right with a non-phrase,
then it will return something of
the same complex type as sleep.
And here's an example of some sentences.
So the sentence, I sleep,
is parsed in the following way.
I is a noun phrase.
Sleep is S back, Word slash NP.
When the two are combined,
the result is an S,
I sleep is a valid sentence
according to this parse.
The sentence, I enjoy books, again
example here is of a transitive verb.
Enjoy gets combined with books by
replacing the noun phrase on the right and
returning the type on the left, so
the type for enjoy books is S\NP.
And then, when that gets combined
derive with I, it becomes S.
So this is a very interesting example,
because it shows you from the S
point of view, enjoy books, and sleep
have the exact same syntactic structure.
Again, they're both things that
can be combined with a subject
to form a sentence.
So the expressive power of CCGs is that
they can generate the language a to the n,
b to the n, c to the n, d to the n, for
n greater than 0, and this is, again,
something that context-free
grammars cannot do.
And I want to acknowledge
Jonathan Kummerfeld,
Aleka Blackwell, and
Patrick Littell for the example.
So the next topic is
going to be very brief.
It's about semantic parsing.
So everything that we've discussed so
far had to do with syntactic parsing.
We wanted to build a syntactic
representation of a sender.
So that we know what our subject and
verb and direct object and so on.
But we never discussed
the semantics of the sentence.
In the last few years there has been
a lot of interest in semantic parsing for
purposes such as question-answering
generation, summarization, and so on.
So what does semantic parsing look like?
Well, it's a very simple idea.
Every who in their grammar is going to
be associated not just with a syntactic
structure, but also with what is
known as compositional semantics.
So in compositional semantics the meaning
of a word is the word itself,
and then the meaning of
a combination of words
is a formula that tells you how to combine
the meanings of the individual words.
This is best illustrated with an example.
So let's look at this
very simple sentence.
Javier eats pizza.
So in this drama,
Javier is associated with a noun,
and the semantics of this
noun is just the word itself,
the object represented by the word Javier.
Pizza is represented by
the meaning of the word Pizza.
And eats, as the verb, is represented as
the logical formula which corresponds to
a longer function of two arguments,
x and y, where the predicate is eat and
x is the entity doing the eating, and
y is the entity receiving the eating.
So at this point we have to combine this
lambda function with it's arguments to be
able to come up with a grounded
expression for the entire sentence.
Provided this works, well, we first
have to combine eat and pizza together.
And we are going to replace pizza with y,
so
this is what is known as
a substitution in formal logic.
We are going to replace the lambda
function of two arguments.
Exy with a new function that
has y bound to pizza and
still remaining unbound variable x.
So we still have a lambda
expression that is looking for
a missing argument to be combined with,
and this happens at the next stage.
When we combine this verb phrase
node with the noun node for Javier,
we get the full meaning of the sentence.
Which is that eat is the predicate,
and Javier is substituted for
x in this expression, and
pizza remains as it was before.
So the entire semantic presentation
of this sentence is that
we have a predicate called eat,
with its first argument being Javier and
the second argument being pizza.
So I'm going to go into semantic
parsing into any more detail, but
I want to encourage you to look at some
of the recent literature in ACL in
this topic, as it seems to be one of the
hottest areas in in the next few years.
So I'm going to conclude this
segment with a short link,
a short set of links to follow on
parsing from the Linguistics Olympiad.
The ones that I have selected
as most relevant to this section
are Twodee about a two dimensional parser.
This is a problem written by
Jason Eisner in the 2013 competition.
A One, Two, Tree,
a problem written by Noah Smith,
Kevin Gimbel and
Jason Eisner for the 2012.
A problem set.
And then,
two problems related to categorical
combinatorial grammars.
CCG, and
combining categories in Tok Pisin.
Both of those were used
in the 2014 competition.
And finally, Grammar Rules, which is
a very simple problem about context-free
grammars from last year's competition
by Andrea Schalley and Pat Littell.
So enjoy those problems, and
when you've solved them,
you can check their solutions
on the same website.
Enough about one more example,
The Parser for Skating Figures,
also written by Pat Littell from one
of the earliest competitions from 2009.
So this concludes the section on
alternative grammatical formulisms and
parsing.
Thank you for your attention.

