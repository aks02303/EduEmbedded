The next thing I'd like to introduce is
term frequency weighting, which is one of
the components of the kind of document
scores that are regularly used in
information retrieval systems. Let's go
back to where we began, with the term
document incidence matrix. So with this
matrix we recorded a number, which was
either one or zero, in each cell of the
matrix depending on whether the word
occurred in the document. If we then think
about what the representation of each
document is, well what we have is a
vector. It's a binary vector, which, the
dimensionality of the vector is the size
of the vocabulary and it's recording these 1s
or 0s. But we don't have to limit
ourselves to a binary vector like this. An
obvious alternative is instead to move to
a count vector. So now we still have a
vector for each document, but rather than
simply putting 1's and 0's in it, we're
putting in the number of times the word
occurs in the document. So we've still got a
vector of size of vocabulary, but it's now a
vector in the natural number vector space.
Previously in the Boolean retrieval model,
we were just looking at a set of words
that occurred in the document and doing
set operations like AND or OR. Now with
this count model, we've moved to the
commonly used bag of words model. So in
the bag of words model, we're not
considering the ordering of the words in
the document, but we are considering how
many times a word occurs in a document.
And this word Bag is commonly used for an
extension to sets, which does record how
often a word is used. So the bag of words
model has some huge limitations. So, John
is quicker than Mary, and Mary is quicker
than John, have exactly the same vectors,
there's no differentiation between them.
And that's obviously has its limitations.
So in a sense, this is a step back.
Earlier on when we introduced positional
indices, they were able to distinguish
these two documents by either proximity or
phrase queries. And we'll want to get back
to that. We'll look later at recovering
positional information, but for now we're
going to develop the bag of words model
and how it's used in vector space
retrieval models. So we have this quantity
of the term frequency of a term in a
document, which is just the number of
times that it occurs. And so the question
then is, how can we use that in a
retrieval score? Thinking about it a
little, I hope you can be convinced that
raw term frequency is perhaps not what we
really want. So the idea underlying making
use of term frequency is, if I'm searching
for something like squirrels, then I
should prefer a document that has the word
squirrel in it three times over one that
just has the word squirrel in it once. But
on the other hand, if I find a document
that has the word squirrel in it 30 times,
it's not clear that I should prefer it 30
times as much as the document that only
mentions squirrel once. And so the
suggestion is that relevance goes up with
number of mentions but not linearly. And so
we want to come up with some way of
scaling term frequencies that is relative
to its frequency but less than linear.
Before I go on to outline such measure let
me just highlight one last point. We talk
here about term frequency. Now the word
frequency actually has two usages. One is
the rate at which something occurs, the
frequency of burglaries. And the other
sense of it is the one that's always used
in information retrieval. So when we talk
about frequency in information retrieval,
frequency just means the count, so the
count of a word in a document. Okay, so
this now is what Is standardly done with
the term frequency. What we do is we take
the log of the term frequency. Now if the
term frequency is zero, the word doesn't
occur in the document, well the log of
zero is negative infinity. So that's
slightly problematic. So the standard fix
for that is we have this two case
construction where we add one to the term
frequency if the term does occur in the
document. So if it occurs once, then its
value will become one. Because the log
will be zero and then we'll add one to it.
And we return an answer of zero if the
word doesn't occur. So that means that if,
going on a little, and if we use base ten
logarithms as here, you can see how we're
getting this less than linear growth. So
if a word occurs twice in a document, it
gets a weight of 1.3, a little more. If it
occurs ten times, it gets a weight of two,
1000 times, a weight of four, And so on.
So in order to score a document query
pair, we're just gonna sum over these
terms for each word in the query and the
document. So it's sufficient to take the
intersection of words that are in both the
query and the document, cause everything
else will contribute nothing to the score.
And then for each of those terms, we're
gonna calculate this quantity and sum them
up. And so note in particular, that the
score is indeed still zero if none of the
query terms is present in the document.
Okay, so that's the idea of term frequency
weighting and how it can be used to give a
score for documents for a particular
query, which can be used to rank the
documents returned.
