In this segment I'm going to introduce the
idea of lexiclizing PCFG's. So let's look
at our basic PCFG here and see how the
probabilities of rules actually work. So,
what we have in the PCFG is local trees
corresponding to rules such as verb phrase
rewrites as VBB PP, and that has some
probability attached to it. Like, maybe
that probability would be zero three,
about three percent of verb phrases expand
as a past tense verb and a prepositional
phrase. Well, similarly, we have other
rules so that we have up here a sentence
goes to NP VP rule.
And that would have a much higher probability.
Maybe its probability is 0.4 or something like that.
But the really important thing to notice
is that these rules make no reference
whatsoever to actual words. So, this is
saying that overall about three percent
of verb phrases consist of a past tense
verb and a prepositional phrase. But
whether that's likely or not depends an
awful lot on which verb we're dealing
with. So in this example here we have the
word, verb walked. And walked is the kind
of motion verb that is really, really likely
to have a PP following after it. Whereas
if we had another verb, for example if we
had the verb saw, then that would be a
past tense verb but it would be really
unlikely to have a prepositional phrase
coming after it, something like he saw in
the mirror or something like that, you'd
normally get a, a noun phrase object after
saw first. So it seems like we can only
really come up with good probability
estimates if we know more about the words
in the sentence. And so, that's precisely
what the idea of lexicalization is. The idea
of lexicalization is, let's define for each
category a way of finding its head. So,
for noun phrase, we'll say that the last
noun in it, whether a proper noun
or common noun will be
declared its head. And so, for this noun
phrase we'll say that its head is Sue, and
for this noun phrase here, we'll say that
its head is the word store, because this is
the last noun in the noun phrase.
And we'll apply that to other ideas. So for a
verb phrase, the head of a verb phrase
will be the verb inside it, so the head of
the verb phrase is walked, and for a
prepositional phrase the head will be the
preposition inside it, into.
And we'll say that the head of a sentence
is the head of the verb phrase.
So we put in this way lexical items that 
represent the head of each phrase next
to each non terminal in the grammar. 
Let me get my more, neatly printed version
of that here. Okay, Well what happens if we do that? 
Well what we then find is that we've now got these
categories like S-walked, which are a
combination or our old nonterminals plus
some lexical item. And so if we do that,
we've enormously, enormously expanded
our effective space of nonterminals,
because if we had something like twenty
nonterminals before, but we had something
like 30,000 words in our vocabulary, well
now we've got 600,000 nonterminals in our
grammar. So that suggests we need to do
start doing some more specialized engineering
to be able to do that. Well let's not worry
about that for a moment, and let's just
think in terms of probabilities what this
will allow us to do. Well the neat thing
this will allow us to do is now if we're
looking at what's the probability of this
sub tree, we won't just be saying, what's
the probability of a verb expanding to a
past tense, verb phrase expanding into a
past tense verb and a PP, we'll be saying
what's the probability of a verb phrase
headed by walked, taking a PP? And in
particular, a PP that's headed by into,
and so we're now gonna be capturing inside
the rule two things. We'll be capturing
both that a VP walk Is likely to take a
PP after it, and we will be capturing the
relationship between the heads here. So,
we will be captur-, capturing the
relationship that it's reasonable to have
someone walk into something.
And so we have a lot of much richer probabilistic
conditioning being captured by our
grammar. This extra information will be
really, really useful for resolving
various kinds of ambiguities. So a classic
example of that is prepositional phrase
attachment. So if we want to work out for
a prepositional phrase whether it is
modifying a noun that precedes or
modifying a verb that precedes. Well, we
can capture quite a lot of that inside a
PCFG once we lexicalize it. Because now
we'll have a rule where an NP-rates is
taking on its right-hand side a PP-for. And
we can ask whether that is a likely thing
to happen or not. Or we can say we have a
VP-announce is taking, expanding on its
right-hand side to a PP-in. Is that a
reasonable thing to happen or not? And so
we can use this information to better
model PP attachments than we could in a
vanilla PCFG. That doesn't mean we can
capture everything about PP attachments
in this model, though. And actually, if
you want to, you could think a little
about what other things that you'd like to
know about attaching a prepositional
phrase, which aren't yet captured in this
model, where we have the head word pinned on
to every phrase like this.
But nevertheless, this kind of
head-lexicalization captures most of the
additional things that you'd like to know
to make the various parsing decisions of a
sentence. So it's also useful for things
like coordination scope, knowing about the
complementation patterns of verbs, and so
on. Indeed, it was the case that doing
this lexicalization of PCFGs was seen as
the parsing breakthrough of the late
1990s. So that's a really
useful notion to know about, is this idea
of lexicalizing PCFGs to capture more of
the necessary probabilistic conditioning
information to make parsing decisions.
