It's hard to really get a sense of the CKY
algorithm from staring at pseudo code, but
I think if we work through an example, it'll
actually seem pretty natural and straightforward
so let's do that. So starting off,
here's the grammar that we're going to
use. It's similar to the ones that I've
shown before, in, when we did the grammar
transformations. It's a grammar with unaries,
but with no empties in it. Okay. So there's
only one conceptual leap that we have to
make to get this to work, and that is,
before I showed you parse triangles the
natural way that make sense, but it turns
out that it's really hard to write text in
diagonal diamonds. And so, by and large, what
everyone has decided is that it's much, much
easier to get human beings to tilt their
necks 45 degrees to the right. And so,
they can see a parse triangle, rather than
to actually print things that way. So
conceptually you should think about it
just as we've had it before, that this is
Fish people fish tanks.
And so, this is the first diagonal of the 
single words, this is the second diagonal
of the pairs of words, the triples of words, 
and the four word constituents.
But we've actually stuck it to make the boxes 
rectangles now, and you have to be holding
your head to the side to look at it. But providing 
you make it past that obstacle, we're ready to go.
Okay. So what do we need to get going? We
have our sentence to parse. Fish people
fish tanks. The second thing that we need
is the grammar. And so then once we've
done that we can start doing things. So
in particular what we going to do is start
filling in the lexical entries. So we start
with the cell here and we say okay, the
word fish that can be a noun or a verb, and
so we put in that we can make a noun from
fish, with probability 0.2. And we can make
a verb from fish with probability 0.6. And
then we keep on doing this for the other
cells along the diagonal. So people can be
a noun with probability 0.5, and people
can be a verb with probability 0.1. And we
keep on going. And this is what we get.
Okay, but now our grammar also has unaries
that rewrite to categories. So we
also then have to apply the unary rules.
And so the way we'll do that is for each
cell we'll find unary rules that apply,
and if they create a category that isn't there
before, or one with higher probability
than what was there before then we'll put
it into our chart. And in this case, the
relevant unary rules here, we've got one
other unary rule over here actually, are all
creating different categories, so we'll be
adding new things to the chart. So, we'll
say, look we can also make a VP here,
which has probability 0.06. So, there I'm
taking the 0.1 and the 0.6 and multiplying
it. We can make an NP, where the
probability of that is 0.7 times .0.2, 0.14.
And in the first round through that's all
we'll do, but after we've been through all
of the rules for nonterminals, we then
say, well, we have added some new things,
so added is true, and so we'll do this
whole while loop once more. And we need to
do that, so we also discover that we can
apply by a second layer of unary rules and
put it, build an S over this span.
So we'll also build an S, and the probability of
that will be 0.1 times the probability of
the VP, so that'll be 0.006. Okay. And so,
then we're going to apply the unary rules
down in the other cells of the lexicon.
And that'll give us this. So at this point,
we're ready to start doing the heart
of the CKY algorithm which is
building constituents over bigger spans,
first applying binary rules and then we'll
do the unaries again on top of them.
Okay, so at this point, the cell that we
next want to fill is this one.
So this is the cell that's covering 
from words position zero to two.
So, this version of the algorithm 
I've written using fence posts,
even though the particular way I'm
doing it, there aren't any empty elements.
Okay. And so, to build a binary
constituent from out of this, we, the only
way to do it is to build it something from
zero to one and something from one to two.
Well the things from zero to one are
contained in this cell, and the things
from one to two are contained in this
cell, and so what we are going to do is
consider ways of combining one of each of
those to make some other constituent,
which will be licensed providing there's
a rule in the grammar over here of X goes
to Y Z, where Y is something here, and Z is
something there. So what of that sort
can we do? Well, there's only, there are
no binary rules that have an N on the
right hand side, only unary rules. So
we won't be able to do anything with the V.
If we then look at this, sorry, we won't 
be able to do anything with the N.
If we look at this V here, that could be made 
with that rule. So that's something that we can
make. So look, we've got a VP goes to V NP,
and so that means we can make a VP
in this cell, and the probability of it
will be 0.6 times 0.35 times the
probability of this rule. And at
this point you're starting to stretch my
ability to do mental arithmetic, so that's
.3 times .35
so that's five carry the one, or 0.105.
But then there are also other things 
that we can build with binary rules.
So what other binary rules do we have? 
We've got a rule for S goes to NP VP.
And, well, we can certainly apply that,
because we've got an NP here and a VP
there. And so we can build an S over this
span. And similarly for some other ones,
so NP goes to NP NP, well we've got an NP
here and NP there, so we can build an NP
over this span. I won't try and work out
all of the probabilities, but if we do
that, and apply them, we then end up with
all of these probabilities here. Now, it
turns out that for each of the, the
constituents that we built here in the
binary phase, there's only one way of
making them, so there wasn't any
contention. But we'll we able to see
contention at the next unary phase. And
our rule for contention is always, if we
discover multiple ways to make a category
over a span, we keep the one of highest
probability. And so the next phase here we
now do, is we do unaries for these cells
down here. So, again, we look for unary
rules, and if they give a way of making
something with higher probability, we put
that into the chart and then we say we've
done some work. And having done some work
will lead us to do the whole while loop
again in case we're able to build up
chains of unary rules. So, in this
particular case, there are no unary rules
that have NP on the right hand side, so we
can't do anything there. But there is a
unary rule that has VP on the right hand
side. And so we discover that there's
another way that we can make an S, where
we make an S goes to VP. And well the
probability of that is actually going to
be 0.105 x 0.1, which is 0.0105,
which is actually a lot more than the probability
of this rule. So, we are going to record
a better way of making a S in our back
pointers, and we are going to record the
probability of that new best way of making
it. We always just keep what constituents
we can make, the best probability way of
making them, and if we're storing back
pointers, the way that was the best way of
making it. So if we go through all the
cells and see if we can make them in
better ways, with unary rules, we find
that that's possible in precisely two places.
This one, and also over here we find a
better a way of making an S. And so we update
those two, and this is now the second row.
Okay, so now we're gonna go on to the
third row where we're building three word
constituents. So this is the "fish people
fish" cell and this is the "people fish tanks"
cell. So at this point, things get a
little bit more interesting for building
the binary constituents. So when we're
going to build things in this cell, which
are things from here to here, well we're
going to build binary constituents, but
there are two ways that we can do it. We
can either take something from here, from
zero to two, and combine it with something
from two to three. Or else, we can take
something from zero to one, and combine it
with something from one to three. And
that's what we do in this part of the
algorithm here. We choose a split point
for dividing the constituent into two.
This is where we exploit it being only
binary rules to get a cubic time
algorithm. So we iterate across split
points, which is another O(n)
operation. And then we consider what rules
that we can apply for each split point.
And then that part is as we did it before.
So first off, we're going to start with
saying okay, we're going to combine stuff
from zero to two, that's this cell, with
something from two to three, that's this cell.
And so then we'll see what we can
build. And well, we have an NP and an NP,
and so that will allow us to build a
bigger NP according to that rule. And we
have NP and a VP. So that'll allow us to build
an S using this rule. And we have.
Well, it seems like we can't actually build a
VP over that span doing things that way.
Okay. Let's then consider the other
possibility, where we do this and this.
Well, this time we can build an NP by
combining this NP, and then looking for
the other one we're looking at this cell.
So this time we're using these two cells.
So we can use this NP and this NP, which
will allow us to build an NP. Umm, we can
use this verb and this NP, which will
allow us to build a VP, and we can use
this NP and this VP which will allow us to
build an S. And so we can build things
with that span. So then what we're going
to want to do is say okay. We can
definitely build an NP over this span but
we want to find out which of these two has
the highest probability one of, way of making
NP and keep only that. And similarly,
we're going to want to find out which of
these two is the highest probability way
of making an S and keep only that. Now,
actually, if you work through the math in
this case, there's a little fine point for
the NP case. Because if you look at the NP
case, for the purple one, it's 0.0049 X
0.14, and the probability of the NP rule
is a constant 0.1. And if you go to the
green one, it's 0.14 here, which is the
same as there, and 0.0049, which is the
same as there, reflecting the fact that
both ways of doing it, you are using the
word fish twice and the world people once.
So actually, both ways of making NP have
identical probability. So in practice,
when you have ties like this, these
parsers just choose one analysis, the
first one they come across. But sort of to
s--, you can just say they choose one
analysis randomly. Okay. So that then
fills in that cell, and we get these
numbers. And then, at that point, we would
again ply, apply unary rules. So the only
unary rule that's active at this level is
this one, cause the other two unary rules
only work on N and V, and so aren't
applicable in a cell like this in this
little grammar. So we'd also find a new
way of making an S, that, by making it from a
VP but its probability would be 0.1
times this number, so it would be
0.000147, which is less than the way we've
already discovered of making an S. So
we'll keep this as the highest probability
analysis. Okay, so then we do the, this
cell here. The one to four cell. And so
again, there are two split points of ways
of making it up. We could either make
something from one to two, and then three
to four. Or else we could make something
from, from one to three, and then from
three to four. And so we explore all those
ways of combining things and fill in the cell.
Okay. Now, we're almost at the end.
We just have to fill in the final cell of the chart.
And it's the same thing apart
from now when we iterate across split
points, there are three ways to do it. So,
you can make things to put in here either
by taking things from zero to one followed
by two to four. Or you can have things
that are from zero to two plus things
that are from three to four. Or, you can
have things that are from zero to three,
plus things from three to four. And so we
explore all the ways of combining things,
and again, choose the highest probability
way of making each of a noun phrase, an S
and a VP, which are the three constituent
types that we can build over that span.
Now by the time you've gone this far up
the math is much too difficult for me to
do in real time in front of you, but I've
worked it all out in advance, and if we
fill it out, this is what we get. So what
we get here is, that we can just ask the
question of what's the highest probability
parse of this sentence regardless of what
category you choose for it? And the answer
to that is that is parsing it as a
sentence. But, if you have a nominated
start category like S, you can instead say
I just want to find an analysis as an S,
regardless. And then that will also return
this analysis. Okay, I say we've finished,
but actually, well, where is the parse
tree? And so the answer to the parse tree
is we can find it by now calling this
build tree routine that traces backwards,
through the tree. And so we know that the
first thing that we built, at the top just
taking this starting point is it's an S
goes to NP VP, and at this point, I have
to confess that I actually left a little
bit out of what's written in the table
just to keep things written larger. But
rather than simply writing the rule that
you used, you'll actually record which
cells you built it from. So that these are
actually constituents over spans, so this
is actually a noun phrase that was built
over zero to two, followed by a verb
phrase that was built over two to four.
Okay, so that means we now look in the
zero to two cell to find the noun phrase
and we look in the three to four cell to
find the verb phrase.
And at that point we'll start to-- 
This is zero to four, zero to two, two to four.
And in this part we start to recurse downwards. 
So the highest probability way of building
a noun phrase was using the noun phrase 
goes to noun phrase noun phrase rule.
And since that is a binary rule we then 
necessarily know that it had to be zero one
and one to two. And the highest way of making 
a verb phrase is using the verb noun phrase rule,
where this goes from two to three, and
this goes from three to four. Okay, so
then, that means, we're building this noun
phrase as a noun phrase in this cell. And
we're building this noun phrase as a noun
phrase in that cell. And then over here,
we're building the verb as a verb in this
cell, and we're building a noun phrase as
a noun phrase in this cell. So at that
point we can then say well, what's the
highest way of building a noun phrase
here, oh, it's by making it as a noun from
zero to one. Well, what's the highest way
of making a noun over zero to one from the
same span because that was a unary rule?
Well it was by realizing it as the word
fish which was a word in our sentence. And
now similarly we do the same for the other
cells. Okay so from one to two the way of
building a noun phrase with highest
probability was building it from a noun as a
unary rule and the way of building a noun
with highest probability was using the
word people, that's a terminal in the sentence.
The highest probability way of
building a verb from two to three is
directly making it from the word fish,
which is a terminal in our sentence. And
then the highest probability of way of
making a noun phrase over three to four is
from making it from a noun from three to
four via a unary rule. And the highest
probability of way of making a noun over
three to four is making it using the word
tanks, which is a terminal. So, at this
point, we've been able to back, follow the
back traces back out, and if, can show you
this is the highest probability parse for
this sentence. Phew, that was a bit to get
through for me as well as you. But I hope
you can see it's actually fairly
straightforward as an algorithm. And it's
actually much easier for computers to do
it than it is for human beings filling in
squares of a rectangle. So I hope you kind
of feel like now you understand how you
could go off and write a program that did
that, this during the same kind of
for loops for yourself.
