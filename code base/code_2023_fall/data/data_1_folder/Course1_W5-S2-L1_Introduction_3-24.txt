Okay, so in this next segment of the class
we're going to talk about the IBM
translation models.
The IBM models go back to the late 19,
1980s, early 1990s.
They were seminal in really starting the
whole statistical approach to the
translation problem.
They form a central part of modern
statistical translation systems, and so
we'll cover these models in some detail in
the segment of the class.
So just to recap, this is a slide from the
last segment of this class.
This is a recap of a noisy channel model.
Which was introduced by the IBM researches
to translation.
So a noisy channel model, as we said, has
two components.
Firstly, p of e, this is a language model,
this is a model that assigns a probability
to any sentence in English.
And secondly, p of f given e.
So this is what's called the translation
model.
This assigns the model for a French
sentence given an English sentence.
One note.
Throughout this lecture, I'll follow
convention which is, I'll always assume
that we're translating from a French
sentence to an English sentence.
So that is our goal.
Okay.
Of course, we might have other languages
for translating between.
But for the purposes of this lecture,
we'll always assume we're translating from
French to English.
Okay, so we have a language model for
strings in English.
We have a translation model which, as I
said, is in a sense backwards.
It says what's the conditional probability
of any French sentence, given an English
sentence.
When we translate on to this model we
search for the English sentence that
maximizes the product of p of e and p of f
given e.
And so when we evaluate for particular
sentence f.
Different English translations, we take
both of these terms into account.
So, the roadmap for the next few lectures
in this course on translation is as
follows.
I'm first going to describe IBM Models 1
and 2.
So IBM went through a series of models, I
think 1 through 5, and we're just going to
do the first two because they will
essentially introduce many of the
important ideas.
And actually model 2 is a pretty decent
model for the problem we're going to be
looking at.
We'll then go on to describe phrase-based
models.
So phrased-based models were invented
around the late 1990s, and they're in some
sense a second generation of s-, school
translation systems.
This is the first generation.
They make direct use of ideas from these
IBM models, and so these IBM models will,
in some sense, form a basis of
phrase-based models, but phrase-based
models work much, much better than IBM
models.
They do, in fact form the basis of many
modern statistical translation systems, so
for example, Google translate is heavily
built on this technology.
So, we'll first talk about IBM Model 1 and
then we'll talk about IBM Model 2 and then
finally we'll talk about parameter
estimation in models 1 and 2.
How do we actually learn the parameters of
these models?
From data consisting of example
translations.
