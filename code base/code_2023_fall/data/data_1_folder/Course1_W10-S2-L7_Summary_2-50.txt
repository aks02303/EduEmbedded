So finally to summarize what we've seen 
in this week's lectures, global linear 
models require definitions of this 
function gen that enumerates a set of 
candidates for any input, and this 
function f that maps any candidate 
structure to a feature vector. 
The key idea is in tagging and dependency 
parsing models that we have described 
this week are as follows. 
Firstly GEN has been defined to be the 
set of all possible structures. 
For example all possible tag sequences or 
all possible dependency parses for a 
particular input. 
And that means the GEN grows 
exponentially quickly with respect to the 
size of the input. 
So, GEN is exponential and in size. 
It grows exponentially fast, with respect 
to the length of sentence. 
To get around this, we've defined this 
global feature back to F, through a sum 
of local feature vectors. 
So F takes an input X and some 
[INAUDIBLE] structure Y as it's input. 
It returns a V2 vector. 
This has been defined as a sum of a local 
feature vectors, which look at sub parts 
of the structure, Y. 
In the tagging case The, the local fea-, 
feature of axis G looked at trigrams of 
text. 
In the dependency parsing case, they 
looked at single dependencies within the 
parsing. 
Given this definition of F, we can use 
dynamic programming to find the highest 
scoring structure under the model. 
And that's critical because we need to 
apply the model to new test examples and 
moreover the perception algorithm, which 
we use for parameter estimation Requires 
us to repeatedly decode our training 
samples. 
To repeatedly find our highest scoring 
structure under the model for each 
training sample in turn. 
Finally, it's worth noting, we've seen 
the perceptron algorithm for parameter 
estimate, but there are certainly other 
options. 
In terms of estimated the parameters of 
these global linear models. 
One very famous example is conditional 
random fields, and these are essentially 
giant log-linear models where we can use 
gradient ascent for parameter estimation. 
Another option that has been widely 
considered is so called large-margin 
methods Related to support vector 
machines. 
And so I covered the perceptron, as it's 
relatively simple and it introduces many 
of the main ideas in global linear 
models. 
But there are certainly computing 
parameter estimation methods, which have 
their own advantages. 

