Okay, so welcome to Natural Language
Processing.
My name is Michael Collins, I'm a
professor in Computer Science at Columbia
University.
I've taught this course for several years
now, most recently at Columbia, and before
that at MIT.
Natural language processing is I think, a
tremendously exciting field.
It builds on insights from computer
science, from linguistics, and as we'll
see, increasingly from probability and
statistics.
It's also having a huge impact in our
daily lives.
Many, many applications and technologies
are now making use of basic ideas from
Natural Language Processing.
So, in this introductory lecture, we're
going to cover a few basic points.
The first question we're going to ask is,
what is Natural Language Processing?
So, we'll discuss a few key applications
in NLP and also a few key problems that
are solved in Natural Language Processing.
The second question we'll consider is, why
is NLP hard?
So, we'll consider some key challenges
that we'll find in natural language
processing.
Finally, I'll talk a little bit about what
this course will be about, what kind of
material we'll cover in this course, and
what in general you should expect taking
this course?
So, at a high level, natural language
processing concerns the use of computers
in processing human or natural languages.
So, on one side of this problem, we have
what is often referred to as natural
language understanding where we take text
as input to the computer and it then
processes that text and, and does
something useful with it.
At the other hand, we have what is often
referred to as natural language
generation.
Where a computer, in some sense produces
language in communicating with a human or
user.
So, we should first consider some key
applications in NLP.
One of the oldest applications and a
problem of great importance is machine
translation.
This is the problem of mapping sentences
in one language to sentences in another
language.
And this is a very, very challenging task.
But remarkable progress is being made in
the last 10 or 20 years in this area.
So here, I have an example translation
from Google translate which many of you
will be familiar with, this is a
translation from Arabic into English.
And, while these translations perfect, you
can still understand a great deal of what
was said in the original language.
So, later in this course, we'll actually
go through all of the key steps in
building a model in the machine
translation system.
So, a second example application is what
is often referred to as information
extraction.
So, the problem in this case is to take
some text as input and to produce some
structured, basically a database
representation of some key content in this
text.
So, in this particular example, we have
input which is a job posting.
And the output captures various important
aspects of this posting.
For example, the industry involved, the
position involved, the location, the
company, the salary, and so on.
And you'll see that this information is
pulled out from this document.
So, the salary in this case comes from
this, this portion here.
So this is a, a critical example of a
natural lang, language on the standing
problem where the promise to, in some
sense understand this input were
unstructured text and to turn it into a
structured data base kind of
representation.
So there's some clear motivation for this
particular problem, information
extraction.
Once we've performed this step, we can,
for example, perform complex searches.
So say I want to find all jobs in the
advertising sector paying at least a
certain salary in a particular location.
This would be a search that is very
difficult to formulate using a regular
search engine, but if I first run my
information extraction system over
websites all of the job postings that I
find in the web.
I can then perform a database query and,
and perform much more complex searches
such as this one.
In addition, we might be able to perform
st, statistical queries.
So we might be able to ask you know how is
the number of jobs in accounting changed
over the years, or what is the number of
jobs in software engineering in the Boston
area posted during the last year.
Another key application in natural
language processing is text summarization.
And the problem in this case is to take a
single document or, potentially a group of
several documents and to try to condense
them down to a summary.
Which, in some sense, preserves the main
information in those documents.
So here, I actually have an example
screenshot from a system developed at
Columbia, which is called News Blaster.
And this is actually a multi-document
system.
It will take multiple documents on the
same news story, and produce a condensed
summary of the main content of those
documents.
So in this particular example, we have a
large group of documents all about
vaccination program.
And here is a summary which attempts to
capture the main information in all of
these documents.
So summarization again has clear
motivation in making sense of the vast
amount of data or text available on the
web and the news sources, and so on.
It's very useful to be able to summarize
that data.
Another key application is what are called
dialogue systems.
And these are systems where a human can
actually interact with a computer to
achieve some task.
So, the example I've shown here is from a
flight domain, where a user is attempting
to book a flight.
And so the user might come with some query
to the system.
And the system then goes and processes
this query, in some sense it understands
that query.
And in this particular case it realizes
that there's a piece of missing
information, namely the day of the flight
and so the system them responds with a
query, what day are you flying on?
The user provides this information and the
system returns a list of flights.
So in dialog systems the basic problem is
to build a system where the user can
interact with a computer using natural
language.
And notice that this type of system
involves both natural language
understanding components, we have to
understand what the user is saying.
And there's also importantly a natural
language generation component.
In that we're going to have to generate
text in some cases.
For example clarification questions as
we've shown here.
So in addition to the applications I've
just described, we'll also consider some
very basic natural language processing
problems which on depend many of these
applications.
And the first, we'll talk about is
something called the tagging problem.
So abstractly tagging problems take the
following form.
As input we have some sequence, in this
case a sequence of letters and as output
we are going to have a tagged sequence
where each letter in the input now has an
associated tag.
This is probably best illustrated through
a couple of examples.
The first one is part-of-speech tagging.
So the problem in this case is to take a
sentences input, for example profits,
soared, at Boeing Co and so on, and to tag
each word in the input with its part of
speech.
So N stands for Noun, V stands for verb, P
stands for preposition, ADV stands for
adverb, and so on and so on.
So, this is one of the sort of, most basic
problems in natural language processing.
If you can perform this mapping with high
accuracy, it's actually useful across a
very wide range of applications.
The second example of a tagging problem is
what's called named entity recognition.
So the basic problem here is, again, to
take some sentence's input.
And now, to identify basic entities in
that sentence.
So, entities are things like companies.
So we have Boeing Co here.
Locations.
Wall Street here.
We might also, for example, identify
people.
Again, named entity recognition is a very
basic problem.
But it's clearly very useful in many
applications to be able to identify
companies, locations, people, and other
entity types.
This problem could be again this problem
could again be framed as a tagging problem
where each word in the input is tagged
either as not belonging to any named
entity, so NA means we're not part of an
entity, or we might be part of a company.
Sc means we have the first word in a
company, it's a start company.
Cc means we have the continuation of a
company.
And similarly, SL means we have the start
of a location.
Cl means we have the continuation of a
location.
So again abstractly, a tagging problem is
a problem of mapping an input sequence of
items, usually words to some tag sequence,
where each word in the sequence has an
associated tag.
These are two key problems, and there are
many, many others.
Another basic problem in natural language
processing is the problem of natural
language parsing.
And this goes back to work in linguistics,
going back to sort of the foundation of
modern linguistics with Noam Chomskys work
in the 1950s.
The problem here is again to take some
sentence as input, and to map this to some
output, which is usually referred to as a
parse tree.
So we'll talk a lot about this later in
this course, but at a very high level.
The parse tree essentially gives some
hierarchical decomposition of a sentence.
Corresponding to its grammatical
structure.
And once we've recovered these kind of
representations.
We can, for example, recover basic
grammatical relations.
The fact that Boeing is the subject of
this verb is, or the fact that this
prepositional phrase in Seattle is a
modifier to located.
So, again this is a very basic problem in
natural language processing.
Well see that, if you can recover these
representations with high accuracy.
It's useful, again across a very wide
range of applications to gain a first
basic step in natural language
understanding.
And trying to make sense of what an
underlying sentence means.
And it's also quite challenging problem.
