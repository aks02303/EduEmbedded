{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name        Course1_W9-S1-L2_Word_Cluster_Representations_...\n",
      "text             input brown clustering algorithm corpus words ...\n",
      "text1            input brown cluster algorithm corpu word rathe...\n",
      "join_text        input brown_clustering_algorithm corpus words ...\n",
      "l_text_topics                                                 [10]\n",
      "l_text_prob                                                 [99.8]\n",
      "Name: 108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def complete(func):\n",
    "    def wrapper(*args):\n",
    "        # print(f\"{func.__name__}...\", end='')\n",
    "        ret_val = func(*args)\n",
    "        # print('done')\n",
    "        return ret_val\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@complete\n",
    "def load_words(path_to_master_list):\n",
    "    df = pd.read_csv(path_to_master_list, names=['0'], header=None)\n",
    "    mstr_list = list(set(df['0'].tolist()))\n",
    "    mstr_list_clean = [vocab for vocab in mstr_list if str(vocab) != 'nan']\n",
    "\n",
    "    # replace space with dash\n",
    "    separator = '_'\n",
    "    word_list = [separator.join(word.split()) for word in mstr_list_clean]\n",
    "\n",
    "    return mstr_list_clean, word_list\n",
    "\n",
    "@complete\n",
    "def load_test_csv(path_to_test_csv):\n",
    "    df = pd.read_csv(path_to_test_csv)\n",
    "    return df\n",
    "\n",
    "@complete\n",
    "def load_data_files(path_to_data_files):\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(path_to_data_files).iterdir():\n",
    "        with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "            if(file.name[-3:]!=\"txt\"):\n",
    "                print(file.name)\n",
    "                continue\n",
    "\n",
    "            results[\"file_name\"].append(file.name.split(\".\")[0])\n",
    "            results[\"text\"].append(file_open.read())\n",
    "    adf = pd.DataFrame(results)\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def extract_details(adf):\n",
    "    adf['course_name'] = adf.file_name.str.split('_').str[0]\n",
    "    adf['temp'] = adf.file_name.str.split('_').str[1]\n",
    "    adf['week'] = adf.temp.str.split('-').str[0]\n",
    "    adf['section'] = adf.temp.str.split('-').str[1]\n",
    "    adf['lesson'] = adf.temp.str.split('-').str[2]\n",
    "    adf['course_title'] = adf.file_name.str.split('_').str[2]\n",
    "    adf.dropna(inplace=True)\n",
    "\n",
    "    adf['week_no'] = adf['week'].str.extract('(\\d+)', expand=False)\n",
    "    adf['week_no'] = adf['week_no'].astype(int)\n",
    "    adf['section_no'] = adf['section'].str.extract('(\\d+)', expand=False)\n",
    "    adf['section_no'] = adf['section_no'].astype(int)\n",
    "    adf['lesson_no'] = adf['lesson'].str.extract('(\\d+)', expand=False)\n",
    "    adf = adf.dropna()\n",
    "    adf['lesson_no'] = adf['lesson_no'].astype(int)\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def clean_text_column(adf):\n",
    "    adf['text'] = adf['text'].apply(lambda x: re.sub(\"\\\\n\", \" \",x))\n",
    "    adf['text'] = adf['text'].apply(lambda x: re.sub(r'\\s+',' ', x))\n",
    "    adf['text'] = adf['text'].apply(lambda x: x.lower())\n",
    "    return adf\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "@complete\n",
    "def stem_and_stopword_removal(adf):\n",
    "\n",
    "    # remove punctuation\n",
    "    adf['text'] = adf['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    adf['text'] = adf['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword)]))\n",
    "    # stopwords \n",
    "\n",
    "    ps = nltk.PorterStemmer()\n",
    "\n",
    "    adf['text1'] = adf['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    return adf\n",
    "\n",
    "def _calculate_nmf(text_list) :\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    # tfidf_feature_names = tfidf_vect.get_feature_names()\n",
    "    # print(tfidf)\n",
    "    nmf = NMF(n_components=15,random_state=1, alpha=.1, l1_ratio=.7, init='nndsvd').fit(tfidf)\n",
    "    nmf_trans = nmf.transform(tfidf)\n",
    "    # print(nmf_trans)\n",
    "    predicted_topics = [np.argsort(each)[::-1][0:5] for each in nmf_trans]\n",
    "    return predicted_topics\n",
    "\n",
    "def calc_text_topics(adf):\n",
    "    adf['text_topics'] = _calculate_nmf(adf['text1'])\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def process_text(text):\n",
    "    def _get_phrases(fle):\n",
    "        phrase_dict = defaultdict(list)\n",
    "        for line in map(str.rstrip, fle):\n",
    "            k, _, phr = line.partition(\" \")\n",
    "            phrase_dict[k].append(line)\n",
    "        return phrase_dict\n",
    "\n",
    "    def _replace(text, dct):\n",
    "        text1 = \"\"\n",
    "        phrases = sorted(chain.from_iterable(dct[word] for word in text.split()\n",
    "        if word in dct) ,reverse=1, key=len)\n",
    "        mysetphrases = set(phrases)\n",
    "        phrases = list(mysetphrases)\n",
    "        for phr in phrases:\n",
    "            text = text.replace(phr, phr.replace(\" \", \"_\"))\n",
    "        text1 =  \"\".join(text)\n",
    "        return text1\n",
    "\n",
    "    def _join_text(text):\n",
    "        text = _replace(text, _get_phrases(mstr_list_clean))\n",
    "        return text\n",
    "\n",
    "    return _join_text(text)\n",
    "\n",
    "@complete\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs\n",
    "\n",
    "def _calculate_lda_p_topic(line):\n",
    "    line = line.split()\n",
    "    line_bow = id2word.doc2bow(line)\n",
    "    doc_lda = ldamodel[line_bow]\n",
    "    return([[doc[0], round(doc[1]*100,2)] for doc in doc_lda])\n",
    "\n",
    "# combined both _calculate_lda and _calculate_lda_p\n",
    "# def _calculate_lda(line):\n",
    "#     line = line.split()\n",
    "#     line_bow = id2word.doc2bow(line)\n",
    "#     doc_lda = ldamodel[line_bow]\n",
    "#     return([doc[0] for doc in doc_lda])\n",
    "\n",
    "\n",
    "# def _calculate_lda_p(line):\n",
    "#     line = line.split()\n",
    "#     line_bow = id2word.doc2bow(line)\n",
    "#     doc_lda = ldamodel[line_bow]\n",
    "#     return([(round(doc[1]*100,2)) for doc in doc_lda])\n",
    "\n",
    "\n",
    "@complete\n",
    "def cn_ci(text):\n",
    "    out = ['vi'+str(index) for index, word in enumerate(mstr_list_clean_joined) if str(word) == str(text)]\n",
    "    return out\n",
    "\n",
    "\n",
    "@complete\n",
    "def cn_ci1(text):\n",
    "    text_l = text.split()\n",
    "    out_l = []\n",
    "    for id, txt in enumerate(text_l):\n",
    "        out = ['vi'+str(index) for index, word in enumerate(mstr_list_clean) if str(word) == str(txt)]\n",
    "        out_l.append(out)\n",
    "    out_l = [item for sublist in out_l for item in sublist]\n",
    "    unique_out_l = []\n",
    "    for x in out_l:\n",
    "      if(x not in unique_out_l):\n",
    "        unique_out_l.append(x)\n",
    "    return unique_out_l\n",
    "\n",
    "def concept_word(text):\n",
    "    text_l = text.split()\n",
    "    out_l = []\n",
    "    for id, txt in enumerate(text_l):\n",
    "        out = [word for index, word in enumerate(mstr_list_clean) if str(word) == str(txt)]\n",
    "        out_l.append(out)\n",
    "    out_l = [item for sublist in out_l for item in sublist]\n",
    "    unique_out_l = []\n",
    "    for x in out_l:\n",
    "      if(x not in unique_out_l):\n",
    "        unique_out_l.append(x)\n",
    "    return unique_out_l\n",
    "\n",
    "\n",
    "def _calculate_tfidf(text_list):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    df = pd.DataFrame(tfidf.todense(), columns = tfidf_vect.get_feature_names())\n",
    "    # df = pd.DataFrame(tfidf[0].T.todense(), index=tfidf_vect.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    return df\n",
    "\n",
    "@complete\n",
    "def _feat(txt):\n",
    "    # print(\"****************************************************************************\")\n",
    "    response = vectorizer.transform([txt])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    arr = []\n",
    "    for col in response.nonzero()[1]:\n",
    "        if feature_names[col] in mstr_list_clean:\n",
    "            f_n = ['vi'+str(index) for index, word in enumerate(mstr_list_clean_joined) if word in feature_names[col]]\n",
    "            arr.append([f_n[0], response[0, col]])\n",
    "        else:\n",
    "            f_n = 'NA'\n",
    "        # print(feature_names[col], response[0, col])\n",
    "        # print(f_n, response[0, col])\n",
    "        # print(\"=====\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _calculate_tfidf(text_list):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    df = pd.DataFrame(tfidf.todense(), columns = tfidf_vect.get_feature_names())\n",
    "    # df = pd.DataFrame(tfidf[0].T.todense(), index=tfidf_vect.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================== Preprocessing ==============================================\n",
    "DEBUG_MODE = False          # set to True to generate intermediate files (in data_intermediates) for debugging\n",
    "PREREQ_LEVEL = False         # set to True to generate prerequisite and level triples\n",
    "\n",
    "\n",
    "MASTER_DATA_PATH = r\"C:\\Users\\patel\\OneDrive - iiit-b\\Desktop\\coursework\\WSL Knowledge Graphs\\codeBase\\EduEmbedd\\data\\data_1_folder\"\n",
    "MASTER_LIST_PATH = '../vocab_master_list.csv'\n",
    "\n",
    "\n",
    "\n",
    "INTERMEDIATES_BASE_DIR = '../debug_output'\n",
    "TRIPLES_WITH_PROB = 'triples.csv'\n",
    "TEST_CSV_PATH = f'{INTERMEDIATES_BASE_DIR}/test.csv'\n",
    "V2_DF_PATH = f'{INTERMEDIATES_BASE_DIR}/v2_df.csv'\n",
    "TW_DF1_PATH = f'{INTERMEDIATES_BASE_DIR}/tw_df1.csv'\n",
    "ADF_PC_1_PATH = f'{INTERMEDIATES_BASE_DIR}/adf_pc_1.csv'\n",
    "ADF_WITH_TFIDF_PATH = f'{INTERMEDIATES_BASE_DIR}/adf_with_tfidf.csv'\n",
    "\n",
    "\n",
    "\n",
    "# extract features from the data files\n",
    "adf = load_data_files(MASTER_DATA_PATH)\n",
    "if PREREQ_LEVEL: adf = extract_details(adf)\n",
    "adf = clean_text_column(adf)\n",
    "adf = stem_and_stopword_removal(adf)\n",
    "if PREREQ_LEVEL: adf = calc_text_topics(adf)\n",
    "\n",
    "if DEBUG_MODE: adf.to_csv(TEST_CSV_PATH)\n",
    "\n",
    "\n",
    "mstr_list_clean, mstr_list_clean_joined = load_words(MASTER_LIST_PATH)\n",
    "v2_df = pd.DataFrame(mstr_list_clean_joined)\n",
    "if DEBUG_MODE: v2_df.to_csv(V2_DF_PATH)\n",
    "# adf = load_test_csv(TEST_CSV_PATH)\n",
    "\n",
    "\n",
    "adf['join_text'] = adf['text'].apply(lambda x: process_text(x))\n",
    "\n",
    "\n",
    "text_list = adf['join_text'].to_list()\n",
    "doc_tokens = docs_preprocessor(text_list)\n",
    "dictionary = gensim.corpora.Dictionary(doc_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_tokens]\n",
    "\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(corpus, num_topics=15, id2word = dictionary, passes=20, iterations=400, chunksize = 500, eval_every = None,random_state=0)\n",
    "id2word = dictionary\n",
    "\n",
    "topics = []\n",
    "for topic_num in range(0,14):\n",
    "    topic = ldamodel.show_topic(topic_num, topn=100)\n",
    "    \n",
    "    topic = [list(t) for t in topic]\n",
    "    topic = [[t[0],t[1],topic_num] for t in topic]\n",
    "    topics.append(topic)\n",
    "\n",
    "topics = [item for items in topics for item in items]   \n",
    "tw_df = pd.DataFrame(topics, columns =['feature', 'proba', 'topic_num'])\n",
    "\n",
    "\n",
    "# adf['l_text_topics'] = adf['join_text'].map(_calculate_lda)\n",
    "# adf['l_text_prob'] = adf['join_text'].apply(lambda x: _calculate_lda_p(x))\n",
    "\n",
    "# above lines replaced with the following due to unexpected behaviour\n",
    "adf['temp_text'] = adf['join_text'].apply(lambda x: _calculate_lda_p_topic(x))\n",
    "adf['l_text_topics'] = adf['temp_text'].apply(lambda x: [item[0] for item in x])\n",
    "adf['l_text_prob'] = adf['temp_text'].apply(lambda x: [item[1] for item in x])\n",
    "adf.drop(columns=['temp_text'], inplace=True)\n",
    "\n",
    "\n",
    "print(adf.iloc[108])\n",
    "\n",
    "adf['concept_vocab_index'] = adf['join_text'].map(lambda s:cn_ci1(s))\n",
    "adf['concept_vocab_word'] = adf['join_text'].map(lambda s:concept_word(s))\n",
    "\n",
    "\n",
    "tw_df['cv_index'] = tw_df['feature'].map(lambda s:cn_ci(s))\n",
    "\n",
    "\n",
    "tw_df1 = tw_df[tw_df['cv_index'].str.len() != 0]\n",
    "tw_df1 = tw_df1.dropna()\n",
    "tw_df1['cv_index_1'] = tw_df1['cv_index'].map(lambda s:s[0])\n",
    "\n",
    "if DEBUG_MODE: tw_df1.to_csv(TW_DF1_PATH)\n",
    "\n",
    "corpus = adf['join_text']\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "adf['new'] = adf['join_text'].apply(lambda x: _feat(x))\n",
    "\n",
    "if PREREQ_LEVEL:\n",
    "    adf['course_no'] = adf['course_name'].str.extract('(\\d+)', expand=False)\n",
    "    adf['course_no'] = adf['course_no'].astype(int)\n",
    "    adf['prerequisite'] = adf.groupby('course_no').file_name.shift(1)\n",
    "    adf['composite'] = adf.groupby('course_no').file_name.shift(-1)\n",
    "\n",
    "\n",
    "if DEBUG_MODE: adf.to_csv(ADF_PC_1_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptfidf = _calculate_tfidf(adf['join_text'])\n",
    "\n",
    "cols = temptfidf.columns\n",
    "\n",
    "cvw_tfidf_score = []\n",
    "for i in range(len(temptfidf)):\n",
    "  cvw = adf.iloc[i]['concept_vocab_word']\n",
    "  tfidf = temptfidf.iloc[i]\n",
    "  cvw_tfidf = []\n",
    "  for word in cvw:\n",
    "    for ind, col in enumerate(cols):\n",
    "      if (word==col):\n",
    "        cvw_tfidf.append(round(tfidf[ind],2))\n",
    "  cvw_tfidf_score.append(cvw_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_name              0\n",
       "text                   0\n",
       "course_name            0\n",
       "temp                   0\n",
       "week                   0\n",
       "section                0\n",
       "lesson                 0\n",
       "course_title           0\n",
       "week_no                0\n",
       "section_no             0\n",
       "lesson_no              0\n",
       "text1                  0\n",
       "text_topics            0\n",
       "join_text              0\n",
       "l_text_topics          0\n",
       "l_text_prob            0\n",
       "concept_vocab_index    0\n",
       "concept_vocab_word     0\n",
       "new                    0\n",
       "course_no              0\n",
       "prerequisite           3\n",
       "composite              3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adf.values:\n",
    "    if (len(i[14]) != len(i[15])):\n",
    "        print(i[0], i[14], i[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_lda(adf.iloc[108]['join_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.8]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_lda_p(adf.iloc[108]['join_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input brown_clustering_algorithm corpus words rather sentences could potentially quite large might use million tens millions even hundred hundreds millions sentences said one advantage brown method doesnt require annotated data use raw unannotated text might find web news wire data various sources produce two types output first partition words word clusters second actually generalization first hierarchical clustering words let actually give examples two types outputs firstly example clusters actually original paper brown colleagues remember input algorithm unlabeled text theyve shown different clusters algorithm recovered basically partition words vocabulary clusters way similar words appear similar clusters really rather striking first one look words friday monday thursday wednesday days week addition weekends case stress derived completely automatically unlabeled text well soon second cluster seems consist month names another cluster words like people guys folks fellows ceos chaps doubters commies unfortunates blokes seem mineral cluster water glass coal man woman boy first names imagine kind word clusters useful pretty wide range natural language applications additional knowledge identity particular word also class words falls actually final part segment ill show kind cluster representations used directly problem named_entity_recognition give big improvements particular task example set word clusters lets look second kind representation brown clustering produce hierarchical representation words illustrate let give simple hierarchy say six words vocabulary say apple pear boy girl maybe said reported might case hierarchical clustering looks like take node tree drawn drawn end cluster words node example corresponds two words boy girl node corresponds two words apple pear node corresponds two words said reported go higher tree node would correspond cluster four words apple pair boy girl finally top node entire tree entire vocabulary least interesting clustering think see kind hierarchical clustering reflects fact boy girl similar apple pear sense four words similar theyre nouns two words verbs hierarchical representation allows clustering different levels granularity itll useful think hierarchies assigning bit strings word vocabulary whenever branch tree left branch right branch similarly versus versus think words apple corresponds bit string pear corresponds bit string boy girl said think reported notice bit strings different lengths example two versus three two lengths different words may different depths tree think prefix prefix bit strings defines clustering cluster apple pear cluster boy girl actually four words apple pear girl said reported case going useful think representation hierarchical clusterings bit strings ive shown actual example hierarchical representation type derived using brown clusters paper scott miller others conference called naacl well look closely paper later lecture used kind clusters within context named_entity_recognition lets take look see brown algorithm really remarkably effective deriving useful hierarchical representations representations derived tens millions words think news wire text see words bit string think common right way first several bits theyre deeply nested hierarchical tree path node words like lawyer newspaperman stewardess toxicologist clearly people general course occasionally errors heres one word slang reason general representations look pretty good similarly company names share pretty long bit string way theyre pretty deep hierarchy finally first names okay share pretty deep bit stream imagine think named_entity_recognition problem example imagine useful representations infrequent rare words words never seen training_data knowing word like consuelo appears class first names extremely useful information trying build named_entity detector'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of adf where file_name is Course1_W9-S1-L2_Word_Cluster_Representations_8-36\n",
    "adf.iloc[108]['join_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1 = adf[['file_name','l_text_topics']]\n",
    "adf1 = adf1.dropna()\n",
    "adf1 = adf1.melt('file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head        0\n",
       "variable    0\n",
       "value       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf1.rename(columns = {\"file_name\": \"head\"}, inplace = True)\n",
    "adf1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head        0\n",
       "variable    0\n",
       "value       0\n",
       "prob        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adf1['prob'] = adf['l_text_prob']\n",
    "# adf1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED TRIPLES FILES AT -> triples.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adf['concept_vocab_word_tfidf'] = cvw_tfidf_score\n",
    "\n",
    "if DEBUG_MODE: adf.to_csv(ADF_WITH_TFIDF_PATH)\n",
    "\n",
    "adf1 = adf[['file_name','l_text_topics']]\n",
    "adf1 = adf1.dropna()\n",
    "adf1 = adf1.melt('file_name')\n",
    "adf1.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf1['prob'] = adf['l_text_prob'].to_list()\n",
    "adf1 = adf1.explode(['value', 'prob'])\n",
    "adf1['value'] = 'topic_' + adf1['value'].astype(str)\n",
    "\n",
    "\n",
    "adf2 = adf[['file_name','concept_vocab_index']]\n",
    "adf2 = adf2.dropna()\n",
    "adf2 = adf2.melt('file_name')\n",
    "adf2.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf2['prob'] = adf['concept_vocab_word_tfidf'].to_list()\n",
    "# adf2['prob'] = adf2['prob'].apply(lambda x:ast.literal_eval(x) if x != [] else 0)\n",
    "# adf2['value'] = adf2['value'].apply(lambda x:ast.literal_eval(x) if x != [] else 0)\n",
    "# print(len(adf2['prob'].iloc[0]), len(adf2['value'].iloc[0]))\n",
    "adf2 = adf2.explode(['value','prob'])\n",
    "adf2 = adf2.drop_duplicates(subset=['head','variable','value','prob'], keep='last')\n",
    "\n",
    "if PREREQ_LEVEL:\n",
    "    adf['prerequisite'] = adf['prerequisite'].fillna('start')\n",
    "    adf['composite'] = adf['composite'].fillna('end')\n",
    "    adf3 = adf[['file_name','prerequisite']]\n",
    "    adf3 = adf3.dropna()\n",
    "    adf3 = adf3.melt('file_name')\n",
    "    adf3.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "    adf3 = adf3.drop_duplicates(subset=['head','variable','value'], keep='last')\n",
    "\n",
    "\n",
    "    adf4 = adf[['file_name']]\n",
    "    adf4['variable'] = 'level'\n",
    "    adf4['value'] = 'level_1'\n",
    "    adf4['value'] = adf4.apply(lambda row: 'level_1' if row['file_name'][6] == '1' else ('level_2' if row['file_name'][6] == '3' else 'level_3'), axis=1)\n",
    "    adf4.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "\n",
    "\n",
    "#topics to vacab have to be done separately\n",
    "# tw_df1 = pd.read_csv('tw_df1.csv')\n",
    "adf5 = tw_df1[['topic_num','cv_index_1']]\n",
    "adf5 = adf5.dropna()\n",
    "adf5 = adf5.melt('topic_num')\n",
    "adf5.rename(columns = {\"topic_num\": \"head\"}, inplace = True) \n",
    "adf5 = adf5.drop_duplicates(subset=['head','variable','value'], keep='last')\n",
    "adf5['head'] = adf5['head'] + 1\n",
    "adf5['head'] = 'topic_' + adf5['head'].astype(str)\n",
    "adf5['variable'] = \"concept_vocab_index\"\n",
    "adf5.head()\n",
    "\n",
    "if PREREQ_LEVEL: fdf = pd.concat([adf1,adf2,adf3,adf4,adf5])\n",
    "else: fdf = pd.concat([adf1,adf2,adf5])\n",
    "\n",
    "fdf = fdf.fillna(1)\n",
    "\n",
    "fdf.drop_duplicates()\n",
    "cvi_triples = fdf[fdf['variable'] == 'concept_vocab_index']\n",
    "l_text_triples = fdf[fdf['variable'] == 'l_text_topics']\n",
    "if PREREQ_LEVEL: prerequisite_triples = fdf[fdf['variable'] == 'prerequisite']\n",
    "if PREREQ_LEVEL: level_triples = fdf[fdf['variable'] == 'level']\n",
    "\n",
    "\n",
    "# print(fdf.shape)\n",
    "cvi_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(cvi_triples['prob'])), columns = ['prob'])\n",
    "l_text_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(l_text_triples['prob'])), columns = ['prob'])\n",
    "if PREREQ_LEVEL: prerequisite_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(prerequisite_triples['prob'])), columns = ['prob'])\n",
    "if PREREQ_LEVEL: level_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(level_triples['prob'])), columns = ['prob'])\n",
    "\n",
    "if PREREQ_LEVEL: fdf = pd.concat([cvi_triples, l_text_triples, prerequisite_triples, level_triples])\n",
    "else: fdf = pd.concat([cvi_triples, l_text_triples])\n",
    "\n",
    "fdf.to_csv(TRIPLES_WITH_PROB)\n",
    "print(f\"GENERATED TRIPLES FILES AT -> {os.path.relpath(TRIPLES_WITH_PROB)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "adf4['hell'].loc[:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'text', 'text1', 'join_text', 'l_text_topics',\n",
       "       'l_text_prob', 'concept_vocab_index', 'concept_vocab_word', 'new',\n",
       "       'concept_vocab_word_tfidf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11970, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
