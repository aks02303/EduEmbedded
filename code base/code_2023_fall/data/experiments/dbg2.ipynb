{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0062125  0.09255381 0.07011316 ... 0.         0.09671562 0.        ]\n",
      " [0.01373551 0.0247265  0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.09010504 ... 0.         0.         0.        ]\n",
      " [0.00242983 0.         0.0763365  ... 0.         0.         0.        ]\n",
      " [0.08137307 0.         0.03096944 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def complete(func):\n",
    "    def wrapper(*args):\n",
    "        # print(f\"{func.__name__}...\", end='')\n",
    "        ret_val = func(*args)\n",
    "        # print('done')\n",
    "        return ret_val\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@complete\n",
    "def load_words(path_to_master_list):\n",
    "    df = pd.read_csv(path_to_master_list, names=['0'], header=None)\n",
    "    mstr_list = list(set(df['0'].tolist()))\n",
    "    mstr_list_clean = [vocab for vocab in mstr_list if str(vocab) != 'nan']\n",
    "\n",
    "    # replace space with dash\n",
    "    separator = '_'\n",
    "    word_list = [separator.join(word.split()) for word in mstr_list_clean]\n",
    "\n",
    "    return mstr_list_clean, word_list\n",
    "\n",
    "@complete\n",
    "def load_test_csv(path_to_test_csv):\n",
    "    df = pd.read_csv(path_to_test_csv)\n",
    "    return df\n",
    "\n",
    "@complete\n",
    "def load_data_files(path_to_data_files):\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(path_to_data_files).iterdir():\n",
    "        with open(file, \"r\", encoding=\"utf8\") as file_open:\n",
    "            if(file.name[-3:]!=\"txt\"):\n",
    "                print(file.name)\n",
    "                continue\n",
    "\n",
    "            results[\"file_name\"].append(file.name.split(\".\")[0])\n",
    "            results[\"text\"].append(file_open.read())\n",
    "    adf = pd.DataFrame(results)\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def extract_details(adf):\n",
    "    adf['course_name'] = adf.file_name.str.split('_').str[0]\n",
    "    adf['temp'] = adf.file_name.str.split('_').str[1]\n",
    "    adf['week'] = adf.temp.str.split('-').str[0]\n",
    "    adf['section'] = adf.temp.str.split('-').str[1]\n",
    "    adf['lesson'] = adf.temp.str.split('-').str[2]\n",
    "    adf['course_title'] = adf.file_name.str.split('_').str[2]\n",
    "    adf.dropna(inplace=True)\n",
    "\n",
    "    adf['week_no'] = adf['week'].str.extract('(\\d+)', expand=False)\n",
    "    adf['week_no'] = adf['week_no'].astype(int)\n",
    "    adf['section_no'] = adf['section'].str.extract('(\\d+)', expand=False)\n",
    "    adf['section_no'] = adf['section_no'].astype(int)\n",
    "    adf['lesson_no'] = adf['lesson'].str.extract('(\\d+)', expand=False)\n",
    "    adf = adf.dropna()\n",
    "    adf['lesson_no'] = adf['lesson_no'].astype(int)\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def clean_text_column(adf):\n",
    "    adf['text'] = adf['text'].apply(lambda x: re.sub(\"\\\\n\", \" \",x))\n",
    "    adf['text'] = adf['text'].apply(lambda x: re.sub(r'\\s+',' ', x))\n",
    "    adf['text'] = adf['text'].apply(lambda x: x.lower())\n",
    "    return adf\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "@complete\n",
    "def stem_and_stopword_removal(adf):\n",
    "\n",
    "    # remove punctuation\n",
    "    adf['text'] = adf['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    adf['text'] = adf['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword)]))\n",
    "    # stopwords \n",
    "\n",
    "    ps = nltk.PorterStemmer()\n",
    "\n",
    "    adf['text1'] = adf['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    return adf\n",
    "\n",
    "def _calculate_nmf(text_list) :\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    # tfidf_feature_names = tfidf_vect.get_feature_names()\n",
    "    # print(tfidf)\n",
    "    nmf = NMF(n_components=15,random_state=1, alpha=.1, l1_ratio=.7, init='nndsvd').fit(tfidf)\n",
    "    nmf_trans = nmf.transform(tfidf)\n",
    "    print(nmf_trans)\n",
    "    predicted_topics = [np.argsort(each)[::-1][0:5] for each in nmf_trans]\n",
    "    return predicted_topics\n",
    "\n",
    "def calc_text_topics(adf):\n",
    "    adf['text_topics'] = _calculate_nmf(adf['text1'])\n",
    "    return adf\n",
    "\n",
    "@complete\n",
    "def process_text(text):\n",
    "    def _get_phrases(fle):\n",
    "        phrase_dict = defaultdict(list)\n",
    "        for line in map(str.rstrip, fle):\n",
    "            k, _, phr = line.partition(\" \")\n",
    "            phrase_dict[k].append(line)\n",
    "        return phrase_dict\n",
    "\n",
    "    def _replace(text, dct):\n",
    "        text1 = \"\"\n",
    "        phrases = sorted(chain.from_iterable(dct[word] for word in text.split()\n",
    "        if word in dct) ,reverse=1, key=len)\n",
    "        mysetphrases = set(phrases)\n",
    "        phrases = list(mysetphrases)\n",
    "        for phr in phrases:\n",
    "            text = text.replace(phr, phr.replace(\" \", \"_\"))\n",
    "        text1 =  \"\".join(text)\n",
    "        return text1\n",
    "\n",
    "    def _join_text(text):\n",
    "        text = _replace(text, _get_phrases(mstr_list_clean))\n",
    "        return text\n",
    "\n",
    "    return _join_text(text)\n",
    "\n",
    "@complete\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs\n",
    "\n",
    "\n",
    "def _calculate_lda(line):\n",
    "    line = line.split()\n",
    "    line_bow = id2word.doc2bow(line)\n",
    "    doc_lda = ldamodel[line_bow]\n",
    "    return([doc[0] for doc in doc_lda])\n",
    "\n",
    "\n",
    "def _calculate_lda_p(line):\n",
    "    line = line.split()\n",
    "    line_bow = id2word.doc2bow(line)\n",
    "    doc_lda = ldamodel[line_bow]\n",
    "    return([(round(doc[1]*100,2)) for doc in doc_lda])\n",
    "\n",
    "\n",
    "@complete\n",
    "def cn_ci(text):\n",
    "    out = ['vi'+str(index) for index, word in enumerate(mstr_list_clean_joined) if str(word) == str(text)]\n",
    "    return out\n",
    "\n",
    "\n",
    "@complete\n",
    "def cn_ci1(text):\n",
    "    text_l = text.split()\n",
    "    out_l = []\n",
    "    for id, txt in enumerate(text_l):\n",
    "        out = ['vi'+str(index) for index, word in enumerate(mstr_list_clean) if str(word) == str(txt)]\n",
    "        out_l.append(out)\n",
    "    out_l = [item for sublist in out_l for item in sublist]\n",
    "    unique_out_l = []\n",
    "    for x in out_l:\n",
    "      if(x not in unique_out_l):\n",
    "        unique_out_l.append(x)\n",
    "    return unique_out_l\n",
    "\n",
    "def concept_word(text):\n",
    "    text_l = text.split()\n",
    "    out_l = []\n",
    "    for id, txt in enumerate(text_l):\n",
    "        out = [word for index, word in enumerate(mstr_list_clean) if str(word) == str(txt)]\n",
    "        out_l.append(out)\n",
    "    out_l = [item for sublist in out_l for item in sublist]\n",
    "    unique_out_l = []\n",
    "    for x in out_l:\n",
    "      if(x not in unique_out_l):\n",
    "        unique_out_l.append(x)\n",
    "    return unique_out_l\n",
    "\n",
    "\n",
    "def _calculate_tfidf(text_list):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    df = pd.DataFrame(tfidf.todense(), columns = tfidf_vect.get_feature_names())\n",
    "    # df = pd.DataFrame(tfidf[0].T.todense(), index=tfidf_vect.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    return df\n",
    "\n",
    "@complete\n",
    "def _feat(txt):\n",
    "    # print(\"****************************************************************************\")\n",
    "    response = vectorizer.transform([txt])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    arr = []\n",
    "    for col in response.nonzero()[1]:\n",
    "        if feature_names[col] in mstr_list_clean:\n",
    "            f_n = ['vi'+str(index) for index, word in enumerate(mstr_list_clean_joined) if word in feature_names[col]]\n",
    "            arr.append([f_n[0], response[0, col]])\n",
    "        else:\n",
    "            f_n = 'NA'\n",
    "        # print(feature_names[col], response[0, col])\n",
    "        # print(f_n, response[0, col])\n",
    "        # print(\"=====\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _calculate_tfidf(text_list):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf = tfidf_vect.fit_transform(text_list)\n",
    "    df = pd.DataFrame(tfidf.todense(), columns = tfidf_vect.get_feature_names())\n",
    "    # df = pd.DataFrame(tfidf[0].T.todense(), index=tfidf_vect.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================== Preprocessing ==============================================\n",
    "MASTER_DATA_PATH = r\"C:\\Users\\patel\\OneDrive - iiit-b\\Desktop\\coursework\\WSL Knowledge Graphs\\codeBase\\EduEmbedd\\data\\data_1_folder\"\n",
    "MASTER_LIST_PATH = 'vocab_master_list.csv'\n",
    "\n",
    "\n",
    "\n",
    "INTERMEDIATES_BASE_DIR = 'data_intermediates'\n",
    "TRIPLES_WITH_PROB = 'triples.csv'\n",
    "TEST_CSV_PATH = f'{INTERMEDIATES_BASE_DIR}/test.csv'\n",
    "V2_DF_PATH = f'{INTERMEDIATES_BASE_DIR}/v2_df.csv'\n",
    "TW_DF1_PATH = f'{INTERMEDIATES_BASE_DIR}/tw_df1.csv'\n",
    "ADF_PC_1_PATH = f'{INTERMEDIATES_BASE_DIR}/adf_pc_1.csv'\n",
    "ADF_WITH_TFIDF_PATH = f'{INTERMEDIATES_BASE_DIR}/adf_with_tfidf.csv'\n",
    "\n",
    "\n",
    "\n",
    "# extract features from the data files\n",
    "adf = load_data_files(MASTER_DATA_PATH)\n",
    "adf = extract_details(adf)\n",
    "adf = clean_text_column(adf)\n",
    "adf = stem_and_stopword_removal(adf)\n",
    "adf = calc_text_topics(adf)\n",
    "\n",
    "# test.csv\n",
    "\n",
    "mstr_list_clean, mstr_list_clean_joined = load_words(MASTER_LIST_PATH)\n",
    "pd.DataFrame(mstr_list_clean_joined).to_csv(V2_DF_PATH)\n",
    "adf = load_test_csv(TEST_CSV_PATH)\n",
    "\n",
    "\n",
    "adf['join_text'] = adf['text'].apply(lambda x: process_text(x))\n",
    "\n",
    "\n",
    "text_list = adf['join_text'].to_list()\n",
    "doc_tokens = docs_preprocessor(text_list)\n",
    "dictionary = gensim.corpora.Dictionary(doc_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_tokens]\n",
    "\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(corpus, num_topics=15, id2word = dictionary, passes=20, iterations=400, chunksize = 500, eval_every = None,random_state=0)\n",
    "id2word = dictionary\n",
    "\n",
    "topics = []\n",
    "for topic_num in range(0,14):\n",
    "    topic = ldamodel.show_topic(topic_num, topn=100)\n",
    "    \n",
    "    topic = [list(t) for t in topic]\n",
    "    topic = [[t[0],t[1],topic_num] for t in topic]\n",
    "    topics.append(topic)\n",
    "\n",
    "topics = [item for items in topics for item in items]   \n",
    "tw_df = pd.DataFrame(topics, columns =['feature', 'proba', 'topic_num'])\n",
    "\n",
    "\n",
    "adf['l_text_topics'] = adf['join_text'].apply(lambda x: _calculate_lda(x))\n",
    "adf['l_text_prob'] = adf['join_text'].apply(lambda x: _calculate_lda_p(x))\n",
    "\n",
    "\n",
    "adf['concept_vocab_index'] = adf['join_text'].map(lambda s:cn_ci1(s))\n",
    "adf['concept_vocab_word'] = adf['join_text'].map(lambda s:concept_word(s))\n",
    "\n",
    "\n",
    "tw_df['cv_index'] = tw_df['feature'].map(lambda s:cn_ci(s))\n",
    "\n",
    "\n",
    "tw_df1 = tw_df[tw_df['cv_index'].str.len() != 0]\n",
    "tw_df1 = tw_df1.dropna()\n",
    "tw_df1['cv_index_1'] = tw_df1['cv_index'].map(lambda s:s[0])\n",
    "\n",
    "tw_df1.to_csv(TW_DF1_PATH)\n",
    "\n",
    "corpus = adf['join_text']\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "adf['new'] = adf['join_text'].apply(lambda x: _feat(x))\n",
    "\n",
    "adf['course_no'] = adf['course_name'].str.extract('(\\d+)', expand=False)\n",
    "adf['course_no'] = adf['course_no'].astype(int)\n",
    "adf['prerequisite'] = adf.groupby('course_no').file_name.shift(1)\n",
    "adf['composite'] = adf.groupby('course_no').file_name.shift(-1)\n",
    "\n",
    "adf.to_csv(ADF_PC_1_PATH)\n",
    "\n",
    "v2_df = pd.read_csv(V2_DF_PATH)\n",
    "\n",
    "temptfidf = _calculate_tfidf(adf['join_text'])\n",
    "\n",
    "cols = temptfidf.columns\n",
    "\n",
    "cvw_tfidf_score = []\n",
    "for i in range(len(temptfidf)):\n",
    "  cvw = adf.loc[i]['concept_vocab_word']\n",
    "  tfidf = temptfidf.loc[i]\n",
    "  cvw_tfidf = []\n",
    "  for word in cvw:\n",
    "    for ind, col in enumerate(cols):\n",
    "      if (word==col):\n",
    "        cvw_tfidf.append(round(tfidf[ind],2))\n",
    "  cvw_tfidf_score.append(cvw_tfidf)\n",
    "\n",
    "\n",
    "adf['concept_vocab_word_tfidf'] = cvw_tfidf_score\n",
    "\n",
    "adf.to_csv(ADF_WITH_TFIDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1 = adf[['file_name','l_text_topics']]\n",
    "adf1 = adf1.dropna()\n",
    "adf1 = adf1.melt('file_name')\n",
    "adf1.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf1['prob'] = adf[['l_text_prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adf1.values:\n",
    "    if (len(i[2]) != len(i[3])):\n",
    "        print(i[2], i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1 = adf1.explode(['value', 'prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\pandas\\core\\frame.py:5042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adf1 = adf[['file_name','l_text_topics']]\n",
    "adf1 = adf1.dropna()\n",
    "adf1 = adf1.melt('file_name')\n",
    "adf1.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf1['prob'] = adf[['l_text_prob']]\n",
    "adf1 = adf1.explode(['value', 'prob'])\n",
    "adf1['value'] = 'topic_' + adf1['value'].astype(str)\n",
    "\n",
    "\n",
    "adf2 = adf[['file_name','concept_vocab_index']]\n",
    "adf2 = adf2.dropna()\n",
    "adf2 = adf2.melt('file_name')\n",
    "adf2.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf2['prob'] = adf['concept_vocab_word_tfidf']\n",
    "# adf2['prob'] = adf2['prob'].apply(lambda x:ast.literal_eval(x) if x != [] else 0)\n",
    "# adf2['value'] = adf2['value'].apply(lambda x:ast.literal_eval(x) if x != [] else 0)\n",
    "print(len(adf2['prob'].iloc[0]), len(adf2['value'].iloc[0]))\n",
    "adf2 = adf2.explode(['value','prob'])\n",
    "adf2 = adf2.drop_duplicates(subset=['head','variable','value','prob'], keep='last')\n",
    "\n",
    "\n",
    "adf['prerequisite'] = adf['prerequisite'].fillna('start')\n",
    "adf['composite'] = adf['composite'].fillna('end')\n",
    "adf3 = adf[['file_name','prerequisite']]\n",
    "adf3 = adf3.dropna()\n",
    "adf3 = adf3.melt('file_name')\n",
    "adf3.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "adf3 = adf3.drop_duplicates(subset=['head','variable','value'], keep='last')\n",
    "\n",
    "\n",
    "adf4 = adf[['file_name']]\n",
    "adf4['variable'] = 'level'\n",
    "adf4['value'] = 'level_1'\n",
    "adf4['value'] = adf4.apply(lambda row: 'level_1' if row['file_name'][6] == '1' else ('level_2' if row['file_name'][6] == '3' else 'level_3'), axis=1)\n",
    "adf4.rename(columns = {\"file_name\": \"head\"}, inplace = True) \n",
    "\n",
    "\n",
    "#topics to vacab have to be done separately\n",
    "# tw_df1 = pd.read_csv('tw_df1.csv')\n",
    "adf5 = tw_df1[['topic_num','cv_index_1']]\n",
    "adf5 = adf5.dropna()\n",
    "adf5 = adf5.melt('topic_num')\n",
    "adf5.rename(columns = {\"topic_num\": \"head\"}, inplace = True) \n",
    "adf5 = adf5.drop_duplicates(subset=['head','variable','value'], keep='last')\n",
    "adf5['head'] = adf5['head'] + 1\n",
    "adf5['head'] = 'topic_' + adf5['head'].astype(str)\n",
    "adf5['variable'] = \"concept_vocab_index\"\n",
    "adf5.head()\n",
    "\n",
    "fdf = pd.concat([adf1,adf2,adf3,adf4,adf5])\n",
    "fdf = fdf.fillna(1)\n",
    "\n",
    "fdf.drop_duplicates()\n",
    "cvi_triples = fdf[fdf['variable'] == 'concept_vocab_index']\n",
    "l_text_triples = fdf[fdf['variable'] == 'l_text_topics']\n",
    "prerequisite_triples = fdf[fdf['variable'] == 'prerequisite']\n",
    "level_triples = fdf[fdf['variable'] == 'level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED TRIPLES FILES AT -> triples.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\Users\\patel\\anaconda3\\envs\\wsl_env\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "cvi_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(cvi_triples['prob'])), columns = ['prob'])\n",
    "l_text_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(l_text_triples['prob'])), columns = ['prob'])\n",
    "prerequisite_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(prerequisite_triples['prob'])), columns = ['prob'])\n",
    "level_triples['prob'] = pd.DataFrame(MinMaxScaler().fit_transform(pd.DataFrame(level_triples['prob'])), columns = ['prob'])\n",
    "\n",
    "fdf = pd.concat([cvi_triples, l_text_triples, prerequisite_triples, level_triples])\n",
    "\n",
    "fdf.to_csv(TRIPLES_WITH_PROB)\n",
    "print(f\"GENERATED TRIPLES FILES AT -> {os.path.relpath(TRIPLES_WITH_PROB)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
