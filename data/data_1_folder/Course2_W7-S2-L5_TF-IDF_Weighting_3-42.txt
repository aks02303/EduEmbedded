We've now introduced two weights for terms and
documents to use in our information
retrieval ranking process: term frequency
and inverse document frequency. In
this segment we're going to put them
together to get the TF-IDF weight of terms.
The TF-IDF weight of a term in a document,
right here, is simply the product of its
TF weight scaled with a log term as we
discussed before, times its inverse
document frequency weight. This is the best
known weighting scheme for terms in
information retrieval. There's been a lot
of research, and there are many others.
But if you only know one, it's the one to
know. Note in particular one fine point.
So this little dash or a hyphen here in this
TF-IDF weighting, that's a hyphen, it's not a
minus sign, that we're taking a product. So
sometimes people indicate that more
explicitly by using a dot or using a
multiplication sign. So, what are the
features of TF-IDF weighting? Tf-idf
weighting increases with the number of
times a term occurs in a document so that
the TF-IDF weight for a query term depends
on the document. It's not independent of
the document. And then the TF-IDF weight
for a term also goes up with the rarity of
the term in the collection. That's from the
IDF weighting here. So using this to find
the ranking of documents for a query, what
we're doing to work out the score of the
query in the document is we're taking the
terms that appear in both the query and
the document. The rest of them have no
weighting, and we're working out this TD-IDF
weight for each of those terms and then
we're summing them to get the score of the
document with respect to the query. So
what have we done here? What we've done is
we've gradually moved from first binary
vectors in the original model of doing
Boolean information retrieval, to
count vectors which were used when we just
had an unscaled term frequency. To now we
have weight vectors for a document and
hence a weight matrix between terms and
documents and that's now what we see here.
So each document is now being represented
by a real-valued vector. So for example, the
document Julius Caesar is being
represented by this vector. So that for
each document, it's in the vector space of
real valued numbers where the
dimensionality is the number of different
terms in our collection again. Okay, and
then when we have a bunch of these vectors
for each document in our collection, we
have a term, a term document matrix which
is now a real valued matrix. We'll say a
little bit more later about what some of
the properties of this is. But hopefully,
seeing this kind of term document matrix
of real numbers is enough to see how
we can do a ranking of documents according
to some query, according to these TF-IDF
scores that we've assigned for each
term and each document So that's TF-IDF
weighting, one of the most central
concepts in information retrieval systems.
