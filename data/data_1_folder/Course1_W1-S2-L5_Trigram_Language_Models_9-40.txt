>> So now, we'll see how we can apply
these ideas very directly to the language
modeling problem.
So and we'll see how trigram language
models can be derived as a direct
application of second-order Markov
processes.
So, a trigram language model will consist
of 2 things.
Firstly, we'll have some finite set V,
which is just going to be the vocabulary
in the language model in exactly the same
way as before.
So, it might have words like the, a,
Beckham, and so on and so on.
Again, this might be a fairly large set.
It might be a few thousands or a few, even
a few tens of thousands of words.
And then, the second part of a language
model is a set of parameters.
So, for every sequence of three words, u,
v, w, so this will be referred to as a
trigram, the sequence of three words, we
have a parameter qw given uv.
We'll see very soon the role these
parameters take.
W could be v sorry, any element of V, or
it can be the STOP symbol.
And u and v could be element, any element
of V in addition to these special star
symbols we saw in the second-order Markov
process.
And given these definitions, the model
defines a distribution as follows.
So, for any sentence, X1 through Xn, where
each Xi is in V, for i equals 1 to n minus
1, and where Xn equals STOP, the
probability under the trigram language
model is define as a product of terms.
So, I have i equals 1 to n at each point.
I have q of Xi, given x minus 2, x minus
1.
I'll illustrate this in a second with a,
an example but basically these parameters
correspond directly to the probability
that Xi is equal to Xi given Xi minus 2 is
equal to Xi minus 2, and Xi minus 1 is
equal to Xi minus 1.
So, this is basically just a rewrite of
the second-order Markov process I've
showed you in the previous section.
So, that's the formal definition, but
things should become a lot clearer if we
go through an example, because what we end
up with is really a very simple model.
So, let's take a simpler sentence, for
example, the sentence, the dog barks.
As before, we have the STOP symbol at the
end of the sentence, we'll always have
that.
So, in this case, the probability assigned
to the sentence under the language model
is a product of terms, so we've notice
that we have each of the words there, dog,
barks, and STOP.
And at each point, we condition on the
previous 2 words in the sequence.
So, for example, barks is conditioned on
the, and dog, because those are the, the
previous 2 words in the sequence.
So, what we're doing here is essentially,
treating sentences as being generated by a
second-order Markov process, where each
word in the sentence is chosen conditioned
only on the two previous words in the
sentence.
And that is a trigram language model.
So, let's just talk briefly about this
assumption, that each word depends only on
the two previous words.
So, that clearly is a very naive
assumption.
And, in fact, it's possible cons, to
construct all kinds of examples, where
that independence assumption is very
clearly violated.
And, in fact, later in the course, we will
see models, for example, probabilistic
variance of context free grammars, which
are arguably much more realistic models of
language and that they capture much longer
range dependencies than just the previous
two words in the sentence.
Having said that, trigram language models
are tremendously useful.
It turns out that they are quite difficult
to improve upon and they have the benefit
of considerable simplicity.
And that all you have to do is estimate
these trigram parameters in the model.
This estimation problem we'll come to a
little later in this lecture.
But to summarize, for any sentence, its
probability is a product of terms, we have
one of these q parameters for each trigram
in a sentence where we condition each word
on the previous two words.
So, this leaves us with a remaining
estimation problem, which is that, we want
to estimate these q parameters in the
model.
So, for example, we might want to estimate
the probability of laughs, given the, the
previous 2 words with and dog.
And we'll spend quite a lot of time on
this estimation problem.
It turns out to be quite a challenging
program.
Let's talk first though about a very
natural estimate for this quantity, and
this is often referred to as the maximum
likelihood estimate.
And it's really a very intuitive estimate.
So, recall again, we've assumed that we
have a training set.
So we have some example sentences in our
language.
We might typically have maybe a few
million, a few tens of millions, or maybe
even a few billion words, or sentences.
From these training samples, we can derive
counts.
So, for example, counts of the and dog,
would be the number of times I've seen the
word, the, followed by dog.
And my training samples, count of the,
dog, laughs, would be the number of times
I've seen the trigram sequence, the,
followed by dog, followed by laughs.
And the maximum likelihood estimate, which
I've written here for this example, is the
ratio of these two terms.
So, I have the ratio of the trigram
divided by the ratio of what we often call
the bigram, the sequence of two words that
we're conditioning upon.
And that's a very intuitive and a very
natural estimate.
Many people would have come up with this
as a first guess for how to estimate the
parameters in this model.
So, maximum like estimators will be very
useful.
They will form a starting point for the
estimation methods we will develop.
But they do have a very clear problem, and
that is the following.
We have a huge number of parameters in our
model.
Even though we've made this assumption,
that we only condition on the previous two
words at each point, we still have a very
large number of parameters.
So, if we define N to be our vocabulary
size, well, there are N possibilities for
this word, N possibilities for this word,
N possibilities for this word, plus a,
plus 1 or 2, if we take the STOP, STOP
symbols and start symbols into account.
But as a good estimate, we have N times N
times N.
We have N cubed parameters in the model.
And so, for example, if we have a
vocabulary size of 20,000, we have 20,000
cubed.
That's 8 times 10 to the 12 different
parameters.
So that is a very large number of
parameters, irrespective of how much
training data we have.
This is going to be a very large number,
in comparison to the number of training
examnples that we have.
And, this manifests itself in the
following way.
In many cases, this counts on the
numerator may be equal to 0.
Because we simply haven't seen this
particular triagram in training data.
And in that case, this estimate will be
equal to 0.
That's problematic because there are so
many trigrams possible, maybe 8 times 10
to the 12 that just because we see the
trigram zero times in training doesn't
mean that we should say this estimate is
equal to 0.
Where still, in some cases, this
denominator may be zero, may be I've never
seen the words, the, followed by dog.
And in that case, this ratio is completely
undefined and the estimate really falls
apart.
So, the main point here is that we have a
very large number of parameters.
We have a large amount of training data.
That would mean that many of these counts
are equal to zero.
And that will lead to estimates being
unrealistically low or actually ill
defined.
But we'll soon see that we can modify
these styles of estimates in ways that
make them quite robust to these problems.
