So, to summarize, we have developed a full
approach to the tagging problem based on
hidden Markov models.
Let's just go over some pros and cons of
the approach I've shown you.
One big advantage is the hidden Markov
model taggers are very, very simple to
train.
It's simply a matter of compiling counts
from the training corpus in the way I
described earlier and then deriving these
simple maximum likelihood of estimates
will be simple lineally interpolated
estimates.
They perform relatively well, so, for
example, for the named entity recognition
task in the work that I cited by Beckel
and others from the late 90s.
They performed over 90% accuracy in terms
of recovering these named entities.
So, that's a pretty good level of
performance.
The main problem with these models is that
as I said, we have this problem of
estimating these parameters e of word
given tag.
And we saw a rather heuristic method for
dealing with low frequency, low count
words by grouping these words into
different classes, depending on their
spelling.
And that's really a black heart.
It takes a lot of human intuition, human
intervention in the problem.
It's a, it's a rather clumsy approach to
this particular problem and it becomes
increasingly difficult if the words that
are input to the model are more complex.
Later in the course, we'll see other
tagging tasks where this approach really
gets out of hand.
And so, later in the class, we will see
alternative tagging methods, which are
slightly more complex than hidden Markov
models, but I think have a much more
satisfactory solution to this particular
problem.
