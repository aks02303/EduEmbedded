Okay, in this section of the course, we're
gonna start talking about how to parse text
using statistical models. But before doing
that, let me just introduce two views of
syntactic structure that we'll explore is
our parsing models.
So the first view is what's called constituency 
parsing and the idea of constituency parsing
is that what we do is that we organize words into
nested constituents. Let me give you a bit
of an example of that. So we had long ago
this sentence Fed raises interest rates.
And so, we had the parts of
speech that we've seen before, that this
is a proper noun, and this is a verb, and
this is, these are both nouns in the
sense that it's the correct parse, but
then above that, what we wanted to say is
these nouns go together to form a noun
phrase. And then the verb and the noun phrase
go together to form a verb phrase.
And everything goes together to form a sentence.
And so, these things up here, these are 
our constituents. And so the claim is
that the part of the sentence, that's also 
sometimes shown with square brackets,
interest rates is a constituent,
that it's a unit of a sentence.
Now, how do we know what a constituent is? 
Well, that's a complex issue, but here are
some of the ideas that we use. So, one big idea
is, by looking at the words that stay
together and go together when phrases move
around because of various syntactic
constructions. So in this example here,
we've got these two and three words
"to the children" and "about drugs". And, well,
something that you'll notice is we can
actually swap them around into the
opposite order, and that that's fine.
John talked about drugs to the children. 
Or you could do other things with them.
So, for example, maybe you could take this one,
and front it. In certain discourse contexts
you could say, to the children John talked
about drugs. And so this is all evidence
that each of these groups of words is a
constituent. On the other hand, if you
randomly reorder the words of a sentence,
normally you get something that
just doesn't work at all. So, here what
I've tried to do is take the word drugs
from the end here and thought, hmm, maybe
I can just take it and stick it after talked.
But that doesn't work at all, so you 
can't say John talked. This is down here.
John talked drugs to the children about. 
There are lots of other tests.
Another test is this idea of phrasal expansion 
and substitution, so for the phrase
on the box, you can make it a
bigger unit, like right on top of the box
and you can make it a smaller unit, like
there and in general you can apply these
substitutions in any place where the
phrase on the box can appear.
There are lots other syntactic tests that 
people use for constituency. Really if you want
to learn a lot about this you should take a
linguistics course and then you'll see this all
in more detail, but I hope that's
enough to give a rough idea. Those were
simple sentences. This is the kind of
thing that happens when you've got big
real sentences. So this sentence is,
Analyst said Mr. Stronach wants to resume
a more influential role in running the
company. And so the idea of constituency
structure or phrase structure is that we have
these units again that are constituents,
so here's a constituent, a more influential
role. Here's another constituent which
just is the name Mr. Stronach. And then
here is some different constituents down at
the bottom. So running the company is a
constituent, but also this bigger in
running the company, is also a constituent, 
so constituents nest together.
These are the kind of parse trees
that NLP people actually spend their time
dealing with, and we'll talk more about these. 
But note just one attribute of them is,
as well as actual pronounced words, 
they're postulating these unpronounced units
in them as well called empty elements.
So the straightforward way to model phrase 
structural constituency structure is just as
a context free grammar, which we assumed
you've seen somewhere in computer science.
That is just the idea that you've got
these rules that says, A rewrites as BCD.
And if you apply these, and let's say you
have B goes to EF. If you apply these,
you get phrase structure rules, where this is 
BCD and then this goes to EF. That gives you
the constituency structure. But this,
these kind of rules you can write for
arbitrary symbols but if you actually look
at what happens in natural languages,
there's always a lot more structure and
this is the idea that gets called X-bar theory.
And what that says is that in
natural languages, by and large, phrases
are headed by particular kinds of words
which then take modifiers and dependents
around them. So informally we call things
a verb phrase precisely because they have
a verb in the middle of them, and we call
things a noun phrase precisely because
they have a noun that is the central
element of them. So a noun phrase,
something like, the man in the corner,
centrally has the noun, man and then has
other modifiers sitting around it. And so,
although according to a context free
grammar, these are just arbitrary symbols,
in practice we give them names that
reflect this headed structure or phrase
structure. And the same is true of other
categories as well, like adjective phrases
and adverb phrases. So they're a large
part of our category inventory. We do have
other kinds of phrases. So we have
sentences, and various kinds of
inverted and questioning sentences
which basically correspond to a subject
noun phrase and a verb phrase. And then
they can appear in bigger complement 
clauses, these SBAR and SBAR(Q)
which also have something like a that
introducing the sentence. And various
other miscellaneous stuff happens as well.
Again, you can learn a lot more about that
in a linguistics syntax class. But this
idea of headed phrase structure is the
connection between phrase structure and
the other big approach to syntactic
structure that I want to show you and
that's dependency structure. So in
dependency structure the idea is we
directly show for the words of a sentence
which other words depend on, that is
modify or are arguments of them. Let's look
at that with this example the boy put, put
the tortoise on the rug. So the head of
this whole sentence is putting. That's the
central word of the sentence. And that's
sometimes represented by having a dependency
arc, coming in from the edge, pointing to
the whole, the head of the whole sentence.
Okay so then put has two arguments itself,
so put's arguments are the boy. I'm sorry
it's got three arguments itself,
the tortoise and on the rug where things are
put, So, who put what? And then where?
Okay, and so then, at that point, we then
sort of say, well, boy, does it have any
modifiers or arguments? Its modifier is
the. Tortoise, does it have any modifiers
or arguments? Its modifier is the. And
then on, as a preposition takes a
complement. So its argument is its
complement, the rug where the rug is the
head and the modifier of rug is the. And
so, this set of arrows I've now drawn
here, is a dependency analysis of this
sentence. And this form of dependency
analysis is the other major syntactic
representation we'll use.
Okay, so I hope that's been enough to give you 
some idea of the representations we use
for syntactic structure and how the two
representations connect to each other.
