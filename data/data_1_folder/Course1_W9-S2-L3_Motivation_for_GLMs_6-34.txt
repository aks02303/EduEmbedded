So, here's an overview for the remain of 
to-, remainder of today's lecture. 
We're going to des-, describe the basic 
framework of global linear models. 
And we'll see that they offer a very 
different way of thinking about these 
supervised learning problems from the 
history based models that I just 
discussed. 
We'll talk about parsing problems in this 
framework, in particular, reranking 
approaches. 
And finally, we'll talk about a variant 
perceptron algorithm, which can be used 
for parameter estimation in these global 
linear models. 
So, one key idea is the following. 
We'll move away from this idea of 
derivations. 
or in particular, we'll move away from 
this idea of attaching probabilities to 
individual decisions, which go into 
building an entire structure. 
Instead, we're going to talk about 
feature vectors over entire structures. 
We'll call these, global features. 
And this is really where the name global 
linear models comes from. 
So, now we're going to define functions, 
which for example, look at an entire 
parse tree and match these to some 
feature vector. 
So, in one sense, these feature vector 
mappings will be highly very closely 
related to the feature vector mappings we 
saw for log linear models. 
But in another sense there's a rather 
more radical move here to a situation 
where entire structures are getting 
mapped to these feature vectors. 
There are several reasons for thinking 
about these models in these terms or, 
there are several reasons for introducing 
global features. 
First piece of motivation is, is that, it 
can offer considerable freedom in 
defining features. 
So, this will allow us to incorporate all 
kinds of features, and all kinds of 
information within our supervised 
learning problems, which were really 
challenging to include in the 
history-based models that we've seen up 
to this point in the course. 
[BLANK_AUDIO]. 
So, let me give a couple of examples of 
features in the parsing problem, which, 
as we'll see, are quite easily 
incorporated within a global linear 
model. 
But which are much more difficult to 
incorporate within, for example, a 
probabilistic context free grammar. 
So, this is one observation by Mark 
Johnson and some others from 1999, which 
is that there's a definite tendency in 
natural languages for something called 
parallelism in coordination. 
And this is essentially the following. 
So, if I look at these two phrases, I 
have bars in New York and pubs in London. 
Second phrase I have bars in New York and 
pubs. 
This one has an instance of parallelism. 
[SOUND]. 
And that's because bars in New York has a 
very similar structure, syntactically 
speaking, to pubs in London. 
So, I have two things being coordinated 
here, which have basically similar 
structures. 
If we look at the second example, this 
does not have parallelism. 
So, we're coordinating bars in New York 
with pubs. 
And these two constituents do not have 
the same internal syntactic structure. 
So, statistically speaking, we seem to 
see a preference for these kind of 
structures, as opposed to these. 
And this kind of preference can come in 
useful when we're trying to disambiguate 
structures. 
So, if I say for example, I visited bars 
in New York and pubs in London. 
There are going to be a few pauses here. 
And some of them are going to, go, going 
to include this sort of parallel 
structure, where these two things are 
noun phrases and their coordinated. 
And some of these structures will not 
include these two parallel structures 
being coordinated. 
Knowing that there's a preference for 
parallelism in coordination can help me 
give a preference for parse trees with 
these kind of parallel structures. 
Now I, I would challenge you to go back 
to the lectures on probabilistic context 
free grammars and try to figure out how 
to incorporate this preference within a 
probabilistic context free grammar 
because it is really not entirely 
straight forward. 
So, that will be one of the motivations 
for global linear models. 
We'll see that it's actually very easy to 
incorporate this kind of feature within 
that model. 
[BLANK_AUDIO]. 
Here's a second example of the kind of 
features which might be useful but, which 
are again rather difficult to incorporate 
within a history-based model, such as a 
probabilistic context free grammar. 
These are semantic features. 
So, imagine we have an ontology or some 
kind of lexicon, which gives properties 
of different nouns and verbs. 
So, one example of such an ontology might 
be a resource called Wordnet. 
This is a very famous resource that has 
an information about a very large 
category of nouns and verbs in English. 
And this ontology might state, for 
example, that the word cappuccino has the 
plus liquid feature, whereas, the word 
book does not have that feature. 
Okay? 
So, cappuccino is a liquid. 
And that's important if we're thinking 
about word like pour, where in general, 
nouns which have a plus liquid feature 
are likely to appear as objects, whereas, 
nouns which are not liquids are much less 
likely to be objects. 
So, these kind of on this kind of 
ontological information, where different 
nouns have different properties, can be 
quite useful again in modeling the 
probability of different path structures 
or in disambiguating different path 
structures. 
So, say we'd like to build a parser that 
has a preference for the verb poor, 
taking nouns with the plus liquid 
feature. 
Again, I would challenge you to think 
about how we would incorporate these kind 
of features within a probabilistic 
context-free grammar. 
It is somewhat challenging, whereas, 
again we'll see with global linear 
models. 
It's relatively easy to incorporate these 
kind of features. 

