So finally to summarize some key ideas we
saw in the IBM translation models.
Really the single key idea was to
introduce these alignment variables,
specifying how words in one language are
aligned with words in another language.
And second to make use of translation
parameters.
For example the probability that dog is
translated to the word chien.
And also distortion parameters.
For example the probability of the
position one in French is aligned to
position two in English.
We saw this parameter estimation
algorithm, the EM algorithm.
So this is an iterative algorithm for
training the Q and T parameters.
It starts off with some initial values for
q and t, and then recalculates them using
the method I described.
And we typically run this for several
iterations, or until convergence.
And critically, once I have recovered
these q and t parameters using the EM
algorithm.
I can go back to my training examples and
fill in alignments.
So earlier I showed you how we can recover
the most likely alignment for a sentence.
Once we have the Q and T parameters.
And this is actually how the idea models
currently use the machine translations
systems.
They are a critical component in that they
allow us to recover these alignments in
our training examples.
And in the next lecture of this course,
we'll talk about phrase-based systems.
And phrase-based systems are going to make
very direct use of the alignments which
are recovered using the IBM models.
