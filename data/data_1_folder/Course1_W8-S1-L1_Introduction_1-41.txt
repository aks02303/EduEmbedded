So in last week's lectures, we developed 
log-linear models. 
This very new way of looking at modelling 
for natural image processing, and also 
parameter estimation and natural image 
processing. 
In the current week's lectures, we going 
to look at various applications of 
log-linear models. 
Two problems in natural language 
processing. 
And, the first problem we're going to 
cover in this segment is going to be the 
application of Log-Linear Models to 
Tagging problems. 
We'll see this is a very important 
example of how Log-Linear models can be 
used in LP. 
And we'll see how to develop powerful 
alternatives to the hidden mark of model 
taggers that you saw earlier in this 
class. 
So historically, Log-linear models for 
tagging are often also referred to as 
maximum-entropy Markov Models. 
Why is that? 
Well, Log-linear models, or sometimes 
referred to, particularly in the earlier 
literature, as Maximum-entropy models. 
So we, we won't really go into the reason 
for that naming. 
But it's worth bearing in mind because 
sometimes you will see the term 
Maximum-entropy models used, and then 
you'll know what it is. 
It's basically just a Log-Linear model. 
And the Markov comes in because these are 
essentailly going to be models for 
tagging. 
That share many of the characteristics of 
hidden Markov models. 
Although, as we'll see, they provide a 
very useful alternative to hidden Markov 
models. 

