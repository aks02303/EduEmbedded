So here's a first sketch of a Gradient 
Ascent Method. 
And it works as follows. 
So our goal is to maximize this function 
L of v. 
Where the derivatives take the following 
form, and what I've actually done here is 
used. 
usual vector calculus definition of 
derivatives, so I have d L v by d v is 
now going to be a vector whose first 
component is the derivative with respect 
to the first parameter and whose second 
component is the derivative with respect 
to the second parameter. 
And these are derived in the way I showed 
you on the previous slide. 
So this thing is a n dimensional vector 
itself, basically giving the gradient. 
And this is easily calculated using the 
definitions I just showed you. 
And so the most naive gradient based 
method would form as follows. 
We would initialize v to be the vector of 
all zeroes. 
At each point we would calculate the 
gradient for this capital delta. 
And then we basically do the search I was 
talking about for okay. 
So we would, start at some point, we'd 
calculate the gradient, we'd do a line 
search to find the optimal distance to 
move. 
And we would move to that point, so beta 
star is basically how far we have to 
move, and so we're calculating the 
optimal value for the function if we just 
move in this one direction. 
So beta is the distance we move in, in 
the, in the, in the direction capital 
delta. 
And then we reset v to be this new value 
here. 
And we iterate into convergence, so again 
we calculate the steepness and sort of 
this steepest line of ascent. 
Either the gradient move the optimal 
distance in that direction and so on and 
so on until we basically have reached 
some optimal value, or some value that we 
think is optimal. 
The good news is that you in general will 
not have to implement these kind of 
gradient ascent methods. 
They've been developed for years, 
actually decades and quite sophisticated 
methods have been derived which in 
practice work extremely well. 
So, one thing to note is that the vanilla 
gradient ascent method I've just showed 
you can be rather slow, it can tend to 
sort of get stuck in these long valleys 
and take quite a few iterations. 
so a more sophisticated method to use, if 
you're interested in looking it up, 
conjugate gradient methods, one of the, 
one very commonly used method for these 
log-linear models is an algorithm called 
LBFGS. 
The good news is that many 
implementations of this algorithm or 
other conjugate gradient style algorithms 
are available. 
And they in general assume a rather 
simple interface. 
Okay. 
So they're basically going to assume that 
given a parameter, vector v, you will 
calculate the value for L v and you'll 
calculate the gradient. 
Okay, so this is the objective value. 
It's one thing you can calculate. 
This is the gradient. 
And with these software packages, they 
will then optimize the function. 
And what they'll basically do is start at 
some initial point for example, all 
zeros, and they'll move in some direction 
which takes into account both the 
gradient. 
And also the previous directions that the 
algorithm has moved in, and these can be 
very, very efficient at optimizing the 
function L v. 
And that's about it. 
So if you can implement functions L of v, 
and d L by d v, this gradient function. 
Given one of these existing packages. 
You can optimize L of v quite efficiently 
and find the maximum likely distance. 

